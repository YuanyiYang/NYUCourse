(dp0
I0
(dp1
S'docBody'
p2
V{{Commons category|Searching}}\u000a[[Category:Information retrieval]]\u000a<!-- used to be on cat search engines that was merged into this, but didn't think it really applied... [[Category:Information technology]] -->
p3
sS'docID'
p4
S'0'
p5
sS'title'
p6
VCategory:Searching
p7
ssI130
(dp8
g2
V{{other uses|question|answer}}\u000a{{multiple issues|\u000a{{cleanup|date=January 2012|reason=extensive use of jargon to define jargon, and inconsistent use of bold and italics font styles}}\u000a{{cleanup-rewrite|date=January 2012}}\u000a{{more footnotes|date=February 2014}}\u000a}}\u000a\u000a'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].\u000a\u000aA QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011  \u000a</ref>\u000a\u000aSome examples of natural language document collections used for QA systems include:\u000a* a local collection of reference texts\u000a\u000a* internal organization documents and web pages\u000a* compiled [[newswire]] reports\u000a* a set of [[Wikipedia]] pages\u000a* a subset of [[World Wide Web]] pages\u000a\u000aQA research attempts to deal with a wide range of question types including: fact, list, definition, ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.\u000a\u000a* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer\u2019s Disease. CLEF 2012 Evaluation Labs and Workshop. September 17 2012</ref>\u000a* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\u000a\u000a==History==\u000a\u000aTwo early QA systems were BASEBALL and LUNAR.{{when|date=November 2012}}{{who|date=November 2012}}{{citation needed|date=November 2012}} BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.\u000a\u000a[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\u000a\u000aIn the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these [[expert system]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.\u000a\u000aThe 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\u000a\u000aRecently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.\u000a\u000a==Architecture==\u000aMost modern QA systems use [[natural language]] text documents as their underlying knowledge source.  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted. An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge. However, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates...) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.\u000a\u000aIn an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s. It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system. Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors. An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.\u000a\u000aCurrent QA systems<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> typically include a '''question classifier''' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a '''document retrieval module''' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a '''filter''' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is "Who invented\u000aPenicillin" the filter returns text that contain names of people. Finally, an '''answer extraction''' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.\u000a\u000aA '''multiagent''' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The meta\u2013agent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124\u000a}}</ref>\u000a\u000a==Question answering methods==\u000aQA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:\u000a# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.\u000a# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\u000a\u000aQuestion answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=http://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].\u000a\u000a===Open domain question answering===\u000aIn [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user\u2019s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.\u000a\u000aThe system takes a [[natural language]] question as an input rather than a set of keywords, for example, \u201cWhen is the national day of China?\u201d The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\u000a\u000aKeyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. \u201cWho\u201d, \u201cWhere\u201d or \u201cHow many\u201d, these words tell the system that the answers should be of type \u201cPerson\u201d, \u201cLocation\u201d, \u201cNumber\u201d respectively. In the example above, the word \u201cWhen\u201d indicates that the answer should be of type \u201cDate\u201d. POS tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is \u201cChinese National Day\u201d, the predicate is \u201cis\u201d and the adverbial modifier is \u201cwhen\u201d, therefore the answer type is \u201cDate\u201d. Unfortunately, some interrogative words like \u201cWhich\u201d, \u201cWhat\u201d or \u201cHow\u201d do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.\u000a\u000aOnce the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as \u201cWho\u201d or \u201cWhere\u201d, a Named Entity Recogniser is used to find relevant \u201cPerson\u201d and \u201cLocation\u201d names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\u000a\u000aA [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is \u201c1st Oct.\u201d\u000a\u000a==Issues==\u000aIn 2002 a group of researchers wrote a roadmap of research in question answering.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R. [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues, Tasks and Program Structures to Roadmap Research in Question Answering (QA)].</ref> The following\u000aissues were identified.<!-- much of the text in this section is copied and pasted from the "roadmap" document; somebody may try and simplify the text -->\u000a\u000a;Question classes : Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}}\u000a\u000a;Question processing : The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.\u000a\u000a;Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.)\u000a\u000a;Data sources for QA : Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained.\u000a\u000a;Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}}\u000a\u000a;Answer formulation : The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.\u000a\u000a;Real time question answering : There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.\u000a\u000a;Multilingual (or cross-lingual) question answering : The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].)\u000a\u000a;Interactive QA : It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions.<ref>Perera, R. and Nand, P. 2014. [http://link.springer.com/chapter/10.1007%2F978-3-319-11716-4_11 Interaction History Based Answer Formulation for Question Answering.]</ref> (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.)\u000a\u000a;Advanced reasoning for QA : More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system.\u000a\u000a;Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process.<ref>Perera, R. 2012. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6305919&isnumber=6305918 IPedagogy: Question Answering System Based on Web Information Clustering.]</ref>\u000a\u000a;User profiling for QA : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}}\u000a\u000a==Progress==\u000aQA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:\u000a\u000a* interactivity\u2014clarification of questions or answers\u000a* answer reuse or caching\u000a* knowledge representation and reasoning\u000a* social media analysis with QA systems\u000a* [[sentiment analysis]]<ref>[http://totalgood.com/bitcrawl/ BitCrawl] by Hobson Lane</ref>\u000a* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://www.aclweb.org/anthology/W12-6004 Towards a thematic role based target identification model for question answering.]</ref>\u000a* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference | author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430\u2013437 | publisher=Springer Berlin Heidelberg | title= [http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 The impact of semantic class identification and semantic role labeling on natural language answer extraction]}}</ref>\u000a* utilization of linguistic resources,<ref>{{cite journal |author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma|title=[http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false The impact of frame semantic annotation levels, frame\u2010alignment techniques, and fusion methods on factoid answer processing] | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247\u2013263 |year =2009}}</ref> such as [[WordNet]], [[FrameNet]], and the similar\u000a\u000aIBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.\u000a<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>\u000a\u000a==References==\u000a* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\u000a* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\u000a*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}\u000a* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.\u000a\u000a<references/>\u000a\u000a==External links==\u000a* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]\u000a* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]\u000a* [http://celct.fbk.eu/QA4MRE/ Question Answering Evaluation at CLEF]\u000a\u000a{{Computable knowledge}}\u000a{{Natural Language Processing}}\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p9
sg4
S'130'
p10
sg6
VQuestion answering
p11
ssI5
(dp12
g2
V{{Cat main|String metrics}}\u000a\u000a[[Category:Algorithms on strings|Similarity]]\u000a[[Category:Information retrieval]]\u000a[[Category:Metric geometry]]\u000a[[Category:Information theory]]\u000a[[Category:String (computer science)]]
p13
sg4
S'5'
p14
sg6
VCategory:String similarity measures
p15
ssI135
(dp16
g2
V'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.\u000a\u000aFurnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].\u000a\u000aThe vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\u000a\u000a== Techniques that solve mismatch ==\u000aZhao provided a survey of common techniques that can solve mismatch in the dissertation on term mismatch.<ref>Zhao, L., Modeling and Solving Term Mismatch in Full-text Retrieval, PhD Dissertation, Carnegie Mellon University, 2012. [http://www.cs.cmu.edu/~lezhao/thesis/diss-Le.pdf URL] retrieved 9/3/2012.</ref>\u000a\u000a===Stemming===\u000a\u000a===Full-text indexing versus only indexing keywords or abstracts===\u000a\u000a===Usages of inlink anchor text or other social tagging===\u000a\u000a===Query expansion===\u000aA recent study by Zhao and Callan (2012)<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].\u000a\u000aA wiki website called [http://www.wikiquery.org WikiQuery] has been developed by one of the authors of the above study, which helps users create, store and share effective Conjunctive normal form queries.\u000a\u000a===Translation based models===\u000a\u000a== References ==\u000a\u000a{{Reflist}}\u000a\u000a[[language code:Title]]\u000a\u000a[[Category:Linguistic research]]\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]
p17
sg4
S'135'
p18
sg6
VVocabulary mismatch
p19
ssI10
(dp20
g2
V{{Original research|date=September 2007}}\u000a{{Other uses|Harsh (disambiguation)}}\u000a'''Harshness''' (also called '''raucousness'''), in [[music information retrieval]], is a Non-Contextual Low-Level Audio Descriptors (NLDs) that represents one dimension of the multi-dimensional [[psychoacoustic]] feature called as musical [[timbre]].\u000a\u000aClassical timbre\u2019 NLDs are [[surface roughness|roughness]], [[spectral centroid]], and [[spectral flux]]. While harmonicity and inharmonicity can also be considered NLDs, harshness differs from them, as well as from roughness, once it reckons for a distinguished perceptual audio feature expressed by the summary spectral periodicity. This feature is especially clear in single-[[Pitch (music)|pitch]], single-[[note]], musical audio, where the timbre of two different musical instruments can greatly differ in levels of harshness (e.g., the difference in harshness between a flute and a saxophone is evident). As it is supposed to be, harshness is independent of all others NLDs.\u000a\u000a[[Category:Musicology]]\u000a[[Category:Music technology]]\u000a[[Category:Information retrieval]]
p21
sg4
S'10'
p22
sg6
VHarshness
p23
ssI140
(dp24
g2
VThe '''Sørensen\u2013Dice index''', also known by other names (see Names, below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]<ref>{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1\u201334 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297\u2013302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.\u000a\u000a==Name==\u000aThe index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the \u2013sen ending.\u000a\u000aOther names include:\u000a*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index<ref name ="gallagher"/>\u000a\u000a==Quantitative version==\u000aThe expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:\u000a* Quantitative Sørensen\u2013Dice index<ref name ="gallagher"/>\u000a* Quantitative Sørensen index<ref name ="gallagher"/>\u000a* Quantitative Dice index<ref name ="gallagher"/>\u000a* [[Bray Curtis dissimilarity|Bray-Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')<ref name ="gallagher"/>\u000a* [[Jan Czekanowski|Czekanowski]]'s quantitative index<ref name ="gallagher"/>\u000a* Steinhaus index<ref name ="gallagher"/>\u000a* [[E. C. Pielou|Pielou]]'s percentage similarity<ref name ="gallagher"/>\u000a* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1948 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326\u2013349 |doi=10.2307/1942268 }}</ref>\u000a\u000a==Formula==\u000aSørensen's original formula was intended to be applied to presence/absence data, and is\u000a\u000a:<math> QS = \u005cfrac{2C}{A + B} = \u005cfrac{2 |A \u005ccap B|}{|A| + |B|}</math>\u000a\u000awhere ''A'' and ''B'' are the number of species in samples A and B, respectively, and ''C'' is the number of species shared by the two samples; QS is the quotient of similarity and ranges between 0 and 1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref>\u000a\u000aIt can be viewed as a similarity measure over sets:\u000a\u000a:<math>s = \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000aSimilarly to Jaccard, the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':\u000a\u000a<math>s_v = \u005cfrac{2 | A \u005ccdot B |}{| A |^2 + | B |^2} </math>\u000a\u000awhich gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.\u000a\u000aFor sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979\u000a|title=Information Retrieval\u000a|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>\u000a\u000aWhen taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003\u000a|title=Cognates Can Improve Statistical Translation Models\u000a|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics\u000a|pages=46\u201348 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>\u000a\u000a:<math>s = \u005cfrac{2 n_t}{n_x + n_y}</math>\u000a\u000awhere ''n''<sub>''t''</sub> is the number of character bigrams found in both strings, ''n''<sub>''x''</sub> is the number of bigrams in string ''x'' and ''n''<sub>''y''</sub> is the number of bigrams in string ''y''. For example, to calculate the similarity between:\u000a\u000a:<code>night</code>\u000a:<code>nacht</code>\u000a\u000aWe would find the set of bigrams in each word:\u000a:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}\u000a:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}\u000a\u000aEach set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.\u000a\u000aInserting these numbers into the formula, we calculate, ''s''&nbsp;=&nbsp;(2&nbsp;·&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.\u000a\u000a==Difference from Jaccard ==\u000aThis coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>\u000a\u000aThe function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function\u000a\u000a:<math>d = 1 -  \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000ais not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.\u000a\u000a==Applications==\u000aThe Sørensen\u2013Dice coefficient is mainly useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409\u2013416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123\u2013131.]</ref>). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref>\u000a\u000a==See also==\u000a* [[Correlation]]\u000a* [[Czekanowski similarity index]]\u000a* [[Jaccard index]]\u000a* [[Hamming distance]]\u000a* [[Horn\u2019s index]]\u000a* [[Hurlbert\u2019s index]]\u000a* [[Kulczy\u0144ski similarity index]]\u000a* [[Pianka's index]]\u000a* [[MacArthur and Levin's index]]\u000a* [[Mantel test]]\u000a* [[Morisita's overlap index]]\u000a* [[Most frequent k characters]]\u000a* [[Overlap coefficient]]\u000a* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])\u000a* [[Simplified Morisita\u2019s index]]\u000a* [[Tversky index]]\u000a* [[Universal adaptive strategy theory (UAST)]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/DiceSorensenMetric.scala Dice / Sorensen] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a{{DEFAULTSORT:Sorensen-Dice coefficient}}\u000a[[Category:Information retrieval]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p25
sg4
S'140'
p26
sg6
VSørensen\u2013Dice coefficient
p27
ssI15
(dp28
g2
V'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.\u000a\u000aLSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.\u000a\u000a[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>\u000a\u000a== See also ==\u000a* [[Latent semantic analysis]]\u000a\u000a== Notes ==\u000a{{reflist}}\u000a\u000a== References ==\u000a* {{cite journal\u000a | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf\u000a | title=Latent semantic mapping [information retrieval]\u000a | author=Bellegarda, J.R.\u000a | date=2005\u000a}}\u000a* {{cite conference\u000a | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp\u000a | title=Latent semantic mapping: Principles and applications\u000a | author=J. Bellegarda\u000a | booktitle=ICASSP 2006\u000a | date=2006\u000a}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a\u000a\u000a{{semantics-stub}}\u000a{{compu-stub}}
p29
sg4
S'15'
p30
sg6
VLatent semantic mapping
p31
ssI145
(dp32
g2
V{{orphan|date=April 2010}}\u000a'''Schoolr''' is a [[front and back ends|front-end]] academic directory that features frequently used third-party search engines and resources from [[Google]], [[Wikipedia]], [[Reference.com]], [[Acronym Finder]], [[Wolfram Alpha]], [[Yahoo! Babel Fish]], and the [[University of North Carolina]].<ref>[http://www.lifehack.org/articles/technology/schoolr-google-wikipedia-dictionarycom-and-more-on-one-site.html "Schoolr: Google, Wikipedia, Dictionary.com, and more on one site]", ''Stepcase Lifehack'', March 19, 2007</ref>\u000a\u000aSchoolr went live on December 3, 2006<ref>[http://lifehacker.com/290027/schoolr-search-start-page "Schoolr search start page]", ''Lifehacker'', August 16, 2007</ref> and was developed by Sasan Aghdasi at the [[University of Victoria]].\u000a\u000a== References ==\u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.schoolr.com/ Schoolr]\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Schoolr}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Internet terminology]]
p33
sg4
S'145'
p34
sg6
VSchoolr
p35
ssI20
(dp36
g2
V{{no footnotes|date=March 2013}}\u000a[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]\u000aIn [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. \u000a\u000aThis enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.\u000a\u000aWith traditional [[information technology]], on the other hand, meanings and relationships must be predefined and \u201chard wired\u201d into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.\u000a\u000aOff-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.\u000a\u000aSemantic technologies are \u201cmeaning-centered.\u201d They include tools for:\u000a\u000a* autorecognition of topics and concepts, \u000a* information and meaning extraction, and\u000a* categorization. \u000a\u000aGiven a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.\u000a\u000aSemantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.\u000a\u000a== See also ==\u000a* [[Business Intelligence 2.0]] (BI 2.0)\u000a* [[Metadata]]\u000a* [[Ontology (computer science)]]\u000a* [[Semantic targeting]]\u000a* [[Semantic web]]\u000a\u000a==References==\u000a\u000a* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004\u000a* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 \u2014 Proc. of the 12th international conference on World Wide Web'', pp 700\u2013709. [[ACM Press]], 2003.\u000a* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.\u000a* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.\u000a* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.\u000a* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, \u000a* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September\u000a2004.\u000a* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5\u000a\u000a== External links ==\u000a* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]\u000a* [http://www.semanticarts.com Semantic Technology and the Enterprise]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Semantics]]
p37
sg4
S'20'
p38
sg6
VSemantic technology
p39
ssI150
(dp40
g2
V{{distinguish|Safeword}}\u000aIn [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref> There is no single universal list of stop words used by all [[Natural language processing|processing of natural language]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].\u000a\u000aAny group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as '[[The Who]]', '[[The The]]', or '[[Take That]]'. Other search engines remove some of the most common words\u2014including [[lexical word]]s, such as "want"\u2014from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It\u2019s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>\u000a\u000a[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept. {{Citation needed|date=March 2013}}\u000a\u000a== See also ==\u000a{{Div col|cols=3}}\u000a* [[Text mining]]\u000a* [[Concept mining]]\u000a* [[Information extraction]]\u000a* [[Natural language processing]]\u000a* [[Query expansion]]\u000a* [[Stemming]]\u000a* [[Index (search engine)|Search engine indexing]]\u000a* [[Poison words]]\u000a* [[Function words]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]\u000a* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]\u000a* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]\u000a* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]\u000a* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]\u000a* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]\u000a* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages]\u000a* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]\u000a\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a{{Natural Language Processing}}\u000a{{SearchEngineOptimization}}
p41
sg4
S'150'
p42
sg6
VStop words
p43
ssI25
(dp44
g2
V[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]\u000aIn [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]]. Suppose a program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.\u000a\u000aIn [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.\u000a\u000aIn simple terms, high '''precision''' means that an algorithm returned substantially more relevant results than irrelevant, while high '''recall''' means that an algorithm returned most of the relevant results.\u000a\u000a==Introduction==\u000aAs an example, in an [[information retrieval]] scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.\u000a\u000aIn a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of '''true positives''''' (i.e. the ''number of items correctly labeled as belonging to the positive class'') ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false positives]]''', which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives'' ''divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false negatives]]''', which are items which were not labeled as belonging to the positive class but should have been).\u000a\u000aIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\u000a\u000aIn a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).\u000a\u000aOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an obvious example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient\u2019s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\u000a\u000aUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s Informedness (DeltaP') and Markedness (DeltaP).<ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Factor  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37\u201363 |url=http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf}}</ref><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97\u2013119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2007"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2007"/>  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.\u000a\u000a== Definition (information retrieval context) ==\u000a\u000aIn [[information retrieval]] contexts, precision and recall are defined in terms of a set of '''retrieved documents''' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of '''relevant documents''' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]].\u000a\u000a===[[Positive predictive value | Precision]]===\u000a\u000aIn the field of [[information retrieval]], '''precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the find:\u000a\u000a:<math> \u005ctext{precision}=\u005cfrac{|\u005c{\u005ctext{relevant documents}\u005c}\u005ccap\u005c{\u005ctext{retrieved documents}\u005c}|}{|\u005c{\u005ctext{retrieved documents}\u005c}|} </math>\u000a\u000aPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called '''precision at n''' or '''P@n'''.\u000a\u000aFor example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.\u000a\u000aPrecision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.\u000a\u000aNote that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.\u000a\u000a===Recall===\u000a\u000aRecall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.\u000a\u000a:<math> \u005ctext{recall}=\u005cfrac{|\u005c{\u005ctext{relevant documents}\u005c}\u005ccap\u005c{\u005ctext{retrieved documents}\u005c}|}{|\u005c{\u005ctext{relevant documents}\u005c}|} </math>\u000a\u000aFor example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned\u000a\u000aIn binary classification, recall is called [[Sensitivity_and_specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.\u000a\u000aIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\u000a\u000a== Definition (classification context) ==\u000a{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"\u000a|+ Terminology and derivations<br \u000a/>from a confusion matrix\u000a|- valign=top\u000a|\u000a; true positive (TP)\u000a:eqv. with hit\u000a; true negative (TN)\u000a:eqv. with correct rejection\u000a; false positive (FP)\u000a:eqv. with [[false alarm]], [[Type I error]]\u000a; false negative (FN)\u000a:eqv. with miss, [[Type II error]]\u000a--------------------------------------------------------\u000a; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)\u000a:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]\u000a:<math>\u005cmathit{TPR} = \u005cmathit{TP} / P = \u005cmathit{TP} / (\u005cmathit{TP}+\u005cmathit{FN})</math>\u000a; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate\u000a:<math>\u005cmathit{SPC} = \u005cmathit{TN} / N = \u005cmathit{TN} / (\u005cmathit{FP} + \u005cmathit{TN}) </math>\u000a; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)\u000a:<math>\u005cmathit{PPV} = \u005cmathit{TP} / (\u005cmathit{TP} + \u005cmathit{FP})</math>\u000a; [[negative predictive value]] (NPV)\u000a:<math>\u005cmathit{NPV} = \u005cmathit{TN} / (\u005cmathit{TN} + \u005cmathit{FN})</math>\u000a; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)\u000a:<math>\u005cmathit{FPR} = \u005cmathit{FP} / N = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TN})</math>\u000a; [[false discovery rate]] (FDR)\u000a:<math>\u005cmathit{FDR} = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TP}) = 1 - \u005cmathit{PPV} </math>\u000a; [[false negative rate]] (FNR)\u000a:<math>\u005cmathit{FNR} = \u005cmathit{FN} / (\u005cmathit{FN} + \u005cmathit{TP}) = 1 - \u005cmathit{TPR} </math>\u000a------------------------------------------------\u000a; [[accuracy]] (ACC)\u000a:<math>\u005cmathit{ACC} = (\u005cmathit{TP} + \u005cmathit{TN}) / (P + N)</math>\u000a;[[F1 score]]\u000a: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]\u000a:<math>\u005cmathit{F1} = 2 \u005cmathit{TP} / (2 \u005cmathit{TP} + \u005cmathit{FP} + \u005cmathit{FN})</math>\u000a; [[Matthews correlation coefficient]] (MCC)\u000a:<math> \u005cfrac{ TP \u005ctimes TN - FP \u005ctimes FN } {\u005csqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\u000a</math>\u000a;\u000a<span style="font-size:90%;">''Source: Fawcett (2006).''<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>\u000a|}\u000a\u000aFor classification tasks, the terms '''true positives''', '''true negatives''', '''false positives''', and '''false negatives''' (see also [[Type I and type II errors]]) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation''). \u000a\u000aLet us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:\u000a\u000a{{DiagnosticTesting_Diagram}}\u000a\u000a<!--\u000a{| border="0" align="center" style="text-align: center; background: #FFFFFF;"\u000a|+\u000a!\u000a! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)\u000a|-\u000a!\u000a|-----\u000a|+\u000a! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)\u000a| '''tp''' <br> (true positive) <br/> Correct result\u000a| '''fp''' <br> (false positive) <br/> Unexpected result\u000a|-bgcolor="#EFEFEF"\u000a| '''fn''' <br> (false negative) <br/> Missing result\u000a| '''tn''' <br> (true negative) <br/> Correct absence of result\u000a|+\u000a|}\u000a\u000a-->\u000a\u000a\u000a\u000aPrecision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>\u000a\u000a: <math>\u005ctext{Precision}=\u005cfrac{tp}{tp+fp} \u005c, </math>\u000a\u000a: <math>\u005ctext{Recall}=\u005cfrac{tp}{tp+fn} \u005c, </math>\u000a\u000aRecall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy_and_precision#In_binary_classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity_(tests)#Specificity|specificity]].\u000a\u000a: <math>\u005ctext{True negative rate}=\u005cfrac{tn}{tn+fp} \u005c, </math>\u000a\u000a: <math>\u005ctext{Accuracy}=\u005cfrac{tp+tn}{tp+tn+fp+fn} \u005c, </math>\u000a\u000a== Probabilistic interpretation ==\u000a\u000aIt is possible to interpret precision and recall not as ratios but as probabilities:\u000a\u000a* '''Precision''' is the probability that a (randomly selected) retrieved document is relevant.\u000a\u000a* '''Recall''' is the probability that a (randomly selected) relevant document is retrieved in a search.\u000a\u000aNote that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by '''randomly selected retrieved document''', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected. \u000a\u000aNote that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation). \u000a\u000aAnother interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.\u000a\u000a== F-measure ==\u000a{{main|F1 score}}\u000aA measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:\u000a\u000a: <math>F = 2 \u005ccdot \u005cfrac{\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall}}{ \u005cmathrm{precision} + \u005cmathrm{recall}}</math>\u000a\u000aThere are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. <ref>{{cite journal|last=POWERS|first=D.M.W.|title=EVALUATION: FROM PRECISION, RECALL AND F-MEASURE TO ROC, INFORMEDNESS, MARKEDNESS & CORRELATION|journal=Journal of Machine Learning Technologies|date=February 27, 2011|volume=2|issue=1|pages=37-63|url=http://www.bioinfo.in/contents.php?id=51}}</ref> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\u000a\u000aIt is a special case of the general <math>F_\u005cbeta</math> measure (for non-negative real values of&nbsp;<math>\u005cbeta</math>):\u000a\u000a:<math>F_\u005cbeta = (1 + \u005cbeta^2) \u005ccdot \u005cfrac{\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall} }{ \u005cbeta^2 \u005ccdot \u005cmathrm{precision} + \u005cmathrm{recall}}</math>\u000a\u000aTwo other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.\u000a\u000aThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\u005cbeta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\u005cbeta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \u005cfrac{1}{\u005cfrac{\u005calpha}{P} + \u005cfrac{1-\u005calpha}{R}}</math>.  Their relationship is <math>F_\u005cbeta = 1 - E</math> where <math>\u005calpha=\u005cfrac{1}{1 + \u005cbeta^2}</math>.\u000a\u000a==Limitations as goals==\u000aThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zaj\u0105c. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref> \u000a\u000aFor [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']</ref>\u000a:''"[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).''\u000a:* ''Search results don't have to be very good.''\u000a:* ''Recall?    Not important (as long as you get at least some good hits).''\u000a:* ''Precision? Not important (as long as at least some of the hits on the first page you return are good)."''\u000a\u000a==See also==\u000a* [[Binary classification]]\u000a* [[Information retrieval]]\u000a* [[Receiver operating characteristic]]\u000a* [[Relevance]]\u000a* [[Sensitivity and specificity]]\u000a* [[Type I and type II errors]], where ''false positives'' and ''false negatives'' are defined\u000a* [[Uncertainty coefficient]], aka Proficiency\u000a\u000a== Sources ==\u000a<references>\u000a* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X\u000a* Hjørland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237\u000a* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''\u000a* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4\u000a</references>\u000a\u000a== External links ==\u000a* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval \u2013 C. J. van Rijsbergen 1979]\u000a* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Information science]]\u000a[[Category:Bioinformatics]]\u000a[[Category:Summary statistics for contingency tables]]\u000a\u000a[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]
p45
sg4
S'25'
p46
sg6
VPrecision and recall
p47
ssI155
(dp48
g2
V[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the usage of a #timetoact hashtag at a 2014 conference]]\u000a\u000aA '''hashtag''' is a word or an unspaced phrase prefixed with the [[Number sign|hash character (or number sign), <code>#</code>]], to form a label.<ref>http://www.merriam-webster.com/dictionary/hashtag</ref> It is a type of [[Tag (metadata)|metadata tag]]. Words or phrases in messages on [[microblogging]] and [[social networking service]]s such as [[Facebook]], [[Google+]], [[Instagram]], [[Twitter]], or [[VK (social network)|VK]] may be tagged by entering # before them,<ref>{{cite web|url=http://support.twitter.com/articles/49309# |title=Using hashtags on Twitter|publisher=support.twitter.com |accessdate=2013-11-25}}</ref> either as they appear in a sentence, e.g., "New artists announced for #SXSW2014 Music Festival"<ref>{{cite web|url=https://dev.twitter.com/media/hashtags |title=Best Practices for Hashtags &#124; Twitter Developers |publisher=Dev.twitter.com |date=2011-07-19 |accessdate=2013-11-12}}</ref> or appended to it. The term hashtag can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary - Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000aA hashtag allows grouping of similarly tagged messages, and also allows an electronic search to return all messages that contain it.\u000a\u000aDue to its widespread use, 'hashtag' was added to the ''[[Oxford English Dictionary]]'' in June 2014.<ref>{{cite web |title='Hashtag' added to the OED \u2013 but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=13 June 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000a==Origin==\u000aThe [[number sign]] was often used in [[information technology]] to highlight a special meaning. In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=2011-08-03 |accessdate=2014-08-25}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W.Kernighan &  d.Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the number sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Techonology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=http://books.google.co.uk/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=7 November 2014|isbn=9781863975858}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=9780195157048|pages=33, 260|url=http://books.google.co.uk/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>\u000a\u000aThe number sign then appeared and was used within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=3 June 2014}}</ref>\u000a\u000aThe use of the number sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=2014-08-29}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitter\u2019s Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter: \u000a{{quote |1=how do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/#!/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}\u000aInternationally, the hashtag became a practice of writing style for Twitter posts during the [[2009\u20132010 Iranian election protests]], as both English- and [[Persian language|Persian]]-language hashtags became useful for Twitter users inside and outside Iran.{{cite web|url=http://www.dw.de/%D8%AD%DA%A9%D8%A7%DB%8C%D8%AA-%D9%87%D8%B4%D8%AA%DA%AF%DB%8C-%DA%A9%D9%87-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86-%D8%A2%D8%BA%D8%A7%D8%B2-%DA%A9%D8%B1%D8%AF%D9%86%D8%AF/g-18012627|title = dw |date= 2009}}\u000a\u000aThe first use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=2013-09-19}}</ref> on 26 August 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.\u000a\u000aBeginning July 2, 2009,{{citation needed|date=November 2013}} Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending_topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=2014-12-03}}</ref>\u000a\u000a==Style==\u000aOn microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").\u000a\u000aThe quantity of hashtags used in a post or tweet is just as important as the type of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks \u201craising the ire of the community.\u201d<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=22 February 2014}}</ref>\u000a\u000aAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=22 February 2014}}</ref>{{failed verification|date=August 2014}}\u000a \u000a[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=http://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=2013-09-24 |accessdate=2014-08-25}}</ref>\u000a\u000a==Function==\u000a[[File:Seguir hashtags.png|300px|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag "#science".]]\u000aHashtags are mostly used as unmoderated ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion using the hashtag. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users, and neither can they be "retired" from public usage, meaning that hashtags can be used in theoretical perpetuity depending upon the longevity of the word or set of characters in a written language. They also do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes as espoused by those who make use of them.\u000a\u000aHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using "#cakefestival" rather than simply "#cake". However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\u000a\u000aHashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.\u000a\u000aHashtags can be used on the social network [[Instagram]], by posting pictures and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic like #photography #iPhone #iphoneography and therefore do not fulfil a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = 7 November 2013|publisher=BBC.co.uk |accessdate=2013-11-25}}</ref> The censorship and ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram\u2019s content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>\u000aOlszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship". Visual Communication Quarterly, 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref> \u000a\u000a\u000aHashtags are also used informally to express context around a given message, with no intent to actually categorize the message for later searching, sharing, or other reasons.  This can help express humor, excitement, sadness or other contextual cues, for example "It's Monday!! #excited #sarcasm"\u000a\u000a==Use outside of social networking websites==\u000aThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]; in the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = Oct 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = Oct 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts which can result from search terms or hashtags.{{citation needed|date=September 2014}}\u000a\u000a==Websites that support hashtags==\u000a{{Cleanup-list|section|date=May 2014}}\u000a{{columns-list|2|\u000a<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\u000a* [[App.net]]\u000a* [[Diaspora (software)|Diaspora software]] and [[Diaspora (social network)|social network]]\u000a* [[DeviantART]]\u000a* [[Facebook]]\u000a* [[Flickr]]\u000a* [[FriendFeed]]\u000a* [[Gawker Media]] websites\u000a* [[GNU Social]]\u000a* [[Google+]]\u000a* [[Instagram]]\u000a* [[Kickstarter]]\u000a* [[Orkut]]<ref>{{cite web|url = http://en.blog.orkut.com/2012/02/hashtags-in-orkut-communities.html|title = Hashtags in Orkut communities|date = February 6, 2012 <!-- , 6:11 PM --> |publisher = Orkut|author = Marco Wisniewski}}</ref>\u000a* [[Sina Weibo]]\u000a* [[SoundCloud]]\u000a* [[Tout (company)|Tout]]\u000a* [[tsu]]\u000a* [[Tumblr]]\u000a* [[Twitter]]\u000a* [[Vine (software)|Vine]]\u000a* [[VK (social network)|VK]]\u000a}}\u000a\u000a==Usage==\u000a\u000a===Mass broadcast media===\u000a\u000aSince 2010, television series on various television channels promote themselves through "branded" hashtag [[digital on-screen graphic|bugs]].<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = Apr 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> This is used as a means of promoting a [[backchannel]] of online side-discussion before, during and after an episode broadcast. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref> (for example, a motion picture trailer).\u000a\u000aWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |publisher=Ibtimes.com |date=2013-03-22 |accessdate=2013-09-19}}</ref>\u000a\u000aThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter\u2019s Hashtag Pages Could Be The New AOL Keywords \u2014 But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>\u000a\u000a===Purchasing===\u000a\u000aSince February 2013 there is a collaboration between the social networking site Twitter and [[American Express]] that makes it possible to buy discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = 12 February 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = 2013-11-25}}</ref> American Express members can sync their card with Twitter and use the offers by tweeting and look for a response in a tweet with the confirmation from American Express.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=2013-11-25}}</ref>\u000a\u000a===Event promotion===\u000a\u000a[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]\u000aOrganized real-world events have also made use of hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other on both Twitter and, in many cases, in real life during events.\u000a\u000aCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\u000a\u000aPolitical protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.\u000a\u000a===Consumer complaints===\u000aHashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a corporate social media hashtag is used to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so customers could share positive experiences about the restaurant chain. The marketing effort was cancelled after just two hours when McDonald's received numerous complaint tweets rather than the positive stories they were expecting.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = 17 May 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = 2012-06-12}}</ref>\u000a\u000a===Sentiment analysis===\u000aThe use of hashtags also reveals things about the sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]]<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>\u2014a difficult [[Artificial Intelligence|AI]] problem.{{citation needed|date=September 2014}}\u000a\u000a==In popular culture==\u000aDuring the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]] [[Jack Layton]] referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably "#fail").<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = Apr 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = Apr 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>\u000a\u000aThe term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of ''[[Houston Press]]'', uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = Jul 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]] and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]]<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>\u000a\u000aOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].<ref>\u000a{{cite web \u000a| title = Twitter / nickbilton: My first byline on A1 of the ... \u000a| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1 \u000a| accessdate = 2013-09-14 \u000a }}\u000a</ref>\u000a\u000a"Hashtag [[Heel (professional wrestling)|heel]]" is a moniker used by [[WWE]] wrestler [[Dolph Ziggler]].\u000a\u000a[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags - social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>\u000a\u000aIn May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>\u000a\u000aIn September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>\u000a\u000a===Adaptations===\u000aIn 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=June 11, 2010 8:05 am |url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=2010-06-11 |accessdate=2014-08-25}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil.<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=2014-06-10 |accessdate=2014-08-25}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=2014-06-10 |accessdate=2014-08-25}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.\u000a\u000aIn July 2012, Twitter adapted the hashtag style to make company [[ticker symbol]]s preceded by the [[dollar sign]] clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols - Jul. 31, 2012 |publisher=Money.cnn.com |date=2012-07-31 |accessdate=2013-11-12}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=2012-07-30 |accessdate=2013-11-12}}</ref> This is intended to allow users to search posts discussing companies and their stocks.\u000a\u000aIn August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=http://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=1 August 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013 it was seen on TV used by [[Jimmy Fallon]], and on ''[[The Colbert Report]]'' among other places.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags\u2014and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref>\u000a\u000a==References==\u000a{{Reflist|colwidth=30em}}\u000a{{commons category|Hashtags}}\u000a\u000a{{Microblogging}}\u000a{{Online social networking}}\u000a{{Web syndication}}\u000a\u000a[[Category:Hashtags| ]]\u000a[[Category:Collective intelligence]]\u000a[[Category:Computer jargon]]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Metadata]]\u000a[[Category:Reference]]\u000a[[Category:Web 2.0]]\u000a[[Category:Social media]]\u000a[[Category:2010s slang]]
p49
sg4
S'155'
p50
sg6
VHashtag
p51
ssI30
(dp52
g2
V'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as \u201cum\u201d and \u201cuh\u201d and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today\u2019s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\u000a\u000a== Techniques for noisy text analysis ==\u000aMissing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[Part-of-speech tagging]]\u000aand [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed. \u000a\u000a== Possible source of noisy text ==\u000a* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.\u000a* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.\u000a* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.\u000a* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.\u000a\u000a== References ==\u000a*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]\u000a*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND); Hyderabad, India."].\u000a<references />\u000a\u000a\u000a==See also==\u000a* [[Text analytics]]\u000a* [[Information extraction]]\u000a* [[Computational linguistics]]\u000a* [[Natural language processing]]\u000a* [[Named entity recognition]]\u000a* [[Text mining]]\u000a* [[Automatic summarization]]\u000a* [[Statistical classification]]\u000a* [[Data quality]]\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Statistical natural language processing]]\u000a\u000a[[es:Extracción de la información]]
p53
sg4
S'30'
p54
sg6
VNoisy text analytics
p55
ssI160
(dp56
g2
VThe '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].\u000a\u000aThe CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. \u000aScience Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.\u000a\u000a==Design==\u000a\u000a[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]\u000aThe CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person\u2019s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer\u2019s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>\u000a\u000a==Antecdents==\u000aThe CMIS antecedents\u2014demographics, personal experience, salience, and beliefs\u2014are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>\u000a\u000a==Information Carrier Characteristics==\u000a\u000aThe information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.<ref name="auto"/>\u000a\u000a==Information Seeking Actions==\u000a\u000aThere are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>\u000a\u000a==Stages in the CMIS==\u000a\u000aA key concept from the CMIS is the notion of \u201cstages,\u201d or \u201ccancer involvement\u201d.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.\u000a\u000aThe first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.\u000a \u000aThe second stage is ''Purposive-Placid''. This is characterized by the question, \u201cWhat can I do to prevent cancer?\u201d Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.\u000a\u000aThe third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.\u000a\u000aThe fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a\u000a\u000a[[Category:Communication]]\u000a[[Category:Information retrieval]]\u000a[[Category:Health sciences]]
p57
sg4
S'160'
p58
sg6
VComprehensive Model of Information Seeking
p59
ssI35
(dp60
g2
V{{Other uses|Relevance}}\u000a\u000aIn [[information science]] and [[information retrieval]], '''relevance''' denotes how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.\u000a\u000a== History ==\u000a\u000aThe concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.\u000a\u000aThe formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810\u2010832.</ref>\u000a\u000aSince 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".\u000a\u000aRecently, Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> showed a connection between the [[Binary Independence Model|relevance probability]] and the [[vocabulary mismatch]] problem in retrieval, which could lead to at least 50-300% gains in retrieval accuracy.<ref>Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\u000a\u000a== Evaluation ==\u000a\u000aThe information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.\u000a\u000aIn order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.\u000a\u000aIn contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).\u000a\u000a== Clustering and relevance ==\u000a\u000aThe [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:\u000a* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, \u201cA model of cluster searching based on classification,\u201d Information Systems, vol. 5, pp. 189\u2013195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, \u201cUsing interdocument similarity information in document retrieval systems,\u201d Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3\u201311, 1986.</ref>\u000a* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, \u201cCluster-based retrieval using language models,\u201d in SIGIR \u201904: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186\u2013193, ACM Press, 2004.</ref>    It is important to ensure that clusters \u2013 either in isolation or combination \u2013 successfully model the set of possible relevant documents.\u000a\u000aA second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, \u201cThe cluster hypothesis revisited,\u201d in SIGIR \u201985: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188\u2013196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,\u000a* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>\u000a* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, \u201cA study of relevance propagation for web search,\u201d in SIGIR \u201905: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408\u2013415, ACM Press, 2005.</ref> methods\u000a* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, \u201cDocument expansion for speech retrieval,\u201d in SIGIR \u201999: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34\u201341, ACM Press, 1999.</ref>\u000a* score regularization<ref name=diazreg>F. Diaz, \u201cRegularizing query-based retrieval scores,\u201d Information Retrieval, vol. 10, pp. 531\u2013562, December 2007.</ref>\u000aLocal methods require an accurate and appropriate document similarity measure.\u000a\u000a==Epistemological issues==\u000a{{Section OR|date=May 2014}}\u000aAre users best at evaluating the relevance of a given document, or is it better to use experts?\u000aMost research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase "relevance" output. An alternative strategy would be to use journal [[impact factor]] to rank output and thus base relevance on expert evaluations. Other strategies, such as including diversity of the search results, may be used as well. The important thing to recognize is, however, that relevance is fundamentally a question of [[epistemology]], not [[psychology]]. (Peoples' psychology reflects certain epistemological influences).\u000a\u000a==References==\u000a {{reflist}}\u000a\u000a==Additional reading==\u000a*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.\u000a\u000a*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])\u000a\u000a[[Category:Information retrieval]]
p61
sg4
S'35'
p62
sg6
VRelevance (information retrieval)
p63
ssI165
(dp64
g2
V{{Lowercase|title=tf\u2013idf}}\u000a{{More footnotes|date=July 2012}}\u000a\u000a'''tf\u2013idf''', short for '''term frequency\u2013inverse document frequency''', is a numerical statistic that is intended to reflect how important a word is to a [[document]] in a collection or [[Text corpus|corpus]].<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref>{{rp|8}} It is often used as a weighting factor in [[information retrieval]] and [[text mining]].\u000aThe tf-idf value increases [[Proportionality (mathematics)|proportionally]] to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\u000a\u000aVariations of the tf\u2013idf weighting scheme are often used by [[search engine]]s as a central tool in scoring and ranking a document's [[Relevance (information retrieval)|relevance]] given a user [[Information retrieval|query]]. tf\u2013idf can be successfully used for [[stop-words]] filtering in various subject fields including [[automatic summarization|text summarization]] and classification.\u000a\u000aOne of the simplest [[ranking function]]s is computed by summing the tf\u2013idf for each query term; many more sophisticated ranking functions are variants of this simple model.\u000a\u000a==Motivation==\u000aSuppose we have a set of English text documents and wish to determine which document is most relevant to the query "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its ''term frequency''.\u000a\u000aHowever, because the term "the" is so common, this will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "brown" and "cow". Hence an ''inverse document frequency'' factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\u000a\u000a==Definition==\u000atf\u2013idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest choice is to use the ''raw frequency'' of a term in a document, i.e. the number of times that term ''t'' occurs in document ''d''. If we denote the raw frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is tf(''t'',''d'') = f(''t'',''d''). Other possibilities include<ref>{{cite doi|10.1017/CBO9780511809071.007}}</ref>{{rp|128}}\u000a\u000a* [[boolean data type|Boolean]] "frequencies": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise;\u000a* [[logarithm]]ically scaled frequency: tf(''t'',''d'') = 1 + log f(''t'',''d''), or zero if f(''t'', ''d'') is zero;\u000a* augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:\u000a:<math>\u005cmathrm{tf}(t,d) = 0.5 + \u005cfrac{0.5 \u005ctimes \u005cmathrm{f}(t, d)}{\u005cmax\u005c{\u005cmathrm{f}(w, d):w \u005cin d\u005c}}</math>\u000a\u000aThe '''inverse document frequency''' is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of [[documents]] by the number of documents containing the term, and then taking the logarithm of that [[quotient]].\u000a\u000a:<math> \u005cmathrm{idf}(t, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000awith\u000a\u000a* <math>N</math>: total number of documents in the corpus\u000a* <math> |\u005c{d \u005cin D: t \u005cin d\u005c}| </math> : number of documents where the term <math> t </math> appears (i.e., <math> \u005cmathrm{tf}(t,d) \u005cneq 0</math>). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <math>1 + |\u005c{d \u005cin D: t \u005cin d\u005c}|</math>.\u000a\u000aMathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.\u000a\u000aThen tf\u2013idf is calculated as\u000a\u000a:<math>\u005cmathrm{tfidf}(t,d,D) = \u005cmathrm{tf}(t,d) \u005ctimes \u005cmathrm{idf}(t, D)</math>\u000a\u000aA high weight in tf\u2013idf is reached by a high term [[frequency (statistics)|frequency]] (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.\u000a\u000a==Justification of idf==\u000aIdf was introduced, as "term specificity", by [[Karen Spärck Jones]] in a 1972 paper. Although it has worked well as a [[heuristic]], its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find [[information theory|information theoretic]] justifications for it.<ref name="understanding">{{cite doi|10.1108/00220410410560582}}</ref>\u000a\u000aSpärck Jones's own explanation didn't propose much theory, aside from a connection to [[Zipf's law]].<ref name="understanding"/> Attempts have been made to put idf on a [[probability theory|probabilistic]] footing,<ref>See also [http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html#p:justificationofidf Probability estimates in practice] in ''Introduction to Information Retrieval''.</ref> by estimating the probability that a given document {{mvar|d}} contains a term {{mvar|t}} as\u000a\u000a<math>\u000aP(t|d) = \u005cfrac{|\u005c{d \u005cin D: t \u005cin d\u005c}|}{N}\u000a</math>\u000a\u000aso that we can define idf as\u000a\u000a<math>\u000a\u005cbegin{align}\u000a\u005cmathrm{idf} & = -\u005clog P(t|d) \u005c\u005c\u000a             & = \u005clog \u005cfrac{1}{P(t|d)} \u005c\u005c\u000a             & = \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}\u000a\u005cend{align}\u000a</math>\u000a\u000aThis probabilistic interpretation in turn takes the same form as that of [[self-information]]. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate [[event space]]s for the required [[probability distribution]]s: not only documents need to be taken into account, but also queries and terms.<ref name="understanding"/>\u000a\u000a==Example of tf\u2013idf==\u000aSuppose we have term frequency tables for a collection consisting of only two documents, as listed on the right, then calculation of tf\u2013idf for the term "this" in document 1 is performed as follows.\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 2\u000a! Term\u000a! | Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| another\u000a| 2\u000a|-\u000a| example\u000a| 3\u000a|}\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 1\u000a! Term\u000a! Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| a\u000a| 2\u000a|-\u000a| sample\u000a| 1\u000a|}\u000a\u000aTf, in its basic form, is just the frequency that we look up in appropriate table. In this case, it's one.\u000a\u000aIdf is a bit more involved:\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000aThe numerator of the fraction is the number of documents, which is two. The number of documents in which "this" appears is also two, giving\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{2}{2} = 0</math>\u000a\u000aSo tf\u2013idf is zero for this term, and with the basic definition this is true of any term that occurs in all documents.\u000a\u000aA slightly more interesting example arises from the word "example", which occurs three times but in only one document. For this document, tf\u2013idf of "example" is:\u000a:<math>\u005cmathrm{tf}(\u005cmathsf{example}, d_2) = 3</math>\u000a:<math>\u005cmathrm{idf}(\u005cmathsf{example}, D) = \u005clog \u005cfrac{2}{1} \u005capprox 0.3010</math>\u000a:<math>\u005cmathrm{tfidf}(\u005cmathsf{example}, d_2) = \u005cmathrm{tf}(\u005cmathsf{example}, d_2) \u005ctimes \u005cmathrm{idf}(\u005cmathsf{example}, D) = 3 \u005ctimes 0.3010 \u005capprox 0.9030</math>\u000a\u000a(using the [[base 10 logarithm]]).\u000a\u000a==See also==\u000a{{Div col||25em}}\u000a* [[Okapi BM25]]\u000a* [[Noun phrase]]\u000a* [[Word count]]\u000a* [[Vector space model]]\u000a* [[PageRank]]\u000a* [[Kullback\u2013Leibler divergence]]\u000a* [[Mutual information]]\u000a* [[Latent semantic analysis]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent Dirichlet allocation]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a* {{Cite doi|10.1108/eb026526}}\u000a* {{Cite book\u000a | last1 = Salton | first1 = G | authorlink1 = Gerard Salton\u000a | last2 = McGill | first2 = M. J.\u000a | year = 1986\u000a | title = Introduction to modern information retrieval\u000a | publisher = [[McGraw-Hill]]\u000a | isbn = 978-0070544840\u000a}}\u000a* {{Cite doi|10.1145/182.358466}}\u000a* {{Cite doi|10.1016/0306-4573(88)90021-0}}\u000a* {{Cite doi|10.1145/1361684.1361686}}\u000a\u000a==External links and suggested reading==\u000a* [[Gensim]] is a Python library for vector space modeling and includes tf\u2013idf weighting.\u000a* [http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&format=html&collection=Wilensky_papers&id=3&show_doc=yes Robust Hyperlinking]: An application of tf\u2013idf for stable document addressability.\u000a* [http://infinova.wordpress.com/2010/01/26/distance-between-documents/ A demo of using tf\u2013idf with PHP and Euclidean distance for Classification]\u000a* [http://www.codeproject.com/KB/IP/AnatomyOfASearchEngine1.aspx Anatomy of a search engine]\u000a* [http://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html tf\u2013idf and related definitions] as used in [[Lucene]]\u000a* [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer TfidfTransformer] in [[scikit-learn]]\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tf\u2013idf.\u000a* [http://blog.christianperone.com/?p=1589 Pyevolve: A tutorial series explaining the tf-idf calculation].\u000a* [http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html TF/IDF with Google n-Grams and POS Tags]\u000a\u000a{{DEFAULTSORT:Tf-Idf}}\u000a[[Category:Statistical natural language processing]]\u000a[[Category:Ranking functions]]\u000a[[Category:Vector space model]]
p65
sg4
S'165'
p66
sg6
VTf\u2013idf
p67
ssI40
(dp68
g2
V{{Other uses of|TREC|TREC (disambiguation)}}\u000aThe '''Text REtrieval Conference (TREC)''' is an on-going series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].\u000a\u000aEach track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.\u000a\u000a== Tracks ==\u000a\u000a===Current Tracks===\u000a''New tracks are added as new research needs are identified, this list is current for TREC 2014.''\u000a* Contextual Suggestion Track - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.\u000a* Clinical Decision Support Track - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care\u000a* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.\u000a* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.\u000a* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. \u000a* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.\u000a* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.\u000a* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.\u000a\u000a===''Past tracks''===\u000a* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.\u000a* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. \u000a* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.\u000a* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.\u000a* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.\u000a* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.\u000a* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.\u000a* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].\u000a* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.\u000a* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.\u000a* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.\u000a* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. \u000a* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.\u000a* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.\u000a* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.\u000a* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.\u000a* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.\u000a* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.\u000a* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].\u000a:In 2003, this track became its own independent evaluation named [[TRECVID]].\u000a\u000a===Related Events===\u000aIn 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).\u000a\u000a== Conference Contributions ==\u000a<!-- contributions of conference to research/IR community -->\u000aNIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."\u000a<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>\u000a<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>\u000a\u000aWhile one study suggests that the state of the art for ad hoc search  has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.\u000a\u000aThe test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.\u000a\u000aTREC systems often provide a baseline for further research.  Examples include:\u000a* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>\u000a* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>\u000a* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC's QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']</ref>\u000a\u000a== Participation ==\u000aThe conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a*[http://trec.nist.gov/ TREC website at NIST]\u000a*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]\u000a*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computer science competitions]]
p69
sg4
S'40'
p70
sg6
VText Retrieval Conference
p71
ssI170
(dp72
g2
V'''{{Redirect|Reverse DNS}}\u000a\u000aIn [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' (rDNS) is the determination of a [[domain name]] that is associated with a given  [[IP address]] using the [[Domain Name System]] (DNS) of the [[Internet]].\u000a\u000aComputer networks use the Domain Name System to determine the IP address associated with a domain name. This process is also known as ''forward'' DNS resolution.  ''Reverse'' DNS lookup is the inverse process, the resolution of an IP address to its designated domain name.\u000a\u000aThe reverse DNS database of the Internet is rooted in the ''Address and Routing Parameter Area'' (<tt>[[.arpa|arpa]]</tt>) [[top-level domain]] of the Internet. [[IPv4]] uses the <tt>in-addr.arpa</tt> domain and the <tt>ip6.arpa</tt> domain is delegated for [[IPv6]]. The process of reverse resolving an IP address uses the ''pointer'' DNS record type ([[List of DNS record types#Resource records|PTR record]]).\u000a\u000aInformational RFCs (RFC 1033, RFC 1912 Section 2.1) specify that ''"Every Internet-reachable host should have a name"'' and that such names match with a reverse pointer record, but it is not a requirement of standards governing operation of the DNS itself.\u000a\u000a==IPv4 reverse resolution==\u000aReverse DNS lookups for [[IPv4]] addresses use a ''reverse IN-ADDR entry'' in the special domain <tt>in-addr.arpa</tt>. In this domain, an IPv4 address is represented as a concatenated sequence of ''four decimal numbers'', separated by dots, to which is appended the second level domain suffix <tt>.in-addr.arpa</tt>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four 8-bit portions and converting each 8-bit portion into a decimal number.  These decimal numbers are then concatenated in the order: least significant 8-bit portion first (leftmost), most significant 8-bit portion last (rightmost). It is important to note that ''this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses'' in textual form.\u000aFor example, an address (A) record for <tt>mail.example.com</tt> points to the IP address 192.0.2.5.\u000aIn pointer records of the reverse database, this IP address is stored as the domain name <tt>5.2.0.192.in-addr.arpa</tt> pointing back to its designated host name <tt>mail.example.com</tt>. \u000aThis allows it to pass the [[Forward Confirmed reverse DNS]] process.\u000a\u000a===Classless reverse DNS method===\u000aHistorically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A.  By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using ''canonical name'' ([[CNAME]]) DNS records.\u000a\u000a==IPv6 reverse resolution==\u000aReverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code>. An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.\u000a\u000a==Multiple pointer records==\u000aWhile most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended, unless there is a specific need.  For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically.  Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause responses to be truncated if they exceed the DNS 512 byte UDP message limit.\u000a\u000a==Records other than PTR records==\u000aRecord types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]] (RFC 4025), [[Secure Shell|SSH]] (RFC 4255) and [[Internet Key Exchange|IKE]] (RFC 4322).\u000a[[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] (RFC 6763) uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC\u202f6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref>\u000aLess standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.\u000a\u000a==Uses==\u000aThe most common uses of the reverse DNS include:\u000a* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.\u000a* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, dynamically assigned addresses, or other inexpensive Internet services.  Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com."  Some corporate anti-spam services take the view that the vast majority, but by no means all, of e-mail that originates from these computers is spam with spam filters refusing e-mail with such rDNS names.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL]</ref> However data has shown that just as much if not more spam has originated from unpatched machines within corporate networks that are more likely to use out of date browsers than cheaper services such as DSL networks not to mention the difficulty of blocking spam from major providers like Yahoo and Hotmail. A recent shift has shown that spamming has switched to mainly coming from hosting companies making using rDNS even less useful.<ref>http://www.mailchannels.com/blog/2013/03/worlds-largest-spam-sources-are-all-hosting-companies/</ref> All of this adds to the argument that the few services that choose to block email servers purely on the basis of rDNS are simply discriminating without merit and often miss out more pro-active and useful indiscriminate anti spam measures.<ref>http://ask.slashdot.org/story/11/10/13/1643202/ask-slashdot-is-reverse-dns-a-worthy-standard-for-fighting-spam</ref>\u000a* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, mainly because [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually can't pass verification for it when they use [[zombie computer]]s to forge domains.\u000a* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address\u000a\u000a==See also==\u000a*[[Forward-confirmed reverse DNS]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a*{{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}\u000a* [http://dns.icann.org ICANN DNS Operations]\u000a* RFC 2317 documents a way to do rDNS delegation for [[Classless Inter-Domain Routing|CIDR]] blocks\u000a* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]\u000a* RDNS policies: [http://postmaster.aol.com/Postmaster.Errors.php#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]\u000a\u000a[[Category:Searching]]\u000a[[Category:Domain name system]]\u000a\u000a[[nl:Domain Name System#Omgekeerde lookups]]'''
p73
sg4
S'170'
p74
sg6
VReverse DNS lookup
p75
ssI45
(dp76
g2
V{{cleanup|date=November 2010}}{{External links|date=January 2012}}\u000a\u000a'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.\u000a\u000a== Importance of expertise ==\u000a\u000aIt can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and \u201clicensing\u201d expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.\u000a\u000aUntil very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one\u2019s judgment about those individuals is justified and that their answers are thoughtful.\u000a\u000aIn the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed \u201cexpertise locating systems\u201d.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|\u201crecommender systems\u201d]].\u000a\u000aAt the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.\u000a\u000aStill other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed \u201cgated objects\u201d, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.\u000a\u000aExamples of the systems outlined above are listed in Table 1.\u000a\u000a'''Table 1: A classification of expertise location systems'''\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Type\u000a! Application domain\u000a! Data source\u000a! Examples\u000a|-\u000a| Social networking\u000a| Professional networking\u000a| User-generated\u000a|\u000a* [[LinkedIn]]\u000a|-\u000a| [[Scientific literature]]\u000a| Identifying publications with strongest research impact\u000a| Third-party generated\u000a|\u000a* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]\u000a|-\u000a| [[Scientific literature]]\u000a| Expertise search\u000a| Software\u000a|\u000a* [[Arnetminer]][http://arnetminer.org]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| User-Generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)\u000a* [[Tacit Software]] (Oracle Corporation)\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| User-generated\u000a|\u000a* [[Community of Science]] Expertise [http://expertise.cos.com]\u000a* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| Third party-generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* MindServer Expertise ([[Recommind]], Inc.)\u000a* Tacit Software\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| Third party-generated\u000a|\u000a* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)\u000a* [http://authoratory.com/ authoratory.com]\u000a* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)\u000a* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)\u000a* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)\u000a* [http://researchcrossroads.org ResearchCrossroads.org] (Innolyst, Inc.)\u000a|-\u000a| Blog [[search engine]]s\u000a|\u000a| Third party-generated\u000a|\u000a* [[Technorati]] [http://technorati.com/]\u000a|}\u000a\u000a== Technical problems ==\u000aA number of interesting problems follow from the use of expertise finding systems:\u000a\u000a* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.\u000a* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).\u000a* Finding ways to avoid \u201cgaming\u201d of the system to reap unjustified expertise [[credibility]].\u000a\u000a== Expertise ranking ==\u000a\u000aMeans of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:\u000a\u000a* How can expertise be assessed objectively? Is that even possible?\u000a* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?\u000a* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?\u000a* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?\u000a\u000a== Sources of data for assessing expertise ==\u000aMany types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.\u000a\u000aUnfiltered data sources that have been used to assess expertise, in no particular ranking order:\u000a\u000a* user recommendations\u000a* help desk tickets: what the problem was and who fixed it\u000a* e-mail traffic between users\u000a* documents, whether private or on the web, particularly publications\u000a* user-maintained web pages\u000a* reports (technical, marketing, etc.)\u000a\u000aFiltered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:\u000a\u000a* [[patent]]s, particularly if issued\u000a* scientific publications\u000a* issued grants (failed grant proposals are rarely know beyond the authors)\u000a* [[clinical trial]]s\u000a* product launches\u000a* pharmaceutical drugs\u000a\u000a== Approaches for creating expertise content ==\u000a* Manual, either by experts themselves (e.g., LinkedIn) or by a curator\u000a* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard])\u000a\u000a== Interesting expertise systems over the years ==\u000aIn no particular order...\u000a\u000a* Autonomy's IDOL\u000a* AskMe\u000a* Tacit Knowledge Systems' ActiveNet\u000a* Triviumsoft's SEE-K\u000a* MIT\u2019s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)\u000a* MITRE\u2019s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]\u000a* MITRE\u2019s XpertNet\u000a* Arnetminer (ref 2)\u000a* Dataware II Knowledge Directory\u000a* Thomson\u2019s tool\u000a* Hewlett-Packard\u2019s CONNEX\u000a* Microsoft\u2019s SPUD project\u000a* [http://www.xperscore.com Xperscore]\u000a* [http://intunex.fi/skillhive/ Skillhive]\u000a\u000a== Conferences ==\u000a# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]\u000a\u000a== References ==\u000a\u000a# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.\u000a# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.\u000a# Maybury, M., D\u2019Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.\u000a# Maybury, M., D\u2019Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.\u000a# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.\u000a# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.\u000a# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.\u000a\u000a[[Category:Evaluation methods]]\u000a[[Category:Metrics]]\u000a[[Category:Analysis]]\u000a[[Category:Impact assessment]]\u000a[[Category:Intellectual works]]\u000a[[Category:Knowledge sharing]]\u000a[[Category:Library science]]\u000a[[Category:Information retrieval]]\u000a[[Category:Science studies]]
p77
sg4
S'45'
p78
sg6
VExpertise finding
p79
ssI175
(dp80
g2
V{{Cat main|Internet search}}\u000a\u000a[[Category:Web services]]\u000a[[Category:Searching]]\u000a[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that "Internet search" redirects to "Web page search" (or something like that)--->
p81
sg4
S'175'
p82
sg6
VCategory:Internet search
p83
ssI50
(dp84
g2
V'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.\u000a\u000a== Explicit feedback ==\u000a\u000aExplicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.\u000a\u000aUsers may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.\u000a\u000aThe relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000aA performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].\u000a\u000a== Implicit feedback ==\u000a\u000aImplicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf].\u000a\u000aThe key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:\u000a\u000a# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and\u000a# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback\u000a\u000aAn example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.\u000a\u000a== Blind feedback ==\u000a\u000aPseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:\u000a\u000a# Take the results returned by initial query as relevant results (only top k with k being between 10 to 50 in most experiments).\u000a# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.\u000a# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.\u000a\u000aSome experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.\u000a\u000aThis automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> \u000aSpecifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.\u000a\u000aBlind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.\u000a\u000a== Using relevance information ==\u000a\u000aRelevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000a==Further reading==\u000a*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's\u000a*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''\u000a*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a[[Category:Information retrieval]]\u000a\u000a[[zh:\u76f8\u5173\u53cd\u9988]]
p85
sg4
S'50'
p86
sg6
VRelevance feedback
p87
ssI180
(dp88
g2
V{{multiple issues|\u000a{{Advert|date=March 2012}}\u000a{{Notability|Products|date=March 2012}}\u000a}}\u000a\u000a{{Infobox software\u000a| name                   = OpenGrok\u000a| logo                   = [[Image:OpenGrok Logo.png|150px|OpenGrok Logo]]\u000a| screenshot             = \u000a| caption                =\u000a| collapsible            = yes\u000a| developer              = [[Sun Microsystems]]/[[Oracle Corporation]]\u000a| latest release version = 0.12.1\u000a| latest release date    = {{release date|2014|04|29}}\u000a| latest preview version =\u000a| latest preview date    =\u000a| operating system       = [[Cross-platform]]\u000a| programming language   = [[Java (programming language)|Java]]\u000a| genre                  = [[Index (search engine)|Index]]er and [[cross-reference]]r with [[Revision control]]\u000a| license                = [[CDDL]]\u000a| website                = http://opengrok.github.com/OpenGrok/\u000a}}\u000a\u000a'''OpenGrok''' is a [[source code]] search and cross reference engine. It helps programmers to search, cross-reference and navigate source code trees.\u000a\u000aIt can understand various [[program (computing)|program]] [[file formats]] and [[version control]] histories like [[Monotone (software)|Monotone]], [[Source Code Control System|SCCS]], [[Revision Control System|RCS]], [[Concurrent Versions System|CVS]], [[Subversion (software)|Subversion]], [[Mercurial (software)|Mercurial]], [[Git (software)|Git]], [[IBM Rational ClearCase|Clearcase]], [[Perforce]] and [[Bazaar (software)|Bazaar]].<ref>https://github.com/OpenGrok/OpenGrok/wiki/Supported-Revision-Control-Systems</ref>\u000a\u000aThe name comes from the term ''[[grok]]'', a [[jargon]] term used in computing to mean "profoundly understand". The term ''[[grok]]'' originated in a science fiction novel by Robert A. Heinlein called ''[[Stranger in a Strange Land]]''.\u000a\u000aOpenGrok is being developed mainly by [[Oracle Corporation]] (former [[Sun Microsystems]]) engineers with help from its community. OpenGrok is released under the terms of the [[Common Development and Distribution License]] (CDDL).\u000a\u000a== Features ==\u000a\u000aOpenGrok's features include:\u000a\u000a* Full text Search\u000a* Definition Search\u000a* Identifier Search\u000a* Path search\u000a* History Search\u000a* Shows matching lines\u000a* Hierarchical Search\u000a* query syntax like ''AND'', ''OR'', ''field'':\u000a* Incremental update\u000a* Syntax highlighting-Xref\u000a* Quick navigation inside the file\u000a* Interface for SCM\u000a* Usable URLs\u000a* Individual file download\u000a* Changes at directory level\u000a* Multi language support\u000a\u000a== See also ==\u000a\u000a* [[LXR Cross Referencer]]\u000a* [[ViewVC]]\u000a* [[FishEye (software)]]\u000a\u000a== References ==\u000a\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://opengrok.github.com/OpenGrok/ OpenGrok project page]\u000a* {{ohloh|opengrok}}\u000a* [http://code.metager.de/source/ Metager]\u000a* [http://BXR.SU/ Super User's BSD Cross Reference]\u000a\u000a{{Sun Microsystems}}\u000a{{Java (Sun)}}\u000a\u000a[[Category:Cross-platform free software]]\u000a[[Category:Free revision control software]]\u000a[[Category:Source code]]\u000a[[Category:Searching]]\u000a[[Category:Java platform software]]\u000a[[Category:Concurrent Versions System]]\u000a[[Category:Subversion]]\u000a[[Category:Code search engines]]\u000a\u000a\u000a{{programming-software-stub}}
p89
sg4
S'180'
p90
sg6
VOpenGrok
p91
ssI55
(dp92
g2
V{{copy edit|for=Use of references (both inline, and ref tags)|date=February 2015}}\u000a\u000a'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications that perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.\u000a\u000aIn August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing <ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>\u000a\u000aCLAMOUR<ref>[http://www.statistics.gov.uk/methods_quality/clamour/coordination/wp03.asp] National Statistics CLAMOUR project</ref><ref>[http://www.statistics.gov.uk/methods_quality/clamour/downloads/Clamour_march2002_final_reportAO.pdf] CLAMOUR Final Report</ref> is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information & statistics. In contrast to the techniques discussed by Concept Searching Limited, CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.\u000a\u000aCompound Term Processing allows information retrieval applications, such as search engines, to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.\u000a\u000aMost [[search engine]]s simply look for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. [[Phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.\u000a\u000aTechniques for probabilistic weighting of single word terms dates back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]] entitled "Relevance weighting of search terms", originally published in the ''Journal of the American Society for Information Science''.<ref>{{cite doi | 10.1002/asi.4630270302}}</ref>  Robertson stated that the assumption of word independence is not justified and exists simply as a matter of mathematical convenience. His objection to term independence is not a new idea, dating back to at least 1964 when H. H. Williams expressed that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>WILLIAMS, J.H., 'Results of classifying documents with multiple discriminant functions', In : Statistical Association Methods for Mechanized Documentation, National Bureau of Standards, Washington, 217-224 (1965).</ref>\u000a\u000aCompound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? By forming compound terms and placing these terms in a search engine's index, searches can be performed with a higher degree of accuracy, as the ambiguity inherent in single words is no longer a problem. Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.\u000a\u000aIn 2004, Anna Lynn Patterson filed a number of patents on "phrase-based searching in an information retrieval system"<ref>[http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PG01&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.html&r=1&f=G&l=50&s1=%2220060031195%22.PGNR.&OS=DN/20060031195&RS=DN/20060031195] US Patent: 20060031195</ref> to which Google subsequently acquired the rights. A full discussion of the patents can be found at [http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]{{dead link|date=February 2015}}.\u000a\u000aStatistical compound term processing is a method more adaptive than the process described by Patterson in her patent applications. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.\u000a\u000aStatistical compound term processing is also more adaptive than the linguistic approach taken by the CLAMOUR project, which must take into consideration the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[Information retrieval]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a==  External links ==\u000a*[http://www.conceptsearching.com/ Concept Searching Limited]\u000a*[http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]\u000a\u000a{{Natural Language Processing}}\u000a\u000a{{DEFAULTSORT:Compound Term Processing}}\u000a[[Category:Information retrieval]]
p93
sg4
S'55'
p94
sg6
VCompound term processing
p95
ssI185
(dp96
g2
V{{sister\u000a|project=wiktionary\u000a|text=See the [[Wiktionary:Wiktionary:Concordances|list of concordances]] in [[Wiktionary]], the free dictionary\u000a}}\u000a\u000aA '''concordance''' is an alphabetical list of the principal words used in a book or body of work, with their immediate [[context (language use)#Verbal context|context]]s.  Because of the time, difficulty, and expense involved in creating a concordance in the pre-[[computer]] era, only works of special importance, such as the [[Vedas]],<ref>{{cite book|first = Maurice | last = Bloomfield| authorlink = Maurice Bloomfield | title = A Vedic Concordance| year  = 1990| publisher = Motilal Banarsidass Publ| isbn = 81-208-0654-9}}</ref> [[Bible]], [[Qur'an]] or the works of [[William Shakespeare|Shakespeare]] or classical Latin and Greek authors,<ref>{{cite journal | first = Roy | last = Wisby | authorlink = Roy Wisby | title = Concordance Making by Electronic Computer: Some Experiences with the Wiener Genesis| journal = The Modern Language Review | publisher = Modern Humanities Research Association | volume = 57 | issue = 2 | pages = 161\u2013172 | date = April 1962 | doi=10.2307/3720960}}</ref> had concordances prepared for them. \u000a[[File:Mordechai nathan hebrew latin concordance.jpg|right|thumb|225px|Mordecai Nathan's Hebrew-Latin Concordance of the Bible]]\u000aA concordance is more than an index; additional material, such as commentary, definitions, and topical cross-indexing make producing them a labor-intensive process, even when assisted by computers.\u000a\u000aAlthough an automatically generated [[subject indexing|index]] lacks the richness of a published concordance, the ability to combine the result of queries concerning multiple terms (such as searching for words near other words) has reduced interest in concordance publishing.  In addition, mathematical techniques such as [[Latent Semantic Indexing]] have been proposed as a means of automatically identifying linguistic information based on word context.\u000a\u000aA '''bilingual concordance''' is a concordance based on [[aligned parallel text]].\u000a\u000aA '''topical concordance''' is a list of subjects that a book (usually The Bible) covers, with the immediate context of the coverage of those subjects. Unlike a traditional concordance, the indexed word does not have to appear in the verse. The most well known topical concordance is [[Nave's Topical Bible]].\u000a\u000aThe first concordance, to the [[Vulgate]] Bible, was compiled by [[Hugh of St Cher]] (d.1262), who employed 500 monks to assist him. In 1448 Rabbi Mordecai Nathan completed a concordance to the Hebrew Bible. It took him ten years. 1599 saw a concordance to the Greek New Testament published by Henry Stephens and the Septuagint was done a couple of years later by Conrad Kircher in 1602. The first concordance to the English bible was published in 1550 by Mr Marbeck. According to Cruden it did not employ the verse numbers devised by Robert Stephens in 1545 but "the pretty large concordance" of Mr Cotton did. Then followed [[Cruden's Concordance]] and [[Strong's Concordance]].\u000a\u000a==Use in linguistics==\u000aConcordances are frequently used in [[linguistics]], when studying a text. For example:    \u000a* comparing different usages of the same word\u000a* analysing keywords\u000a* analysing [[word frequencies]]\u000a* finding and analysing phrases and [[idioms]]\u000a* finding [[translation]]s of subsentential elements, e.g. [[terminology]], in [[Bitext#Bitexts and translation memories|bitexts and translation memories]]\u000a* creating indexes and word lists (also useful for publishing)\u000a\u000aConcordancing techniques are widely used in national corpora such as [[American National Corpus]], [[British National Corpus]], and [[Corpus of Contemporary American English]] available on-line.  Stand-alone applications that employ concordancing techniques are known as concordancers.<ref>[http://www.lexically.net/wordsmith/introduction.htm?gclid=COjFnvGKhakCFVJX4Qod-RqjjQ Introduction to WordSmith]</ref> Some of them have integrated part-of-speech taggers and enable the user to create his/her own pos-annotated corpora to conduct various type of searches adopted in corpus linguistics.<ref>[http://yatsko.zohosites.com/linguistic-toobox-a-concordancer.html Linguistic Toolbox]</ref>\u000a\u000a==Inversion==\u000a\u000aThe reconstruction of the text of some of the [[Dead Sea Scrolls]] involved a concordance.\u000a\u000aAccess to some of the scrolls was governed by a "secrecy rule" that allowed only the original International Team or their designates to view the original materials. After the death of [[Roland de Vaux]] in 1971, his successors repeatedly refused to even allow the publication of photographs to other scholars. This restriction was circumvented by [[Martin Abegg]] in 1991, who used a computer to "invert" a concordance of the missing documents made in the 1950s which had come into the hands of scholars outside of the International Team, to obtain an approximate reconstruction of the original text of 17 of the documents.<ref>{{cite web |last= Hawrysch |first= George |title= Dr. George Hawrysch's speech on concordance book launch |work= The Ukrainian Weekly, No. 31, Vol. LXX |publisher= Ukrainian National Association |date= 2002-08-04 |url= http://www.ukrweekly.com/old/archive/2002/310217.shtml |accessdate= 2008-06-19}}</ref><ref>{{cite web |last= Jillette |first= Penn |title= You May Already be a "Computer Expert" |url= http://pennandteller.com/sincity/penn-n-teller/pcc/deadsea.html |accessdate= 2008-06-14}}</ref> This was soon followed by the release of the original text of the scrolls.\u000a\u000a== See also ==\u000a* [[Back-of-the-book index]]\u000a* [[A Vedic Word Concordance]]\u000a* [[Bible concordance]]\u000a* [[Bitext]]\u000a* [[Concordancer]]\u000a* [[Cross-reference]]\u000a* [[Index (publishing)|Index]]\u000a* [[Key Word in Context|KWIC]]\u000a* [[Text mining]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.opensourceshakespeare.org/concordance/ Shakespeare concordance] - A concordance of Shakespeare's complete works (from Open Source Shakespeare)\u000a* [http://www.arts.ualberta.ca/~ukr/skovoroda/NEW/ Online Concordance to the Complete Works of Hryhorii Skovoroda] - A concordance to Hryhorii Skovoroda's complete works (University of Alberta, Edmonton, Canada)\u000a* [http://infomotions.com/alex/ Alex Catalogue of Electronic Texts] - The Alex Catalogue is a collection of public domain electronic texts from American and English literature as well as Western philosophy. Each of the 14,000 items in the Catalogue are available as full-text but they are also complete with a concordance. Consequently, you are able to count the number of times a particular word is used in a text or list the most common (10, 25, 50, etc.) words.\u000a* [http://victorian.lang.nagoya-u.ac.jp/concordance/ Hyper-Concordance] - The Hyper-Concordance is written in C++, a program that scans and displays lines based on a command entered by the user. The main advantage of the C++ program is that it not only identifies the concordance lines but the words occurring to the left and the right of the word or phrase searched. It also reports the total number of text lines, the total word count and the number of occurrences of the word or phrase searched. The full text of the book is displayed in a box at the bottom of the screen. Each line of the text is numbered, and the line number and the term(s) searched provide a link to the full text.\u000a* [http://cherry.conncoll.edu/cohar/Programs.htm Concord] - Page includes link to Concord, an on-the-fly KWIC concordance generator.  Works with at least some non-Latin scripts (modern Greek, for instance).  Multiple choices for sorting results; multi-platform; Open Source.\u000a* [http://buschmeier.org/bh/study/ccd/ ConcorDance] - A concordance interface to the WorldWideWeb, it uses Google's or Yahoo's search engine to find concordances and can be used directly from the browser.\u000a* [http://ctext.org/tools/concordance Chinese Text Project Concordance Tool] - Concordance lookup and discussion of the continued importance of printed concordances in [[Sinology]] - [[Chinese Text Project]]\u000a* [http://khc.sourceforge.net/en/ KH Coder] - A free software for KWIC concordance and collocation stats generation. Various statistical analysis functions are also available such as co-occurrence network, multidimensional scaling, hierarchical cluster analysis, and correspondence analysis of words.\u000a\u000a{{DEFAULTSORT:Concordance (Publishing)}}\u000a[[Category:Concordances (publishing)| ]]\u000a[[Category:Indexing]]\u000a[[Category:Searching]]\u000a[[Category:Library science]]\u000a[[Category:Information science]]\u000a[[Category:Reference works]]
p97
sg4
S'185'
p98
sg6
VConcordance (publishing)
p99
ssI60
(dp100
g2
VThe '''European Summer School in Information Retrieval''' (ESSIR) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.\u000aThe aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: \u201c''The term IR identifies the activities that a person \u2013 the user \u2013 has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''\u201d<ref>Agosti, M.: \u201cInformation Access using the Guide of User Requirements\u201d. In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>\u000a\u000aIR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[Computer Science]] to [[Information Science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].\u000a\u000aESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the [[Internet]].\u000a\u000aTwo books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval''\u000a,<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): \u201cLectures on Information Retrieval\u201c. Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11\u201315, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is ''Advanced Topics in Information Retrieval''.<ref>Melucci, M., and Baeza-Yates, R. (Eds): \u201cAdvanced Topics in Information Retrieval\u201c. The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>\u000a\u000a== ESSIR Editions ==\u000aESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by Nick Belkin, [[Rutgers University]], U.S.A., and Maristella Agosti, [[University of Padua]], Italy, for an Italian audience in 1989.\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Edition\u000a! Web Site\u000a! Location\u000a! Organiser(s)\u000a|-\u000a|  9th\u000a|  [http://www.ugr.es/~essir2013/ ESSIR 2013]\u000a|  Granada, Spain\u000a|  Juan M. Fernadez-Luna and Juan F. Huete\u000a|-\u000a|  8th\u000a|  [http://essir.uni-koblenz.de/ ESSIR 2011]\u000a|  Koblenz, Germany\u000a|  Sergej Sizov and Steffen Staab\u000a|-\u000a|  7th\u000a|  [http://essir2009.dei.unipd.it/ ESSIR 2009]\u000a|  Padua, Italy\u000a|  Massimo Melucci and Ricardo Baeza-Yates\u000a|-\u000a|  6th\u000a|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]\u000a|  Glasgow, Scotland, United Kingdom\u000a|  Iadh Ounis and Keith van Rijsbergen\u000a|-\u000a|  5th\u000a|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]\u000a|  Dublin, Ireland\u000a|  Alan Smeaton\u000a|-\u000a|  4th\u000a|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]\u000a|  Aussois (Savoie), France\u000a|  Catherine Berrut and Yves Chiaramella\u000a|-\u000a|  3rd\u000a|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]\u000a|  Varenna, Italy\u000a|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi\u000a|-\u000a|  2nd\u000a|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]\u000a|  Glasgow, United Kingdom\u000a|  Keith van Rijsbergen\u000a|-\u000a|  1st\u000a|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]\u000a|  Brixen, Italy\u000a|  Maristella Agosti\u000a|}\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]\u000a* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.dei.unipd.it/ Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.unipd.it/en/index.htm University of Padua, Italy]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Summer schools]]
p101
sg4
S'60'
p102
sg6
VEuropean Summer School in Information Retrieval
p103
ssI190
(dp104
g2
V'''Bayesian search theory''' is the application of [[Bayesian statistics]] to the search for lost objects. It has been used several times to find lost sea vessels, for example the [[USS Scorpion (SSN-589)|USS ''Scorpion'']]. It also played a key role in the recovery of the flight recorders in the [[Air France Flight 447]] disaster of 2009. \u000a\u000a==Procedure==\u000a\u000aThe usual procedure is as follows:\u000a\u000a# Formulate as many reasonable hypotheses as possible about what may have happened to the object. \u000a# For each hypothesis, construct a [[probability density function]] for the location of the object.\u000a# Construct a function giving the probability of actually finding an object in location&nbsp;X when searching there if it really is in location&nbsp;X. In an ocean search, this is usually a function of water depth \u2014 in shallow water chances of finding an object are good if the search is in the right place. In deep water chances are reduced.\u000a# Combine the above information coherently to produce an overall probability density map. (Usually this simply means multiplying the two functions together.) This gives the probability of finding the object by looking in location&nbsp;X, for all possible locations&nbsp;X. (This can be visualized as a [[contour map]] of probability.)\u000a# Construct a search path which starts at the point of highest probability and 'scans' over high probability areas, then intermediate probabilities, and finally low probability areas.\u000a# Revise all the probabilities continuously during the search. For example, if the hypotheses for location&nbsp;X imply the likely disintegration of the object and the search at location&nbsp;X has yielded no fragments, then the probability that the object is somewhere around there is greatly reduced (though not usually to zero) while the probabilities of its being at other locations is correspondingly increased. The revision process is done by applying [[Bayes' theorem]].\u000a\u000aIn other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.\u000a\u000aThe advantages of the Bayesian method are that all information available is used coherently (i.e., in a "leak-proof" manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, "there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15&nbsp;days" or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.\u000a\u000aApart from the [[USS Scorpion (SSN-589)|USS ''Scorpion'']], other vessels located by Bayesian search theory include the [[MV Derbyshire|MV&nbsp;''Derbyshire'']], the largest British vessel ever lost at sea, and the [[SS Central America|SS&nbsp;''Central America'']]. It also proved successful in the search for a lost [[hydrogen bomb]] following the [[1966 Palomares B-52 crash]] in Spain, and the recovery in the Atlantic Ocean of the crashed [[Air France Flight 447]].\u000a\u000aBayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the [[United States Coast Guard]] for [[search and rescue]]. This program was later adapted for inland search by adding terrain and ground cover factors for use by the [[United States Air Force]] and [[Civil Air Patrol]].\u000a\u000a==Mathematics==\u000a\u000aSuppose a grid square has a probability ''p'' of containing the wreck and that the probability of successfully detecting the wreck if it is there is ''q''. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by\u000a\u000a: <math>  p' = \u005cfrac{p(1-q)}{(1-p)+p(1-q)} = p \u005cfrac{1-q}{1-pq} < p.</math>\u000aFor each other grid square, if its prior probability is ''r'', its posterior probability is given by\u000a\u000a: <math> r' = r \u005cfrac{1}{1- pq} > r. </math>\u000a\u000a\u000a==Optimal Distribution of Search Effort==\u000a\u000aThe classical book on this subject, based on probabilistic information, by [[Lawrence D. Stone]], won the 1975 [[Frederick W. Lanchester Prize]] by the [[Operations Research Society of America]].\u000a\u000a\u000a\u000a<!-- Material on USS Scorpion, moved from Bayesian inference\u000a\u000aIn May 1968, the [[U.S. Navy]]'s [[nuclear submarine]] [[USS Scorpion (SSN-589)|USS ''Scorpion'' (SSN-589)]] failed to arrive as expected at her home port of [[Norfolk, Virginia]]. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the [[East Coast of the United States|Eastern Seaboard]],  but an extensive search there failed to discover the remains of the ''Scorpion''.\u000a\u000aThen, a Navy deep-water expert, [[John Craven USN|John P. Craven]], suggested that the USS ''Scorpion'' had sunk elsewhere. Craven organised a search southwest of the [[Azores]] based on a controversial approximate triangulation by [[hydrophone]]s. He was allocated only a single ship, the [[USNS Mizar (AGOR-11)|''Mizar'']], and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the ''Scorpion''.\u000a\u000aThe sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.\u000a\u000a\u000a==Optimal Distribution of Search Effort==\u000a\u000aThe classical book on this subject by [[Lawrence D. Stone]] won the 1975 [[Lancaster Prize]] by the American Operations Research Society.\u000a\u000a-->\u000a\u000a==See also==\u000a* [[Bayesian inference]]\u000a* [[Search games]]\u000a\u000a== References ==\u000a* [[Stone, Lawrence D.]], ''The Theory of Optimal Search'', published by the [[Operations Research Society of America]], 1975\u000a* [[Stone, Lawrence D.]], In search of Air France Flight 447. Institute of Operations Research and the Management Sciences, 2011\u000a* Iida, Koji., '' Studies on the Optimal Search Plan'', Vol.&nbsp;70, Lecture Notes in Statistics, Springer-Verlag, 1992.\u000a* De Groot, Morris H., ''Optimal Statistical Decisions'', Wiley Classics Library, 2004.\u000a* Richardson, Henry R; and Stone, Lawrence D. Operations Analysis during the underwater search for ''Scorpion''. ''Naval Research Logistics Quarterly'', June&nbsp;1971, Vol.&nbsp;18, Number&nbsp;2. Office of Naval Research.\u000a* Stone, Lawrence D. Search for the SS ''Central America'': Mathematical Treasure Hunting. Technical Report, Metron Inc. Reston, Virginia.\u000a* [[Bernard Koopman|Koopman, B.O.]] ''Search and Screening'', Operations Research Evaluation Group Report 56, Center for Naval Analyses, Alexandria, Virginia. 1946.\u000a* Richardson, Henry R; and Discenza, J.H. The United States Coast Guard computer-assisted search planning system (CASP). ''Naval Research Logistics Quarterly''. Vol.&nbsp;27 number&nbsp;4. pp.&nbsp;659\u2013680. 1980.\u000a* [[Ross, Sheldon M.]], ''An Introduction to Stochastic Dynamic Programming'', Academic Press. 1983.\u000a\u000a[[Category:Bayesian statistics|Search theory]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]\u000a[[Category:Operations research]]
p105
sg4
S'190'
p106
sg6
VBayesian search theory
p107
ssI65
(dp108
g2
V'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>\u000a\u000a==Architecture==\u000aTeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.\u000a\u000a==References==\u000a<references/>\u000a\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing software]]
p109
sg4
S'65'
p110
sg6
VTeLQAS
p111
ssI195
(dp112
g2
V{{Infobox Software\u000a| name = Lookeen\u000a| screenshot =\u000a| caption =\u000a| developer = [[Axonic Informationssysteme GmbH]]\u000a| latest_release_version = 8.3.1.5156\u000a| latest_release_date = May 21, 2013\u000a| latest_preview_version =\u000a| latest_preview_date =\u000a| operating_system = [[Microsoft Windows]]\u000a| genre = [[Search Tool|Email search]]\u000a| company_type   = Private (venture-backed)\u000a| foundation     = 2006\u000a| location       = [[Karlsruhe]], [[Germany]]\u000a| key_people     = [[Martin Welker]], CEO<br>[[Peter Oehler]], COO\u000a| industry       = Email Applications\u000a| website = [http://www.lookeen.com www.lookeen.com]\u000a}}\u000a'''Lookeen''' is a business search [[Plug-in (computing)|add-on]] for [[Microsoft Outlook]], produced under shareware license. The program uses [[Apache Software Foundation|Apache]]'s search engine [[Lucene]] and helps searching for [[Computer file|files]], [[emails]], [[contacts]], [[Email attachment|attachements]] as well as [[desktop environment|desktop]] elements on [[personal computers]] as well as in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://email.about.com/od/outlookaddons/gr/lookeen.htm ''Lookeen 2010'']. Editor Review on about.com. Retrieved on August 22, 2014.</ref>\u000a==Using==\u000aLookeen is an add-on for Microsoft Outlook. The [[shareware]] program is developed according to the Microsoft company recommendation on the add-ons design. After installation the program automatically integrates into Microsoft Outlook workspace. After the indexing process, Lookeen easily allows to search whole [[Personal Storage Table|Outlook archives]] and the [[My Documents]] folder.<ref>[http://www.pcworld.com/article/233114/lookeen.html ''Lookeen'']. Editor Review on pcworld.com. Retrieved on August 22, 2014.</ref>\u000aIn contrast to the Microsoft Outlook [[native (computing)|native]] search engine, Lookeen indexes the complete [[folder (computing)|folder]] structure. Whereas the native Outlook search only allows searches within the presently used and active folder, Lookeen searches in complete Outlook archives for needed information. \u000a\u000a===Supported mailbox storages===\u000aLookeen supports the following types of mail accounts: [[POP3]], [[IMAP]], [[HTTP]] and [[Microsoft Exchange Server]]. Both uncached and [[cache (computing)|cached]] exchange server modes are supported.\u000a===Supported filetypes===\u000aThe following filetypes can be indexed and searched for with Lookeen (in alphabetical order]: [[.bmp]], [[.doc]], [[.docx]], [[.gif]], [[.htm]], [[.html]], [[.jpeg]], [[.jpg]], [[.msg]], [[.pdf]], [[.php]], [[.png]], [[.pps]], [[.ppsx]], [[.ppt]], [[.pptx]], [[.rtf]], [[.txt]], [[.tif]], [[.tiff]], [[.xls]], [[.xlsm]], [[.xlsx]], [[.xml]]. \u000a===Central indexing===\u000aLookeen 8 supports central indexing of shared resources (e.g. network files, public exchange folders). This shared index is created once and integrated by the clients via its URL. Goal is to reduce network- and server-traffic and reduce the index storage cost for local indexes.<ref>[http://www.techmynd.com/outlook-search-tool-lookeen-licenses-giveaway/ ''Excellent Outlook Search Tool \u2013 Lookeen'']. Editor Review on Techmynd.com. Retrieved on August 22, 2014.</ref>\u000a===Enterprise Roll-Out Support===\u000aLookeen 8 supports [[Group Policies]] for advanced software distributions in companies. Many options (e.g. index location, settings location, included sources, index intervals, license keys, etc.) can be defined by the administrator. That enables enterprises to use Lookeen in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://www.itwire.com/featured-news/54892-lookeen-8-accelerates-outlook-e-mail-search ''Lookeen 8 accelerates Outlook E-Mail-Search'']. Official Press Release on ITwire.com. Retrieved on August 22, 2014.</ref>\u000a\u000a==History==\u000aStructure and Design of the first version strongly resembled the e-mail search software Lookout as developed by the [[Silicon Valley]] [[Startup company|Startup]] [[Lookout Software LCC]]. In 2004, Microsoft bought Lookout for allegedly 6 Million US-Dollars in order to integrate the search technology into its [[Windows Desktop Search]].<ref>[http://www.microsoft.com/presspass/press/2004/jul04/07-16lookoutpr.mspx ''MSN Announces Investment in Search Technology'']. Press Release on Microsoft.com. Retrieved on August 12, 2014.</ref> Lookout continued being available as [[Freeware]], but was not compatible anymore with Microsoft Outlook with the Release of [[Microsoft Windows Vista]] in January 2007. \u000aIn 2007, the German IT company [[Axonic Informationssysteme GmbH]] started working on a follow-up software for Lookout and finally released Lookeen in January 2008 as a professional solution for file and e-mail searches.<ref>[http://unternehmen.wikia.com/wiki/Axonic ''Company history of the creators of Lookeen'']. Official company registry entry on unternehmens.wikia.com. Retrieved on August 22, 2014.</ref> Within eight months, Lookeen was then sold in more than 40 countries.\u000a\u000a==Lookeen Server Enterprise Search==\u000aIn Juli 2011, a corresponding [[enterprise search]] version has been released. The [[Lookeen Server]] supports global indexing functions taking privacy and data security concerns into account by totally centralizing control options.<ref>[http://www.lookeen-server.com/en/product/overview ''Overview: Lookeen Server'']. From lookeen-server.com. Retrieved on August 20, 2014.</ref> \u000a\u000a==See also==\u000a* [[Comparison of enterprise search software]]\u000a* [[List of enterprise search vendors]]\u000a* [[List of Search Engines]]\u000a\u000a==References==\u000a<references />\u000a== External links ==\u000a* [http://www.crunchbase.com/company/lookeen CrunchBase: Lookeen Profile]\u000a* [http://www.lookeen.net Lookeen homepage]\u000a* [http://www.lookeen-server.com Lookeen Server homepage]\u000a* [http://www.axonic.net Creators of Lookeen]\u000a\u000a[[Category:Shareware]]\u000a[[Category:Software]]\u000a[[Category:Microsoft Office-related software]]\u000a[[Category:Desktop search engines]]\u000a[[Category:Searching]]
p113
sg4
S'195'
p114
sg6
VLookeen
p115
ssI70
(dp116
g2
V{{Lowercase}}\u000a\u000a{{Infobox company |\u000a  name   = dtSearch Corp. |\u000a  slogan = "The Smart Choice for Text Retrieval since 1991" |\u000a  type   =  Private company |\u000a  foundation     = 1991 |\u000a  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]] |\u000a  key_people     = David Thede, President |\u000a  industry       = [[Software]] |\u000a\u000a  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]\u000a}}\u000a\u000a'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.\u000a\u000a==History==\u000adtSearch Corp was founded by David Thede<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm; Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[AskSAM]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.\u000a\u000aIn the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft\u2019s initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]</ref>\u000a\u000aIn 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>\u000a\u000a==Products==\u000aThe current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.\u000a\u000a*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)\u000a*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)\u000a*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)\u000a*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)\u000a*dtSearch Engine for Linux - SDK with C++ and Java APIs\u000a*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\u005cDVD publishing (32 and 64 bit indexers)\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[List of enterprise search vendors]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.dtsearch.com/ Company Website]\u000a*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]\u000a*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]\u000a*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]\u000a*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan\u2013Feb]\u000a*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233\u20131246] \u000a*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]\u000a\u000a{{DEFAULTSORT:Dtsearch Corp.}}\u000a[[Category:Desktop search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies based in Maryland]]
p117
sg4
S'70'
p118
sg6
VDtSearch
p119
ssI200
(dp120
g2
V{{contradict-other-multiple|Permutation|Lehmer code|Factorial number system|date=March 2013}}\u000a[[File:Inversion set and vector of a permutation.svg|thumb|right|380px|The permutation (4,1,5,2,6,3) has the inversion vector (0,1,0,2,0,3) and the inversion set {(1,2),(1,4),(3,4),(1,6),(3,6),(5,6)}. The inversion vector [[w:Factorial number system|converted]] to decimal is 373.]]\u000a[[File:Inversion set 16; wp(13,11, 7,15).svg|thumb|250px|Inversion set of the permutation<br>(0,15, 14,1, 13,2, 3,12,<br>11,4, 5,10, 6,9, 8,7)<br>showing the pattern of the<br>[[Thue\u2013Morse sequence]]]]\u000aIn [[computer science]] and [[discrete mathematics]], an '''inversion''' is a pair of places of a sequence where the elements on these places are out of their natural [[total order|order]].\u000a\u000a== Definitions ==\u000a\u000aFormally, let <math>(A(1), \u005cldots, A(n))</math> be a sequence of ''n'' distinct numbers.  If <math>i < j</math> and <math>A(i) > A(j)</math>, then the pair <math>(i, j)</math> is called an inversion of <math>A</math>.{{sfn|Cormen|Leiserson|Rivest|Stein|2001|pp=39}}{{sfn|Vitter|Flajolet|1990|pp=459}}\u000a\u000aThe '''inversion number''' of a sequence is one common measure of its sortedness.{{sfn|Barth|Mutzel|2004|pp=183}}{{sfn|Vitter|Flajolet|1990|pp=459}}  Formally, the inversion number is defined to be the number of inversions, that is, \u000a:<math>\u005ctext{inv}(A) = \u005c# \u005c{(A(i),A(j)) \u005cmid i < j \u005ctext{ and } A(i) > A(j)\u005c}</math>.{{sfn|Barth|Mutzel|2004|pp=183}}  \u000aOther measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted "runs" within the sequence, and the smallest number of exchanges needed to sort the sequence.{{sfn|Mahmoud|2000|pp=284}} Standard [[comparison sort]]ing algorithms can be adapted to compute the inversion number in time {{math|O(''n'' log ''n'')}}.\u000a\u000aThe '''inversion vector''' ''V(i)'' of the sequence is defined for ''i'' = 2, ..., ''n'' as <math>V[i] = \u005cleft\u005cvert\u005c{k \u005cmid k < i \u005ctext{ and } A(k) > A(i)\u005c}\u005cright\u005cvert</math>.  In other words each element is the number of elements preceding the element in the original sequence that are greater than it.  Note that the inversion vector of a sequence has one less element than the sequence, because of course the number of preceding elements that are greater than the first is always zero.  Each permutation of a sequence has a unique inversion vector and it is possible to construct any given permutation of a (fully sorted) sequence from that sequence and the permutation's inversion vector.{{sfn|Pemmaraju|Skiena|2003|pp=69}}\u000a\u000a==Weak order of permutations==\u000aThe set of permutations on ''n'' items can be given the structure of a [[partial order]], called the '''weak order of permutations''', which forms a [[lattice (order)|lattice]].\u000a\u000aTo define this order, consider the items being permuted to be the integers from 1 to ''n'', and let Inv(''u'') denote the set of inversions of a permutation ''u'' for the natural ordering on these items. That is, Inv(''u'') is the set of ordered pairs (''i'', ''j'') such that 1 \u2264 ''i'' < ''j'' \u2264 ''n'' and ''u''(''i'') > ''u''(''j''). Then, in the weak order, we define ''u'' \u2264 ''v'' whenever Inv(''u'') \u2286 Inv(''v'').\u000a\u000aThe edges of the [[Hasse diagram]] of the weak order are given by permutations ''u'' and ''v'' such that ''u < v'' and such that ''v'' is obtained from ''u'' by interchanging two consecutive values of ''u''. These edges form a [[Cayley graph]] for the [[symmetric group|group of permutations]] that is isomorphic to the [[skeleton (topology)|skeleton]] of a [[permutohedron]].\u000a\u000aThe identity permutation is the minimum element of the weak order, and the permutation formed by reversing the identity is the maximum element.\u000a\u000a== See also ==\u000a{{wikiversity|Inversion (discrete mathematics)}}\u000a{{commons|Category:Inversion (discrete mathematics)|Inversion (discrete mathematics)}}\u000a* [[Factorial number system]] (a factorial number is a reflected inversion vector)\u000a* [[Permutation group#Transpositions, simple transpositions, inversions and sorting|Transpositions, simple transpositions, inversions and sorting]]\u000a* [[Damerau\u2013Levenshtein distance]]\u000a* [[Parity of a permutation]]\u000a\u000a'''Sequences in the [[On-Line Encyclopedia of Integer Sequences|OEIS]]:'''\u000a* [https://oeis.org/wiki/Index_to_OEIS:_Section_Fa#factorial Index entries for sequences related to factorial numbers]\u000a* Reflected inversion vectors: {{OEIS link|A007623}} and {{OEIS link|A108731}}\u000a* Sum of inversion vectors, cardinality of inversion sets: {{OEIS link|A034968}}\u000a* Inversion sets of finite permutations interpreted as binary numbers: {{OEIS link|A211362}} &nbsp; (related permutation: {{OEIS link|A211363}})\u000a* Finite permutations that have only 0s and 1s in their inversion vectors: {{OEIS link|A059590}} &nbsp; (their inversion sets: {{OEIS link|A211364}})\u000a* Numbers of permutations of n elements with k inversions; Mahonian numbers: {{OEIS link|A008302}} &nbsp; (their row maxima; Kendall-Mann numbers: {{OEIS link|A000140}})\u000a* Number of connected labeled graphs with n edges and n nodes: {{OEIS link|A057500}}\u000a* Arrays of permutations with similar inversion sets and inversion vectors: {{OEIS link|A211365}}, {{OEIS link|A211366}}, {{OEIS link|A211367}}, {{OEIS link|A211368}}, {{OEIS link|A211369}}, {{OEIS link|A100630}}, {{OEIS link|A211370}}, {{OEIS link|A051683}}\u000a\u000a== References ==\u000a{{reflist|4|refs=}}\u000a\u000a=== Source bibliography ===\u000a{{refbegin|1}}\u000a* {{cite journal|ref=harv|first1=Wilhelm|last1=Barth|first2=Petra|last2=Mutzel|author2-link=Petra Mutzel|title=Simple and Efficient Bilayer Cross Counting|journal=[[Journal of Graph Algorithms and Applications]]|volume=8|issue=2|pages=179&ndash;194|year=2004|doi=10.7155/jgaa.00088}}\u000a* {{cite book|ref=harv\u000a | first1=Thomas H.|last1=Cormen|authorlink1=Thomas H. Cormen\u000a | last2=Leiserson|first2=Charles E.|authorlink2=Charles E. Leiserson\u000a | last3=Rivest|first3=Ronald L.|authorlink3=Ron Rivest\u000a | last4=Stein|first4=Clifford|authorlink4=Clifford Stein\u000a | title = [[Introduction to Algorithms]]\u000a | publisher = MIT Press and McGraw-Hill\u000a | year = 2001\u000a | isbn = 0-262-53196-8\u000a | edition = 2nd\u000a }}\u000a* {{cite book|ref=harv|title=Sorting: a distribution theory|chapter=Sorting Nonrandom Data|volume=54|series=Wiley-Interscience series in discrete mathematics and optimization|first=Hosam Mahmoud|last=Mahmoud|publisher=Wiley-IEEE|year=2000|isbn=978-0-471-32710-3}}\u000a* {{cite book|ref=harv|title=Computational discrete mathematics: combinatorics and graph theory with Mathematica|chapter=Permutations and combinations|first1=Sriram V.|last1=Pemmaraju|first2=Steven S.|last2=Skiena|publisher=Cambridge University Press|year=2003|isbn=978-0-521-80686-2}}\u000a* {{cite book|ref=harv|title=Algorithms and Complexity|volume=1|editor1-first=Jan|editor1-last=van Leeuwen|editor1-link=Jan van Leeuwen|edition=2nd|publisher=Elsevier|year=1990|isbn=978-0-444-88071-0|chapter=Average-Case Analysis of Algorithms and Data Structures|first1=J.S.|last1=Vitter|first2=Ph.|last2=Flajolet}}\u000a{{refend}}\u000a\u000a=== Further reading ===\u000a* {{cite journal|ref=harv|journal=Journal of Integer Sequences|volume=4|year=2001|title=Permutations with Inversions|first=Barbara H.|last=Margolius}}\u000a\u000a=== Presortedness measures ===\u000a* {{cite journal|ref=harv|journal=Lecture Notes in Computer Science|year=1984|volume=172|pages=324&ndash;336|doi=10.1007/3-540-13345-3_29|title=Measures of presortedness and optimal sorting algorithms|first=Heikki|last=Mannila|authorlink=Heikki Mannila}}\u000a* {{cite journal|ref=harv|first1=Vladimir|last1=Estivill-Castro|first2=Derick|last2=Wood|title=A new measure of presortedness|journal=Information and Computation|volume=83|issue=1|pages=111&ndash;119|year=1989|doi=10.1016/0890-5401(89)90050-3}}\u000a* {{cite journal|ref=harv|first=Steven S.|last=Skiena|year=1988|title=Encroaching lists as a measure of presortedness|journal=BIT|volume=28|issue=4|pages=755&ndash;784|doi=10.1007/bf01954897}}\u000a\u000a[[Category:Permutations]]\u000a[[Category:Order theory]]\u000a[[Category:String similarity measures]]\u000a[[Category:Sorting algorithms]]\u000a[[Category:Combinatorics]]\u000a[[Category:Discrete mathematics]]
p121
sg4
S'200'
p122
sg6
VInversion (discrete mathematics)
p123
ssI75
(dp124
g2
V'''Learning to rank'''<ref name="liu">{{citation\u000a|author=Tie-Yan Liu\u000a|title=Learning to Rank for Information Retrieval\u000a|series=Foundations and Trends in Information Retrieval: Vol. 3: No 3\u000a|year=2009\u000a|isbn=978-1-60198-244-5\u000a|doi=10.1561/1500000016\u000a|pages=225\u2013331\u000a|journal=Foundations and Trends® in Information Retrieval\u000a|volume=3\u000a|issue=3\u000a}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]\u000a</ref> or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The\u000aMIT Press ISBN 9780262018258.</ref> Training data consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.\u000a\u000aLearning to rank is a relatively new research area which has emerged in the past decade.\u000a\u000a== Applications ==\u000a\u000a=== In information retrieval ===\u000a[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]\u000aRanking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], [[computational advertising]] (online ad placement).\u000a\u000aA possible architecture of a machine-learned search engine is shown in the figure to the right.\u000a\u000aTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),\u000a<!-- "assessor" is the more standard term, used e.g. by TREC conference -->\u000awho check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used \u2014 only the top few documents, retrieved by some existing ranking models are checked. <!--\u000a  TODO: write something about selection bias caused by pooling\u000a--> Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation\u000a | author=Joachims, T.\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf\u000a | title=Optimizing Search Engines using Clickthrough Data\u000a | year=2002\u000a}}</ref> ''query chains'',<ref>{{citation\u000a | author=Joachims T., Radlinski F.\u000a | title=Query Chains: Learning to Rank from Implicit Feedback\u000a | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf\u000a | year=2005\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a}}</ref> or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].\u000a\u000aTraining data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.\u000a\u000aTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation\u000a | author=B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt.\u000a | title=Early exit optimizations for additive machine learned ranking systems\u000a | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. (to appear)\u000a | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf\u000a}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation\u000a | author=Broder A., Carmel D., Herscovici M., Soffer A., Zien J.\u000a | title=Efficient query evaluation using a two-level retrieval process\u000a | journal=Proceedings of the twelfth international conference on Information and knowledge management\u000a | year=2003\u000a | pages=426\u2013434\u000a | isbn=1-58113-723-0\u000a | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf\u000a }}</ref> [[Okapi BM25|BM25]]. This phase is called ''top-<math>k</math> document retrieval'' and many good heuristics were proposed in the literature to accelerate it, such as using document's static quality score and tiered indexes.<ref name="manning-q-eval">{{citation\u000a | author=Manning C.,  Raghavan P. and Schütze H.\u000a | title=Introduction to Information Retrieval\u000a | publisher=Cambridge University Press\u000a | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\u000a\u000a=== In other areas ===\u000aLearning to rank algorithms have been applied in areas other than information retrieval:\u000a* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation\u000a | author=Kevin K. Duh\u000a | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data\u000a | year=2009\u000a | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf\u000a}}</ref>\u000a* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />\u000a* In [[proteomics]] for the identification of frequent top scoring peptides.<ref name="Hen09">{{citation\u000a | author=Henneges C., Hinselmann G., Jung S., Madlung J., Schütz W., Nordheim A., Zell A.\u000a | title=Ranking Methods for the Prediction of Frequent Top Scoring Peptides from Proteomics Data\u000a | year=2009\u000a | url=http://www.omicsonline.com/ArchiveJPB/2009/May/01/JPB2.226.pdf\u000a}}</ref>\u000a* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.</ref>\u000a\u000a== Feature vectors ==\u000aFor convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such approach is sometimes called ''bag of features'' and is analogous to [[bag of words]] and [[vector space model]] used in information retrieval for representation of documents.\u000a\u000aComponents of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):\u000a* ''Query-independent'' or ''static'' features \u2014 those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>\u000a{{cite conference\u000a | first=M. |last=Richardson\u000a | coauthors=Prakash, A. and Brill, E.\u000a | title=Beyond PageRank: Machine Learning for Static Ranking\u000a | booktitle=Proceedings of the 15th International World Wide Web Conference\u000a | pages=707\u2013715\u000a | publisher=\u000a | year=2006\u000a | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf\u000a | accessdate=\u000a }}</ref>\u000a* ''Query-dependent'' or ''dynamic'' features \u2014 those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.\u000a* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''\u000a\u000aSome examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>\u000a* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;\u000a* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;\u000a* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.\u000a\u000aSelecting and designing good features is an important area in machine learning, which is called [[feature engineering]].\u000a\u000a== Evaluation measures ==\u000aThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\u000a\u000aExamples of ranking quality measures:\u000a* [[Mean average precision]] (MAP);\u000a* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];\u000a* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;\u000a* [[Mean reciprocal rank]];\u000a* [[Kendall's tau]]\u000a* [[Spearman's rank correlation coefficient|Spearman's Rho]]\u000a\u000aDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.\u000a\u000aRecently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:\u000a* [[Expected reciprocal rank]] (ERR);<ref>{{citation\u000a|author=Olivier Chapelle, Donald Metzler, Ya Zhang, Pierre Grinspan\u000a|title=Expected Reciprocal Rank for Graded Relevance\u000a|url=http://research.yahoo.com/files/err.pdf\u000a|journal=CIKM\u000a|year=2009\u000a|pages=\u000a}}</ref>\u000a* [[Yandex]]'s pfound.<ref>{{citation\u000a|author=Gulin A., Karpovich P., Raskovalov D., Segalovich I.\u000a|title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods\u000a|url=http://romip.ru/romip2009/15_yandex.pdf\u000a|journal=Proceedings of ROMIP'2009\u000a|year=2009\u000a|pages=163\u2013168\u000a}} (in Russian)</ref>\u000aBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\u000a\u000a== Approaches ==\u000a{{Expand section|date=December 2009}}\u000aTie-Yan Liu of [[Microsoft Research Asia]] in his paper "Learning to Rank for Information Retrieval"<ref name="liu" /> and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and [[loss function]]:\u000a\u000a=== Pointwise approach ===\u000aIn this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem \u2014 given a single query-document pair, predict its score.\u000a\u000aA number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.\u000a\u000a=== Pairwise approach ===\u000aIn this case learning-to-rank problem is approximated by a classification problem \u2014 learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.\u000a\u000a=== Listwise approach ===\u000aThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.\u000a\u000a=== List of methods ===\u000aA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\u000a:{|class="wikitable sortable"\u000a! Year || Name || Type || Notes\u000a|-\u000a| 1989 || OPRF <ref name="Fuhr1989">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=ACM Transactions on Information Systems\u000a | title=Optimum polynomial retrieval functions based on the probability ranking principle\u000a | volume=7\u000a | number=3\u000a | pages=183\u2013204 \u000a | year=1989\u000a | doi=10.1145/65943.65944\u000a}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)\u000a|-\u000a| 1992 || SLR <ref name="Cooperetal1992">{{citation\u000a | author=Cooper, William S.; Gey, Frederic C.; Dabney, Daniel P.\u000a | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval \u000a | title=Probabilistic retrieval based on staged logistic regression\u000a | pages=198\u2013210 \u000a | year=1992\u000a | doi=10.1145/133160.133199\u000a}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression\u000a|-\u000a| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.\u000a|-\u000a| 2002 || Pranking<ref>{{cite paper | id = {{citeseerx|10.1.1.20.378}} | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.\u000a|-\u000a| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || <span style="display:none">3</span> pairwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.\u000a|-\u000a| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || \u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||\u000a|-\u000a| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || RankGP<ref>{{cite paper | id = {{citeseerx|10.1.1.90.220}} | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||\u000aRegularized least-squares based ranking. The work is extended in\u000a<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio|coauthors=Tsivtsivadze, Evgeni, Airola, Antti, Järvinen, Jouni, Boberg, Jorma|title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129\u2013165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.\u000a|-\u000a| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || <span style="display:none">3</span> listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.\u000a|-\u000a| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)\u000a|-\u000a| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. \u000a|-\u000a| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.\u000a|-\u000a| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.\u000a|-\u000a| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2010 || [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.\u000a|-\u000a| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.\u000a|-\u000a| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || \u000a|-\u000a| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.\u000a|}\u000a\u000aNote: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\u000a\u000a== History ==\u000a[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=Computer Journal\u000a | title=Probabilistic Models in Information Retrieval\u000a | volume=35\u000a | number=3\u000a | pages=243\u2013255\u000a | year=1992\u000a | doi=10.1093/comjnl/35.3.243\u000a}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author=Manning C.,  Raghavan P. and Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\u000a\u000aSeveral conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\u000a\u000a=== Practical usage by search engines ===\u000aCommercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>\u000a\u000a[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.\u000a\u000aIn November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary MatrixNet algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]</ref> based on their own search engine's production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>\u000a\u000aAs of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web\u000a  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html\u000a  | archiveurl = http://www.webcitation.org/5sq8irWNM\u000a  | archivedate = 2010-09-18\u000a  | title = Are Machine-Learned Models Prone to Catastrophic Errors?\u000a  | date = 2008-05-24\u000a  | last = Rajaraman\u000a  | first = Anand\u000a  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web\u000a  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing\u000a  | archiveurl = http://www.webcitation.org/5sq7DX3Pj\u000a  | archivedate = 2010-09-15\u000a  | title = Cuil Blog: So how is Bing doing?\u000a  | date = 2009-06-26\u000a  | last = Costello\u000a  | first = Tom}}</ref>\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a; Competitions and public datasets\u000a* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]\u000a* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]\u000a* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]\u000a* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]\u000a\u000a; Open Source code\u000a* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]\u000a* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]\u000a* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]\u000a[[Category:Ranking functions]]
p125
sg4
S'75'
p126
sg6
VLearning to rank
p127
ssI205
(dp128
g2
VIn [[mathematics]], the '''Euclidean distance''' or '''Euclidean metric''' is the "ordinary" [[distance]] between two points in [[Euclidean space]]. With this distance, Euclidean space becomes a [[metric space]]. The associated [[Norm (mathematics)|norm]] is called the '''[[Norm (mathematics)#Euclidean norm|Euclidean norm]].''' Older literature refers to the metric as '''Pythagorean metric'''.\u000a\u000a==Definition==\u000aThe '''Euclidean distance''' between points '''p''' and '''q''' is the length of the [[line segment]] connecting them (<math>\u005coverline{\u005cmathbf{p}\u005cmathbf{q}}</math>).\u000a\u000aIn [[Cartesian coordinates]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>,...,&nbsp;''p''<sub>''n''</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>,...,&nbsp;''q''<sub>''n''</sub>) are two points in [[Euclidean space|Euclidean ''n''-space]], then the distance (d) from '''p''' to '''q''', or from '''q''' to '''p''' is given by the [[Pythagorean theorem|Pythagorean formula]]:\u000a\u000a{{NumBlk|:|<math>\u005cbegin{align}\u005cmathrm{d}(\u005cmathbf{p},\u005cmathbf{q}) = \u005cmathrm{d}(\u005cmathbf{q},\u005cmathbf{p}) & = \u005csqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \u005ccdots + (q_n-p_n)^2} \u005c\u005c[8pt]\u000a& = \u005csqrt{\u005csum_{i=1}^n (q_i-p_i)^2}.\u005cend{align}</math>|{{EquationRef|1}}}}\u000a\u000aThe position of a point in a Euclidean ''n''-space is a [[Euclidean vector]]. So, '''p''' and '''q''' are Euclidean vectors, starting from the origin of the space, and their tips indicate two points. The '''[[Euclidean norm]]''', or '''Euclidean length''', or '''magnitude''' of a vector measures the length of the vector:\u000a:<math>\u005c|\u005cmathbf{p}\u005c| = \u005csqrt{p_1^2+p_2^2+\u005ccdots +p_n^2} = \u005csqrt{\u005cmathbf{p}\u005ccdot\u005cmathbf{p}}</math>\u000awhere the last equation involves the [[dot product]].\u000a\u000aA vector can be described as a directed line segment from the [[Origin (mathematics)|origin]] of the Euclidean space (vector tail), to a point in that space (vector tip). If we consider that its length is actually the distance from its tail to its tip, it becomes clear that the Euclidean norm of a vector is just a special case of Euclidean distance: the Euclidean distance between its tail and its tip.\u000a\u000aThe distance between points '''p''' and '''q''' may have a direction (e.g. from '''p''' to '''q'''), so it may be represented by another vector, given by\u000a\u000a:<math>\u005cmathbf{q} - \u005cmathbf{p} = (q_1-p_1, q_2-p_2, \u005ccdots, q_n-p_n)</math>\u000a\u000aIn a three-dimensional space (''n''=3), this is an arrow from '''p''' to '''q''', which can be also regarded as the position of '''q''' relative to '''p'''. It may be also called a [[displacement (vector)|displacement]] vector if '''p''' and '''q''' represent two positions of the same point at two successive instants of time.\u000a\u000aThe Euclidean distance between '''p''' and '''q''' is just the Euclidean length of this distance (or displacement) vector:\u000a{{NumBlk|:|<math>\u005c|\u005cmathbf{q} - \u005cmathbf{p}\u005c| = \u005csqrt{(\u005cmathbf{q}-\u005cmathbf{p})\u005ccdot(\u005cmathbf{q}-\u005cmathbf{p})}.</math>|{{EquationRef|2}}}}\u000a\u000awhich is equivalent to equation 1, and also to:\u000a\u000a:<math>\u005c|\u005cmathbf{q} - \u005cmathbf{p}\u005c| = \u005csqrt{\u005c|\u005cmathbf{p}\u005c|^2 + \u005c|\u005cmathbf{q}\u005c|^2 - 2\u005cmathbf{p}\u005ccdot\u005cmathbf{q}}.</math>\u000a\u000a===One dimension===\u000aIn one dimension, the distance between two points on the [[real line]] is the [[absolute value]] of their numerical difference.  Thus if ''x'' and ''y'' are two points on the real line, then the distance between them is given by:\u000a:<math>\u005csqrt{(x-y)^2} = |x-y|.</math>\u000a\u000aIn one dimension, there is a single homogeneous, translation-invariant [[Metric (mathematics)|metric]] (in other words, a distance that is induced by a [[Norm (mathematics)|norm]]), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.\u000a\u000a===Two dimensions===\u000aIn the [[Euclidean plane]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>) then the distance is given by\u000a\u000a:<math>\u005cmathrm{d}(\u005cmathbf{p},\u005cmathbf{q})=\u005csqrt{(p_1-q_1)^2 + (p_2-q_2)^2}.</math>\u000a\u000aThis is equivalent to the [[Pythagorean theorem]].\u000a\u000aAlternatively, it follows from ({{EquationRef|2}}) that if the [[polar coordinates]] of the point '''p''' are (''r''<sub>1</sub>,&nbsp;\u03b8<sub>1</sub>) and those of '''q''' are (''r''<sub>2</sub>,&nbsp;\u03b8<sub>2</sub>), then the distance between the points is\u000a\u000a:<math>\u005csqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \u005ccos(\u005ctheta_1 - \u005ctheta_2)}.</math>\u000a\u000a===Three dimensions===\u000aIn three-dimensional Euclidean space, the distance  is\u000a\u000a:<math>d(p, q) = \u005csqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+(p_3 - q_3)^2}.</math>\u000a\u000a===''n'' dimensions <!-- This is a lower-case italicized "n" for a reason. -->===\u000aIn general, for an ''n''-dimensional space, the distance is\u000a\u000a:<math>d(p, q) = \u005csqrt{(p_1- q_1)^2 + (p_2 - q_2)^2+\u005ccdots+(p_i - q_i)^2+\u005ccdots+(p_n - q_n)^2}.</math>\u000a\u000a===Squared Euclidean distance===\u000aThe standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes\u000a\u000a:<math>d^2(p, q) = (p_1 - q_1)^2 + (p_2 - q_2)^2+\u005ccdots+(p_i - q_i)^2+\u005ccdots+(p_n - q_n)^2.</math>\u000a\u000aSquared Euclidean Distance is not a metric as it does not satisfy the [[triangle inequality]], however it is frequently used in optimization problems in which distances only have to be compared.\u000a\u000aIt is also referred to as [[rational trigonometry#Quadrance|quadrance]] within the field of [[rational trigonometry]].\u000a\u000a==See also==\u000a*[[Chebyshev distance]] measures distance assuming only the most significant dimension is relevant.\u000a*[[Euclidean distance matrix]]\u000a*[[Hamming distance]] identifies the difference bit by bit of two strings\u000a*[[Mahalanobis distance]] normalizes based on a covariance matrix to make the distance metric scale-invariant.\u000a*[[Manhattan distance]] measures distance following only axis-aligned directions.\u000a*[[Metric (mathematics)|Metric]]\u000a*[[Minkowski distance]] is a generalization that unifies Euclidean distance, Manhattan distance, and Chebyshev distance.\u000a*[[Pythagorean addition]]\u000a\u000a==References==\u000a* {{cite book |first=Elena |last=Deza |first2=Michel Marie |last2=Deza |year=2009 |title=Encyclopedia of Distances |page=94 |publisher=Springer }}\u000a* {{cite web |url=http://www.statsoft.com/textbook/cluster-analysis/ |title=Cluster analysis |date=March 2, 2011 }}\u000a\u000a{{DEFAULTSORT:Euclidean Distance}}\u000a[[Category:Metric geometry]]\u000a[[Category:Length]]\u000a[[Category:String similarity measures]]
p129
sg4
S'205'
p130
sg6
VEuclidean distance
p131
ssI80
(dp132
g2
V{{context|date=June 2012}}\u000aThe '''Binary Independence Model''' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.\u000a\u000a==Definitions==\u000aThe Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\u000aThe representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x<sub>1</sub>, ..., x<sub>m</sub>)'' where ''x<sub>t</sub>=1'' if term ''t'' is present in the document ''d'' and ''x<sub>t</sub>=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\u000a"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.\u000a\u000aThe probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:\u000a\u000a<math>P(R|x,q) = \u005cfrac{P(x|R,q)*P(R|q)}{P(x|q)}</math>\u000a\u000awhere ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.\u000aThe exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.\u000a\u000a''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\u000aSince a document is either relevant or nonrelevant to a query we have that:\u000a\u000a<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>\u000a\u000a=== Query Terms Weighting ===\u000aGiven a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the\u000aterms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \u005cfrac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight\u000aof term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.\u000a\u000aThe Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Spärck Jones.<ref name="jones77"/>\u000a\u000a== See also ==\u000a\u000a* [[Bag of words model]]\u000a\u000a==Further reading==\u000a* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | coauthors=Prabhakar Raghavan & Hinrich Schütze | publisher=Cambridge University Press | year=2008}}\u000a* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | coauthors=Charles L. A. Clarke & Gordon V. Cormack | publisher=MIT Press | year=2010}}\u000a\u000a==References==\u000a{{Reflist|refs=\u000a<ref name="cyu76">{{cite doi | 10.1145/321921.321930 }}</ref>\u000a<ref name="jones77">{{cite doi | 10.1002/asi.4630270302 }}</ref> \u000a}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]
p133
sg4
S'80'
p134
sg6
VBinary Independence Model
p135
ssI210
(dp136
g2
VIn [[information theory]] and [[computer science]], the '''Damerau\u2013Levenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir I. Levenshtein]]<ref>{{cite conference |last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286\u2013293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf}}</ref><ref name="bard"/><ref>{{cite conference |last1=Li |last2=et al. |year=2006|title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1025\u20131032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf}}</ref>) is a [[Metric (mathematics)|distance]] ([[string metric]]) between two [[string (computer science)|strings]], i.e., finite sequence of symbols, given by counting the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a [[transposition (mathematics)|transposition]] of two '''adjacent''' characters.  In his seminal paper,<ref>{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |publisher=ACM |volume=7 |issue=3 |pages=171\u2013176 |date=March 1964 |doi=10.1145/363958.363994}}</ref> Damerau not only distinguished these four edit operations but also stated that they correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation.\u000a\u000aThe Damerau\u2013Levenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations. The classical Levenshtein distance only allows insertion, deletion, and substitution operations.<ref>{{citation |url= <!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf--> |first=Vladimir I. |last=Levenshtein |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |date=February 1966 |volume=10 |issue=8 |pages=707&ndash;710}}</ref> Modifying this distance by including transpositions of adjacent symbols produces a different distance measure, known as the Damerau\u2013Levenshtein distance.<ref name="bard">{{citation\u000a | last = Bard | first = Gregory V.\u000a | contribution = Spelling-error tolerant, order-independent pass-phrases via the Damerau\u2013Levenshtein string-edit distance metric\u000a | isbn = 1-920-68285-6\u000a | location = Darlinghurst, Australia\u000a | pages = 117\u2013124\u000a | publisher = Australian Computer Society, Inc.\u000a | series = Conferences in Research and Practice in Information Technology\u000a | title = Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007\u000a | url = http://dl.acm.org/citation.cfm?id=1274531.1274545\u000a | volume = 68\u000a | year = 2007}}. The isbn produces two hits: a 2007 work and a 2010 work at World Cat.</ref>\u000a\u000aWhile the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, Damerau\u2013Levenshtein distance has also seen uses in biology to measure the variation between [[DNA]].<ref>The method used in: {{Citation\u000a| last1  = Majorek         | first1 = Karolina A.\u000a| last2  = Dunin-Horkawicz | first2 = Stanis\u0142aw\u000a| last3  = Steczkiewicz    | first3 = Kamil\u000a| last4  = Muszewska       | first4 = Anna\u000a| last5  = Nowotny         | first5 = Marcin\u000a| last6  = Ginalski        | first6 = Krzysztof\u000a| last7  = Bujnicki        | first7 = Janusz M.\u000a| display-authors = 2\u000a| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification\u000a| journal = Nucleic Acids Research\u000a| volume  = 42\u000a| issue   = 7\u000a| pages   = 4160\u20134179\u000a| year    = 2013\u000a| doi     = 10.1093/nar/gkt1414\u000a| url     = http://nar.oxfordjournals.org/content/42/7/4160.full\u000a}}</ref>\u000a\u000a== Definition ==\u000aThe Damerau\u2013Levenshtein distance between two strings <math>a</math> and <math>b</math> is given by <math>d_{a,b}(|a|,|b|)</math> where:\u000a\u000a<math>\u005cqquad d_{a,b}(i,j) = \u005cbegin{cases}\u000a  \u005cmax(i,j) & \u005ctext{ if} \u005cmin(i,j)=0, \u005c\u005c\u000a\u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} \u005c\u005c\u000a          d_{a,b}(i-2,j-2) + 1 \u000a       \u005cend{cases} & \u005ctext{ if } i,j > 1 \u005ctext{ and } a_i = b_{j-1} \u005ctext{ and } a_{i-1} = b_j \u005c\u005c\u000a  \u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)}\u000a       \u005cend{cases} & \u005ctext{ otherwise.}\u000a\u005cend{cases}</math>\u000a\u000awhere  <math>1_{(a_i \u005cneq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\u000a\u000aEach recursive call matches one of the cases covered by the Damerau\u2013Levenshtein distance:\u000a* <math>d_{a,b}(i-1,j) + 1</math> corresponds to a deletion (from a to b).\u000a* <math>d_{a,b}(i,j-1) + 1</math> corresponds to an insertion (from a to b).\u000a* <math>d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} </math> corresponds to a match or mismatch, depending on whether the respective symbols are the same.\u000a* <math>d_{a,b}(i-2,j-2) + 1 </math> corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.\u000a\u000a== Algorithm ==\u000aPresented here are two algorithms: the first,<ref>{{cite paper | author1 = B. J. Oommen | author2 = R. K. S. Loke | title = Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions | id = {{citeseerx|10.1.1.50.1459}} | doi=10.1016/S0031-3203(96)00101-X }}</ref> simpler one, computes what is known as the [[optimal string alignment]]{{Citation needed|date=May 2013}} (sometimes called the ''restricted edit distance''{{Citation needed|date=May 2013}}), while the second one<ref name="LW75">{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=JACM |volume=22 |issue=2 |pages=177\u2013183 |date=April 1975 |doi=10.1145/321879.321880}}</ref> computes the Damerau\u2013Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.\u000a\u000aTake for example the edit distance between '''CA''' and '''ABC'''. The Damerau\u2013Levenshtein distance LD('''CA''','''ABC''') = 2 because '''CA''' \u2192 '''AC''' \u2192 '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA''' \u2192 '''AC''' is used, it is not possible to use '''AC''' \u2192 '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA''' \u2192 '''A''' \u2192 '''AB''' \u2192 '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') < OSA('''CA''','''ABC'''), and so it is not a true metric.\u000a \u000a===Optimal string alignment distance===\u000aFirstly, let us consider a direct extension of the formula used to calculate [[Levenshtein distance]].  Below is [[pseudocode]] for a function ''OptimalStringAlignmentDistance'' that takes two strings, ''str1'' of length ''lenStr1'', and ''str2'' of length ''lenStr2'', and computes the optimal string alignment distance between them:\u000a\u000a<syntaxhighlight lang="pascal">\u000a int OptimalStringAlignmentDistance(char str1[1..lenStr1], char str2[1..lenStr2])\u000a    // d is a table with lenStr1+1 rows and lenStr2+1 columns\u000a    declare int d[0..lenStr1, 0..lenStr2]\u000a\u000a    // i and j are used to iterate over str1 and str2\u000a    declare int i, j, cost\u000a\u000a    // for loop is inclusive, need table 1 row/column larger than string length\u000a    for i from 0 to lenStr1\u000a        d[i, 0] := i\u000a    for j from 1 to lenStr2\u000a        d[0, j] := j\u000a\u000a    // pseudo-code assumes string indices start at 1, not 0\u000a    // if implemented, make sure to start comparing at 1st letter of strings\u000a    for i from 1 to lenStr1\u000a        for j from 1 to lenStr2\u000a            if str1[i] = str2[j] then cost := 0\u000a                                 else cost := 1\u000a            d[i, j] := minimum(\u000a                                 d[i-1, j  ] + 1,     // deletion\u000a                                 d[i  , j-1] + 1,     // insertion\u000a                                 d[i-1, j-1] + cost   // substitution\u000a                             )\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )                        \u000a  \u000a    return d[lenStr1, lenStr2]\u000a</syntaxhighlight>\u000a\u000aBasically this is the algorithm to compute [[Levenshtein distance]] with one additional recurrence:\u000a\u000a<syntaxhighlight lang="pascal">\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )\u000a</syntaxhighlight>\u000a\u000a===Distance with adjacent transpositions===\u000aHere is the second algorithm that computes the true Damerau\u2013Levenshtein distance with adjacent transpositions (ActionScript 3.0); this function requires as an additional parameter the size of the alphabet (''C''), so that all entries of the arrays are in 0..(''C''&minus;1):\u000a\u000a<syntaxhighlight lang='actionscript3'>static public function damerauLevenshteinDistance(a:Array, b:Array, C:uint):uint\u000a{\u000a    // "infinite" distance is just the max possible distance\u000a    var INF:uint = a.length + b.length;\u000a\u000a    // make and initialize the character array indices            \u000a    var DA:Array = new Array(C);\u000a    for (var k:uint = 0; k < C; ++k) DA[k]=0;\u000a\u000a    // make the distance matrix H[-1..a.length][-1..b.length]\u000a    var H:matrix = new matrix(a.length+2,b.length+2);\u000a    \u000a    // initialize the left and top edges of H\u000a    H[-1][-1] = INF;\u000a    for (var i:uint = 0; i <= a.length; ++i)\u000a    {\u000a        H[i][-1] = INF;\u000a        H[i][ 0] = i;\u000a    }\u000a    for (var j:uint = 0; j <= b.length; ++j)\u000a    {\u000a        H[-1][j] = INF;\u000a        H[ 0][j] = j;\u000a    }\u000a\u000a    // fill in the distance matrix H\u000a    // look at each character in a\u000a    for (var i:uint = 1; i <= a.length; ++i)\u000a    {\u000a        var DB:uint = 0;\u000a        // look at each character in b\u000a        for (var j:uint = 1; j <= b.length; ++j)\u000a        {\u000a            var i1:uint = DA[b[j-1]];\u000a            var j1:uint = DB;\u000a            var cost:uint;\u000a            if (a[i-1] == b[j-1])\u000a               {\u000a                 cost = 0;\u000a                 DB   = j;\u000a               }\u000a            else\u000a               cost = 1;\u000a            H[i][j] = Math.min(    H[i-1 ][j-1 ] + cost,  // substitution\u000a                                   H[i   ][j-1 ] + 1,     // insertion\u000a                                   H[i-1 ][j   ] + 1,     // deletion\u000a                                   H[i1-1][j1-1] + (i-i1-1) + 1 + (j-j1-1));\u000a        }\u000a        DA[a[i-1]] = i;\u000a    }\u000a    return H[a.length][b.length];\u000a}\u000a</syntaxhighlight>\u000a\u000a:<small>'''Note''': the algorithm given in the paper uses alphabet 1..C rather than the 0..''C''&minus;1 used here; the paper indexes arrays: H[&minus;1..|A|,&minus;1..|B|] and DA[1..C]; here DA[0..C&minus;1] is used; the paper seems to be missing the necessary line H[&minus;1,&minus;1]&nbsp;=&nbsp;INF</small>\u000a\u000aTo devise a proper algorithm to calculate unrestricted Damerau\u2013Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, <math>W_T</math>, is at least the average of the cost of an insertion and deletion, i.e., <math>2W_T \u005cge W_I+W_D</math>.<ref name="LW75"/>) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: <math>O\u005cleft (M \u005ccdot N \u005ccdot \u005cmax(M, N) \u005cright )</math>, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,<ref name="LW75"/> this naive algorithm can be improved to be <math>O\u005cleft (M \u005ccdot N \u005cright)</math> in the worst case.\u000a\u000aIt is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.\u000a\u000a== Applications ==\u000aDamerau\u2013Levenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke{{ref|OO}} even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.\u000a\u000a=== DNA ===\u000aSince [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau\u2013Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[Needleman\u2013Wunsch algorithm]] or [[Smith\u2013Waterman algorithm]].\u000a\u000a=== Fraud detection ===\u000aThe algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as "Rich Heir Estate Services" versus a false vendor "Rich Hier State Services". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau\u2013Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.\u000a\u000a== See also ==\u000a* [[Approximate string matching]]\u000a* [[Levenshtein automata]]\u000a* [[Typosquatting]]\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a== Further reading ==\u000a* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31\u201388 |date=March 2001 |doi=10.1145/375360.375365 }}\u000a\u000a{{DEFAULTSORT:Damerau-Levenshtein Distance}}\u000a[[Category:String similarity measures]]\u000a[[Category:Information theory]]\u000a[[Category:Dynamic programming]]
p137
sg4
S'210'
p138
sg6
VDamerau\u2013Levenshtein distance
p139
ssI85
(dp140
g2
VThere are several categories of search engine software:  Web search or full-text search (example: [[Lucene]]), database or structured data search (example: [[Dieselpoint]]), and mixed or [[enterprise search]] (example: [[Google Search Appliance]]).  The largest web search engines such as [[Google]] and [[Yahoo!]] utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:\u000a\u000aSearching for text-based content in [[databases]] or other [[structured data]] formats ([[XML]], [[Comma-separated values|CSV]], etc.) presents some special challenges and opportunities which a number of specialized search engines resolve.  Databases are slow when solving complex queries (with multiple logical or [[string matching]] arguments.  Databases allow logical queries which full-text search doesn't (use of multi-field boolean logic for instance).  There is no crawling necessary for a database since the data is already structured but it is often necessary to index the data in a more compact form designed to allow for faster search.\u000a\u000aDatabase search engines were initially (and still usually are) included with major database software products.  As such, they are usually called indexing engines.  However, these indexing engines are relatively limited in their ability to customize indexing formats (compounding, normalization, transformation, [[transliteration]], etc.)   Usually they do not provide sophisticated data matching technology ([[string matching]], [[boolean logic]], algorithmic methods, search scripting, etc.).\u000a\u000aIn more advanced Database search systems relational databases are indexed by compounding multiple tables into a single table containing only the fields that need to be queried (or displayed in search results).  The actual data matching engines can include any functions from basic string matching, normalization, transformation,  Database search technology is heavily used by government database services, e-commerce companies, web advertising platforms, telecommunications service providers, etc.\u000a\u000a==See also==\u000a\u000a*[[Search engine]]\u000a*[[Web crawler]]\u000a*[[Search engine indexing]]\u000a*[[Enterprise search]]\u000a\u000a==External links==\u000a* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\u000a\u000a{{DEFAULTSORT:Search Engine Technology}}\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p141
sg4
S'85'
p142
sg6
VDatabase search engine
p143
ssI215
(dp144
g2
VThe '''Simple Matching Coefficient (SMC)''' is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets.<ref>http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sdaugherty/similarity.html</ref>\u000a\u000aGiven two objects, A and B, each with n binary attributes, SMC is defined as:\u000a:<math> SMC = {\u005ctext{Number of Matching Attributes}\u005cover \u005ctext{Number of Attributes}} = {{M_{00}+M_{11}}\u005cover{M_{00}+M_{01}+M_{10}+M_{11}}}</Math>\u000a\u000aWhere:\u000a:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.\u000a:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.\u000a:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.\u000a:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.\u000a\u000a== See also ==\u000a* [[Jaccard index]]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a[[Category:Index numbers]]\u000a[[Category:Measure theory]]\u000a[[Category:Clustering criteria]]\u000a[[Category:String similarity measures]]
p145
sg4
S'215'
p146
sg6
VSimple matching coefficient
p147
ssI90
(dp148
g2
V{{Citations missing|date=June 2008}}\u000a\u000a'''Federated search''' is an [[information retrieval]] technology that allows the simultaneous search of multiple searchable resources.  A user makes a single query request which is distributed to the [[search engine]]s participating in the federation.  The federated search then aggregates the results that are received from the [[search engine]]s for presentation to the user.\u000a\u000a==Purpose==\u000aFederated search came about to meet the need of searching multiple  disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user.\u000a\u000a==Process==\u000aAs described by Peter Jacso (2004<ref>Thoughts About Federated Searching.  Jacsó, Péter, Information Today,  Oct 2004, Vol. 21, Issue 9</ref>), federated searching consists of (1) transforming a [[Web search query|query]] and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set.\u000a\u000aFederated search portals, either commercial or open access, generally search public access [[bibliographic databases]], public access Web-based library catalogues ([[OPAC]]s), Web-based search engines like [[Google]] and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely [[screen scrape]] the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources.\u000a\u000aThis process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time.\u000a\u000a==Implementation==\u000a[[File:Fed_search.png|thumb|alt=federated search engine|Federating across three search engines]]\u000a\u000aOne application of federated searching is the [[metasearch engine]]; however, this is not a complete solution as many documents are not currently indexed. These documents are on what is known as the [[deep Web]], or invisible Web. Many more information sources are not yet stored in electronic form. [[Google Scholar]] is one example of many projects trying to address this.\u000a\u000aWhen the search vocabulary or [[data model]] of the search system is different from the data model of one or more of the foreign target systems the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require [[semantic translation]].\u000a\u000aA challenge faced in the implementation of federated search engines is scalability, in other words, the performance of the site as the number of information sources comprising the federated search engine increase. One federated search engine that has begun to address this issue is [[WorldWideScience]], hosted by the [[U.S. Department of Energy]]'s [[Office of Scientific and Technical Information]].  WorldWideScience <ref>[http://www.worldwidescience.org WorldWideScience]</ref> is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov <ref name="Science.gov">[http://www.science.gov Science.gov]</ref> which itself federates more than 30 information sources representing most of the R&D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience.<ref name="Science.gov"/> This approach of cascaded federated search enables large number of information sources to be searched via a single query.\u000a\u000aAnother application [[Sesam]] running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat,<ref>[http://sesat.no Sesat]</ref> an acronym for [[Sesam Search Application Toolkit]], is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning.\u000a\u000a==Challenges==\u000a\u000aWhen federated search is performed against secure data sources, the users' credentials must be passed on\u000ato each underlying search engine, so that appropriate security is maintained.  If the user has different\u000alogin credentials for different systems, there must be a means to map their login ID to each search\u000aengine's security domain.<ref>[http://www.ideaeng.com/tabId/98/itemId/124/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search]</ref>\u000a\u000aAnother challenge is mapping results list navigators into a common form.  Suppose 3 real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges.<ref>[http://www.ideaeng.com/tabId/98/itemId/154/20-Differences-Between-Internet-vs-Enterprise-Se.aspx#fed_facets 20+ Differences Between Internet vs. Enterprise Search - part 1]</ref>  The system also needs to understand "next page" links if it's going to allow the user to page through the combined results.\u000a\u000a==Further reading==\u000a*[http://www.libraryjournal.com/article/CA6571320.html Federated Search 101. Linoski, Alexis, Walczyk, Tine, Library Journal, Summer 2008 Net Connect, Vol. 133]{{Dead link|date=November 2010}} Note: this content has been moved [http://www.accessmylibrary.com/article-1G1-182034526/federated-search-101-alexis.html here], but you will need a remote access account through your local library to get the whole article.\u000a\u000a*Cox, Christopher N. Federated Search: Solution or Setback for Online Library Services. Binghamton, NY: Haworth Information Press, 2007.[http://lccn.loc.gov/2006101753 Table of Contents]\u000a*[http://www.altsearchengines.com/2009/01/11/federated-search-finds-content-that-google-cant-reach-part-i-of-iii/ Federated Search Primer. Lederman, S., AltSearchEngines, January 2009] {{Dead link|date=July 2010}} Note: This material has been reposted [http://deepwebtechblog.com/federated-search-finds-content-that-google-can%E2%80%99t-reach-part-i-of-iii/ here], on the blog of a commercial search engine company.\u000a\u000a* Milad Shokouhi and Luo Si, Federated Search, Foundations and Trends® in Information Retrieval: Vol. 5: No 1, pp 1-102., [http://dx.doi.org/10.1561/1500000010 http://dx.doi.org/10.1561/1500000010]\u000a\u000a==See also==\u000a* [[Search aggregator]]\u000a* [[Deep Web]]\u000a\u000a==References==\u000a{{Reflist}}\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Federated Search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]\u000a[[Category:Searching]]\u000a[[Category:Internet search algorithms]]\u000a[[Category:Applications of distributed computing]]
p149
sg4
S'90'
p150
sg6
VFederated search
p151
ssI220
(dp152
g2
V{{Use dmy dates|date=August 2013}}\u000a{{other uses}}\u000a{{infobox bibliographic database\u000a| title = Scopus\u000a| image = [[File:Scopus_type_logo.jpg]]\u000a| caption = Scopus logo\u000a| producer = [[Elsevier]]\u000a| country = \u000a| history = \u000a| languages = English\u000a| providers = \u000a| cost = Subscription\u000a| disciplines= \u000a| depth = \u000a| formats = \u000a| temporal = 1995-present\u000a| geospatial = Worldwide\u000a| number = 55 million\u000a| updates = \u000a| p_title = \u000a| p_dates = \u000a| ISSN = \u000a| web = http://www.scopus.com\u000a| titles = \u000a}}\u000a'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092\u20136 |pmid=19738094}}</ref>\u000a\u000aSince Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.\u000a\u000aA 2008 study  compared [[PubMed]], Scopus, [[Web of Science]], and [[Google Scholar]] and concluded: <blockquote>"PubMed and Google Scholar are accessed for free [...] Scopus offers about 20% more coverage than Web of Science, whereas Google Scholar offers results of inconsistent accuracy. PubMed remains an optimal tool in biomedical electronic research. Scopus covers a wider journal range [...] but it is currently limited to recent articles (published after 1995) compared with Web of Science. Google Scholar, as for the Web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information."<ref>{{Cite journal |pmid=17884971 |year=2008 |last1=Falagas |first1=ME |last2=Pitsouni |first2=EI |last3=Malietzis |first3=GA |last4=Pappas |first4=G |title=Comparison of PubMed, Scopus, Web of Science, and Google Scholar: Strengths and weaknesses |volume=22 |issue=2 |pages=338\u201342 |doi=10.1096/fj.07-9492LSF |journal=[[FASEB Journal]]}}</ref></blockquote>\u000a\u000aEvaluating ease of use and coverage of Scopus and the Web of Science (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. [...] The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive. [...]".<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>\u000a\u000aScopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].\u000a\u000aScopus can be integrated with [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>\u000a\u000a== References ==\u000a{{reflist|30em}}\u000a\u000a== External links ==\u000a* {{Official website|http://www.scopus.com/}}\u000a* [http://www.elsevier.com/online-tools/scopus Scopus information]\u000a\u000a\u000a{{Reed Elsevier}}\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Elsevier]]\u000a[[Category:Citation indices]]\u000a[[Category:Library cataloging and classification]]\u000a[[Category:Scholarly search services]]
p153
sg4
S'220'
p154
sg6
VScopus
p155
ssI95
(dp156
g2
V{{refimprove|date=September 2014}}\u000a\u000a'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.  CLIR techniques can be classified into different categories based on different translation resources: \u000a* Dictionary-based CLIR techniques\u000a* Parallel corpora based CLIR techniques\u000a* Comparable corpora based CLIR techniques\u000a* Machine translator based CLIR techniques\u000a\u000aThe first workshop on CLIR was held in Zürich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).\u000a\u000aThe term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.\u000a\u000a==See also==\u000a*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)\u000a\u000a==References==\u000a<references />\u000a\u000a==External links==\u000a*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]\u000a\u000a{{DEFAULTSORT:Cross-Language Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a\u000a\u000a{{linguistics-stub}}
p157
sg4
S'95'
p158
sg6
VCross-language information retrieval
p159
ssI225
(dp160
g2
VA '''citation index''' is a kind of [[bibliographic database]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]].\u000a\u000a==History==\u000a\u000aThe earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study.  The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998,  p. 51''ff''</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.\u000a\u000aIn English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name='shapiro'/>\u000a\u000aThe first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].<ref name='shapiro'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)</ref>\u000a\u000a==Major citation indexing services==\u000a{{main|Indexing and abstracting service}}\u000a{{main cat|Citation indices}}\u000a\u000aGeneral-purpose academic citation indexes include:\u000a\u000a*ISI (now part of [[Thomson Reuters]]) publishes the ISI citation indexes in print and [[compact disc]]. They are now generally accessed through the Web under the name '' [[Web of Science]]'', which is in turn part of the group of databases in the ''[[Web of Knowledge]].''\u000a*[[Elsevier]] publishes [[Scopus]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].\u000a*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.\u000aEach of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: the ISI databases and Scopus are available by subscription (generally to libraries).\u000a\u000aIn addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.\u000a\u000a==Citation analysis==\u000a{{main|Citation analysis}}\u000a{{merge|section=yes|Citation analysis|date=December 2013}}\u000a{{duplication|dupe=Citation analysis|date=December 2013}}\u000a\u000aWhile citation indexes were originally designed for [[information retrieval]], they are increasingly used for [[bibliometrics]] and other studies involving research evaluation. Citation data is also the basis of the popular [[journal impact factor]].\u000a\u000aThere is a large body of literature on [[citation analysis]], sometimes called [[scientometrics]], a term invented by [[Vasily Nalimov]], or more specifically [[bibliometrics]]. The field blossomed with the advent of the [[Science Citation Index]], which now covers source literature from 1900 on. The leading journals of the field are ''[[Scientometrics]],'' ''Informetrics,'' and the ''[[Journal of the American Society of Information Science and Technology]]''. [[American Society for Information Science and Technology|ASIST]] also hosts an [[electronic mailing list]] called SIGMETRICS at  ASIST.<ref>{{cite web | title=The American Society for Information Science & Technology | work=The Information Society for the Information Age | url=http://www.asis.org| accessdate=2006-05-21}}</ref> This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as  [[CiteBase]],  [[CiteSeerX]], [[Google Scholar]], and the former [[Windows Live Academic]] (now available with extra features as [[Microsoft Academic Search]]).\u000a\u000a[[Legal citation]] analysis is a citation analysis technique for analyzing [[legal documents]] to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a [[citation graph]] extracted from a regulatory document, which could supplement [[E-discovery]] - a process that leverages on technological innovations in [[big data analytics]].<ref>[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=5070630&tag=1 ]{{dead link|date=December 2013}}</ref><ref>Mohammad Hamdaqa and A. Hamou-Lhadj, "Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents",  In Proc. of the 6th International Conference on Information Technology, Las Vegas, USA</ref><ref name=BD-HB-R-01>{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez</ref><ref name=BD-HB-R-02>{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology - Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}</ref>\u000a\u000a===History===\u000aIn a 1965 paper, [[Derek J. de Solla Price]] described the inherent linking characteristic of the SCI as "Networks of Scientific Papers".<ref>{{cite journal | author=Derek J. de Solla Price | title=Networks of Scientific Papers | journal=[[Science (journal)|SCIENCE]] | date=July 30, 1965 | volume=149 | issue=3683| pages=510&ndash;515 | url=http://garfield.library.upenn.edu/papers/pricenetworks1965.pdf | pmid=14325149 | doi=10.1126/science.149.3683.510|format=PDF}}</ref> The links between citing and cited papers became dynamic when the SCI began to be published online. The [[Social Sciences Citation Index]] became one of the first databases to be mounted on the [[Dialog]] system<ref>{{cite web | title=Dialog, A Thomson Business | work="Dialog invented online information services" | url=http://www.dialog.com| accessdate=2006-05-21}}</ref> in 1972. With the advent of the [[CD-ROM]] edition, linking became even easier and enabled the use of [[bibliographic coupling]] for finding related records. In 1973, Henry Small published his classic work on [[Co-Citation analysis]] which became a [[self-organizing]] classification system that led to [[document clustering]] experiments and eventually an "Atlas of Science" later called "Research Reviews".\u000a\u000aThe inherent topological and graphical nature of the worldwide citation network which is an inherent property of the [[scientific literature]] was described by [[Ralph Garner]] ([[Drexel University]]) in 1965.<ref>http://www.garfield.library.upenn.edu/rgarner.pdf</ref>\u000a\u000aThe use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts  to rank authors and [[academic paper|papers]].  In a landmark paper of 1965 he and [[Irving Sher]] showed the correlation between citation frequency and eminence in demonstrating that [[Nobel Prize]] winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon.  The usual summary measure is known as [[impact factor]], the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposes\u2014in particular, the use of this measure alone for  ranking authors and papers is therefore [[Impact factor#|quite controversial.]]\u000a\u000aIn an early study in 1964 of the use of Citation Analysis in writing the history of [[DNA]], Garfield and Sher demonstrated the potential for generating [[historiograph]]s, [[topological map]]s of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, [[A. I. Pudovkin]] of the [[Institute of Marine Biology]], [[Russian Academy of Sciences]] and [[V. S. Istomin]] of [[Center for Teaching, Learning, and Technology]], [[Washington State University]] and led to the creation of the [[Histcite|HistCite]] <ref>{{cite web | author=Eugene Garfield, A. I. Pudovkin,  V. S. Istomin | year=2002 | title=Algorithmic Citation-Linked Historiography\u2014Mapping the Literature of Science | work=Presented the ASIS&T 2002: Information, Connections and Community. 65th Annual Meeting of ASIST in Philadelphia, PA. November 18\u201321, 2002  | url=http://www.garfield.library.upenn.edu/papers/asis2002/asis2002presentation.html | accessdate=2006-05-21}}</ref> software around 2002.\u000a\u000aAutomatic citation indexing was introduced in 1998 by [[Lee Giles]], [[Steve Lawrence]] and [[Kurt Bollacker]] <ref>C.L. Giles, K. Bollacker, S. Lawrence, "CiteSeer: An Automatic Citation Indexing System," DL'98 Digital Libraries, 3rd ACM Conference on Digital Libraries, pp. 89-98, 1998.</ref> and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being [[CiteSeer]] (now [[CiteSeerX]], soon followed by Cora, which focused primarily on the field of [[computer science]] and [[information science]]. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as [[Ann Arbor, Michigan|Ann Arbor]], [[Milton Keynes]], and [[Walton Hall, Milton Keynes|Walton Hall]] being credited with extensive academic output.<ref name="pmid18354457">{{cite journal |author=Postellon DC |title=Hall and Keynes join Arbor in the citation indexes |journal=[[Nature (journal)|Nature]] |volume=452 |issue=7185 |page=282 |date=March 2008 |pmid=18354457 |doi=10.1038/452282b}}</ref>  SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.\u000a\u000a==See also==\u000a* [[Impact factor]]\u000a* [[Citation impact]]\u000a* [[Eigenfactor]]\u000a* [[Microsoft Academic Search]]\u000a* [[Google Scholar]]\u000a* [[Scopus]]\u000a* [[H-index]] or [[Hirsch number]]\u000a* [[Citation analysis]]\u000a* [[Acknowledgment index]]\u000a* [[CiteSeer]]\u000a* [[CiteSeerX]]\u000a* [[Scientific journal]]\u000a* [[Science Citation Index]]\u000a* [[Indian Citation Index]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links ==\u000a* Official [http://admin-apps.isiknowledge.com/JCR/JCR Journal Citation Report] from the [http://www.isinet.com ISI website]\u000a* [http://www.librijournal.org/2005-4toc.html Google Scholar: The New Generation of Citation Indexes]\u000a* [http://www.atlasofscience.net/ Atlas of Science: Mapping Science by means of citation relations]\u000a* [http://www.dlib.org/dlib/september05/bauer/09bauer.html An Examination of Citation Counts in a New Scholarly Communication Environment]\u000a* [http://cids.fc.ul.pt/ CIDS] online tool that calculates the h-index and [[g-index]] based on [[Google Scholar]] data and discerning self-citations\u000a\u000a{{DEFAULTSORT:Citation Index}}\u000a[[Category:Academic publishing]]\u000a[[Category:Bibliometrics]]\u000a[[Category:Library science]]\u000a[[Category:Reputation management]]\u000a[[Category:Citation indices]]
p161
sg4
S'225'
p162
sg6
VCitation index
p163
ssI100
(dp164
g2
V'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].  \u000a\u000a== Overview ==\u000a\u000aIn a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref> \u000a\u000aLegal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change. \u000a\u000aTechniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.\u000a\u000a== Problems ==\u000a\u000aApplication of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>\u000a\u000aLegal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref> \u000a\u000a#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.\u000a#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;\u000a#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;\u000a#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;\u000a\u000aIn addition, it also has the common meaning: \u000a<ol start="5">\u000a<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>\u000a</ol>\u000a\u000aThough the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. \u000a\u000aEven if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.\u000a\u000aAdditionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />\u000a\u000aOvercoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>\u000a\u000a== Techniques ==\u000a\u000a===Boolean searches===\u000a\u000a[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above. \u000a\u000aThe recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>\u000a\u000a===Manual classification===\u000a\u000aIn order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s \u201cNatural Language\u201d<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers<ref name=WL /> or Lexis' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>\u000a\u000aThese systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>\u000a\u000aThe major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>\u000a\u000a===Natural language processing===\u000a\u000aIn order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, \u201cSMILE,\u201d which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>\u000a\u000aDespite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>\u000a\u000a== Notes ==\u000a{{Reflist|2}}\u000a\u000a==References==\u000a{{Refbegin}}\u000a*{{cite journal\u000a|author     = Maxwell, K.T., and Schafer, B.\u000a|year       = 2008\u000a|title      = Concept and Context in Legal Information Retrieval\u000a|url        = http://portal.acm.org/citation.cfm?id=1564016\u000a|journal    = Frontiers in Artificial Intelligence and Applications\u000a|volume     = 189\u000a|pages      = 63\u201372\u000a|publisher  = IOS Press\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Jackson, P. et al.\u000a|year       = 1998\u000a|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation\u000a|url        = http://portal.acm.org/citation.cfm?id=288627.288642\u000a|journal    = Conference on Information and Knowledge Management\u000a|pages      = 60\u201367\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Blair, D.C., and Maron, M.E.\u000a|year       = 1985\u000a|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval\u000a|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997\u000a|journal    = Communications of the ACM\u000a|volume     = 28\u000a|issue      = 3 \u000a|pages      = 289\u2013299\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a|doi=10.1145/3166.3197\u000a}}\u000a*{{cite journal\u000a|author     = Peters, W. et al.\u000a|year       = 2007\u000a|title      = The structuring of legal knowledge in LOIS\u000a|url        = http://www.springerlink.com/content/d04l7h2507700g45/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 15\u000a|issue      = 2\u000a|pages      = 117\u2013135\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-007-9034-4\u000a}}\u000a*{{cite journal\u000a|author     = Saravanan, M. et al.\u000a|year       = 2007\u000a|title      = Improving legal information retrieval using an ontological framework \u000a|url        = http://www.springerlink.com/content/h66412k08h855626/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 17\u000a|issue      = 2\u000a|pages      = 101\u2013124\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-009-9075-y\u000a}}\u000a*{{cite journal\u000a|author     = Schweighofer, E. and Liebwald, D.\u000a|year       = 2007\u000a|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary\u000a|url        = http://www.springerlink.com/content/v62v7131x10413v0/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 15\u000a|issue      = 2\u000a|pages      = 103\u2013115\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-007-9029-1\u000a}}\u000a*{{cite journal\u000a|author     = Gelbart, D. and Smith, J.C.\u000a|year       = 1993\u000a|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management\u000a|url        = http://portal.acm.org/citation.cfm?id=158994\u000a|journal    = International Conference on Artificial Intelligence and Law\u000a|pages      = 142\u2013151\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Ashley, K.D. and Bruninghaus, S.\u000a|year       = 2009\u000a|title      = Automatically classifying case texts and predicting outcomes\u000a|url        = http://www.springerlink.com/content/lhg8837331hgu024/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 17\u000a|issue      = 2\u000a|pages      = 125\u2013165\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-009-9077-9\u000a}}\u000a{{Refend}}\u000a\u000a{{DEFAULTSORT:Legal Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Legal research]]
p165
sg4
S'100'
p166
sg6
VLegal information retrieval
p167
ssI230
(dp168
g2
V{{Merge from|Web of Knowledge|date=December 2014|discuss=Talk:Web of Science#Merge}}\u000a{{Infobox Bibliographic Database\u000a|title = Web of Science\u000a|image = [[File:Web of science next generation.png|thumb|350px|Web of Science]]\u000a|caption = \u000a|producer = Thomson Reuters \u000a|country = United States \u000a|history = \u000a|languages = \u000a|providers = Various institutions and commercial organizations \u000a|cost = \u000a|disciplines = Science, social science, arts, humanities (supports 256 disciplines) \u000a|depth = citation indexing,  author, topic title, subject keywords, abstract,  periodical title, author's address, publication year \u000a|formats = full text articles, reviews, editorials, chronologies, abstracts, proceedings (journals and book-based ), technical papers \u000a|temporal = 1900 to present \u000a|geospatial = \u000a|number = 90 million + \u000a|updates = \u000a|p_title = \u000a|p_dates = \u000a|ISSN =\u000a|web =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science \u000a|titles =http://thomsonreuters.com/content/science/pdf/Web_of_Science_factsheet.pdf\u000a}}\u000a'''Web of Science''' (WoS, previously known as [[Web of Knowledge]]) is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] that provides a comprehensive citation search. It gives access to multiple databases that reference cross-disciplinary research, which allows for in-depth exploration of specialized sub-fields within an [[academic discipline|academic or scientific discipline]].<ref>Drake, Miriam A. Encyclopedia of Library and Information Science. New York, N.Y.: Marcel Dekker, 2004.</ref>\u000a\u000a==Background==\u000aA citation index is built on the fact that citations in science serve as linkages between similar research items, and lead to matching or related scientific literature, such as [[academic journal|journal articles]], [[conference proceedings]], abstracts, etc. In addition, literature which shows the greatest impact in a particular field, or more than one discipline, can be easily located through a citation index. For example, a paper's influence can be determined by linking to all the papers that have cited it. In this way, current trends, patterns, and emerging fields of research can be assessed. [[Eugene Garfield]], the "father of citation indexing of academic literature,"<ref>Jacso, Peter. The impact of Eugene Garfield through the prizm of Web of Science. Annals of Library and Information Studies, Vol. 57, September 2010, P. 222. [http://nopr.niscair.res.in/bitstream/123456789/10235/4/ALIS%2057%283%29%20222-247.pdf PDF]</ref> who launched the [[Science Citation Index]] (SCI), which in turn led to the Web of Science,<ref>Garfield, Eugene, Blaise Cronin, and Helen Barsky Atkins. The Web of Knowledge: A Festschrift in Honor of Eugene Garfield. Medford, N.J.: Information Today, 2000.</ref> wrote: \u000a\u000a{{Quote|Citations are the formal, explicit linkages between papers that have particular points in common. A citation index is built around these linkages. It lists publications that have been cited and identifies the sources of the citations. Anyone conducting a literature search can find from one to dozens of additional papers on a subject just by knowing one that has been cited. And every paper that is found provides a list of new citations with which to continue the search.\u000aThe simplicity of citation indexing is one of its main strengths. <ref>Garfield, Garfield, Eugene. Citation indexing: Its theory and application in science, technology, and humanities. New York: Wiley, 1979, P. 1. [http://garfield.library.upenn.edu/ci/chapter1.PDF PDF]</ref>}}\u000a\u000a==Coverage==\u000a[[File:Web_of_Science_Core_Collection.png|thumb|200px|Accessing the Web of Science via the [[Web of Knowledge]]]]\u000aExpanding the coverage of Web of Science, in November 2009 Thomson Reuters introduced ''Century of Social Sciences''. This service contains files which trace social science research back to the beginning of the 20th century,<ref name=InfoToNov2009>"''Thomson Reuters introduces century of social sciences''". Information Today 26.10 (2009): 10. General OneFile. Web. 23 June 2010.  [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211794482&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0  Document URL].</ref><ref name=ComLibNov2009>Thomson Reuters introduces century of social sciences." Computers in Libraries 29.10 (2009): 47. General OneFile. Internet. 23 June 2010. [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211236981&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0 Document URL]</ref> and Web of Science now has indexing coverage from the year 1900 to the present.<ref name=oview>\u000a{{Cite web |title =Overview - Web of Science| publisher =Thomson Reuters| year = 2010\u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\u000a  | format =Overview of coverage gleaned from promotional language.  \u000a  | accessdate =2010-06-23}}</ref><ref name=UoOL>\u000a{{Cite web| last = Lee| first =Sul H.| title =Citation Indexing and ISI's Web of Science \u000a  | publisher =The University of Oklahoma Libraries| year =2010\u000a  | url =http://www.ou.edu/webhelp/librarydemos/isi/ | format =Discussion of finding literature manually. Description of [[citation index]]ing, and Web of Science.| accessdate =2010-06-23}}</ref> The multidisciplinary coverage of the Web of Science encompasses over 50,000 scholarly books, 12,000 journals and 160,000 conference proceedings<ref name="Web of Science">[http://wokinfo.com/citationconnection/realfacts/#regional Web of Science. Thomson Reuters, 2014]</ref> (as of September 3, 2014). The selection is made on the basis of [[impact factor|impact evaluations]] and comprise [[open-access journal]]s, spanning multiple [[academic discipline]]s. The coverage includes: the [[science]]s, [[social science]]s, [[the arts|arts]], and humanities, and goes across disciplines.<ref name=oview/><ref name=facts/> However, Web of Science does not index all journals, and its coverage in some fields is less complete than in others.\u000a\u000aFurthermore, as of September 3, 2014 the total file count of the Web of Science was 90 million records, which included over a billion cited references. This citation service on average indexes around 65 million items per year, and it is described as the largest accessible citation database.<ref name=facts>[http://wokinfo.com/citationconnection/  Bulleted fact sheet]. Thomson Reuters. 2014.</ref>\u000a\u000aTitles of foreign-language publications are translated into English and so cannot be found by searches in the original language.<ref name=harvard-search>{{Cite web\u000a  |title =Some Searching Conventions\u000a  | publisher =President and Fellows of Harvard College   | date = December 3, 2009   | url =http://hcl.harvard.edu/research/guides/citationindex/#some   | format =    | accessdate =2010-06-23}}</ref>\u000a\u000a==Citation databases==\u000aWeb of Science consist of seven online databases:<ref name=included/><ref name="Web of Science">[http://wokinfo.com/media/pdf/WoSFS_08_7050.pdf Jo Yong-Hak. Web of Science. Thomson Reuters, 2013]</ref>\u000a*[[Conference Proceedings Citation Index]] covers more than 160,000 conference titles in the Sciences starting from 1990 to the present day\u000a*[[Science Citation Index Expanded]] covers more than 8,500 notable journals encompassing 150 disciplines. Coverage is from the year 1900 to the present day.\u000a*[[Social Sciences Citation Index]] covers more than 3,000 journals in social science disciplines.  Range of coverage is from the year 1900 to the present day.\u000a*[[Arts & Humanities Citation Index]] covers more than  1,700 arts and humanities journals starting from 1975. In addition, 250 major scientific and social sciences journals are also covered. \u000a*[[Index Chemicus]] lists more than 2.6 million compounds. The time of coverage is from 1993 to present day.\u000a*[[Current Chemical Reactions]] indexes over one million reactions, and the range of coverage is from 1986 to present day. The '' INPI '' archives from 1840 to 1985 are also indexed in this database.\u000a*[[Book Citation Index]] covers more than 60,000 editorially selected books starting from 2005.\u000a\u000a=== Contents ===\u000aThe seven [[citation index|citation indices]] listed above contain references which have been cited by other articles. One may use them to undertake cited reference search, that is, locating articles that cite an earlier, or current publication. One may search citation databases by topic, by author, by source title, and by location. Two chemistry databases,  ''Index Chemicus'' and  ''Current Chemical Reactions'' allow for the creation of structure drawings, thus enabling users to locate [[chemical compound]]s and reactions. Institutions such as universities and research departments generally access the Web of Science through the [[Web of Knowledge]] platform. (An example of a typical search.<ref>[http://cires.colorado.edu/~jjose/P-Cited/DeCarlo06_ISI.pdf A typical Web of Science search example.]</ref>)\u000a\u000a===Abstracting and indexing===\u000aThe following  types of literature are indexed: scholarly books, [[peer review]]ed journals, original research articles, reviews, editorials, chronologies, abstracts, as well as other items. Disciplines included in this index are  [[agriculture]], [[biological sciences]], [[engineering]], medical and [[life sciences]], [[physics|physical]] and [[chemical sciences]], [[anthropology]], law, [[library science]]s, [[architecture]], dance, music, film, and theater. Seven citation databases encompasses coverage of the above disciplines.<ref name=UoOL/><ref name=included>\u000a{{Cite web |title =Coverage - Web of Science| publisher =Thomson Reuters| year = 2010\u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\u000a  | format =Overview of coverage gleaned from promotional language.  \u000a  | accessdate =2010-06-23}}</ref><ref name="Web of Science" />\u000a\u000a==Limitations in the use of citation analysis==\u000aAs with other scientific approaches, scientometrics and bibliometrics have their own limitations. Recently, a criticism was voiced pointing toward certain deficiencies of the journal impact factor (JIF) calculation process, based on Thomson Reuters Web of Science, such as: journal citation distributions usually are highly skewed towards established journals; journal impact factor properties are field-specific and can be easily manipulated by editors, or even by changing the editorial policies; this makes the entire process essentially nontransparent.<ref name="Declaration">[http://am.ascb.org/dora/ San Francisco Declaration on Research Assessment: Putting science into the assessment of research, December 16, 2012]</ref>\u000a\u000aRegarding the more objective journal metrics, there is a growing view that for greater accuracy it must be supplemented with an article-based assessment and peer-review.<ref name="Declaration" /> Thomson Reuters replied to criticism in general terms by stating that "no one metric can fully capture the complex contributions scholars make to their disciplines, and many forms of scholarly achievement should be considered."<ref>Thomson Reuters Statement Regarding the San Francisco Declaration on Research Assessment [http://researchanalytics.thomsonreuters.com/]</ref>\u000a\u000a== See also ==\u000a\u000a{{Div col|3}}\u000a*[[List of academic journal search engines]]\u000a*[[CSA (database company)|CSA databases]]\u000a*[[Dialog (online database)]]\u000a*[[Energy Citations Database]]\u000a*[[Energy Science and Technology Database]]\u000a*[[ETDEWEB]]\u000a*[[Geographic Names Information System]]\u000a*[[Materials Science Citation Index]]\u000a*[[PASCAL (database)|PASCAL database]]\u000a* [[PubMed Central]]\u000a* [[SciELO]]\u000a* [[VINITI Database RAS]]\u000a* [[Web development tools]]\u000a{{Div col end}}\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* [http://scientific.thomson.com/products/wos/ Web of Science]\u000a* [http://www.webofknowledge.com/ Web of Knowledge]\u000a* [http://web.archive.org/web/20110521161422/http://hcl.harvard.edu/research/guides/citationindex/ Searching the Citation Indexes (Web of Science)] Harvard College Library. 2010. (archive)\u000a* [http://video.mit.edu/watch/web-of-science-12339/ MIT Web of Science video tutorial]. 2008.\u000a\u000a{{Thomson Reuters}}\u000a{{DEFAULTSORT:Web Of Science}}\u000a[[Category:Bibliographic databases]]\u000a[[Category:Full text scholarly online databases]]\u000a[[Category:Thomson family]]\u000a[[Category:Thomson Reuters]]\u000a[[Category:Citation indices]]\u000a[[Category:Scholarly search services]]
p169
sg4
S'230'
p170
sg6
VWeb of Science
p171
ssI105
(dp172
g2
VThe '''query likelihood model''' is a [[language model]] used in [[Information Retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.\u000a\u000a==Calculating the likelihood==\u000aUsing [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:\u000a\u000a:<math>\u000a P(d|q) = \u005cfrac{P(q|d) P(d)}{P(q)}\u000a</math>\u000a\u000aSince the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.\u000a\u000a:<math>\u000a P(d|q) = P(q|d)\u000a</math>\u000a\u000aDocuments are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:\u000a:<math>\u000a P(q|M_d) = K_q \u005cprod_{t \u005cin V} P(t|M_d)^{tf_{t,q}}\u000a</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tM,q}!)</math> for query {{math|q}}.\u000a\u000aIn practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.\u000a\u000a<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>\u000a\u000a==References==\u000a <references/>\u000a\u000a[[Category:Information retrieval]]
p173
sg4
S'105'
p174
sg6
VQuery likelihood model
p175
ssI235
(dp176
g2
V{{Refimprove|date=April 2010}}\u000a{{Infobox Software|\u000a|name                   = SensMe\u000a|logo                   = [[File:SensMelogo.png|center|64px|SensMe Logo]]\u000a|screenshot             = [[File:SensMe.jpg|200px|center]]\u000a|caption                = Screenshot of SensMe on Media Go\u000a|developer              = [[Sony Corporation]]\u000a|genre                  = Mood detection software for music data.\u000a|platform               = [[Sony Walkman]] MP3/4 players<br />[[Sony Ericsson]]<br />[[Media Go]]<br />[[PlayStation Portable]]\u000a|license                = [[Proprietary software|Proprietary]]\u000a|website                = [http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w910i?cc=gb&lc=en http://www.sonyericsson.com]\u000a}}\u000a\u000a'''SensMe''' is a [[Proprietary software|proprietary]] music mood and tempo detection system created by [[Sony|Sony Corporation]], and employed in numerous Sony branded products, most notably the [[Walkman]] MP3/MP4 players (E <ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6052&NewsAreaId=2 Your cool, colourful music partner Feature-packed WALKMAN® E450 Video MP3 player from Sony with premium sound for young music fans (15 July 2010 )]</ref> and S series<ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6203&NewsAreaId=2 Sony introduces super-slim WALKMAN® S750 (15 September 2010)]</ref>), [[Media Go]], [[PlayStation Portable]], and [[Sony Ericsson]] phone series, .\u000a\u000a==Technical specifications==\u000a\u000a''SensMe'' works by mapping music to a dual axis map based on the mood and tempo of music tracks.<ref>What is SensMe? http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w980</ref> Mood and tempo is determined by using the appropriate Sony compatible software which analyzes music tracks individually and computes the relevant track information. Analyzed tracks can then be plotted onto an intuitive dual axis map through which the music library on the device can be navigated, and playlists can be generated based on relative speed and mood. The horizontal axis is based on mood and the vertical axis is based on [[tempo]].\u000a\u000a==PlayStation Portable==\u000a\u000aSensMe was made available on the PlayStation Portable as of system software version 6.10.<ref name="update610" /> It can be downloaded via the [[XrossMediaBar|XMB]] or by using a computer.<ref name="pspdownload">SensMe PSP Download http://www.playstation.com/psp-app/sensme/en/</ref> The application features twelve channels by which music is categorized. These include Favorites, Newly Added, Dance, Extreme, Lounge, Emotional, Mellow, Upbeat, Relax, Energetic, Morning/Day/Night/Midnight, and Shuffle All.\u000a\u000a===PlayStation Portable Version History===\u000a{| class="wikitable"\u000a!width="180"|Version<br> Release date (UTC)\u000a!class="unsortable"|Description\u000a|-\u000a|align=center|'''1.50'''<br>March 31, 2010\u000a|\u000a* Music tracks transferred using a PlayStation 3 system or music management application other than Media Go are now also categorized into channels.\u000a* Users can now add music tracks to a block list so they do not play.\u000a* Users can now activate or deactivate the [Dynamic Normalizer] feature.\u000a|-\u000a|align=center|'''1.01'''<br>October 22, 2009\u000a|\u000a* Descriptions of some menu items in some languages have been revised.\u000a|-\u000a|align=center|'''1.00'''<br>October 1, 2009\u000a|\u000a* Initial release.\u000a|}\u000a\u000a==SensMe compatible products==\u000a* [[Walkman]]\u000a* [[Media Go]]\u000a* [[PlayStation Portable]]<ref name="update610">PSP Firmware Update (v6.10) http://blog.us.playstation.com/2009/09/psp-firmware-update-v6-10/</ref>\u000a\u000a[[File:Sony Ericsson W760i running SensMe.JPG|thumb|right|Screenshot of SensMe on a Sony Ericsson]]\u000a\u000a===Sony Ericsson handsets===\u000a*''[[Sony Ericsson Aino|Aino]]''\u000a*''[http://www.sony.co.uk/product/nws-s-series/nwz-s639f Sony NWZ-S639F Media Player]''\u000a*''[[Sony Ericsson Elm|elm]]''\u000a*''[[Sony Ericsson W380|W380]]''\u000a*''[[Sony Ericsson W518a|W508]]''\u000a*''[[Sony Ericsson W518a|W518a]]''\u000a*''[[Sony Ericsson W595|W595]]''\u000a*''[[Sony Ericsson W705|W705]]''\u000a*''[[Sony Ericsson W705|W715]]''\u000a*''[[Sony Ericsson W760|W760]]''\u000a*''[[Sony Ericsson W890i|W890i]]''\u000a*''[[Sony Ericsson W902|W902]]''\u000a*''[[Sony Ericsson W910|W910i]]''\u000a*''[[Sony Ericsson W980|W980]]''\u000a*''[[Sony Ericsson W995|W995]]''\u000a*''[[Sony Ericsson Xperia X10|Xperia X10]]''\u000a*''[[Sony Ericsson Xperia Neo|Xperia Neo]]''\u000a*''[[Sony Ericsson Xperia Play|Xperia Play]]''\u000a*''[[Sony Ericsson Xperia ray|Xperia Ray]]''\u000a===Sony handsets===\u000a*''[[Sony Xperia E|Xperia E]]''\u000a*''[[Sony Xperia M|Xperia M]]''\u000a*''[[Sony Xperia Sola|Xperia Sola]]''\u000a* [[Sony_Xperia_L|Xperia L]]\u000a*''[[Sony Xperia S|Xperia S]]''\u000a*''[[Sony Xperia P|Xperia P]]''\u000a*''[[Sony Xperia U|Xperia U]]''\u000a*''[[Sony Xperia T|Xperia T]]''\u000a*''[[Sony Xperia TX|Xperia TX]]''\u000a*''[[Sony Xperia TL|Xperia TL]]''\u000a*''[[Sony Xperia tipo|Xperia tipo]]''\u000a*''[[Sony Xperia T|Xperia Go]]''\u000a*''[[Sony Xperia V|Xperia V]]''\u000a*''[[Sony Xperia Z|Xperia Z]]''\u000a*''[[Sony Xperia Z1|Xperia Z1]]''\u000a*''[[Sony Xperia Z1 Compact|Xperia Z1 Compact]]''\u000a*''[[Sony Xperia Z Ultra|Xperia Z Ultra]]''\u000a*''[[Sony Xperia Z1f|Xperia Z1f/Z1s]]''\u000a*''[[Sony Xperia ZL|Xperia ZL]]''\u000a*''[[Sony Xperia ZL|Xperia SP]]''\u000a*''[[Sony Xperia Z2|Xperia Z2]]''\u000a*''[[Sony Xperia Z3|Xperia Z3]]''\u000a*''[[Sony Xperia Z3 Compact|Xperia Z3 Compact]]''\u000a*''[http://www.sonyericsson.com/cws/products/mobilephones/overview/zylo?cc=ph&lc=en#view=features_specifications Zylo (W20i)]''\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Sony software]]\u000a[[Category:Music search engines]]
p177
sg4
S'235'
p178
sg6
VSensMe
p179
ssI110
(dp180
g2
VThe '''International Society for Music Information Retrieval''' ('''ISMIR''') is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11 ISSN: 1082-9873.]</ref> which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>\u000a\u000a==Purpose==\u000aGiven the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.\u000a\u000aAs the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science and many others.\u000a\u000a==Annual Conference==\u000aSince its inception in 2000, ISMIR has been the world\u2019s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.\u000a* [http://ismir2012.ismir.net ISMIR 2012], 8\u201312 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2012'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2011.ismir.net ISMIR 2011], 24\u201328 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2011'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2010.ismir.net ISMIR 2010], 9\u201313 August 2010, Utrecht (Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2010'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2009.ismir.net ISMIR 2009], 26\u201330 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2009'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2008.ismir.net ISMIR 2008], 14\u201318 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2008'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2007.ismir.net ISMIR 2007], 23\u201330 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2007'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2006.ismir.net ISMIR 2006], 8\u201312 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2006'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2005.ismir.net ISMIR 2005], 11\u201315 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2005'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2004.ismir.net ISMIR 2004], 10\u201315 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2004'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2003.ismir.net ISMIR 2003], 26\u201330 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2003'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2002.ismir.net ISMIR 2002], 13\u201317 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2002'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2001.ismir.net ISMIR 2001], 15\u201317 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2001'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2000.ismir.net ISMIR 2000], 23\u201325 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='200'&page=0&order=Authors&order_type=ASC proceedings]\u000a\u000aThe [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences.\u000a\u000a==MIREX==\u000aThe Music Information Retrieval Evaluation eXchange (MIREX)<ref>[http://www.music-ir.org/mirex MIREX Wiki]</ref> is an annual evaluation campaign for Music Information Retrieval (MIR) algorithms, coupled to the ISMIR conference.\u000a\u000aMIR tasks evaluated at past MIREXs include:\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Train/Test_Tasks Audio Train/Test Tasks]\u000a**Audio Artist Identification\u000a**Audio Genre Classification\u000a**Audio Music Mood Classification\u000a**Audio Classical Composer Identification\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Genre_Classification Symbolic Genre Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Onset_Detection Audio Onset Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Key_Detection Audio Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Key_Detection Symbolic Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tag_Classification Audio Tag Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Cover_Song_Identification Audio Cover Song Identification]\u000a*[http://www.music-ir.org/mirex/wiki/Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following) Real-time Audio to Score Alignment (a.k.a Score Following)]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Singing/Humming Query by Singing/Humming]\u000a*[http://www.music-ir.org/mirex/wiki/Multiple_Fundamental_Frequency_Estimation_&_Tracking Multiple Fundamental Frequency Estimation & Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Chord_Estimation Audio Chord Estimation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Melody_Extraction Audio Melody Extraction]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Tapping Query by Tapping]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Beat_Tracking Audio Beat Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Music_Similarity_and_Retrieval Audio Music Similarity and Retrieval]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Melodic_Similarity Symbolic Melodic Similarity]\u000a*[http://www.music-ir.org/mirex/wiki/Structural_Segmentation Structural Segmentation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Drum_Detection Audio Drum Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tempo_Extraction Audio Tempo Extraction]\u000a\u000a==See also==\u000a* [[International Conference on Digital Audio Effects]]\u000a* [[Music Technology]]\u000a* [[Sound and music computing|Sound and Music Computing]]\u000a* [[Sound and Music Computing Conference]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a[[Category:Music technology]]\u000a[[Category:Multimedia]]\u000a[[Category:Information retrieval]]
p181
sg4
S'110'
p182
sg6
VInternational Society for Music Information Retrieval
p183
ssI115
(dp184
g2
V{{mergefrom|Latent semantic indexing|date=July 2012}}\u000a{{semantics}}\u000a'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular in [[vectorial semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188}}</ref>\u000a\u000aAn information retrieval method using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853]) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>\u000a\u000a== Overview ==\u000a\u000a=== Occurrence matrix ===\u000aLSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency\u2013inverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\u000a\u000aThis matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.\u000a\u000a=== Rank lowering ===\u000aAfter the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:\u000a\u000a* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").\u000a* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).\u000a* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document\u2014generally a much larger set due to [[synonymy]].\u000a\u000aThe consequence of the rank lowering is that some dimensions are combined and depend on more than one term:\u000a\u000a:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}\u000a\u000aThis mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.\u000a\u000a=== Derivation ===\u000aLet <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:\u000a\u000a:<math>\u000a\u005cbegin{matrix} \u000a & \u005ctextbf{d}_j \u005c\u005c\u000a & \u005cdownarrow \u005c\u005c\u000a\u005ctextbf{t}_i^T \u005crightarrow &\u000a\u005cbegin{bmatrix} \u000ax_{1,1} & \u005cdots & x_{1,n} \u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000ax_{m,1} & \u005cdots & x_{m,n} \u005c\u005c\u000a\u005cend{bmatrix}\u000a\u005cend{matrix}\u000a</math>\u000a\u000aNow a row in this matrix will be a vector corresponding to a term, giving its relation to each document:\u000a\u000a:<math>\u005ctextbf{t}_i^T = \u005cbegin{bmatrix} x_{i,1} & \u005cdots & x_{i,n} \u005cend{bmatrix}</math>\u000a\u000aLikewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:\u000a\u000a:<math>\u005ctextbf{d}_j = \u005cbegin{bmatrix} x_{1,j} \u005c\u005c \u005cvdots \u005c\u005c x_{m,j} \u005cend{bmatrix}</math>\u000a\u000aNow the [[dot product]] <math>\u005ctextbf{t}_i^T \u005ctextbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\u005ctextbf{t}_i^T \u005ctextbf{t}_p</math> (<math> = \u005ctextbf{t}_p^T \u005ctextbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\u005ctextbf{d}_j^T \u005ctextbf{d}_q = \u005ctextbf{d}_q^T \u005ctextbf{d}_j</math>.\u000a\u000aNow, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\u005cSigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):\u000a\u000a:<math>\u000a\u005cbegin{matrix}\u000aX = U \u005cSigma V^T\u000a\u005cend{matrix}\u000a</math>\u000a\u000aThe matrix products giving us the term and document correlations then become\u000a\u000a:<math>\u000a\u005cbegin{matrix}\u000aX X^T &=& (U \u005cSigma V^T) (U \u005cSigma V^T)^T = (U \u005cSigma V^T) (V^{T^T} \u005cSigma^T U^T) = U \u005cSigma V^T V \u005cSigma^T U^T = U \u005cSigma \u005cSigma^T U^T \u005c\u005c\u000aX^T X &=& (U \u005cSigma V^T)^T (U \u005cSigma V^T) = (V^{T^T} \u005cSigma^T U^T) (U \u005cSigma V^T) = V \u005cSigma^T U^T U \u005cSigma V^T = V \u005cSigma^T \u005cSigma V^T\u000a\u005cend{matrix}\u000a</math>\u000a\u000aSince <math>\u005cSigma \u005cSigma^T</math> and <math>\u005cSigma^T \u005cSigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\u005cSigma \u005cSigma^T</math>, or equally, by the non-zero entries of <math>\u005cSigma^T\u005cSigma</math>. Now the decomposition looks like this:\u000a\u000a:<math>\u000a\u005cbegin{matrix} \u000a & X & & & U & & \u005cSigma & & V^T \u005c\u005c\u000a & (\u005ctextbf{d}_j) & & & & & & & (\u005chat{\u005ctextbf{d}}_j) \u005c\u005c\u000a & \u005cdownarrow & & & & & & & \u005cdownarrow \u005c\u005c\u000a(\u005ctextbf{t}_i^T) \u005crightarrow \u000a&\u000a\u005cbegin{bmatrix} \u000ax_{1,1} & \u005cdots & x_{1,n} \u005c\u005c\u000a\u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000a\u005c\u005c\u000ax_{m,1} & \u005cdots & x_{m,n} \u005c\u005c\u000a\u005cend{bmatrix}\u000a&\u000a=\u000a&\u000a(\u005chat{\u005ctextbf{t}}_i^T) \u005crightarrow\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005cbegin{bmatrix} \u005c, \u005c\u005c \u005c, \u005c\u005c \u005ctextbf{u}_1 \u005c\u005c \u005c, \u005c\u005c \u005c,\u005cend{bmatrix} \u000a\u005cdots\u000a\u005cbegin{bmatrix} \u005c, \u005c\u005c \u005c, \u005c\u005c \u005ctextbf{u}_l \u005c\u005c \u005c, \u005c\u005c \u005c, \u005cend{bmatrix}\u000a\u005cend{bmatrix}\u000a&\u000a\u005ccdot\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005csigma_1 & \u005cdots & 0 \u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000a0 & \u005cdots & \u005csigma_l \u005c\u005c\u000a\u005cend{bmatrix}\u000a&\u000a\u005ccdot\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005cbegin{bmatrix} & & \u005ctextbf{v}_1 & & \u005cend{bmatrix} \u005c\u005c\u000a\u005cvdots \u005c\u005c\u000a\u005cbegin{bmatrix} & & \u005ctextbf{v}_l & & \u005cend{bmatrix}\u000a\u005cend{bmatrix}\u000a\u005cend{matrix}\u000a</math>\u000a\u000aThe values <math>\u005csigma_1, \u005cdots, \u005csigma_l</math> are called the singular values, and <math>u_1, \u005cdots, u_l</math> and <math>v_1, \u005cdots, v_l</math> the left and right singular vectors.\u000aNotice the only part of <math>U</math> that contributes to <math>\u005ctextbf{t}_i</math> is the <math>i\u005ctextrm{'th}</math> row.\u000aLet this row vector be called <math>\u005chat{\u005ctextrm{t}}_i</math>.\u000aLikewise, the only part of <math>V^T</math> that contributes to <math>\u005ctextbf{d}_j</math> is the <math>j\u005ctextrm{'th}</math> column, <math>\u005chat{ \u005ctextrm{d}}_j</math>.\u000aThese are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.\u000a\u000aIt turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The vector <math>\u005chat{\u005ctextbf{t}}_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the vector <math>\u005chat{\u005ctextbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as\u000a\u000a:<math>X_k = U_k \u005cSigma_k V_k^T</math>\u000a\u000aYou can now do the following:\u000a* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\u005cSigma_k \u005chat{\u005ctextbf{d}}_j </math> and <math>\u005cSigma_k \u005chat{\u005ctextbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).\u000a* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\u005cSigma_k \u005chat{\u005ctextbf{t}}_i^T </math> and <math>\u005cSigma_k \u005chat{\u005ctextbf{t}}_p^T </math>.\u000a* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.\u000a* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.\u000a\u000aTo do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:\u000a\u000a:<math>\u005chat{\u005ctextbf{d}}_j = \u005cSigma_k^{-1} U_k^T \u005ctextbf{d}_j</math>\u000a\u000aNote here that the inverse of the diagonal matrix <math>\u005cSigma_k</math> may be found by inverting each nonzero value within the matrix.\u000a\u000aThis means that if you have a query vector <math>q</math>, you must do the translation <math>\u005chat{\u005ctextbf{q}} = \u005cSigma_k^{-1} U_k^T \u005ctextbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:\u000a\u000a:<math>\u005ctextbf{t}_i^T = \u005chat{\u005ctextbf{t}}_i^T \u005cSigma_k V_k^T</math>\u000a\u000a:<math>\u005chat{\u005ctextbf{t}}_i^T = \u005ctextbf{t}_i^T V_k^{-T} \u005cSigma_k^{-1} = \u005ctextbf{t}_i^T V_k \u005cSigma_k^{-1}</math>\u000a\u000a:<math>\u005chat{\u005ctextbf{t}}_i = \u005cSigma_k^{-1}  V_k^T \u005ctextbf{t}_i</math>\u000a\u000a== Applications ==\u000a\u000aThe new low-dimensional space typically can be used to:\u000a* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).\u000a* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).\u000a* Find relations between terms ([[synonymy]] and [[polysemy]]).\u000a* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).\u000a* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model | author=Alain Lifchitz, Sandra Jhean-Larose, Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201\u20131209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>\u000a\u000aSynonymy and polysemy are fundamental problems in [[natural language processing]]: \u000a* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.\u000a* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.\u000a\u000a=== Commercial applications ===\u000a\u000aLSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\u000a\u000a=== Applications in human memory ===\u000a\u000aThe use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall | author=Marc W. Howard and Michael J. Kahana |year=1999}}</ref>\u000a\u000aWhen participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb et al. | booktitle=Interspeech'2005|year=2006}}</ref>\u000a\u000aAnother model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=5/8/2011}}</ref>\u000a\u000a== Implementation ==\u000a\u000aThe [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis | author=Geneviève Gorrell and Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}</ref>\u000aA fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20\u201330 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.\u000aIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>doi: 10.1109/ICCSNT.2011.6182070</ref>\u000a\u000a== Limitations ==\u000aSome of LSA's drawbacks include:\u000a\u000a* The resulting dimensions might be difficult to interpret. For instance, in\u000a:: {(car), (truck), (flower)} \u21a6  {(1.3452 * car + 0.2828 * truck), (flower)}\u000a:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to\u000a:: {(car), (bottle), (flower)} \u21a6  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}\u000a:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.\u000a\u000a* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word){{Citation needed|date=October 2013}}.  Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).\u000a\u000a* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words.\u000a\u000a* To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>[http://www.translational-medicine.com/content/12/1/324 J Transl Med. 2014 Nov 27;12(1):324.]</ref>\u000a\u000a* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>\u000a\u000a== See also ==\u000a* [[Compound term processing]]\u000a* [[Explicit semantic analysis]]\u000a* [[Latent semantic mapping]]\u000a* [[Latent Semantic Structure Indexing]]\u000a* [[Principal components analysis]]\u000a* [[Probabilistic latent semantic analysis]]\u000a* [[Spamdexing]]\u000a* [[Topic model]]\u000a** [[Latent Dirichlet allocation]]\u000a* [[Vectorial semantics]]\u000a* [[Coh-Metrix]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a* {{cite journal\u000a | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\u000a |format=PDF| title=Introduction to Latent Semantic Analysis\u000a | author=[[Thomas Landauer]], Peter W. Foltz, & Darrell Laham\u000a | journal=Discourse Processes\u000a | volume=25\u000a | pages=259\u2013284\u000a |year=1998\u000a | doi=10.1080/01638539809545028\u000a | issue=2\u20133\u000a}}\u000a* {{cite journal\u000a | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf \u000a |format=PDF| title=Indexing by Latent Semantic Analysis\u000a | author=[[Scott Deerwester]], [[Susan Dumais|Susan T. Dumais]], [[George Furnas|George W. Furnas]], [[Thomas Landauer|Thomas K. Landauer]], [[Richard Harshman]]\u000a | journal=Journal of the American Society for Information Science\u000a | volume=41\u000a | issue=6\u000a | pages=391\u2013407\u000a | year=1990 \u000a | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\u000a}} Original article where the model was first exposed.\u000a* {{cite journal\u000a | url=http://citeseer.ist.psu.edu/berry95using.html\u000a | title=Using Linear Algebra for Intelligent Information Retrieval\u000a | author=Michael Berry, [[Susan Dumais|Susan T. Dumais]], Gavin W. O'Brien\u000a |year=1995\u000a}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.\u000a* {{cite web\u000a | url=http://iv.slis.indiana.edu/sw/lsa.html\u000a | title=Latent Semantic Analysis\u000a | publisher=InfoVis\u000a}}\u000a* {{cite web\u000a | url=http://cran.at.r-project.org/web/packages/lsa/index.html\u000a | title=An Open Source LSA Package for R\u000a | publisher=CRAN\u000a | author=Fridolin Wild\u000a | date=November 23, 2005\u000a | accessdate=2006-11-20\u000a}}\u000a* {{ cite web\u000a | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM\u000a | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge\u000a | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]\u000a | accessdate=2007-07-02\u000a}}\u000a\u000a==External links==\u000a\u000a===Articles on LSA===\u000a* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.\u000a\u000a===Talks and demonstrations===\u000a* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].\u000a* [http://www.semanticsearchart.com/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.\u000a\u000a===Implementations===\u000a\u000aDue to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.\u000a* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA\u000a* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA\u000a* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices\u000a* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)\u000a* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA\u000a* [[Gensim]] contains a fast, online Python implementation of LSA for matrices larger than RAM.\u000a\u000a{{DEFAULTSORT:Latent Semantic Analysis}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Latent variable models]]\u000a\u000a[[fa:\u0622\u0646\u0627\u0644\u06cc\u0632 \u067e\u0646\u0647\u0627\u0646 \u0645\u0641\u0647\u0648\u0645\u06cc \u0627\u062d\u062a\u0645\u0627\u0644\u06cc]]
p185
sg4
S'115'
p186
sg6
VLatent semantic analysis
p187
ssI120
(dp188
g2
V'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]].  \u000aHe is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.\u000a\u000aRadev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006\u2013present) and associate editor of [http://www.jair.org JAIR].\u000a\u000a== Awards ==\u000aAs [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].\u000a\u000a== IOL==\u000aRadev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].\u000a\u000a== Books ==\u000a* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>\u000a* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']\u000a\u000a== Selected Papers ==\u000a* SIGIR 1995 Generating summaries of multiple news articles\u000a* ANLP 1997 Building a generation knowledge source using internet-accessible newswire\u000a* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources\u000a* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities\u000a* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation\u000a* CIKM 2001 Mining the web for answers to natural language questions\u000a* AAAI 2002 Towards CST-enhanced summarization\u000a* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project\u000a* Information Processing and Management 2004 Centroid-based summarization of multiple documents\u000a* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization\u000a* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web\u000a* Communications of the ACM 2005 NewsInEssence: summarizing online news topics\u000a* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing\u000a* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network\u000a* IEEE Intelligent Systems 2008 natural language processing and the web\u000a* NAACL 2009 Generating surveys of scientific paradigms\u000a* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways\u000a* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts\u000a* KDD 2010 Divrank: the interplay of prestige and diversity in information networks\u000a* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs\u000a* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language\u000a* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology\u000a\u000a==External links==\u000a* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]\u000a* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]\u000a\u000a== References ==\u000a{{reflist}}\u000a<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\u000a*\u000a*\u000a*\u000a*\u000a\u000a{{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. -->\u000a| NAME              = Radev, Dragomir R.\u000a| ALTERNATIVE NAMES =\u000a| SHORT DESCRIPTION = American computer scientist\u000a| DATE OF BIRTH     =\u000a| PLACE OF BIRTH    =\u000a| DATE OF DEATH     =\u000a| PLACE OF DEATH    =\u000a}}\u000a\u000a{{DEFAULTSORT:Radev, Dragomir R.}}\u000a[[Category:Year of birth missing (living people)]]\u000a[[Category:Living people]]\u000a\u000a[[Category:Columbia University alumni]]\u000a[[Category:American computer scientists]]\u000a[[Category:University of Michigan faculty]]\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p189
sg4
S'120'
p190
sg6
VDragomir R. Radev
p191
ssI125
(dp192
g2
VThe [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183\u2013190|doi=10.1109/21.87068}}</ref>  have been widely used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decisin making, multi-criteria multi-expert decision making).<ref>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref><ref>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref> It is widely accepted that fuzzy sets<ref>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338\u2013353|doi=10.1016/S0019-9958(65)90241-X}}</ref> are more suitable for representing preferences of criteria in decision making. But fuzzy sets are not crisp values, how can we aggregate fuzzy sets in OWA mechanism? \u000a\u000aThe type-1 OWA operators<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281\u20133296|doi=10.1016/j.fss.2008.06.018}}</ref><ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455\u20131468|doi=10.1109/TKDE.2010.191}}</ref>  have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\u000a\u000aFirst, there are two definitions for type-1 OWA operators, one is based on Zadeh's Extension Principle, the other is based on <math>\u005calpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.\u000a\u000a==Definitions==\u000a\u000a'''Definition 1.<ref name="fssT1OWA" /> '''\u000aLet <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:\u000a\u000aGiven n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\u005cPhi</math>,\u000a\u000a:<math>\u005cPhi \u005ccolon F(X)\u005ctimes \u005ccdots \u005ctimes F(X)  \u005clongrightarrow  F(X)</math>\u000a:<math>(A^1 , \u005ccdots ,A^n)  \u005cmapsto   Y</math>\u000a\u000asuch that\u000a\u000a:<math>\u005cmu _{Y} (y) =\u005cdisplaystyle \u005csup_{\u005cdisplaystyle \u005csum_{k =1}^n \u005cbar {w}_i a_{\u005csigma (i)}  = y }\u005cleft({\u005cbegin{array}{*{1}l}\u005cmu _{W^1 } (w_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu_{W^n } (w_n )\u005cwedge \u005cmu _{A^1 } (a_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu _{A^n } (a_n )\u005cend{array}}\u005cright)</math>\u000a\u000awhere <math>\u005cbar {w}_i = \u005cfrac{w_i }{\u005csum_{i = 1}^n {w_i } }</math>,and <math>\u005csigma \u005ccolon \u005c{1, \u005ccdots ,n\u005c} \u005clongrightarrow \u005c{1, \u005ccdots ,n\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cgeq a_{\u005csigma (i + 1)},\u005c \u005cforall i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma(i)} </math> is the <math>i</math>th highest element in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a'''Definition 2.<ref name="kdeT1OWA" /> '''\u000a\u000aThe definition below is based on the alpha-cuts of fuzzy sets:\u000a\u000aGiven the n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, then for each <math>\u005calpha \u005cin [0,\u005c;1]</math>, an <math>\u005calpha </math>-level type-1 OWA operator with <math>\u005calpha </math>-level sets <math>\u005cleft\u005c{ {W_\u005calpha ^i } \u005cright\u005c}_{i = 1}^n </math> to aggregate the <math>\u005calpha </math>-cuts of fuzzy sets <math>\u005cleft\u005c{ {A^i} \u005cright\u005c}_{i =1}^n </math> is given as\u000a\u000a: <math>\u000a\u005cPhi_\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright) =\u005cleft\u005c{ {\u005cfrac{\u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} } }{\u005csum\u005climits_{i = 1}^n {w_i } }\u005cleft| {w_i \u005cin W_\u005calpha ^i ,\u005c;a_i } \u005cright. \u005cin A_\u005calpha ^i ,\u005c;i = 1, \u005cldots ,n} \u005cright\u005c}</math>\u000a\u000awhere  <math>W_\u005calpha ^i= \u005c{w| \u005cmu_{W_i }(w) \u005cgeq \u005calpha \u005c}, A_\u005calpha ^i=\u005c{ x| \u005cmu _{A_i }(x)\u005cgeq \u005calpha \u005c}</math>, and <math>\u005csigma :\u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c} \u005cto \u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cge a_{\u005csigma (i + 1)} ,\u005c;\u005cforall \u005c;i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma (i)} </math> is the <math>i</math>th largest\u000aelement in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a== Representation theorem of Type-1 OWA operators<ref name="kdeT1OWA" />==\u000a\u000aGiven the ''n'' linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, and the fuzzy sets <math>A^1, \u005ccdots ,A^n</math>, then we have that<ref name="kdeT1OWA" />\u000a:<math>Y=G</math>\u000a\u000awhere <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.\u000a\u000a==Programming problems for Type-1 OWA operators==\u000a\u000aAccording to the '''''Representation Theorem of Type-1 OWA Operators''''',a general type-1 OWA operator can be decomposed into a series of <math>\u005calpha</math>-level type-1 OWA operators. In practice, these series of  <math>\u005calpha</math>-level type-1 OWA operators are used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots\u000a,A_\u005calpha ^n } \u005cright)_\u005calpha } \u005calpha </math>\u000a\u000aFor the left end-points, we need to solve the following programming problem:\u000a:<math> \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)_{-} = \u005cmathop {\u005cmin }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i = 1}^n {w_i } } </math>\u000a\u000awhile for the right end-points, we need to solve the following programming problem:\u000a:<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots , A_\u005calpha ^n } \u005cright)_{+} = \u005cmathop {\u005cmax }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i  A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i =\u000a1}^n {w_i } } </math>\u000a\u000aA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.<ref name="kdeT1OWA" />\u000a\u000a== Alpha-level approach to Type-1 OWA operation<ref name="kdeT1OWA" />==\u000a* '''Step 1'''.To set up the <math>\u005calpha </math>- level resolution in [0, 1].\u000a* '''Step 2'''. For each <math>\u005calpha \u005cin [0,1]</math>,\u000a''Step 2.1.'' To calculate <math>\u005crho _{\u005calpha +} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha +} ^{i_0 } \u005cge A_{\u005calpha + }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha +} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.1-3''.\u000a# <math>i_0 \u005cleftarrow i_0 + 1</math>, go to ''Step 2.1-2''.\u000a\u000a''Step 2.2.'' To calculate<math>\u005crho _{\u005calpha -} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha -} ^{i_0 } \u005cge A_{\u005calpha - }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha -} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.2-3.''\u000a#<math>i_0 \u005cleftarrow i_0 + 1</math>, go to step ''Step 2.2-2.''\u000a\u000a'''Step 3.'''To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]</math>: \u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]} \u005calpha </math>\u000a\u000a==Special cases of Type-1 OWA operators==\u000a* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />\u000a* Join operators of (type-1) fuzzy sets,<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312\u201340|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199\u2013249|doi=10.1016/0020-0255(75)90036-5}}</ref> i.e., fuzzy maximum operators;\u000a* Meet operators of (type-1) fuzzy sets,<ref name="MT"/><ref name="zadehJ"/> i.e., fuzzy minimum operators;\u000a* Join-like operators of (type-1) fuzzy sets;<ref name="kdeT1OWA"/><ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91\u2013109|doi=10.1007/978-3-642-17910-5_5}}</ref>\u000a* Meet-like operators of (type-1) fuzzy sets.<ref name="kdeT1OWA"/><ref name="bookT1OWA"/>\u000a\u000a==Generalizations==\u000aType-2 OWA operators<ref>{{cite journal|last=Zhou|first=S.M.|coauthors=R. I. John, F. Chiclana and J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540\u2013558|doi=10.1002/int.20420}}</ref> have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Artificial intelligence]]\u000a[[Category:Logic in computer science]]\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]
p193
sg4
S'125'
p194
sg6
VType-1 OWA operators
p195
ss.
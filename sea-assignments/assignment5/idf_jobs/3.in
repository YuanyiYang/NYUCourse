(dp0
I128
(dp1
S'docBody'
p2
VIn [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build \u000aa textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. \u000aAs a result, the same ideas can be represented using a smaller set of words.\u000a\u000aSemantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document \u000acannot be reconstructed in a reverse process.\u000a\u000a==Semantic compression by generalization==\u000aSemantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:\u000a#	determining cumulated term frequencies to identify target lexicon,\u000a#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>\u000a\u000aStep 1 requires assembling word frequencies and \u000ainformation on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, \u000aa cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:\u000a<math>cum f(k_{i}) = f(k_{i}) + \u005csum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.\u000aThen, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.\u000a\u000aIn the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence \u000aof a less frequent hyponym as its hypernym in output text.\u000a\u000a;Example\u000a\u000aThe below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.\u000a\u000a<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' \u000ain very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects \u000a'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the \u000a'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of \u000a'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>\u000a\u000aThe procedure outputs the following text:\u000a\u000a<blockquote>They are both '''facility''' building '''insect''', but '''insect''' and honey '''insects''' '''arrange''' their '''biological groups''' \u000ain very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects \u000a'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the \u000a'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of \u000a'''organic process''', and there are '''impinging difference of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>\u000a\u000a==Implicit semantic compression==\u000aA natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)\u000a.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],\u000aCOLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>\u000a\u000a==Applications and advantages==\u000aIn the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less \u000a[[computational complexity]] and a positive influence on efficiency. \u000a\u000aSemantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity \u2013 limited language redundancy, a step towards a controlled dictionary).\u000a\u000aAs in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).\u000a\u000a==See also==\u000a* [[Text simplification]]\u000a* [[Lexical substitution]]\u000a* [[Information theory]]\u000a* [[Quantities of information]]\u000a\u000a==References==\u000a<references/>\u000a\u000a==External links==\u000a* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Quantitative linguistics]]\u000a[[Category:Computational linguistics]]
p3
sS'docID'
p4
S'128'
p5
sS'title'
p6
VSemantic compression
p7
ssI3
(dp8
g2
VIn research communities (for example, [[earth science]]s), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client\u2014server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.\u000a\u000aSubsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=http://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>\u000a* restrict the time range\u000a* select [[Cross-sectional data|cross section]]s of data\u000a* select particular kinds of [[time series]]\u000a* exclude particular obersvations\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a==External links==\u000a*[http://www.subset.org/index.jsp Subset.org]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Statistics-stub}}
p9
sg4
S'3'
p10
sg6
VSubsetting
p11
ssI133
(dp12
g2
V'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.\u000a\u000aAccording to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078\u20132091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a document\u2019s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company\u2019s earnings or information on already-happened or invalid predictions.\u000a\u000aT-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.\u000a\u000aThis page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.\u000a\u000a== Temporal dynamics (T-dynamics) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&nbsp;117 \u2013 130). Lisbon, Portugal. September 11\u201313: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||\u000a|-\u000a|'''Cho, J., & Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||\u000a|-\u000a| '''Fetterly, D., Manasse, M., Najork, M., & Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&nbsp;669 \u2013 678). Budapest, Hungary. May 20\u201324: ACM Press. || 2003 || WWW || T-Dynamics ||\u000a|-\u000a| '''Ntoulas, A., Cho, J., & Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1 \u2013 12). New York, NY, United States. May 17\u201322: ACM Press. || 2004 || WWW || T-Dynamics ||\u000a|-\u000a| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \u2013 142). Paris, France. June 13\u201318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\u000a|-\u000a| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909 \u2013 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||\u000a|-\u000a| '''Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;282 \u2013 291). Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 \u2013 10). New York, United States. February 3\u201306: ACM Press. || 2010 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 \u2013 1124). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || F-IRetrieval ||\u000a|-\u000a| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction'' (pp.&nbsp;273 \u2013 281). Washington DC, United States. March 30\u201331: Springer-Verlag. || 2010 || SBP || T-Dynamics ||\u000a|-\u000a| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 \u2013 176). Hong Kong, China. February 9\u201312: ACM Press. || 2011 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||\u000a|-\u000a| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 \u2013 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\u000a|-\u000a| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&nbsp;1171 \u2013 1172). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || T-Dynamics ||\u000a|-\u000a| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 \u2013 596). Lisboa, Portugal. October 10\u201313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 \u2013 1240). Glasgow, Scotland, UK. October 24\u201328: ACM Press. || 2011 || CIKM || C-Memory ||\u000a|-\u000a| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\u201311: ACM Press. || 2014 || SIGIR || T-RModels ||\u000a|}\u000a\u000a== Temporal markup languages (T-MLanguages) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Setzer, A., & Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||\u000a|-\u000a| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||\u000a|-\u000a| '''Ferro, L., Mani, I., Sundheim, B., & Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||\u000a|-\u000a| '''Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&nbsp;28 \u2013 34). Tilburg, Netherlands. January 15\u201317. || 2003 || IWCS || T-MLanguages ||\u000a|-\u000a| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||\u000a|}\u000a\u000a== Temporal taggers (T-taggers) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., & Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;69 \u2013 76). Hong Kong, China. October 1\u20138: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||\u000a|-\u000a| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;168 \u2013 175). Philadelphia, PA, United States. July 6\u201312: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||\u000a|-\u000a| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||\u000a|-\u000a| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&nbsp;321 \u2013 324). Uppsala, Sweden. July 11\u201316.|| 2010 || ACL - SemEval || T-Taggers ||\u000a|-\u000a| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\u000a|-\u000a| '''Chang, A., & Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\u000a|-\u000a| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||\u000a|-\u000a| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. & Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]\u000a|}\u000a\u000a== Temporal indexing (T-indexing) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;519 \u2013 526). Amsterdam, Netherlands. July 23\u201327: ACM Press. || 2007 || SIGIR || W-Archives ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM ||| T-RModels ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 \u2013 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||\u000a|}\u000a\u000a== Temporal query understanding (TQ-understanding) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \u2013 142). Paris, France. June 13\u201318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\u000a|-\u000a| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 \u2013 1438). Napa Valley, California, United States. October 26\u201330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;182 \u2013 191). Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM || TQ-Understanding ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''König, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;347 \u2013 354). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 \u2013 20). New York, United States. February 3\u20136: ACM Press. || 2010 || WSDM || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6\u201310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\u000a|-\u000a| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&nbsp;1129 \u2013 1139). Massachusetts, United States. October 9\u201311: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||\u000a|-\u000a| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 \u2013 176). Hong Kong, China. February 9\u201312: ACM Press. || 2011 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&nbsp;1325). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 \u2013 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\u000a|-\u000a| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;1171 \u2013 1172). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;41 \u2013 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||\u000a|-\u000a| '''Shokouhi, M., & Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;601 \u2013 610). Portland, United States. August 12\u201316.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2035 \u2013 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 \u2013 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\u000a|}\u000a\u000a== Time-aware retrieval/ranking models (T-RModels) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Li, X., & Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;469 \u2013 475). New Orleans, Louisiana, United States. November 2\u20138: ACM Press. || 2003 || CIKM || T-RModels ||\u000a|-\u000a| '''Sato, N., Uehara, M., & Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&nbsp;215 \u2013 220). Prague, Czech Republic. September 1\u20135: IEEE. || 2003 || DEXA || T-RModels ||\u000a|-\u000a| '''Berberich, K., Vazirgiannis, M., & Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||\u000a|-\u000a| '''Cho, J., Roy, S., & Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551 \u2013 562). Baltimore, United States. June 13\u201316: ACM Press. || 2005 || SIGMOD || T-RModels ||\u000a|-\u000a| '''Perkiö, J., Buntine, W., & Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;647 \u2013 648). Salvador, Brazil. August 15\u201316: ACM Press. || 2005 || SIGIR || T-RModels ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|-\u000a| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 \u2013 1438). Napa Valley, California, United States. October 26\u201330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM ||| T-RModels ||\u000a|-\u000a| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&nbsp;165 \u2013 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 \u2013 10). New York, United States. February 3\u201306: ACM Press. || 2010 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;629 \u2013 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||\u000a|-\u000a| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 \u2013 20). New York, United States. February 3\u20136: ACM Press. || 2010 || WSDM || T-RModels ||\u000a|-\u000a| '''Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&nbsp;13 \u2013 25). Milton Keynes, UK. March 28\u201331: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||\u000a|-\u000a| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;331 \u2013 340). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || T-RModels ||\u000a|-\u000a| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&nbsp;331 \u2013 340). Atlanta, United States. June 11\u201315: AAAI Press. || 2010 || AAAI || T-RModels ||\u000a|-\u000a| '''Dai, N., & Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;114 \u2013 121). Geneve, Switzerland. July 19\u201323: ACM Press. || 2010 || SIGIR || T-RModels ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6\u201310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\u000a|-\u000a| '''Efron, M., & Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;495 \u2013 504). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || T-RModels ||\u000a|-\u000a| '''Dai, N., Shokouhi, M., & Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;95 \u2013 104). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 \u2013 764). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\u000a|-\u000a| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1101 \u2013 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||\u000a|-\u000a| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2463 \u2013 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||\u000a|-\u000a| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 \u2013 172). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || T-IRetrieval ||\u000a|-\u000a| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\u201311: ACM Press. || 2014 || SIGIR || T-RModels ||\u000a|}\u000a\u000a== Temporal clustering (T-clustering) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 \u2013 174). Houston, United States. November 27\u201330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 \u2013 124). Austin, United States. June 15\u201319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292 \u2013 296). Funchal - Madeira, Portugal. October 6\u20138. || 2009 || KDIR || T-Clustering ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 \u2013 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\u000a|}\u000a\u000a== Temporal text classification (T-classification) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Jong, F., Rode, H., & Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&nbsp;161 \u2013 168). Amsterdam, Netherlands. September 14\u201317 || 2005 || AHC || T-Classification ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&nbsp;233 \u2013 241). Edinburgh, Scotland. May 23\u201326: ACM Press. || 2006 || WWW || T-Classification ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;129 \u2013 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;137 \u2013 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&nbsp;358 \u2013 370). Aarhus, Denmark. September 14\u201319: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Strötgen, J., Alonso, O., & Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;33 \u2013 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||\u000a|-\u000a| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7\u201313. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]\u000a|}\u000a\u000a== Temporal visualization (T-interfaces) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 \u2013 56). Athens, Greece. July 24\u201328: ACM Press. || 2000 || SIGIR || TDT ||\u000a|-\u000a| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 \u2013 80). Boston, Massachusetts, United States. August 20\u201323: ACM Press. || 2000 || KDD - TM || TDT ||\u000a|-\u000a| [http://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||\u000a|-\u000a| '''Cousins, S., & Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||\u000a|-\u000a| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&nbsp;125 \u2013 137). Seattle, Washington, United States. August 17\u201319: ACM Press. || 1994 || ISSTA || T-Interfaces ||\u000a|-\u000a| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&nbsp;221 \u2013 227). Vancouver, British Columbia, Canada. April 13\u201318: ACM Press. || 1996 || CHI || T-Interfaces ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 \u2013 160). Salzburg, Austria. September 6\u20139: ACM Press. || 2005 || HT || W-Archives ||\u000a|-\u000a| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 \u2013 120). Palo Alto, California, United States. July 11\u201314: ACM Press. || 2006 || ICWE || W-Archives ||\u000a|-\u000a| '''Catizone, R., Dalli, A., & Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24\u201326: ELDA. || 2006 || LREC || T-Interfaces ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&nbsp;1221 \u2013 1222). Beijing, China. April 21\u201325: ACM Press. || 2008 || WWW || W-Archives ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8\u201310: ACM Press. || 2008 || WikiSym || T-Interfaces ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&nbsp;601 \u2013 604). Aveiro, Portugal. October 12\u201315. || 2009 || EPIA || T-Interfaces ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&nbsp;549 \u2013 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||\u000a|}\u000a\u000a== Temporal search engines (T-SEngine) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|}\u000a\u000a== Temporal question answering (T-QAnswering) ==\u000a{| class="wikitable sortable"\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|}\u000a\u000a== Temporal snippets (T-snippets) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20\u201324: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&nbsp;26 \u2013 31). Pisa, Italy. October 17\u201321.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||\u000a|-\u000a| '''Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1045 \u2013 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||\u000a|}\u000a\u000a== Future information retrieval (F-IRetrieval) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15\u201319: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 \u2013 124). Austin, United States. June 15\u201319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 \u2013 1124). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || F-IRetrieval ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\u000a|-\u000a| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 \u2013 764). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\u000a|-\u000a| '''Kanazawa, K., Jatowt, A., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;278 \u2013 283). Lyon, France. August 22\u201327: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 \u2013 596). Lisboa, Portugal. October 10\u201313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Weerkamp, W., & Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||\u000a|-\u000a| '''Radinski, K., & Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;255 \u2013 264). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || F-IRetrieval ||\u000a|}\u000a\u000a== Temporal image retrieval (T-IRetrieval) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&nbsp;199 \u2013 204). Cartagena de Indias, Colombia. October 21\u201325: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||\u000a|-\u000a| '''Palermo, F., Hays, J., & Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&nbsp;499 \u2013 512). Firenze, Italy. October 07\u201313: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||\u000a|-\u000a| '''Kim, G., & Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 \u2013 172). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || T-IRetrieval ||\u000a|-\u000a| '''Martin, P., Doucet, A., & Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||\u000a|}\u000a\u000a== Collective memory (C-memory) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||\u000a|-\u000a| '''Hall, D., Jurafsky, D., & Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&nbsp;363 \u2013 371). Waikiki, Honolulu, Hawaii. October 25\u201327: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||\u000a|-\u000a| '''Shahaf, D., & Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623 \u2013 632). Washington, United States. July 25\u201328: ACM Press. || 2010 || KDD || C-Memory ||\u000a|-\u000a| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;83 \u2013 92). Eindhoven, Netherlands. June 6\u20139: ACM Press. || 2011 || HT || C-Memory ||\u000a|-\u000a| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||\u000a|-\u000a| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 \u2013 1240). Glasgow, Scotland, UK. October 24\u201328: ACM Press. || 2011 || CIKM || C-Memory ||\u000a|}\u000a\u000a== Web archives (W-archives) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||\u000a|-\u000a| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&nbsp;72 \u2013 73. || 1997 || SAM || W-Archives ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 \u2013 160). Salzburg, Austria. September 6\u20139: ACM Press. || 2005 || HT || W-Archives ||\u000a|-\u000a| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 \u2013 120). Palo Alto, California, United States. July 11\u201314: ACM Press. || 2006 || ICWE || W-Archives ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&nbsp;135 \u2013 144). Odense, Denmark. August 22\u201325: ACM Press. || 2006 || HT || W-Archives ||\u000a|-\u000a| '''Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&nbsp;239 \u2013 248). Monterey, CA, United States. October 19\u201322: ACM Press. || 2008 || UIST || W-Archives ||\u000a|-\u000a| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\u000a|-\u000a| '''Gomes, D., Miranda, J., & Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&nbsp;408 \u2013 420). Berlin, Germany. September 25\u201329: Springer-Verlag || 2011 || TPDL || W-Archives ||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 \u2013 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||\u000a||\u000a|-\u000a| '''Costa, M., & Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||\u000a|}\u000a\u000a== Topic detection and tracking (TDT) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Allan, J., Carbonell, J., Doddington, G., & Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194 \u2013 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||\u000a|-\u000a| '''Swan, R., & Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;38 \u2013 45). Kansas City, Missouri, United States. November 2\u20136: ACM Press. || 1999 || CIKM || TDT ||\u000a|-\u000a| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 \u2013 80). Boston, Massachusetts, United States. August 20\u201323: ACM Press. || 2000 || KDD - TM || TDT ||\u000a|-\u000a| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 \u2013 56). Athens, Greece. July 24\u201328: ACM Press. || 2000 || SIGIR || TDT ||\u000a|-\u000a| '''Makkonen, J., & Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&nbsp;393 \u2013 404). Trondheim, Norway. August 17\u201322: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||\u000a|-\u000a| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 \u2013 174). Houston, United States. November 27\u201330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&nbsp;227 \u2013 242). New York, United States. || 2004 || TALIP || TDT ||\u000a|}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]
p13
sg4
S'133'
p14
sg6
VTemporal information retrieval
p15
ssI8
(dp16
g2
VThe '''National Centre for Text Mining''' (NaCTeM)\u000a<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community. \u000a\u000aThe [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest - examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].\u000a\u000aThe Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[information extraction]], [[natural language processing]] and parallel and distributed data mining systems in biomedical and clinical applications.\u000a\u000a==Services==\u000a[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them. <ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117\u2013132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref> \u000a\u000a[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[Disambiguation|disambiguates]] them.<ref name="pmid17050571">{{cite journal| author=Okazaki N, Ananiadou S| title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089\u201395 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }} </ref>\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts.\u000a\u000a[http://refine1-nactem.mc.man.ac.uk/facta/ ''' Facta+'''] is a MEDLINE search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal| author=Tsuruoka Y, Tsujii J, Ananiadou S| title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559\u201360 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }} </ref>\u000a\u000a[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system based on MEDLINE.\u000a\u000a[https://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].\u000a\u000a==Resources==\u000a\u000a[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] a large-scale terminological resource for the biomedical domain\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+corpus '''GENIA'''] a collection of reference materials for the development of biomedical text mining systems\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* http://www.nactem.ac.uk\u000a\u000a[[Category:Computational linguistics]]\u000a[[Category:Computer science organizations]]\u000a[[Category:Information retrieval]]\u000a[[Category:Linguistics organizations]]\u000a[[Category:School of Computer Science, University of Manchester]]
p17
sg4
S'8'
p18
sg6
VNational Centre for Text Mining
p19
ssI138
(dp20
g2
VIn [[information retrieval]], '''Okapi BM25''' is a [[ranking function]] used by [[search engine]]s to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query. It is based on the [[Probabilistic relevance model|probabilistic retrieval framework]] developed in the 1970s and 1980s by [[Stephen E. Robertson]], [[Karen Spärck Jones]], and others.\u000a\u000aThe name of the actual ranking function is BM25. To set the right context, however, it usually referred to as "Okapi BM25", since the Okapi information retrieval system, implemented at [[London]]'s [[City University, London|City University]] in the 1980s and 1990s, was the first system to implement this function.\u000a\u000aBM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art [[TF-IDF]]-like retrieval functions used in document retrieval, such as [[web search]].\u000a\u000a== The ranking function ==\u000a\u000aBM25 is a [[Bag of words model|bag-of-words]] retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.\u000a\u000aGiven a query <math>Q</math>, containing keywords <math>q_1, ..., q_n</math>, the BM25 score of a document <math>D</math> is:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})},</math>\u000a\u000awhere <math>f(q_i, D)</math> is <math>q_i</math>'s [[term frequency]] in the document <math>D</math>, <math>|D|</math> is the length of the document <math>D</math> in words, and <math>avgdl</math> is the average document length in the text collection from which documents are drawn. <math>k_1</math> and <math>b</math> are free parameters, usually chosen, in absence of an advanced optimization, as <math>k_1 \u005cin [1.2,2.0]</math> and <math>b = 0.75</math>.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. ''An Introduction to Information Retrieval'', Cambridge University Press, 2009, p. 233.</ref> <math>\u005ctext{IDF}(q_i)</math> is the IDF ([[inverse document frequency]]) weight of the query term <math>q_i</math>. It is usually computed as:\u000a\u000a:<math>\u005ctext{IDF}(q_i) = \u005clog \u005cfrac{N - n(q_i) + 0.5}{n(q_i) + 0.5},</math>\u000a\u000awhere <math>N</math> is the total number of documents in the collection, and <math>n(q_i)</math> is the number of documents containing <math>q_i</math>.\u000a\u000aThere are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the [[Binary Independence Model]].\u000a\u000aPlease note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score.\u000aThis means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way:\u000a\u000a* Each summand can be given a floor of 0, to trim out common terms;\u000a* The IDF function can be given a floor of a constant <math>\u005cepsilon</math>, to avoid common terms being ignored at all;\u000a* The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all.\u000a\u000a== IDF information theoretic interpretation ==\u000aHere is an interpretation from information theory. Suppose a query term <math>q</math> appears in <math>n(q)</math> documents. Then a randomly picked document <math>D</math> will contain the term with probability <math>\u005cfrac{n(q)}{N}</math> (where <math>N</math> is again the cardinality of the set of documents in the collection). Therefore, the [[information]] content of the message "<math>D</math> contains <math>q</math>" is:\u000a\u000a:<math>-\u005clog \u005cfrac{n(q)}{N} = \u005clog \u005cfrac{N}{n(q)}.</math>\u000a\u000aNow suppose we have two query terms <math>q_1</math> and <math>q_2</math>. If the two terms occur in documents entirely independently of each other, then the probability of seeing both <math>q_1</math> and <math>q_2</math> in a randomly picked document <math>D</math> is:\u000a\u000a:<math>\u005cfrac{n(q_1)}{N} \u005ccdot \u005cfrac{n(q_2)}{N},</math>\u000a\u000aand the information content of such an event is:\u000a\u000a:<math>\u005csum_{i=1}^{2} \u005clog \u005cfrac{N}{n(q_i)}.</math>\u000a\u000aWith a small variation, this is exactly what is expressed by the IDF component of BM25.\u000a\u000a== Modifications ==\u000a* At the extreme values of the coefficient <math>b</math> BM25 turns into ranking functions known as '''BM11''' (for <math>b=1</math>) and '''BM15''' (for <math>b=0</math>).<ref>http://xapian.org/docs/bm25.html</ref>\u000a* '''BM25F'''<ref>Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. [http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf ''Microsoft Cambridge at TREC-13: Web and HARD tracks.''] In Proceedings of TREC-2004.</ref>  is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance.\u000a* '''BM25+'''<ref>Yuanhua Lv and ChengXiang Zhai. [http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf ''Lower-bounding term frequency normalization.''] In Proceedings of CIKM'2011, pages 7-16.</ref> is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter <math>\u005cdelta</math> (a default value is <math>1.0</math> in absence of a training data) as compared with BM25:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cleft[ \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})} + \u005cdelta \u005cright]</math>\u000a\u000a== Footnotes ==\u000a{{Reflist}}\u000a\u000a== References ==\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford | title=Okapi at TREC-3 | conference=[http://trec.nist.gov/pubs/trec3/t3_proceedings.html Proceedings of the Third Text REtrieval Conference (TREC 1994)]|location=Gaithersburg, USA|date=November 1994|url=http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}}\u000a\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu|title=Okapi at TREC-7|conference=[http://trec.nist.gov/pubs/trec7/t7_proceedings.html Proceedings of the Seventh Text REtrieval Conference]|location=Gaithersburg, USA|date=November 1998|url=http://trec.nist.gov/pubs/trec7/papers/okapi_proc.pdf.gz}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00015-7}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00016-9}}\u000a\u000a== External links ==\u000a* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}\u000a\u000a[[Category:Ranking functions]]\u000a[[Category:Information retrieval]]
p21
sg4
S'138'
p22
sg6
VOkapi BM25
p23
ssI13
(dp24
g2
V{{For|the observation regarding integrated circuits|Moore's law}}\u000a{{Refimprove|date=September 2011}}\u000a\u000a'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.\u000a\u000a{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=http://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers'%20law&f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}\u000a\u000a==Original interpretation==\u000a\u000aMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the users personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>\u000a\u000a==Out-of-context interpretation==\u000a\u000aThe more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:\u000a\u000a{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}\u000a{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}\u000a\u000aIn this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=pp 607\u2013609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}\u000a\u000a==External links==\u000a* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.\u000a* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.\u000a* [http://www.phillyimc.org/en/gasoline-7-17-moors-law-kent-moors-authority Another empirical observation with a similar-sounding name is Moors' law], named for Kent Moors of Duquesne University, which states crude oil prices double every five years. \u000a[[Category:Empirical laws]]\u000a[[Category:Library science]]\u000a[[Category:Information retrieval]]
p25
sg4
S'13'
p26
sg6
VMooers' law
p27
ssI143
(dp28
g2
V{{Infobox company\u000a|name= Artificial Solutions\u000a|logo=[[Image:Artificial Solutions Logo.png]]\u000a|type=[[Private company]]\u000a|foundation=(2001)\u000a|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström \u000a|location=[[Stockholm]], [[Sweden]]\u000a|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]] and [[Stockholm]] \u000a|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], \u000a|products= Teneo platform\u000a|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]\u000a}}\u000a\u000a'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>\u000a\u000a==History==\u000aArtificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>\u000a\u000aThe company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>\u000a\u000aIn 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>\u000a[[Elbot]], Artificial Solutions\u2019 test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>\u000a\u000aWith a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>\u000aIn 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>\u000aA new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>\u000a\u000aIn February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==External links==\u000a*[http://www.hello-indigo.com Indigo]\u000a*[http://www.elbot.com Elbot]\u000a\u000a[[Category:Natural language processing software]]\u000a[[Category:Intelligent software assistants]]\u000a[[Category:User interfaces]]\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p29
sg4
S'143'
p30
sg6
VArtificial Solutions
p31
ssI18
(dp32
g2
V'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].\u000a\u000a==Features==\u000a\u000a* Custom [[query language]].\u000a* Two-level [[regular expressions]]:\u000a** operating at the level of characters in words\u000a** operating at the level of words in statements/paragraphs\u000a* Good performance\u000a* Compact corpus representation (compared to similar projects)\u000a* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]\u000a* Lack of portability across [[endianness]] (current release works only on little endian devices)\u000a\u000a==External links==\u000a\u000a* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]\u000a* [http://poliqarp.sourceforge.net/ Project website on SourceForge]\u000a* [http://poliqarp.suxx.pl/ Search plugin for Firefox]\u000a<br />\u000a[[Category:Information retrieval]]
p33
sg4
S'18'
p34
sg6
VPoliqarp
p35
ssI148
(dp36
g2
VThe '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].\u000a\u000a==Chronological honorees and lectures==\u000a* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."\u000a* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : "A look back and a look forward."\u000a* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."\u000a* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"\u000a* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." \u000a* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''\u000a* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''\u000a* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."\u000a* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."\u000a* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."\u000a\u000a==External links==\u000a* [http://www.acm.org/sigir/ ACM SIGIR homepage]\u000a* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]\u000a\u000a[[Category:Association for Computing Machinery]]\u000a[[Category:Computer science awards]]\u000a[[Category:Information retrieval]]
p37
sg4
S'148'
p38
sg6
VGerard Salton Award
p39
ssI23
(dp40
g2
V{{primary sources|date=October 2011}}\u000a'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.\u000a\u000aDynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.\u000a\u000aDynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996.\u000a\u000aDynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as the origin of the notion of [[well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].\u000a\u000a[[Inso]] corporation went out of business in 2002. \u000a\u000a==References==\u000a*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{markup-languages-stub}}
p41
sg4
S'23'
p42
sg6
VDynatext
p43
ssI153
(dp44
g2
V{{Infobox company\u000a|name             = Swiftype\u000a|logo             =Black_Swiftype_Logo.png \u000a|type             = [[Privately held company|Private]]\u000a|industry         = [[Software]] <br/> [[Information Technology]] <br/> [[Search Engines]]\u000a|area_served      = Worldwide\u000a|location_city    = [[San Francisco, California|San Francisco]], [[California (state)|California]]\u000a|location_country = U.S.\u000a|founders       = {{unbulleted list|Matt Riley, Quin Hoxie}}\u000a|key_people       = {{unbulleted list|Matt Riley (CEO), Quin Hoxie (CTO)}}\u000a|services         = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}\u000a|genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]]\u000a|num_employees    = 25\u000a|foundation       = [[San Francisco, California|San Francisco]], [[California (state)|California]], [[U.S.A]] [[January 2012]]\u000a|homepage         = {{URL|https://www.swiftype.com/|Swiftype.com}}\u000a|intl             = yes\u000a|footnotes             = {{unbulleted list|[http://www.crunchbase.com/organization/swiftype Crunchbase] [http://www.Swiftype.com Official Website]}}\u000a|alt = Black text and red icon edition of the full Swiftype logo|products = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}}}\u000a\u000a'''Swiftype''' is a company that sells [[search engines]] for websites and mobile applications (also known as [[enterprise search]]) and creates a [[PageRank]] specific to individual websites and mobile applications.<ref name="TechCrunch-Ha">{{cite news|last1=Ha|first1=Anthony|title=Y Combinator-Backed Swiftype Builds Site Search That Doesn\u2019t Suck|url=http://techcrunch.com/2012/05/08/swiftype-launch/|accessdate=21 July 2014|publisher=TechCrunch|date=May 8, 2012|ref=TechCrunch-Ha}}</ref><ref name=BetaBeat>{{cite news|last1=Roy|first1=Jessica|title=Can This Y Combinator Startup Solve the Site Search Problem?|url=http://betabeat.com/2012/05/can-this-y-combinator-startup-solve-the-site-search-problem/|accessdate=21 July 2014|publisher=BetaBeat|date=July 21, 2014|ref=BetaBeat}}</ref><ref name="Crunchbase">{{cite web|url=http://www.crunchbase.com/organization/swiftype|website=Crunchbase|accessdate=19 July 2014|title = <nowiki>Swiftype | CrunchBase</nowiki>}}</ref><ref name=VatorNews>{{Cite news|url = http://vator.tv/news/2013-08-15-swiftype-bags-17m-from-big-names-for-better-search|title = Swiftype bags $1.7M from big names for better search|last = Marino|first = Faith|date = August 15, 2013|work = |accessdate = July 21, 2014|ref = VatorNews|publisher = VatorNews}}</ref>  The company is based in [[San Francisco, CA]] and is funded mainly through [[venture capital]].<ref name=Crunchbase />\u000a\u000a==History==\u000aSwiftype was founded in 2012 by former [[Scribd]] engineers Matt Riley and Quin Hoxie.<ref name=Crunchbase /> The two met while working on an internal search tool for [[Scribd]].<ref name="TechCrunch-Ha" /><ref name="BetaBeat" /><ref name=Forbes>{{cite news|last1=Casserly|first1=Meghan|title=Site Search (Should Be) Sexy: How Swiftype Raised $1.7M In Seed Funding From SV Bigwigs|url=http://www.forbes.com/sites/meghancasserly/2013/08/15/site-search-should-be-sexy-how-swiftype-raised-1-7-in-seed-funding-from-sv-bigwigs/|accessdate=19 July 2014|publisher=Forbes|ref = Forbes|date=2013-08-15}}</ref> Swiftype participated in [[Y Combinator (company)|Y Combinator]] in 2012 and received investment from a number of prominent sources.<ref name=VentureBeat>{{Cite news|url = http://venturebeat.com/2013/08/15/yc-startup-swiftype-raises-1-7m-seed-round-from-andreessen-nea-kleiner/|title = YC startup Swiftype raises $1.7M seed round from Andreessen; NEA; Kleiner|last = Grant|first = Rebecca|date = August 15, 2013|work = |accessdate = July 21, 2014|publisher = VentureBeat|ref = VentureBeat}}</ref><ref name=VatorNews /><ref name="TechCrunch-Yang">{{cite news|last1=Yang|first1=Anthony|title=Site Search Engine Creator Swiftype Raises $1.7M From A16Z, Others|url=http://techcrunch.com/2013/08/15/swiftype-1-7m/|accessdate=19 July 2014|publisher=TechCrunch|date=2013-08-15|ref = TechCrunch-Yang}}</ref><ref name=AllThingsD>{{cite news|last1=Gannes|first1=Liz|title=Swiftype Raises $1.7M for Smarter Site Search|url=http://allthingsd.com/20130815/swiftype-raises-1-7m-for-site-search/|accessdate=19 July 2014|publisher=All Things D|ref = AllThingsD|date=2013-08-15}}</ref> In September 2013, the company obtained [[Series A]] funding.<ref name=Crunchbase /><ref name=VatorNews /><ref name=VentureBeat /><ref name=TechCrunch-Yang /><ref name=AllThingsD /><ref name=StartUpBeatBeat>{{Cite news|url = http://startupbeat.com/2013/08/26/swiftype-wants-to-dramatically-improve-search-on-websites-and-mobile-apps-of-all-types-and-sizes-id3402/|title = Swiftype wants to dramatically improve search on websites and mobile apps of all types and sizes|last = Editor|first = |date = August 26, 2013|work = |accessdate = July 21, 2014|publisher = StartUpBeat|ref = StartUpBeat}}</ref>\u000a\u000aAs of August 2013, Swiftype had over 70,000 websites using their search bar, powering over 130 million queries per month.<ref name=Forbes /><ref name=AllThingsD />\u000a\u000a==Features==\u000aSwiftype is available as an [[API]] or [[web crawler]] based engine.<ref name=TechCrunch-Yang />  The company also offers a VIP-approved [[WordPress]] Plugin, a [[Shopify]] App, and a [[Magento]] extension.<ref>{{Cite web|url = https://apps.shopify.com/swiftype|title = Autocomplete & Site Search by Swiftype \u2013 Ecommerce Plugins for Online Stores \u2013 Shopify App Store|date = 2014-09-26|accessdate = 2014-09-26|website = Shopify App Store|publisher = Shopify|last = |first = }}</ref><ref>{{Cite web|url = http://www.magentocommerce.com/magento-connect/modern-site-search-by-swiftype.html|title = Modern Site Search by Swiftype - Magento Connect|date = 2014-09-26|accessdate = 2014-09-26|website = Magento Connect|publisher = Magento|last = |first = }}</ref><ref>{{Cite web|url = https://wordpress.org/plugins/swiftype-search/|title = <nowiki>WordPress | Swiftype Search | WordPress Plugins</nowiki>|date = 2014-09-26|accessdate = 2014-09-26|website = WordPress Plugin Directory|publisher = WordPress|last = |first = }}</ref> Swiftype sells [[eCommerce]] search, [[enterprise search]], [[faceted search]], [[full text search]], [[enterprise search]], [[real-time search]], [[concept search]], and website [[search engines]] for websites and mobile applications.<ref name=Crunchbase /><ref name=VatorNews /> The company's paid plans offer on demand and live recrawls and indexing of websites.<ref name=AllThingsD /> Other features include drag and drop result customization<ref name=Forbes /><ref name=VentureBeat /><ref name=AllThingsD /> and  real-time analytics.<ref name=TechCrunch-Ha /><ref name=Forbes />\u000a<!--Swiftype website lists several additional features that I've been unable to find neutral third party discussion of -->\u000a\u000a==Competitors==\u000a* [[Algolia]]<ref name=Crunchbase />\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[Search engines]]\u000a* [[Faceted search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Concept search]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* {{Official website|swiftype.com}}\u000a\u000a__FORCETOC__\u000a__INDEX__\u000a__NEWSECTIONLINK__\u000a\u000a[[Category:Search engine software]]\u000a[[Category:Companies established in 2012]]\u000a[[Category:Companies based in San Francisco, California]]\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies]]\u000a[[Category:Y Combinator companies]]\u000a[[Category:Semantic Web]]\u000a[[Category:Software startup companies]]\u000a[[Category:Online companies]]
p45
sg4
S'153'
p46
sg6
VSwiftype
p47
ssI28
(dp48
g2
VIn applied mathematics \u2013 specifically in [[fuzzy logic]] \u2013 the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.\u000a\u000a== Definition ==\u000a\u000aFormally an '''OWA''' operator of dimension <math> \u005c n </math> is a mapping <math> F: R_n \u005crightarrow R </math> that has an associated collection of weights <math> \u005c  W = [w_1, \u005cldots, w_n] </math> lying in the unit interval and summing to one and with 		\u000a\u000a:<math> F(a_1, \u005cldots , a_n) =  \u005csum_{j=1}^n  w_j b_j</math>\u000a\u000awhere <math> b_j </math> is the ''j''<sup>th</sup> largest of the <math> a_i </math>.\u000a\u000aBy choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''<sub>''j''</sub>.\u000a\u000a== Properties ==\u000a\u000aThe OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.\u000a\u000a{|class="wikitable"\u000a|[[Bounded operator|Bounded]]\u000a|<math>   \u005cmin(a_1, \u005cldots, a_n) \u005cle F(a_1, \u005cldots, a_n) \u005cle \u005cmax(a_1, \u005cldots, a_n) </math>\u000a|-\u000a|[[Monotonic]]\u000a|<math>   F(a_1, \u005cldots, a_n) \u005cge F(g_1, \u005cldots, g_n) </math> if <math> a_i \u005cge g_i </math> for <math>\u005c i = 1,2,\u005cldots,n </math>\u000a|-\u000a|[[symmetric operator|Symmetric]]\u000a|<math>   F(a_1, \u005cldots, a_n)  = F(a_\u005cboldsymbol{\u005cpi(1)}, \u005cldots, a_\u005cboldsymbol{\u005cpi(n)})</math> if <math>\u005cboldsymbol{\u005cpi} </math> is a permutation map\u000a|-\u000a|[[Idempotent]]\u000a|<math>  \u005c F(a_1, \u005cldots, a_n)  =  a </math> if all <math> \u005c a_i = a </math>\u000a|}\u000a\u000a== Notable OWA operators ==\u000a:<math> \u005c F(a_1, \u005cldots, a_n) = \u005cmax(a_1, \u005cldots, a_n) </math> if <math> \u005c w_1 = 1 </math> and <math> \u005c w_j = 0 </math> for <math> j \u005cne 1 </math>\u000a\u000a:<math> \u005c F(a_1, \u005cldots, a_n) = \u005cmin(a_1, \u005cldots, a_n) </math> if <math> \u005c w_n = 1 </math> and <math> \u005c w_j = 0 </math> for <math> j \u005cne n </math>\u000a\u000a== Characterizing features ==\u000a\u000aTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\u000a\u000aThis is defined as\u000a:<math>A-C(W)= \u005cfrac{1}{n-1} \u005csum_{j=1}^n (n - j) w_j. </math>\u000a\u000aIt is known that <math> A-C(W) \u005cin [0, 1] </math>.\u000a\u000aIn addition ''A''&nbsp;&minus;&nbsp;''C''(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\u000a\u000aThe second feature is the dispersion. This defined as\u000a\u000a:<math>H(W) = -\u005csum_{j=1}^n w_j \u005cln (w_j).</math>\u000a\u000aAn alternative definition is <math>E(W) = \u005csum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used\u000a\u000a== A literature survey: OWA (1988-2014)==\u000aThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183\u2013190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full) \u000a\u000a== Type-1 OWA aggregation operators ==\u000a\u000aThe above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\u000a'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\u000a\u000aThe '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:\u000a\u000aGiven the ''n'' linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, then for each <math>\u005calpha \u005cin [0,\u005c;1]</math>, an <math>\u005calpha </math>-level type-1 OWA operator with <math>\u005calpha </math>-level sets <math>\u005cleft\u005c{ {W_\u005calpha ^i } \u005cright\u005c}_{i = 1}^n </math> to aggregate the <math>\u005calpha </math>-cuts of fuzzy sets <math>\u005cleft\u005c{ {A^i} \u005cright\u005c}_{i =1}^n </math> is given as\u000a\u000a: <math>\u000a\u005cPhi_\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright) =\u005cleft\u005c{ {\u005cfrac{\u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} } }{\u005csum\u005climits_{i = 1}^n {w_i } }\u005cleft| {w_i \u005cin W_\u005calpha ^i ,\u005c;a_i } \u005cright. \u005cin A_\u005calpha ^i ,\u005c;i = 1, \u005cldots ,n} \u005cright\u005c}</math>\u000a\u000awhere <math>W_\u005calpha ^i= \u005c{w| \u005cmu_{W_i }(w) \u005cgeq \u005calpha \u005c}, A_\u005calpha ^i=\u005c{ x| \u005cmu _{A_i }(x)\u005cgeq \u005calpha \u005c}</math>, and <math>\u005csigma :\u005c{\u005c;1, \u005cldots ,n\u005c;\u005c} \u005cto \u005c{\u005c;1, \u005cldots ,n\u005c;\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cge a_{\u005csigma (i + 1)} ,\u005c;\u005cforall \u005c;i = 1, \u005cldots ,n - 1</math>, i.e., <math>a_{\u005csigma (i)} </math> is the <math>i</math>th largest\u000aelement in the set <math>\u005cleft\u005c{ {a_1 , \u005cldots ,a_n } \u005cright\u005c}</math>.\u000a\u000aThe computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals <math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)</math>:\u000a<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)_{-} </math> and <math>\u000a\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)_ {+},</math>\u000awhere <math>A_\u005calpha ^i=[A_{\u005calpha-}^i, A_{\u005calpha+}^i], W_\u005calpha ^i=[W_{\u005calpha-}^i, W_{\u005calpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:\u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots\u000a,A_\u005calpha ^n } \u005cright)_\u005calpha } \u005calpha </math>\u000a\u000aFor the left end-points, we need to solve the following programming problem:\u000a\u000a:<math> \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)_{-} = \u005cmathop {\u005cmin }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i = 1}^n {w_i } } </math>\u000a\u000awhile for the right end-points, we need to solve the following programming problem:\u000a\u000a:<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots , A_\u005calpha ^n } \u005cright)_{+} = \u005cmathop {\u005cmax }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i  A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i =\u000a1}^n {w_i } } </math>\u000a\u000a[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\u000a\u000a== References ==\u000a\u000a* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183\u2013190, 1988.\u000a\u000a* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.\u000a\u000a* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68\u201381, 2007.\u000a\u000a* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]\u000a\u000a* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988\u20132014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d]. \u000a\u000a* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.\u000a\u000a* Majlender, P., "OWA operators with maximal Rényi entropy," Fuzzy Sets and Systems 155, 340\u2013360, 2005.\u000a\u000a* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439\u2013456.\u000a\u000a* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;3281\u20133296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]\u000a\u000a* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;1455\u20131468.[http://dx.doi.org/10.1109/TKDE.2010.191]\u000a\u000a* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540\u2013558, 2010.[http://dx.doi.org/10.1002/int.20420]\u000a\u000a[[Category:Artificial intelligence]]\u000a[[Category:Logic in computer science]]\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]
p49
sg4
S'28'
p50
sg6
VOrdered weighted averaging aggregation operator
p51
ssI158
(dp52
g2
V{{cat main|Deep Web}}\u000a\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:World Wide Web]]
p53
sg4
S'158'
p54
sg6
VCategory:Deep Web
p55
ssI33
(dp56
g2
V{{Infobox company\u000a| name = Coveo Solutions Inc.\u000a| logo = [[Image:Coveo logo.png|120px]]\u000a| type = Private\u000a| slogan = \u000a| foundation =  2004\u000a| location_city = [[Quebec City]], [[Canada]]\u000a| key_people = Louis Têtu, Chairman and CEO <br />Laurent Simoneau, President and CTO\u000a| num_employees =\u000a| industry = [[Enterprise search]]\u000a| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce\u000a| homepage = http://www.coveo.com\u000a}}\u000a\u000a'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for Salesforce.com, Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.\u000a\u000a==History==\u000aCoveo Solutions Inc. was founded in 2004 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>\u000a\u000a==Products==\u000a'''Coveo Search & Relevance Platform'''\u000a\u000aCoveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.\u000a\u000a'''Coveo for Sitecore'''\u000a\u000aCoveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore\u2019s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.\u000a\u000a'''Coveo for Salesforce'''\u000a\u000aCoveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.\u000a\u000a==Customers==\u000aCoveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://www.coveo.com/ Coveo.com]\u000a\u000a[[Category:Companies based in Quebec City]]\u000a[[Category:Information retrieval]]\u000a[[Category:BlackBerry development software]]
p57
sg4
S'33'
p58
sg6
VCoveo
p59
ssI163
(dp60
g2
VIn [[natural language processing]] and [[information retrieval]], '''explicit semantic analysis''' ('''ESA''') is a [[Vector space model|vectorial]] representation of text (individual words or entire documents) that uses a document corpus as a [[knowledge base]]. Specifically, in ESA, a word is represented as a column vector in the [[tf*idf|tf\u2013idf]] matrix of the text corpus and a document (string of words) is represented as the [[centroid]] of the vectors representing its words. Typically, the text corpus is [[Wikipedia]], though other corpora including the [[Open Directory Project]] have been used.<ref name="infosys">{{cite journal |authors=Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich |year=2011 |title=Concept-Based Information Retrieval using Explicit Semantic Analysis |url=http://www.cs.technion.ac.il/~gabr/publications/papers/Egozi2011CBI.pdf|format=pdf|accessdate=January 3, 2015|journal=ACM Transactions on Information Systems |volume=29 |issue=2}}</ref>\u000a\u000aESA was designed by [[Evgeniy Gabrilovich]] and Shaul Markovitch as a means of improving [[document classification|text categorization]]<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Overcoming the brittleness bottleneck using Wikipedia: enhancing text categorization with encyclopedic knowledge |conference=Proc. 21st National Conference on Artificial Intelligence (AAAI) |pages=1301\u20131306 |year=2006 |url=http://www.aaai.org/Papers/AAAI/2006/AAAI06-204.pdf}}</ref>\u000aand has been used by this pair of researchers to compute what they refer to as "[[Semantics|semantic]] relatedness" by means of [[cosine similarity]] between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.\u000aThe name "explicit semantic analysis" contrasts with [[latent semantic analysis]] (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis |conference=Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI) |pages=1606\u20131611 |year=2007 |url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf}}</ref><ref name="infosys"/>\u000a\u000aESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically [[Orthogonality|orthogonal]] concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of [[information retrieval]] systems when it is based not on Wikipedia, but on the [[Reuters]] corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".<ref>Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2009c.pdf The ESA retrieval model revisited]. Proceedings of the 32nd International ACM Conference on Research and Development in Information Retrieval (SIGIR), pp. 670-671, 2009.</ref>\u000aTo explain this observation, links have been shown between ESA and the [[generalized vector space model]].<ref>Thomas Gottron, Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011o.pdf Insights into explicit semantic analysis]. Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.</ref>\u000aGabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".<ref name="infosys" />\u000a\u000a'''Cross-language explicit semantic analysis''' ('''CL-ESA''') is a multilingual generalization of ESA.<ref>Martin Potthast, Benno Stein, and Maik Anderka. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2008b.pdf A Wikipedia-based multilingual retrieval model]. Proceedings of the 30th European Conference on IR Research (ECIR), pp. 522-530, 2008.</ref>\u000aCL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.\u000a\u000a== See also ==\u000a* [[Topic model]]\u000a\u000a== External links ==\u000a* [http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html Explicit semantic analysis] on Evgeniy Gabrilovich's homepage; has links to implementations\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a[[Category:Natural language processing]]\u000a[[Category:Vector space model]]
p61
sg4
S'163'
p62
sg6
VExplicit semantic analysis
p63
ssI38
(dp64
g2
V'''Globrix''' was a UK [[real estate]] [[Web search engine|search engine]] that was launched in January 2008. It was launched as a joint venture with [[News International]], publishers of ''[[The Sunday Times]]'', ''[[The Sun (newspaper)|The Sun]]'', ''[[The Times]]'', ''[[The News of the World]]'' and ''[[Thelondonpaper]]''.<ref>[http://www.nma.co.uk/news/news-international-invests-in-property-site-globrix/35492.article News International invests in property site, Globrix - NMA article]</ref>\u000a\u000a[[Estate agent]]s and [[letting agent]]s could list their properties for free. This competed with traditional paid-listings sites such as [[Rightmove]] (originally a joint venture between four of the UK's largest property agents, now a [[public limited company]]), [[Zoopla|Propertyfinder]] (also partly backed by News International) and [[Primelocation]] (owned by [[Daily Mail and General Trust]]). Unlike most property websites, Globrix directed users to agent websites rather than hosting the property details and capturing the lead on Globrix itself. Globrix gathered its property listings in three different ways; crawling agent websites, taking data feeds and by agents manually uploading via the Globrix extranet. Because Globrix was 'free to list', Globrix was able to gain substantial market coverage and claimed to list more properties than any other UK property website. Unlike websites like [[Gumtree]] and [[Oodle]], private sellers and landlords were not allowed to list their properties on the site.\u000a\u000aThe website charged property professionals and property related services companies for geo-targeted [[Web banner|banner ads]]. There were also premium services available to estate and letting agents (such as [[Search Engine Optimization]] consultancy, branded email alerts and increased traffic) and [[Google Ads]] were displayed in unsold advertising positions on the right hand side of search results.\u000a\u000a==Functionality==\u000a\u000aThe basic property search functionality is kept simple with just one text box on the homepage. Users can search for property by location (e.g. city, town, full postcode, partial postcode or, unusually for property portals, street name), places of interest (e.g. schools, stations, landmarks) or by key features (e.g. swimming pool, garden, double glazing, helipad).\u000a\u000aSearch results can then be refined further by changing the price parameters, number of bedrooms and bathrooms, property type (e.g. detached, bungalow, flat), outside space, nearby stations and schools and property features (e.g. wooden floors, sea view). Registered users are able to search by additional parameters such as price change.\u000a\u000aAs an alternative to the regular 'list view' of property results, users can also opt to see the search results plotted on [[Bing Maps]] (previously they used [[Google map]]) to allow users to look for property by location. (Some users are unimpressed with the lack of precision of the inferior Bing offering, which often manages to put the marker in a field, compared to the accuracy and ease of use of Googlemaps).  Users are able to drag and zoom the map, with relevant properties automatically placed in view. It is also possible for users to draw a catchment area directly onto the map of where they would like to search.\u000a\u000a==Data==\u000a\u000aGlobrix data was sometimes used by the national media to illustrate stories on house prices,<ref>House prices drop £100,000 in two weeks in race to sell before Christmas - Daily Mail [http://www.dailymail.co.uk/news/article-1089563/House-prices-drop-100-000-WEEKS-race-sell-Christmas.html]</ref> the economy, area trends, consumer confidence<ref>[http://news.bbc.co.uk/1/hi/business/7737507.stm House sales rise as prices fall - BBC News]</ref> and the property market.<ref>[http://www.telegraph.co.uk/finance/personalfinance/borrowing/mortgages/3268208/Housing-market-stagnates-as-buyers-disappear.html Housing market stagnates as buyers disappear - Daily Telegraph]</ref>\u000a\u000a==Awards==\u000a\u000aIn 2008, Globrix was awarded 'Best Property Portal UK' which is awarded by one of the group's own newspapers, the [[The Daily Mail]].<ref>[http://www.residentialpropertyawards.net/index.php/International/Winners/Winners-of-2008.html Daily Mail Property Awards 2008]</ref> Globrix also won 'Estate Agency Service Firm of the Year' at The Negotiator Awards.<ref>[http://negotiator-magazine.co.uk/events/awards/categories-and-finalists/agency-service-firm-of-the-year/ The Negotiator Awards 2008]</ref>\u000a\u000a==Founders==\u000a\u000aGlobrix was founded by Dan Lee and Ian Parry, both ex employees of UK-based search company [[Autonomy Corporation|Autonomy]] and the Norwegian search company [[Fast Search & Transfer|FAST]].\u000a\u000a==Merged with Zoopla==\u000a\u000aIn December 2012 Globrix merged with [[Zoopla]].<ref name="Estate Agent Today">{{cite web|title=Zoopla acquires Globrix as it steps up battle against Rightmove|url=http://www.estateagenttoday.co.uk/news_features/Zoopla-acquires-Globrix-as-it-steps-up-battle-against-Rightmove|work=Estate Agent Today|accessdate=8 September 2013}}</ref>\u000a\u000a== References ==\u000a<references/>\u000a\u000a==External links==\u000a* [http://www.globrix.com/ Globrix homepage]\u000a* [http://www.ft.com/cms/s/0/bc401968-824e-11dc-8a8f-0000779fd2ac.html News International invests in search engine] - Financial Times\u000a* [http://www.independent.co.uk/news/business/analysis-and-features/home-search-sites-have-a-new-kid-on-the-block-786342.html Home search sites have a new kid on the block] - The Independent\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Online real estate companies]]
p65
sg4
S'38'
p66
sg6
VGlobrix
p67
ssI168
(dp68
g2
V{{lowercase|title=agrep}}\u000a{{Infobox software\u000a| name                   = agrep\u000a| logo                   = <!-- Image name is enough -->\u000a| logo caption           = \u000a| logo_size              = \u000a| logo_alt               = \u000a| screenshot             = <!-- Image name is enough -->\u000a| caption                = \u000a| screenshot_size        = \u000a| screenshot_alt         = \u000a| collapsible            = \u000a| developer              = {{Plainlist|\u000a* [[Udi Manber]]\u000a* Sun Wu\u000a}}\u000a| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| discontinued           = \u000a| latest release version = \u000a| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| latest preview version = \u000a| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| status                 = \u000a| programming language   = \u000a| operating system       = {{Plainlist|\u000a* [[Unix-like]]\u000a* [[OS/2]]\u000a* [[DOS]]\u000a* [[Microsoft Windows|Windows]]\u000a}}\u000a| platform               = \u000a| size                   = \u000a| language               = \u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| genre                  = [[Pattern matching]]\u000a| license                = \u000a| website                = <!-- {{URL|example.org}} -->\u000a| standard               = \u000a}}\u000a\u000a'''agrep''' (approximate [[grep]]) is a [[proprietary software|proprietary]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].\u000a\u000aIt selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.\u000a\u000aagrep is also the [[search engine]] in the indexer program [[GLIMPSE]]. agrep is free for private and non-commercial use only, and belongs to the University of Arizona.\u000a\u000a== Alternative implementations ==\u000aA more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].\u000a\u000aFREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* Wu-Manber agrep\u000a**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)\u000a**[http://www.tgries.de/agrep For DOS, Windows and OS/2 home page]\u000a*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]\u000a\u000a*See also\u000a**[http://laurikari.net/tre TRE regexp matching package]\u000a**[http://www.bell-labs.com/project/wwexptools/cgrep/ cgrep a command line approximate string matching tool]\u000a**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool\u000a**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]\u000a\u000a[[Category:Searching]]\u000a[[Category:Unix text processing utilities]]
p69
sg4
S'168'
p70
sg6
VAgrep
p71
ssI43
(dp72
g2
V'''SimRank''' is a general [[Semantic similarity|similarity measure]], based on a simple and intuitive [[Graph theory|graph-theoretic model]].\u000aSimRank is applicable in any [[Domain model|domain]] with object-to-object [[Relation (mathematics)|relationships]], that measures similarity of the structural context in which objects occur, based on their relationships with other objects.\u000aEffectively, SimRank is a measure that says "'''two objects are considered to be similar if they are referenced by similar objects'''."\u000a\u000a== Introduction ==\u000a\u000aMany [[Application software|applications]] require a measure of "similarity" between objects.\u000aOne obvious example is the "find-similar-document" query,\u000aon traditional text corpora or the [[World Wide Web|World-Wide Web]].\u000aMore generally, a similarity measure can be used to [[Cluster analysis|cluster objects]], such as for [[collaborative filtering]] in a [[recommender system]], in which \u201csimilar\u201d users and items are grouped based on the users\u2019 preferences.\u000a\u000aVarious aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain.\u000aIn a [[Text corpus|document corpus]], matching text may be used, and for collaborative filtering, similar users may be identified by common preferences.\u000aSimRank is a general approach that exploits the object-to-object relationships found in many domains of interest.\u000aOn the [[World Wide Web|Web]], for example, two pages are related if there are [[hyperlink]]s between them.\u000aA similar approach can be applied to scientific papers and their citations, or to any other document corpus with [[cross-reference]] information.\u000aIn the case of recommender systems, a user\u2019s preference for an item constitutes a relationship between the user and the item.\u000aSuch domains are naturally modeled as [[Graph (mathematics)|graphs]], with [[Vertex (graph theory)|nodes]] representing objects and [[Edge (graph theory)#Graph|edges]] representing relationships.\u000a\u000aThe intuition behind the SimRank algorithm is that, in many domains, '''similar objects are referenced by similar objects'''.\u000aMore precisely, objects <math>a</math> and <math>b</math> are considered to be similar if they are pointed from objects <math>c</math> and <math>d</math>, respectively, and <math>c</math> and <math>d</math> are themselves similar.\u000aThe [[Recursion (computer science)#Recursive programming|base case]] is that objects are maximally similar to themselves\u000a.<ref name=jeh_widom>G. Jeh and J. Widom. SimRank: A Measure of Structural-Context Similarity. In [[SIGKDD|KDD'02]]: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543. [[Association for Computing Machinery|ACM Press]], 2002. [http://www-cs-students.stanford.edu/~glenj/simrank.pdf]</ref>\u000a\u000aIt is important to note that SimRank is a general algorithm that determines only the similarity of structural context.\u000aSimRank applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships.\u000aObviously, similarity of other domain-specific aspects are important as well; these can \u2014 and should be combined with relational structural-context similarity for an overall similarity measure.\u000aFor example, for [[Web page]]s SimRank can be combined with traditional textual similarity; the same idea applies to scientific papers or other document corpora.\u000aFor recommendation systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level).\u000aAgain, these similarities can be combined with the similarity scores that are computed based on preference patterns, in order to produce an overall similarity measure.\u000a\u000a== Basic SimRank equation ==\u000a\u000aFor a node <math>v</math> in a directed graph, we denote by <math>I(v)</math> and <math>O(v)</math> the set of in-neighbors and out-neighbors of <math>v</math>, respectively.\u000aIndividual in-neighbors are denoted as <math>I_i(v)</math>, for <math>1 \u005cle i \u005cle \u005cleft|I(v)\u005cright|</math>, and individual\u000aout-neighbors are denoted as <math>O_i(v)</math>, for <math>1 \u005cle i \u005cle \u005cleft|O(v)\u005cright|</math>.\u000a\u000aLet us denote the similarity between objects <math>a</math> and <math>b</math> by <math>s(a, b) \u005cin [0, 1]</math>. \u000aFollowing the earlier motivation, a recursive equation is written for <math>s(a, b)</math>.\u000aIf <math>a = b</math> then <math>s(a, b)</math> is defined to be <math>1</math>.\u000aOtherwise,\u000a:<math>s(a, b) = \u005cfrac{C}{\u005cleft|I(a)\u005cright| \u005cleft|I(b)\u005cright|}\u000a \u005csum_{i=1}^{\u005cleft|I(a)\u005cright|}\u005csum_{j=1}^{\u005cleft|I(b)\u005cright|}\u000a s(I_i(a), I_j(b))</math>\u000awhere <math>C</math> is a constant between <math>0</math> and <math>1</math>.\u000aA slight technicality here is that either <math>a</math> or <math>b</math> may not have any in-neighbors.\u000aSince there is no way to infer any similarity between <math>a</math> and <math>b</math> in this case, similarity is set to <math>s(a, b) = 0</math>, so the summation in the above equation is defined to be <math>0</math> when <math>I(a) = \u005cemptyset</math> or <math>I(b) = \u005cemptyset</math>.\u000a\u000a== Matrix representation of SimRank ==\u000a\u000aLet <math>\u005cmathbf{S}</math> be the similarity matrix whose entry <math>[\u005cmathbf{S}]_{a,b}</math> denotes the similarity score <math>s(a,b)</math>, and <math>\u005cmathbf{A}</math> be the column normalized adjacency matrix whose entry <math>[\u005cmathbf{A}]_{a,b}=\u005ctfrac{1}{|\u005cmathcal{I}(b)|}</math> if there is an edge from <math>a</math> to <math>b</math>, and 0 otherwise. Then, in matrix notations, SimRank can be formulated as\u000a\u000a:<math>\u000a   {{\u005cmathbf{S}}}= \u005cmax\u005c{C\u005ccdot (\u005cmathbf{A}^{T} \u005ccdot {{\u005cmathbf{S}}}\u005ccdot {{\u005cmathbf{A}}} ) , {{\u005cmathbf{I}}}\u005c},</math>\u000a\u000awhere <math>\u005cmathbf{I}</math> is an identity matrix.\u000a\u000a== Computing SimRank ==\u000a\u000aA solution to the SimRank equations for a graph <math>G</math> can be reached by [[Iterative method|iteration]] to a [[Fixed point (mathematics)|fixed-point]].\u000aLet <math>n</math> be the number of nodes in <math>G</math>.\u000aFor each iteration <math>k</math>, we can keep <math>n^2</math> entries <math>s_k(*, *)</math>, where <math>s_k(a, b)</math> gives the score between <math>a</math> and <math>b</math> on iteration <math>k</math>.\u000aWe successively compute <math>s_{k+1}(*, *)</math> based on <math>s_k(*, *)</math>.\u000aWe start with <math>s_0(*, *)</math> where each <math>s_0(a, b)</math> is a lower bound on the actual SimRank score <math>s(a, b)</math>:\u000a:<math> s_0(a, b) =\u000a \u005cbegin{cases}\u000a  1 \u005cmbox{  } , \u005cmbox{    } \u005cmbox{if } a = b  \u005cmbox{  } , \u005c\u005c\u000a  0 \u005cmbox{  } , \u005cmbox{    } \u005cmbox{if } a \u005cneq b \u005cmbox{  } .\u000a \u005cend{cases}</math>\u000a\u000aTo compute <math>s_{k+1}(a, b)</math> from <math>s_k(*, *)</math>, we use the basic SimRank equation to get:\u000a:<math>s_{k + 1}(a, b) = \u000a \u005cfrac{C}{\u005cleft|I(a)\u005cright| \u005cleft|I(b)\u005cright|}\u000a \u005csum_{i=1}^{\u005cleft|I(a)\u005cright|}\u005csum_{j=1}^{\u005cleft|I(b)\u005cright|}\u000a  s_k(I_i(a), I_j(b))</math>\u000afor <math>a \u005cne b</math>, and <math>s_{k+1}(a, b) = 1</math> for <math>a = b</math>.\u000aThat is, on each iteration <math>k + 1</math>, we update the similarity of <math>(a, b)</math> using the similarity scores of the neighbours of <math>(a, b)</math> from the previous iteration <math>k</math> according to the basic SimRank equation.\u000aThe values <math>s_k(*, *)</math> are [[Monotonic function|nondecreasing]] as <math>k</math> increases.\u000aIt was shown in <ref name="jeh_widom"/> that the values [[Limit of a sequence|converge]] to [[Limit of a sequence|limits]] satisfying the basic SimRank equation, the SimRank scores <math>s(*, *)</math>, i.e., for all <math>a, b \u005cin V</math>, <math>\u005clim_{k \u005cto \u005cinfty} s_k(a, b) = s(a, b)</math>.\u000a\u000aThe original SimRank proposal suggested choosing the decay factor <math>C = 0.8</math> and a fixed number <math>K = 5</math> of iterations to perform.\u000aHowever, the recent research <ref name="lizorkin">D. Lizorkin, P. Velikhov, M. Grinev and D. Turdakov. Accuracy Estimate and Optimization Techniques for\u000aSimRank Computation. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 422--433. [http://modis.ispras.ru/Lizorkin/Publications/simrank_accuracy.pdf]</ref> showed that the given values for <math>C</math> and <math>K</math> generally imply relatively low [[Accuracy and precision|accuracy]] of iteratively computed SimRank scores.\u000aFor guaranteeing more accurate computation results, the latter paper suggests either using a smaller decay factor (in particular, <math>C = 0.6</math>) or taking more iterations.\u000a\u000a== Partial Sums Memoization ==\u000a\u000aThe recent work of Lizorkin et al.<ref name="lizorkin"/> proposed three optimization techniques for speeding up the computation of SimRank:\u000a\u000a(1) Essential nodes selection may eliminate the computation of a fraction of node pairs with a-priori zero scores.\u000a\u000a(2) Partial sums memoization can effectively reduce repeated calculations of the similarity among different node pairs by caching part of similarity summations for later reuse.\u000a\u000a(3) A threshold setting on the similarity enables a further reduction in the number of node pairs to be computed. \u000a\u000aIn particular, the second observation of partial sums memoization plays a paramount role in greatly speeding up the computation of SimRank from <math>O(Kd^2n^2)</math> to <math>O(Kdn^2)</math>, where <math>K</math> is the number of iterations, <math>d</math> is average degree of a graph, and <math>n</math> is the number of nodes in a graph. The central idea of partial sums memoization consists of two steps:\u000a\u000aFirst, the partial sums over <math>{\u005cmathcal I}(a)</math> are memoized as\u000a:<math>\u000aPartial_{{\u005cmathcal I}(a)}^{s_{k}}(j)=\u005csum_{i\u005cin{\u005cmathcal I}(a)}s_{k}(i,j), \u005cqquad (\u005cforall j \u005cin {\u005cmathcal I}(b))\u000a</math>\u000aand then <math>s_{k+1} (a,b)</math> is iteratively computed from <math>Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j)</math> as\u000a:<math>\u000as_{k+1}( a,b )=\u005ctfrac{C}{| \u005cmathsf{\u005cmathcal{I}}( a ) | | \u005cmathsf{\u005cmathcal{I}}( b ) |}\u005csum_{j \u005cin \u005cmathsf{\u005cmathcal{I}}( b ) } Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j).\u000a</math>\u000aConsequently, the results of <math>Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j)</math>, <math>\u005cforall j \u005cin {\u005cmathcal I}(b)</math>,\u000acan be reused later when we compute the similarities <math>s_{k+1}(a,*)</math> for a given vertex <math>a</math> as the first argument.\u000a\u000a== Further research on SimRank ==\u000a\u000a* Fogaras and Racz <ref name="fogaras_racz">D. Fogaras and B. Racz. Scaling link-based similarity search. In [[World Wide Web Conference|WWW '05]]: Proceedings of the 14th international conference on World Wide Web, pages 641--650, New York, NY, USA, 2005. [[Association for Computing Machinery|ACM]]. [http://www2005.org/docs/p641.pdf]</ref> suggested speeding up SimRank computation through [[Probability theory|probabilistic]] computation using the [[Monte Carlo method]].\u000a\u000a* Antonellis et al.<ref name="simrank_plusplus">I. Antonellis, H. Garcia-Molina and C.-C. Chang. Simrank++: Query Rewriting through Link Analysis of the Click Graph. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 408--421. [http://dbpubs.stanford.edu/pub/showDoc.Fulltext?lang=en&doc=2008-17&format=pdf&compression=&name=2008-17.pdf]</ref> extended SimRank equations to take into consideration (i) evidence factor for [[Graph (mathematics)#Properties of graphs|incident nodes]] and (ii) link weights.\u000a\u000a* Lizorkin et al.<ref name="lizorkin"/> proposed several [[Optimization (computer science)|optimization]] techniques for speeding up SimRank iterative computation.\u000a\u000a* Yu et al.<ref name="yu_icde13">W. Yu, X. Lin, W. Zhang. Towards Efficient SimRank Computation on Large Networks. In [[International Conference on Data Engineering|ICDE '13]]: Proceedings of the 29th IEEE International Conference on Data Engineering, pages 601--612. [http://www.cse.unsw.edu.au/~weirenyu/pubs/icde13.pdf]</ref> further improved SimRank computation via a fine-grained [[memoization]] method to share small common parts among different partial sums.\u000a\u000a== See also ==\u000a\u000a* [[PageRank]]\u000a\u000a== Citations ==\u000a{{reflist|colwidth=30em}}\u000a\u000a[[Category:Information retrieval]]
p73
sg4
S'43'
p74
sg6
VSimRank
p75
ssI173
(dp76
g2
V'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.\u000aMultimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.\u000aWe can distinguish two methodologies in multimedia search:\u000a*'''Metadata search''': the search is made on the layers of [[metadata]].\u000a* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.\u000a\u000a\u000a==Metadata search==\u000a\u000aSearch is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.\u000a\u000aThere are three processes which should be done in this method:\u000a*'''[[Multimedia Information Retrieval#Feature Extraction Methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.\u000a*'''[[ Multimedia Information Retrieval#Feature Extraction Methods |Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])\u000a*'''[[ Multimedia Information Retrieval#Categorization Methods | Categorization of media descriptions ]]''' into classes.\u000a\u000a==[[Query by Example]]==\u000a\u000aIn [[query by example]] the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often it\u2019s used [[Search engine indexing |audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:\u000a*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].\u000a*Compare descriptors of the query and our database\u2019s media.\u000a*List the media sorted by maximum coincidence.\u000a\u000a==Multimedia search engine==\u000aThere are two big search families, in function of the content:\u000a* [[Visual search engine]]\u000a*[[Audio search engine]]\u000a\u000a===[[Visual search engine]]===\u000aInside this family we can distinguish two topics: [[image search]] and [[video search]]\u000a\u000a*'''[[Image search]]''': Although usually it\u2019s used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example [[QR codes]].\u000a*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.\u000a\u000a===[[Audio search engine]]===\u000aThere are different methods of audio searching:\u000a*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].\u000a*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album\u2026) . There are some programs of [[music recognition]]. for example: [[Shazam (service)|Shazam]] or [[SoundHound]].\u000a\u000a==See also==\u000a*[[Search engine indexing]]\u000a*[[Multimedia]]\u000a*[[Multimedia Information Retrieval]]\u000a*[[Streaming media]]\u000a*[[Journal of Multimedia]]\u000a*[[List of search engines#Multimedia|List of search engines]]\u000a*[[Video search engine]]\u000a\u000a==External links==\u000a\u000a[[Category:Searching]]\u000a[[Category:Multimedia]]
p77
sg4
S'173'
p78
sg6
VMultimedia search
p79
ssI48
(dp80
g2
V{{Orphan|date=February 2009}}\u000aA '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computing terminology]]\u000a\u000a\u000a{{Web-stub}}
p81
sg4
S'48'
p82
sg6
VCommunication engine
p83
ssI178
(dp84
g2
V{{unreferenced|date=October 2007}}\u000aThe use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.\u000a\u000aIn a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.\u000a\u000aIn a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.\u000a\u000aThe benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. \u000a\u000a== Contrast with ==\u000a* [[Service-oriented architecture]] (SOA)\u000a* [[Service-Oriented Modeling]]\u000a\u000a== See also ==\u000a* [[Hibernate search]]\u000a \u000a[[Category:Software architecture]]\u000a[[Category:Data search engines]]\u000a[[Category:Searching]]
p85
sg4
S'178'
p86
sg6
VSearch-oriented architecture
p87
ssI53
(dp88
g2
V{{Multiple issues|\u000a{{unreferenced|date=March 2009}}\u000a{{orphan|date=February 2009}}\u000a{{confusing|date=March 2009}}\u000a}}\u000a\u000a'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.\u000a\u000aNegative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.\u000a\u000aNegative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.\u000a\u000aNegative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.\u000a\u000aExamples of Negative Intent are:\u000a\u000a- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.\u000a\u000a- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.\u000a\u000a- An investigator is looking for a car but has no other information on that car on which to base a search.\u000a\u000a==Negative Search Classifiers==\u000a\u000aIf there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.\u000a\u000a[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.\u000a\u000a==Irrelevancy as a Desirable Construct==\u000a\u000aPositive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.\u000a\u000aIt follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.\u000a\u000a==Degrees of Passivity==\u000a\u000aPositive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\u005c's Eve|New Years Eve]]."\u000a\u000aDiscovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"\u000a\u000aNegative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."\u000a\u000aSearchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]
p89
sg4
S'53'
p90
sg6
VNegative search
p91
ssI183
(dp92
g2
V{{Use dmy dates|date=February 2011}}\u000a{{Infobox Windows component\u000a| name                = Indexing Service\u000a| screenshot          = Indexing Service Query Form.PNG\u000a| screenshot_size     = 300px\u000a| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].\u000a| type                = [[Desktop search]]\u000a| service_name        = Indexing Service\u000a| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.\u000a| replaced_by         = [[Windows Search]]\u000a| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />\u000a}}\u000a\u000a'''Indexing Service''' (originally called '''Index server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by [[Windows Search]].\u000a\u000a== History ==\u000aIndexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}\u000a\u000aIn [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />\u000a\u000aIndexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].\u000a\u000a== Search interfaces ==\u000a\u000aComprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.\u000a\u000aOnce the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.\u000a\u000aMicrosoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web\u000a | url = http://support.microsoft.com/kb/319506\u000a | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)\u000a | work = Microsoft Support\u000a | publisher = 10 May 2002\u000a | accessdate = 1 February 2011\u000a}}</ref>\u000a\u000a== References ==\u000a{{Reflist|refs=\u000a<ref name = "MIS-Intro">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx\u000a  |title = Introduction to Microsoft Index Server\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date = 15 October 1997\u000a  |accessdate = 1 February 2011\u000a  |first1 = Krishna\u000a  |last1 = Nareddy\u000a  }}</ref>\u000a<ref name = "MIS-v3">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx\u000a  |title = Indexing Service Version 3.0\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name = "WIS-What">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx\u000a  |title = What is Indexing Service?\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name="WIS-Install2008">{{Cite web\u000a  |url = http://support.microsoft.com/kb/954822\u000a  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)\u000a  |work = Microsoft Support\u000a  |publisher = Microsoft Corporation\u000a  |date = 3 May 2010\u000a  |accessdate = 1 February 2011\u000a  }}</ref>\u000a<ref name="TnC-144">{{Cite book\u000a  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en\u000a  |format = Microsoft Word\u000a  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP\u000a  |edition = 2.0\u000a  |publisher = Microsoft Corporation\u000a  |page = 144\u000a  |date=December 2005\u000a  |first1 = Mike\u000a  |last1  = Danseglio\u000a  |first2 = Kurt\u000a  |last2  = Dillard\u000a  |first3 = José\u000a  |last3  = Maldonado\u000a  |first4 = Paul\u000a  |last4  = Robichaux\u000a  |editor1-first = Reid\u000a  |editor1-last  = Bannecker\u000a  |editor2-first = John\u000a  |editor2-last  = Cobb\u000a  |editor3-first = Jon\u000a  |editor3-last  = Tobey\u000a  |editor4-first = Steve\u000a  |editor4-last  = Wacker\u000a  }}</ref>\u000a}}\u000a\u000a{{DEFAULTSORT:Indexing Service}}\u000a[[Category:Windows communication and services]]\u000a[[Category:Desktop search engines|Desktop search engines]]\u000a[[Category:Searching]]\u000a[[Category:Windows components]]
p93
sg4
S'183'
p94
sg6
VIndexing Service
p95
ssI58
(dp96
g2
V{{Cleanup|date=March 2011}}\u000a''' \u000aA Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query \u201c''apple''\u201d might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.\u000a\u000a== KDDCUP 2005 ==\u000a\u000aKDDCUP 2005 competition<ref>[http://www.sigkdd.org/kdd2005/kddcup.html KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query \u201c''apple''\u201d, it should be classified into ranked categories: \u201c''Computers \u005c Hardware''; ''Living \u005c Food & Cooking''\u201d.\u000a\u000a{| class="wikitable"\u000a|-\u000a! Query\u000a! Categories\u000a|-\u000a| apple\u000a| Computers \u005c Hardware<br />Living \u005c Food & Cooking\u000a|-\u000a| FIFA 2006\u000a| Sports \u005c Soccer<br />Sports \u005c Schedules & Tickets<br />Entertainment \u005c Games & Toys\u000a|-\u000a| cheesecake recipes\u000a| Living \u005c Food & Cooking<br />Information \u005c Arts & Humanities\u000a|-\u000a| friendships poem\u000a| Information \u005c Arts & Humanities<br />Living \u005c Dating & Relationships\u000a|}\u000a\u000a[[Image:Web query length.gif]]\u000a[[Image:Web query meaning.gif]]\u000a\u000a== Difficulties ==\u000a\u000aWeb query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:\u000a\u000a=== How to derive an appropriate feature representation for Web queries? ===\u000a\u000aMany queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.\u000a\u000a* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).\u000a\u000aHow about disadvantages and advantages??\u000agive the answers:\u000a\u000a=== How to adapt the changes of the queries and categories over time? ===\u000a\u000aThe meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.\u000a\u000a* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.\u000a\u000a=== How to use the unlabeled query logs to help with query classification? ===\u000a\u000aSince the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.\u000a\u000a* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.</ref> tries to associate related queries by clustering \u201csession data\u201d, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.\u000a\u000a* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.\u000a\u000a== Applications ==\u000a\u000a* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.\u000a* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.\u000a* '''[[Online advertising]]'''<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.\u000aAll these services rely on the understanding Web users' search intents through their Web queries.\u000a\u000a== See also ==\u000a\u000a* [[Document classification]]\u000a* [[Web search query]]\u000a* [[Information retrieval]]\u000a* [[Query expansion]]\u000a* [[Naive Bayes classifier]]\u000a* [[Support vector machines]]\u000a* [[Meta search]]\u000a* [[Vertical search]]\u000a* [[Online advertising]]\u000a\u000a== References ==\u000a\u000a{{reflist}}\u000a\u000a== Further reading ==\u000a* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Web Query Classification}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search]]
p97
sg4
S'58'
p98
sg6
VWeb query classification
p99
ssI188
(dp100
g2
V{{copy edit|date=October 2013}}\u000a\u000a{{Use dmy dates|date=November 2011}}\u000a\u000a'''Search-based software engineering''' ('''SBSE''') is an approach to apply [[metaheuristic]] search techniques like [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. It is inspired by the observation that many activities in [[software engineering]] can be formulated as [[Optimization (mathematics)|optimization]] problems. Due to the [[computational complexity]] of these problems, exact [[Optimization (mathematics)|optimization]] techniques of [[operations research]] like [[linear programming]] or [[dynamic programming]] are mostly impractical for large scale [[software engineering]] problems. Because of this, researchers and practitioners have used [[metaheuristic]] search techniques to find near optimal or good-enough solutions.\u000a\u000aBroadly speaking SBSE problems can be divided into two types. The first are black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem). \u000aWith this sort of problem domain, the underlying problem could have come from the software industry, but equally it could have originated from any domain where people are assigned tasks. \u000aThe second type are white-box problems where operations on source code need to be considered.<ref>\u000a{{Cite conference\u000a| doi = 10.1109/SCAM.2010.28\u000a| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| pages = 7\u201319\u000a| last = Harman\u000a| first = Mark\u000a| title = Why Source Code Analysis and Manipulation Will Always be Important\u000a| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| year = 2010\u000a}}</ref>\u000a\u000a__TOC__\u000a\u000a==Definition==\u000a\u000aThe basic idea of SBSE is to take a software engineering problem and convert it into a computational search problem which can be tackled with a [[metaheuristic]]. \u000aThis essentially involves a number of stages. Firstly defining a search space (the set of possible solutions to the problem). \u000aThis space is typically too large to be explored exhaustively and therefore a  [[metaheuristic]] is employed to sample this space. \u000aSecondly, a metric <ref>\u000a{{Cite conference\u000a| doi = 10.1109/METRIC.2004.1357891\u000a| conference = 10th International Symposium on Software Metrics, 2004\u000a| pages = 58\u201369\u000a| last = Harman\u000a| first = Mark\u000a|author2=John A. Clark\u000a | title = Metrics are fitness functions too\u000a| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 \u000a| year = 2004\u000a}}</ref> (also called a fitness function, cost function, objective function or quality measure) is used to measure the quality of a potential solution. Many software engineering problems can be reformulated as a computational search problem.<ref>{{Cite journal\u000a| doi = 10.1049/ip-sen:20030559\u000a| issn = 1462-5970\u000a| volume = 150\u000a| issue = 3\u000a| pages = 161\u2013175\u000a| last = Clark\u000a| first = John A.\u000a| coauthors = Dolado, José Javier; Harman, Mark; Hierons, Robert M.; Jones, Bryan F.; Lumkin, M.; Mitchell, Brian S.; Mancoridis, Spiros; Rees, K.; Roper, Marc; Shepperd, Martin J.\u000a| title = Reformulating software engineering as a search problem\u000a| journal = IEE Proceedings - Software \u000a| year = 2003\u000a}}</ref>\u000a\u000aThe term "[[search-based application]]", in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.\u000a\u000a==Brief history==\u000a\u000aOne of the earliest attempts in applying [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of software testing.<ref>\u000a{{Cite journal\u000a| doi = 10.1109/TSE.1976.233818\u000a| issn = 0098-5589\u000a| volume = SE-2\u000a| issue = 3\u000a| pages = 223\u2013226\u000a| last = Miller\u000a| first = Webb\u000a| last2 = Spooner\u000a| first2 = David L. \u000a| title = Automatic Generation of Floating-Point Test Data\u000a| journal = IEEE Transactions on Software Engineering\u000a| year = 1976\u000a}}</ref> \u000aIn 1992, Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.<ref>S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, "Application of genetic algorithms to software testing," in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&nbsp;625\u2013636</ref> \u000aThe term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00189-6\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 833\u2013839\u000a| last = Harman\u000a| first = Mark\u000a| last2 = Jones\u000a| first2 = Bryan F.\u000a| title = Search-based software engineering\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001896\u000a}}</ref> Since then, the research community has grown to include more than 800 authors in 2013, from approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}\u000a\u000a==Application areas==\u000a\u000aSearch-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications of search techniques in [[software engineering]].<ref>\u000a{{Cite journal\u000a| doi = 10.1002/stvr.294\u000a| issn = 1099-1689\u000a| volume = 14\u000a| issue = 2\u000a| pages = 105\u2013156\u000a| last = McMinn\u000a| first = Phil\u000a| title = Search-based software test data generation: a survey\u000a| journal = Software Testing, Verification and Reliability\u000a| accessdate = 2013-10-31\u000a| year = 2004\u000a| url = http://onlinelibrary.wiley.com/doi/10.1002/stvr.294/abstract\u000a}}</ref> Search techniques have also been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.infsof.2003.07.002\u000a| issn = 0950-5849\u000a| volume = 46\u000a| issue = 4\u000a| pages = 243\u2013253\u000a| last = Greer\u000a| first = Des\u000a| last2 = Ruhe\u000a| first2 = Guenther\u000a| title = Software release planning: an evolutionary and iterative approach\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-09-06\u000a| date = 2004-03-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S095058490300140X\u000a}}</ref>\u000a<ref>{{Cite conference\u000a| doi = 10.1109/SBES.2009.23\u000a| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| pages = 207\u2013215\u000a| last = Colares\u000a| first = Felipe\u000a| last2 = Souza\u000a| first2 = Jerffeson\u000a| last3 = Carmo\u000a| first3 = Raphael\u000a| last4 = Pádua\u000a| first4 = Clarindo\u000a| last5 = Mateus\u000a| first5 = Geraldo R.\u000a| title = A New Approach to the Software Release Planning\u000a| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| year = 2009\u000a}}</ref> [[software design]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00195-1\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 891\u2013904\u000a| last = Clark\u000a| first = John A.\u000a| last2 = Jacob\u000a| first2 = Jeremy L. \u000a| title = Protocols are programs too: the meta-heuristic search for security protocols\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001951\u000a}}</ref> [[software development]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.ins.2006.12.020\u000a| issn = 0020-0255\u000a| volume = 177\u000a| issue = 11\u000a| pages = 2380\u20132401\u000a| last = Alba\u000a| first = Enrique\u000a| last2 = Chicano\u000a| first2 = J. Francisco \u000a| title = Software project management with GAs\u000a| journal = Information Sciences\u000a| accessdate = 2013-10-31\u000a| date = 2007-06-01\u000a| url = http://www.sciencedirect.com/science/article/pii/S0020025507000175\u000a}}</ref> and [[software maintenance]].<ref>\u000a{{Cite conference\u000a| doi = 10.1109/ICSM.2005.79\u000a| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| pages = 240\u2013249\u000a| last = Antoniol\u000a| first = Giuliano\u000a| last2 = Di Penta\u000a| first2 = Massimiliano \u000a| last3 = Harman\u000a| first3 = Mark\u000a| title = Search-based techniques applied to optimization of project planning for a massive maintenance project\u000a| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| year = 2005\u000a}}</ref>\u000a\u000a===Requirements engineering===\u000a\u000a[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches users' requests and different constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally speaking, involves presenting the decision maker with a range of good compromises between cost and user satisfaction.<ref>\u000a{{Cite thesis\u000a| type = Ph.D.\u000a| publisher = University of London\u000a| last = Zhang\u000a| first = Yuanyuan\u000a| title = Multi-Objective Search-based Requirements Selection and Optimisation\u000a| location = Strand, London, UK\u000a| date = February 2010\u000a| url = http://eprints.ucl.ac.uk/170695/\u000a}}</ref>\u000a<ref>\u000aY.&nbsp;Zhang and M.&nbsp;Harman and S.&nbsp;L.&nbsp;Lim, "[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management]," Department of Computer Science, University College London, Research Note RN/11/12, 2011.\u000a</ref>\u000a\u000a===Debugging and maintenance===\u000a\u000aIdentifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is supported by a number of tools. One objective of SBSE is to automatically identify bugs (for example via [[mutation testing]]), then automatically fix them.\u000a\u000a[[Genetic programming]], a biologically-inspired technique which involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software was shown to be able to repair 55 out of 105 bugs for approximately $8 each.<ref>{{Cite conference\u000a| doi = 10.1109/ICSE.2012.6227211\u000a| conference = 2012 34th International Conference on Software Engineering (ICSE)\u000a| pages = 3\u201313\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Dewey-Vogt\u000a| first2 = Michael\u000a| last3 = Forrest\u000a| first3 = Stephanie\u000a| last4 = Weimer\u000a| first4 = Westley \u000a| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each\u000a| booktitle = 2012 34th International Conference on Software Engineering (ICSE)\u000a| year = 2012\u000a}}</ref>\u000a\u000a[[Coevolution]] has also been used as an approach. It follows a predator and prey metaphor where a population of programs and a population of [[Unit Testing|unit tests]] evolve together and influence each other.<ref>{{Cite conference\u000a| doi = 10.1109/CEC.2008.4630793\u000a| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| pages = 162\u2013168\u000a| last = Arcuri\u000a| first = Andrea\u000a| last2 = Yao\u000a| first2 = Xin \u000a| title = A novel co-evolutionary approach to automatic software bug fixing\u000a| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| year = 2008\u000a}}</ref>\u000a\u000a===Testing===\u000a\u000aSearch-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.\u000a\u000a===Optimizing software===\u000aThe use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of developing research interest and success. Genetic programming has been used to improve programs. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.<ref>\u000a{{Cite journal\u000a| last = Langdon\u000a| first = William B.\u000a| last2 = Harman\u000a| first2 = Mark \u000a| title = Optimising Existing Software with Genetic Programming\u000a| journal = IEEE Transactions on Evolutionary Computation\u000a| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf\u000a}}</ref>\u000a\u000a===Project management===\u000aA number of decisions which are normally made by a project manager can be done automatically, for example, project scheduling.<ref>\u000a{{Cite conference\u000a| publisher = ACM\u000a| doi = 10.1145/2330163.2330332\u000a| isbn = 978-1-4503-1177-9\u000a| pages = 1221\u20131228\u000a| last = Minku\u000a| first = Leandro L.\u000a| last2 = Sudholt\u000a| first2 = Dirk\u000a| last3 = Yao\u000a| first3 = Xin \u000a| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design\u000a| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference\u000a| location = New York, NY, USA\u000a| series = GECCO '12\u000a| accessdate = 2013-10-31\u000a| year = 2012\u000a| url = http://doi.acm.org/10.1145/2330163.2330332\u000a}}</ref>\u000a\u000a==Tools==\u000a\u000aThere are a number of tools available for SBSE approaches. These include tools like [[OpenPAT]].<ref> \u000a{{cite conference\u000a|ref        = harv\u000a|last       = Mayo\u000a|first      = M.\u000a|coauthors  = Spacey, S.\u000a|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics\u000a|url        = http://rd.springer.com/chapter/10.1007/978-3-642-39742-4_13\u000a|format     = PDF\u000a|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)\u000a|volume     = 8084\u000a|pages      = 158\u2013171\u000a|year       = 2013\u000a}}</ref>\u000aand Evosuite <ref>(http://www.evosuite.org/)</ref>\u000aand a code coverage measurement for Python\u000a<ref>\u000ahttps://pypi.python.org/pypi/coverage\u000a</ref>\u000a\u000a==Methods and techniques==\u000a\u000aThere are a number of methods and techniques available. \u000aA non-exhaustive list of these tools includes:\u000a\u000a\u2022[[profiling (computer programming)|Profiling]]\u000a<ref>http://java-source.net/open-source/profilers</ref> via [[instrumentation]] in order to monitor certain parts of a program as it is executed.\u000a\u000a\u2022Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into the structure of a program.\u000a\u000a\u2022Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]], [[Program analysis (computer science)|program analysis]].\u000a\u000a\u2022[[Code coverage]] allows measuring how much of the code is executed with a given \u000aset of input data.\u000a\u000a\u2022[[Static program analysis]]\u000a\u000a==Industry acceptance==\u000a\u000aAs a relatively new area of research, SBSE does not yet benefit from broad industry acceptance. One issue is that software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are quite different from the ones humans would produce.<ref>\u000a{{cite web\u000a |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/\u000a |title      = Programming using genetic algorithms: isn\u2019t that what humans already do ;-)\u000a |last       = Jones\u000a |first      = Derek\u000a |date       = 18 October 2013\u000a |website    = The Shape of Code\u000a |accessdate = 31 October 2013\u000a}}\u000a</ref>\u000aIn the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to favor program maintainability.<ref>\u000a{{Cite journal\u000a| doi = 10.1007/s11219-013-9208-0\u000a| issn = 1573-1367\u000a| volume = 21\u000a| issue = 3\u000a| pages = 421\u2013443\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Forrest \u000a| first2 = Stephanie \u000a| last3 = Weimer\u000a| first3 = Westley\u000a| title = Current challenges in automatic software repair\u000a| journal = Software Quality Journal\u000a| accessdate = 2013-10-31\u000a| date = 2013-09-01\u000a| url = http://link.springer.com/article/10.1007/s11219-013-9208-0\u000a}}\u000a</ref>\u000a\u000aAnother concern is that SBSE might make the software engineer redundant. Researchers have argued that, on the contrary, the motivation for SBSE is to enhance the relationship between the engineer and the program.<ref>\u000a{{Cite conference\u000a| publisher = IEEE Press\u000a| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering\u000a| pages = 49\u201350\u000a| last = Simons\u000a| first = Christopher L.\u000a| title = Whither (away) software engineers in SBSE?\u000a| location = San Francisco, USA\u000a| accessdate = 2013-10-31\u000a| date = May 2013\u000a| url = http://eprints.uwe.ac.uk/19938/\u000a}}</ref>\u000a\u000a==See also==\u000a{{Portal|Software Testing}}\u000a*[[Program analysis (computer science)]]\u000a*[[Dynamic program analysis]]\u000a\u000a==References==\u000a{{reflist|colwidth=30em}}\u000a\u000a==External links==\u000a*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]\u000a*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]\u000a*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]\u000a*[http://2013.icse-conferences.org/ International Conference on Software Engineering]\u000a*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]\u000a*[http://scholar.google.co.uk/citations?view_op=search_authors&hl=en&mauthors=label:sbse Google Scholar page on Search-based software engineering]\u000a\u000a[[Category:2001 introductions]]\u000a[[Category:Software engineering]]\u000a[[Category:Software testing]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]\u000a[[Category:Optimization algorithms and methods]]\u000a[[Category:Genetic algorithms]]
p101
sg4
S'188'
p102
sg6
VSearch-based software engineering
p103
ssI63
(dp104
g2
VThe '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the intersection divided by the smaller of the size of the two sets:\u000a\u000a:<math>\u005cmathrm{overlap}(X,Y) = \u005cfrac{| X \u005ccap Y | }{\u005cmin(|X|,|Y|)}</math>\u000a\u000aIf set ''X'' is a subset of ''Y'' or the converse then the overlap coefficient is equal to one.\u000a\u000a== External links==\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/OverlapMetric.scala Overlap] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p105
sg4
S'63'
p106
sg6
VOverlap coefficient
p107
ssI193
(dp108
g2
V[[Category:Searching]]\u000a[[Category:Data search engines]]\u000a[[Category:Utility software by type]]\u000a[[Category:Marketing software]]\u000a[[Category:Web software]]
p109
sg4
S'193'
p110
sg6
VCategory:Search engine software
p111
ssI68
(dp112
g2
V{{mergeto|Faceted classification|date=January 2015}}\u000a'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>\u000a\u000aFacets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.\u000a\u000aWithin the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].{{fact|date=May 2014}}\u000a\u000a==Development==\u000a\u000aThe [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:\u000a<blockquote>\u000aThe web search world, since its very beginning, has offered two paradigms:\u000a*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.\u000a*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. \u000aOver the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>\u000a</blockquote>\u000a\u000a==Technology==\u000a\u000aVarious search engine software supports faceted classification.\u000a\u000a* [[Apache Lucene]] and derived software:\u000a**  [[Apache Solr]]\u000a** [[Swiftype]]\u000a** [[Elasticsearch]]\u000a* A number of major vendors listed at [[Comparison of enterprise search software#Faceted_Navigation]]\u000a* [[Dieselpoint]]\u000a* [[Endeca]]\u000a* iSeek, search engine for general web and education<ref>[http://www.iseek.com iSeek]</ref>\u000a* [[SpeedTrack]]<ref>http://www.speedtrack.com/technology</ref>\u000a* XSEARCH<ref>[http://www.weitkamper.com]</ref>\u000a\u000a==Mass market use==\u000a\u000aFaceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] such as [[Swiftype]] provide software for implementing faceted search applications.\u000a\u000aOnline retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.\u000a\u000a==Libraries and information science==\u000a\u000a\u000a\u000a\u000aIn 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[http://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>\u000a\u000aModern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system.\u000a\u000aInfoHarness<ref>{{cite journal|last1=Shklar|first1=Leon|last2=Thatte|first2=Satish|last3=Marcus|first3=Howard|last4=Sheth|first4=Amit|title=The "InfoHarness" Information Integration Platform|journal=Proceedings of the Second International Conference on the World Wide Web|date=1994|url=http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.43.4042|accessdate=5 January 2015}}</ref> <ref>{{cite journal|last1=Shklar|first1=Leon|last2=Sheth|first2=Amit|last3=Kashyap|first3=Vipul|last4=Shah|first4=Kshitij|title=InfoHarness: Use of automatically generated metadata for search and retrieval of heterogeneous information|journal=Advanced Information Systems Engineering|date=20 July 2005|doi=10.1007/3-540-59498-1_248|url=http://link.springer.com/chapter/10.1007/3-540-59498-1_248|accessdate=5 January 2015|ref=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.7442}}</ref> is one of the first Web System (developed in 1994) that provided faceted search over heterogeneous information artifacts such as Web pages, images, videos and documents. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.\u000a\u000a==See also==\u000a* [[Enterprise Search]]\u000a* [[Exploratory search]]\u000a* [[Faceted classification]]\u000a* [[Human\u2013computer information retrieval]]\u000a* [[Information Extraction]]\u000a* [[NoSQL]]\u000a* [[Trove (website)]]\u000a\u000a==References==\u000a<References/>\u000a\u000a{{DEFAULTSORT:Faceted Search}}\u000a[[Category:Information retrieval]]
p113
sg4
S'68'
p114
sg6
VFaceted search
p115
ssI198
(dp116
g2
VSearching by sound for now has limited uses. There are a handful of applications, specifically for mobile devises that utilizes searching by sound. [[Shazam (service)]], [[Soundhound]], Midomi, and others has seen considerable success by using a simple algorithm to match an acoustic fingerprint to a song in a library. These applications takes a sample clip of a song, or a user generated melody and checks a music library to see where the clip matches with the song. From there, song information will be pulled up and displayed to the user. \u000a\u000aThese kind of applications is mainly used for finding a song that the user does not already know. \u000a\u000aSearching by sound is not limited so just identifying [[songs]], but also for identifying [[melodies]], [[Music|tunes]] or [[advertisements]], [[sound library management]] and [[video files]].\u000a\u000a==Acoustic Fingerprinting==\u000aThe way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. A microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. Using the same method of fingerprinting sounds, when Shazam picks up a sound clip, it will generate a signature for that clip. Then it\u2019s simple pattern matching from there using an extensive audio music database. \u000a\u000aThe practice of using [[acoustic fingerprints]] is not limited to just music however, but other areas of the entertainment business as well. Shazam also can identify television shows with the same technique of acoustic fingerprinting. Of course, this method of breaking down a sound sample into a unique signature is useless unless there is an extensive database of music with keys to match with the samples. Shazam has over 11 million songs in its database. <ref> http://www.slate.com/articles/technology/technology/2009/10/that_tune_named.html </ref>\u000a\u000aOther services such as Midomi and Soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound. \u000a\u000a==Spectogram==\u000aGenerating a signature from the song is essential for searching by sound, and can be tricky. However, the way certain applications such as Shazam found a way around this issue by creating a spectrogram. \u000a\u000aAny piece of music can be translated to a time frequency graph called a spectrogram. For each song in its database, each song is basically a graph that plots the three dimensions of music, frequency vs amplitude (intensity) vs time. The algorithm then picks out the points which peaks in the graph, labeled as \u201chigher energy content\u201d. In practice, this seems to work out to about three points per song. <ref> http://www.soyoucode.com/2011/how-does-shazam-recognize-song </ref>\u000a\u000aThis is how a song can be identified with just two or three notes. This greatly reduces the impact that [[background noise]] has on searching by sound. The key values taken away from this would be frequency in hertz and time in seconds. Shazam builds their fingerprint catalog out as a hash table, where the key is the frequency. They do not just mark a single point in the spectrogram, rather they mark a pair of points: the \u201cpeak intensity\u201d plus a second \u201canchor point\u201d. <ref> Li-Chun Wang, Avery. "An Industrial-Strength Audio Search Algorithm." Columbia University. Web. 1 Dec. 2014. <http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf>. </ref> So their key is not just a single frequency, it is a hash of the frequencies of both points.  This leads to less hash collisions which in turn speeds up catalog searching by several orders of magnitude by allowing them to take greater advantage of the table\u2019s constant (O(1)) look-up time. <ref> "How Shazam Works." Free Wont. Web. 1 Dec. 2014. <http://laplacian.wordpress.com/2009/01/10/how-shazam-works/>. </ref>\u000a\u000aThis method of acoustic fingerprinting allows applications such as Shazam to have the ability to differentiate between two closely related covers, as well as not having to account for popularity of a certain song. \u000a\u000a==Query by Humming==\u000aMidomi and Soundhound both utilize Query by Humming, or QbH. This is a branch off of acoustic fingerprints, but is still a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query. \u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a\u000a\u000a[[Category:Searching]]
p117
sg4
S'198'
p118
sg6
VSearch by sound
p119
ssI73
(dp120
g2
V{{Orphan|date=September 2012}}\u000a\u000a'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.\u000a\u000a==Berrypicking==\u000aOne way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." http://www.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>\u000a\u000aBates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.\u000a\u000a==Exploratory Search==\u000aResearchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.\u000a\u000a==Natural language searching==\u000a\u000aAnother way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Cognitive modeling]]
p121
sg4
S'73'
p122
sg6
VCognitive models of information retrieval
p123
ssI203
(dp124
g2
V{{redirect|String distance|the distance between strings and the fingerboard in musical instruments|Action (music)}}\u000a\u000aIn [[mathematics]] and [[computer science]], a '''string metric''' (also known as a '''string similarity metric''' or '''string distance function''') is a [[metric (mathematics)|metric]] that measures [[distance]] ("inverse similarity") between two [[string (computer science)|text strings]] for [[approximate string  matching]] or comparison and in [[approximate string  matching|fuzzy string searching]]. Necessary requirement for a string ''metric'' (e.g. in contrast to [[string matching]]) is fulfillment of the [[triangle inequality]]. For example the strings "Sam" and "Samuel" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.\u000a\u000aThe most widely known string metric is a rudimentary one called the [[Levenshtein distance|Levenshtein Distance]] (also known as Edit Distance).  It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as [[Levenshtein distance]] have expanded to include phonetic, [[token (parser)|token]], grammatical and character-based methods of statistical comparisons.\u000a\u000aA widespread example of a string metric is [[DNA]] [[sequence analysis]] and RNA analysis, which are performed by optimized string metrics to identify matching sequences.\u000a\u000aString metrics are used heavily in [[information integration]] and are currently used in areas including [[Data analysis techniques for fraud detection|fraud detection]], [[fingerprint analysis]], [[plagiarism detection]], [[ontology merging]], [[DNA analysis]], RNA analysis, [[image analysis]], evidence-based machine learning, [[database]] [[data deduplication]], [[data mining]], Web interfaces, e.g. [[Ajax (programming)|Ajax]]-style suggestions as you type, [[data integration]], and semantic [[knowledge integration]].\u000a\u000a==List of string metrics==\u000a\u000a<!-- This can be a separate article, someday. -->\u000a* [[Sørensen\u2013Dice coefficient]]\u000a* [[Hamming distance]]\u000a* [[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a* [[Block distance]] or [[L1 distance]] or [[City block distance]]\u000a* [[Simple matching coefficient]] (SMC)\u000a* [[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a* [[Most frequent k characters]]\u000a* [[Tversky index]]\u000a* [[Overlap coefficient]]\u000a* [[Variational distance]]\u000a* [[Hellinger distance]] or [[Bhattacharyya distance]]\u000a* [[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a* [[Skew divergence]]\u000a* [[Confusion probability]]\u000a* [[Kendall_tau_distance|Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a* [[Fellegi and Sunters metric]] (SFS)\u000a* [[Maximal matches]]\u000a* [[Lee distance]]\u000a\u000a==Selected string measures examples==\u000a\u000a{| class="wikitable"\u000a|-\u000a! Name\u000a! Example\u000a|-\u000a|[[Hamming distance]]\u000a| "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.\u000a|-\u000a|[[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a| \u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a<!--|-\u000a|[[Simple matching coefficient]] (SMC)\u000a|-->\u000a<!--|-\u000a|-\u000a|[[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a|-->\u000a|-\u000a|[[Most frequent k characters]]\u000a|MostFreqKeySimilarity('<span style="color:red;">r</span><span style="color:#0082ff">e</span>s<span style="color:#0082ff">e</span>a<span style="color:red;">r</span>ch', 's<span style="color:#0082ff">ee</span>king', 2) = 2\u000a<!--|-\u000a|[[Tversky index]]\u000a|-->\u000a<!--|-\u000a|[[Overlap coefficient]]\u000a|-->\u000a<!--|-\u000a|[[Variational distance]]\u000a|-->\u000a<!--|-\u000a|[[Hellinger distance]] or [[Bhattacharyya distance]]\u000a|-->\u000a<!--|-\u000a|[[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a|-->\u000a<!--|-\u000a|[[Skew divergence]]\u000a|-->\u000a<!--|-\u000a|[[Confusion probability]]\u000a|-->\u000a<!--|-\u000a|[[Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a|-->\u000a<!--|-\u000a|[[Fellegi and Sunters metric]] (SFS)\u000a|-->\u000a<!--|-\u000a|[[Maximal matches]]\u000a|-->\u000a|}\u000a\u000a==See also==\u000a* [[approximate string  matching]]\u000a* [[String matching]]\u000a* [http://www.speech.cs.cmu.edu/ Carnegie Mellon University open source library]\u000a* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms\u000a* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics\u000a\u000a==External links==\u000a*http://www.dcs.shef.ac.uk/~sam/stringmetrics.html {{Dead link|date=July 2011}} A fairly complete overview {{wayback|url=http://www.dcs.shef.ac.uk/~sam/stringmetrics.html#ukkonen}}\u000a\u000a{{DEFAULTSORT:String Metric}}\u000a[[Category:String similarity measures| ]]\u000a[[Category:Metrics]]\u000a\u000a[[de:Ähnlichkeitsanalyse]]
p125
sg4
S'203'
p126
sg6
VString metric
p127
ssI78
(dp128
g2
V{{Unreferenced|date=January 2010}}\u000a\u000aThe '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}\u000a\u000a==European Conference on Information Retrieval==\u000aOrganising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.\u000a\u000a== External links ==\u000a* [http://irsg.bcs.org/ IRSG website]\u000a\u000a[[Category:Information retrieval|Specialist Group]]\u000a[[Category:BCS Specialist Groups]]
p129
sg4
S'78'
p130
sg6
VInformation Retrieval Specialist Group
p131
ssI208
(dp132
g2
V{{About|the measure|other uses|Jaro (disambiguation){{!}}Jaro}}\u000a\u000a{{Original research|date=May 2013}}\u000a{{Notability|date=May 2013}}\u000a\u000aIn [[computer science]] and [[statistics]], the '''Jaro\u2013Winkler distance''' (Winkler, 1990) is a measure of similarity between two [[String (computer science)|strings]].  It is a variant of the '''Jaro distance''' metric (Jaro, 1989, 1995), a type of string [[edit distance]], and was developed in the area of [[record linkage]] (duplicate detection) (Winkler, 1990). The higher the Jaro\u2013Winkler distance for two strings is, the more similar the strings are.  The Jaro\u2013Winkler distance metric is designed and best suited for short strings such as person names.  The score is normalized such that 0 equates to no similarity and 1 is an exact match.\u000a\u000a== Definition ==\u000a\u000aThe Jaro distance <math>d_j</math> of two given strings <math>s_1</math> and <math>s_2</math> is\u000a\u000a: <math>d_j = \u005cleft\u005c{\u000a\u000a\u005cbegin{array}{l l}\u000a  0 & \u005ctext{if }m = 0\u005c\u005c\u000a  \u005cfrac{1}{3}\u005cleft(\u005cfrac{m}{|s_1|} + \u005cfrac{m}{|s_2|} + \u005cfrac{m-t}{m}\u005cright) & \u005ctext{otherwise} \u005cend{array} \u005cright.</math>\u000a\u000aWhere:\u000a\u000a* <math>m</math> is the number of ''matching characters'' (see below);\u000a* <math>t</math> is half the number of ''transpositions'' (see below).\u000a\u000aTwo characters from <math>s_1</math> and <math>s_2</math> respectively, are considered ''matching'' only if they are the same and not farther than <math>\u005cleft\u005clfloor\u005cfrac{\u005cmax(|s_1|,|s_2|)}{2}\u005cright\u005crfloor-1</math>.\u000a\u000aEach character of <math>s_1</math> is compared with all its matching\u000acharacters in <math>s_2</math>. The number of matching (but different sequence order) characters\u000adivided by 2 defines the number of ''transpositions''.\u000aFor example, in comparing CRATE with TRACE, only 'R'   'A'   'E'  are the matching characters, i.e. m=3. Although 'C', 'T' appear in both strings, they are farther than 1, i.e., floor(5/2)-1=1. Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.\u000a\u000aJaro\u2013Winkler distance uses a [[prefix]] scale <math>p</math> which gives more favourable ratings to strings that match from the beginning for a set prefix length <math>\u005cell</math>.  Given two strings <math>s_1</math> and <math>s_2</math>, their Jaro\u2013Winkler distance <math>d_w</math> is:\u000a\u000a: <math>d_w = d_j + (\u005cell p (1 - d_j))</math>\u000a\u000awhere:\u000a\u000a* <math>d_j</math> is the Jaro distance for strings <math>s_1</math> and <math>s_2</math>\u000a* <math>\u005cell</math> is the length of common prefix at the start of the string up to a maximum of 4 characters\u000a* <math>p</math> is a constant [[scaling factor]] for how much the score is adjusted upwards for having common prefixes.  <math>p</math> should not exceed 0.25, otherwise the distance can become larger than 1.  The standard value for this constant in Winkler's work is <math>p = 0.1</math>\u000a\u000aAlthough often referred to as a ''distance metric'', the Jaro\u2013Winkler distance is actually not a [[metric (mathematics)|metric]] in the mathematical sense of that term because it does not obey the [[triangle inequality]] [http://richardminerich.com/tag/jaro-winkler/].\u000a\u000aIn some implementations of Jaro-Winkler, the prefix bonus <math>\u005cell p (1 - d_j)</math> is only added when the compared strings have a Jaro distance above a set "boost threshold" <math>b_t</math>. The boost threshold in Winkler's implementation was 0.7.\u000a\u000a: <math>d_w = \u005cleft\u005c{\u000a\u000a\u005cbegin{array}{l l}\u000a  d_j & \u005ctext{if }d_j < b_t\u005c\u005c\u000a  d_j + (\u005cell p (1 - d_j)) & \u005ctext{otherwise} \u005cend{array} \u005cright.</math>\u000a\u000a== Example ==\u000a\u000a''Note that Winkler's "reference" C code differs in at least two ways from published accounts of the Jaro\u2013Winkler metric. First is his use of a typo table (adjwt) and also some optional additional tolerance for long strings.''\u000a\u000aGiven the strings <math>s_1</math> ''MARTHA'' and <math>s_2 </math> ''MARHTA'' we find:\u000a\u000a* <math>m = 6</math>\u000a* <math>|s_1| = 6</math>\u000a* <math>|s_2| = 6</math>\u000a* There are mismatched characters T/H and H/T leading to <math>t = \u005cfrac{2}{2} = 1</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a<math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{6}{6} + \u005cfrac{6}{6} + \u005cfrac{6-1}{6}\u005cright) = 0.944</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 3</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.944 + (3 * 0.1 (1 - 0.944)) = 0.961</math>\u000a\u000aGiven the strings <math>s_1</math> ''DWAYNE'' and <math>s_2</math> ''DUANE'' we find:\u000a\u000a* <math>m = 4</math>\u000a* <math>|s_1| = 6</math>\u000a* <math>|s_2| = 5</math>\u000a* <math>t = 0</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a: <math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{4}{6} + \u005cfrac{4}{5} + \u005cfrac{4-0}{4}\u005cright) = 0.822</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 1</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.822 + (1 * 0.1 (1 - 0.822)) = 0.84</math>\u000a\u000aGiven the strings <math>s_1</math> ''DIXON'' and <math>s_2</math> ''DICKSONX'' we find:\u000a\u000a{{elucidate|date=March 2013}}\u000a\u000a{| class="wikitable"\u000a|-\u000a|\u000a| D\u000a| I\u000a| X\u000a| O\u000a| N\u000a|-\u000a| D\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| 0\u000a|-\u000a| I\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| C\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| K\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| S\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| O\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| N\u000a| 0\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a|-\u000a| X\u000a| 0\u000a| 0\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a|}\u000a\u000a* <math>m = 4</math>  Note that the two ''X''s are not considered matches because they are outside the match window of 3.\u000a\u000a* <math>|s_1| = 5</math>\u000a* <math>|s_2| = 8</math>\u000a* <math>t = 0</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a: <math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{4}{5} + \u005cfrac{4}{8} + \u005cfrac{4-0}{4}\u005cright) = 0.767</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 2</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.767 + (2 * 0.1 (1 - 0.767)) = 0.814</math>\u000a\u000a== See also ==\u000a\u000a* [[Levenshtein distance]]\u000a* [[Record linkage]]\u000a* [[Census]]\u000a\u000a== References ==\u000a\u000a* {{cite journal | last=Cohen |first=W. W. |last2=Ravikumar |first2=P. |last3=Fienberg |first3=S. E. |year=2003 |title=A comparison of string distance metrics for name-matching tasks |journal=KDD Workshop on Data Cleaning and Object Consolidation |volume=3 |pages=73-8 |url=https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf}}\u000a* {{cite journal | author = [[Matthew A. Jaro|Jaro, M. A.]] | title = Advances in record linkage methodology as applied to the 1985 census of Tampa Florida | journal = Journal of the American Statistical Association | year = 1989 | volume = 84 | issue = 406 |pages=414\u201320| url = | doi = 10.1080/01621459.1989.10478785 }}\u000a* {{cite journal |author=Jaro, M. A. |title=Probabilistic linkage of large public health data file  |journal= Statistics in Medicine |year=1995 |volume=14 |issue=5\u20137 |pages=491\u20138  |pmid=7792443 |doi=10.1002/sim.4780140510}}\u000a* {{cite journal\u000a\u000a  | author = [[William E. Winkler|Winkler, W. E.]]\u000a  | title = String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage\u000a  | journal = Proceedings of the Section on Survey Research Methods\u000a  | publisher = American Statistical Association\u000a  | pages = 354\u2013359\u000a  | year = 1990\u000a  | url = http://www.amstat.org/sections/srms/Proceedings/papers/1990_056.pdf }}\u000a\u000a* {{cite journal | author = [[William E. Winkler|Winkler, W. E.]] | title = Overview of Record Linkage and Current Research Directions | journal = Research Report Series, RRS | year = 2006 | volume = | issue = | url = http://www.census.gov/srd/papers/pdf/rrs2006-02.pdf}}\u000a\u000a== External links ==\u000a\u000a* [http://web.archive.org/web/20100227020019/http://www.census.gov/geo/msb/stand/strcmp.c strcmp.c - Original C Implementation by the author of the algorithm]\u000a\u000a{{DEFAULTSORT:Jaro-Winkler distance}}\u000a\u000a[[Category:String similarity measures]]
p133
sg4
S'208'
p134
sg6
VJaro\u2013Winkler distance
p135
ssI83
(dp136
g2
V{{orphan|date=January 2011}}\u000a\u000aA '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].\u000a\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{computing-stub}}
p137
sg4
S'83'
p138
sg6
VChampion list
p139
ssI213
(dp140
g2
VIn [[probability theory|probability]] and [[mathematical statistics|statistics]], the '''Hellinger distance''' (also called [[Bhattacharyya distance]] as this was originally introduced by [[Anil Kumar Bhattacharya]]) is used to quantify the similarity between two [[probability distributions]]. It is a type of [[f-divergence|''f''-divergence]].  The Hellinger distance is defined in terms of the [[Hellinger integral]], which was introduced by [[Ernst Hellinger]] in 1909.<ref>{{SpringerEOM|title=Hellinger distance|id=h/h046890|first=M.S. |last=Nikulin}}</ref><ref>{{Citation \u000a| last = Hellinger \u000a| first = Ernst\u000a| author-link = Ernst Hellinger\u000a| title = Neue Begründung der Theorie quadratischer Formen von unendlichvielen Veränderlichen \u000a| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 \u000a| year = 1909 \u000a| journal = [[Journal für die reine und angewandte Mathematik]]\u000a| language = German\u000a| volume = 136 \u000a| pages = 210\u2013271\u000a| jfm = 40.0393.01\u000a| doi=10.1515/crll.1909.136.210\u000a}}</ref>\u000a\u000a==Definition==\u000a\u000a===Measure theory===\u000aTo define the Hellinger distance in terms of [[measure theory]], let ''P'' and ''Q'' denote two [[probability measure]]s that are [[absolute continuity|absolutely continuous]] with respect to a third probability measure &lambda;.  The square of the Hellinger distance between ''P'' and ''Q'' is defined as the quantity\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cdisplaystyle \u005cint \u005cleft(\u005csqrt{\u005cfrac{dP}{d\u005clambda}} - \u005csqrt{\u005cfrac{dQ}{d\u005clambda}}\u005cright)^2 d\u005clambda. </math>\u000a\u000aHere, ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are the [[Radon\u2013Nikodym derivative]]s of ''P'' and ''Q'' respectively.  This definition does not depend on &lambda;, so the Hellinger distance between ''P'' and ''Q'' does not change if &lambda; is replaced with a different probability measure with respect to which both  ''P'' and ''Q'' are absolutely continuous.  For compactness, the above formula is often written as\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{dP} - \u005csqrt{dQ}\u005cright)^2. </math>\u000a\u000a===Probability theory using Lebesgue measure===\u000aTo define the Hellinger distance in terms of elementary probability theory, we take &lambda; to be [[Lebesgue measure]], so that ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are simply [[probability density function]]s.  If we denote the densities as ''f'' and ''g'', respectively, the squared Hellinger distance can be expressed as a standard calculus integral\u000a\u000a:<math>\u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{f(x)} - \u005csqrt{g(x)}\u005cright)^2 dx = 1 - \u005cint \u005csqrt{f(x) g(x)} \u005c, dx,</math>\u000a\u000awhere the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain must be one.\u000a\u000aThe Hellinger distance ''H''(''P'',&nbsp;''Q'') satisfies the property (derivable from the [[Cauchy-Schwarz inequality#L2|Cauchy-Schwarz inequality]])\u000a\u000a: <math>0\u005cle H(P,Q) \u005cle 1.</math>\u000a\u000a===Discrete distributions===\u000aFor two discrete probability distributions <math>P=(p_1 \u005cldots p_k)</math> and <math>Q=(q_1 \u005cldots q_k)</math>,\u000atheir Hellinger distance is defined as\u000a\u000a: <math>\u000a  H(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005csqrt{\u005csum_{i=1}^{k} (\u005csqrt{p_i} - \u005csqrt{q_i})^2},\u000a</math>\u000a\u000awhich is directly related to the [[Euclidean distance|Euclidean norm]] of the difference of the square root vectors, i.e.\u000a: <math>\u000aH(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005cbigl\u005c|\u005csqrt{P} - \u005csqrt{Q} \u005cbigr\u005c|_2 .\u000a</math>\u000a\u000a== Connection with the statistical distance ==\u000a\u000aThe Hellinger distance <math>H(P,Q)</math> and the [[total variation distance]] (or statistical distance) <math>\u005cdelta(P,Q)</math> are related as follows:<ref>[http://www.tcs.tifr.res.in/~prahladh/teaching/2011-12/comm/lectures/l12.pdf Harsha's lecture notes on communication complexity]</ref>\u000a\u000a: <math>\u000aH^2(P,Q) \u005cleq \u005cdelta(P,Q) \u005cleq \u005csqrt 2 H(P,Q)\u005c,.\u000a</math>\u000a\u000aThese inequalities follow immediately from the inequalities between the [[Lp space#The p-norm in finite dimensions|1-norm]] and the [[Lp space#The p-norm in finite dimensions|2-norm]].\u000a\u000a==Properties==\u000aThe maximum distance 1 is achieved when ''P'' assigns probability zero to every set to which ''Q'' assigns a positive probability, and vice versa.\u000a\u000aSometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.\u000a\u000aThe Hellinger distance is related to the [[Bhattacharyya distance|Bhattacharyya coefficient]] <math>BC(P,Q)</math> as it can be defined as\u000a\u000a: <math>H(P,Q) = \u005csqrt{1 - BC(P,Q)}.</math>\u000a\u000aHellinger distances are used in the theory of [[sequential analysis|sequential]] and [[asymptotic statistics]].<ref>Erik Torgerson (1991) ''Comparison of Statistical Experiments'', volume 36 of Encyclopedia of Mathematics. Cambridge University Press.\u000a</ref><ref>{{cite book\u000a  | author = Liese, Friedrich and Miescke, Klaus-J.\u000a  | title = Statistical Decision Theory: Estimation, Testing, and Selection\u000a  | year = 2008\u000a  | publisher = Springer\u000a  | isbn = 0-387-73193-8\u000a  }}\u000a</ref>\u000a\u000a==Examples==\u000aThe squared Hellinger distance between two [[normal distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_1,\u005csigma_1^2)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_2,\u005csigma_2^2)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005csqrt{\u005cfrac{2\u005csigma_1\u005csigma_2}{\u005csigma_1^2+\u005csigma_2^2}} \u005c,  e^{-\u005cfrac{1}{4}\u005cfrac{(\u005cmu_1-\u005cmu_2)^2}{\u005csigma_1^2+\u005csigma_2^2}}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[exponential distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Exp}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Exp}(\u005cbeta)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 \u005csqrt{\u005calpha \u005cbeta}}{\u005calpha + \u005cbeta}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[Weibull distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{W}(k,\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{W}(k,\u005cbeta)</math> (where <math> k </math> is a common shape parameter and <math> \u005calpha\u005c, , \u005cbeta </math> are the scale parameters respectively):\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 (\u005calpha \u005cbeta)^{k/2}}{\u005calpha^k + \u005cbeta^k}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Poisson distribution]]s with rate parameters <math>\u005calpha</math> and <math>\u005cbeta</math>, so that <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Poisson}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Poisson}(\u005cbeta)</math>, is:\u000a: <math>\u000a  H^2(P,Q) = 1-e^{-\u005cfrac{1}{2}(\u005csqrt{\u005calpha} - \u005csqrt{\u005cbeta})^2}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Beta distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005ctext{Beta}(a_1,b_1)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005ctext{Beta}(a_2, b_2)</math> is:\u000a: <math>\u000aH^{2}(P,Q)	=1-\u005cfrac{B\u005cleft(\u005cfrac{a_{1}+a_{2}}{2},\u005cfrac{b_{1}+b_{2}}{2}\u005cright)}{\u005csqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}\u000a  </math>\u000awhere <math>B</math> is the [[Beta function]].\u000a\u000a==See also==\u000a* [[Kullback Leibler divergence]]\u000a* [[Fisher information metric]]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==References==\u000a* {{cite book |author=Yang, Grace Lo; Le Cam, Lucien M. |title=Asymptotics in Statistics: Some Basic Concepts |publisher=Springer |location=Berlin |year=2000 |pages= |isbn=0-387-95036-2 |oclc= |doi=}}\u000a* {{cite book |author=Vaart, A. W. van der |title=Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) |publisher=Cambridge University Press |location=Cambridge, UK |year= |pages= |isbn=0-521-78450-6 |oclc= |doi=}}\u000a* {{cite book |author=Pollard, David E. |title=A user's guide to measure theoretic probability |publisher=Cambridge University Press |location=Cambridge, UK |year=2002 |pages= |isbn=0-521-00289-3 |oclc= |doi=}}\u000a\u000a[[Category:Probability theory]]\u000a[[Category:F-divergences]]\u000a[[Category:Statistical distance measures]]\u000a[[Category:String similarity measures]]
p141
sg4
S'213'
p142
sg6
VHellinger distance
p143
ssI88
(dp144
g2
V{{multiple issues|\u000a{{technical|date=October 2012}}\u000a{{Expert-subject|date=July 2010}}\u000a{{Expert-subject|Science|date=July 2010}}\u000a}}\u000a\u000a\u000a'''Music information retrieval''' ('''MIR''') is the interdisciplinary science of retrieving [[information]] from [[music]]. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in [[musicology]], [[psychology]], academic music study, [[signal processing]], [[machine learning]] or some combination of these.\u000a\u000a== Applications ==\u000aMIR is being used by businesses and academics to categorize, manipulate and even create music.\u000a\u000a=== Recommender systems ===\u000aSeveral [[recommender systems]] for music already exist, but surprisingly few are based upon MIR techniques, instead making use of similarity between users or laborious data compilation. [[Pandora]], for example, uses experts to tag the music with particular qualities such as "female singer" or "strong bassline". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems.\u000a\u000a=== Track separation and instrument recognition ===\u000aTrack separation is about extracting the original tracks as recorded, which could have more than one instrument played per track. Instrument recognition is about identifying the instruments involved and/or separating the music into one track per instrument. Various programs have been developed that can separate music into its component tracks without access to the master copy. In this way e.g. karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments.\u000a\u000a===Automatic music transcription===\u000aAutomatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a [[MIDI_file#File_formats|MIDI file]].<ref>A. Klapuri and M. Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.</ref> This process involves several subtasks, which include multi-pitch detection, [[Onset_detection#Onset_detection|onset detection]], duration estimation, instrument identification, and the extraction of rhythmic information. This task becomes more difficult with greater numbers of instruments and a greater [[Polyphony and monophony in instruments|polyphony level]].\u000a\u000a===Automatic categorization===\u000aMusical genre categorization is a common task for MIR and is the usual task for the yearly Music Information Retrieval Evaluation eXchange(MIREX).<ref>http://www.music-ir.org/mirex/wiki/MIREX_HOME - Music Information Retrieval Evaluation eXchange.</ref> Machine learning techniques such as [[Support Vector Machines]] tend to perform well, despite the somewhat subjective nature of the classification. Other potential classifications include identifying the artist, the place of origin or the mood of the piece. Where the output is expected to be a number rather than a class, [[regression analysis]] is required.\u000a\u000a===Music generation===\u000aThe automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results.\u000a\u000a==Methods used==\u000a\u000a===Data source===\u000a[[Sheet music|Scores]] give a clear and logical description of music from which to work, but access to sheet music, whether digital or otherwise, is often impractical. [[MIDI]] music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as [[WAV]], [[mp3]], and [[ogg]] are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently includes analysis of [[social tagging|social tags]] for music.\u000a\u000a===Feature representation===\u000aAnalysis can often require some summarising,<ref>Eidenberger, Horst (2011). \u201cFundamental Media Understanding\u201d, atpress. ISBN 978-3-8423-7917-6.</ref> and for music (as with many other forms of data) this is achieved by feature extraction, especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the [[Mel-frequency cepstral coefficient|Mel-Frequency Cepstral Coefficient]] (MFCC) which is a measure of the [[timbre]] of a piece of music. Other features may be employed to represent the [[Tonality#Computational_methods_to_determine_the_key|key]], chords, harmonies, melody, main pitch, beats per minute or rhythm in the piece.\u000a\u000a===Statistics and machine learning===\u000a*Computational methods for classification, clustering, and modelling \u2014 musical feature extraction for mono- and [[polyphonic]] music, similarity and [[pattern matching]], retrieval\u000a* Formal methods and databases \u2014 applications of automated [[music identification]] and recognition, such as [[score following]], automatic accompaniment, routing and filtering for music and music queries, query languages, standards and other metadata or protocols for music information handling and [[information retrieval|retrieval]], [[multi-agent system]]s, distributed search)\u000a*Software for music information retrieval \u2014 [[Semantic Web]] and musical digital objects, intelligent agents, collaborative software, web-based search and [[semantic retrieval]], [[query by humming]], [[acoustic fingerprinting]]\u000a* Music analysis and knowledge representation \u2014 automatic summarization, citing, excerpting, downgrading, transformation, formal models of music, digital scores and representations, music indexing and [[metadata]].\u000a\u000a==Other issues==\u000a*Human-computer interaction and interfaces \u2014 multi-modal interfaces, [[user interface]]s and [[usability]], mobile applications, user behavior\u000a* Music perception, cognition, affect, and emotions \u2014 music [[similarity metrics]], syntactical parameters, semantic parameters, musical forms, structures, styles ands, music annotation methodologies\u000a* Music archives, libraries, and digital collections \u2014 music [[digital library|digital libraries]], public access to musical archives, benchmarks and research databases\u000a* [[Intellectual property]] rights and music \u2014 national and international [[copyright]] issues, [[digital rights management]], identification and traceability\u000a* Sociology and Economy of music \u2014 music industry and use of MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental design and metrics\u000a\u000a== See also ==\u000a* [[Audio mining]]\u000a* [[Artificial intelligence]]\u000a* [[Digital rights management]]\u000a* [[Digital signal processing]]\u000a* [[Ethnomusicology]]\u000a* [[Multimedia Information Retrieval]]\u000a* [[Music notation]]\u000a* [[Musicology]]\u000a* [[Parsons code]]\u000a* [[Sound and music computing]]\u000a* [[Music OCR]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a* Michael Fingerhut (2004). [http://mediatheque.ircam.fr/articles/textes/Fingerhut04b "Music Information Retrieval, or how to search for (and maybe find) music and do away with incipits"], ''IAML-IASA Congress'', Oslo (Norway), August 8\u201313, 2004.\u000a\u000a==External links==\u000a* [http://www.ismir.net/ International Society for Music Information Retrieval]\u000a* [http://music-ir.org/ Music Information Retrieval research]\u000a* [http://www.music-ir.org/jdownie_papers/downie_mir_arist37.pdf J. Stephen Downie: Music information retrieval]\u000a* [http://dx.doi.org/10.1561/1500000042 M. Schedl, E. Gómez and J. Urbano: Music Information Retrieval: Recent Developments and Applications]\u000a* [http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000002 Nicola Orio: Music Retrieval: A Tutorial and Review]\u000a* [https://ccrma.stanford.edu/wiki/MIR_workshop_2011 Intelligent Audio Systems: Foundations and Applications of Music Information Retrieval, introductory course at Stanford University's Center for Computer Research in Music and Acoustics]\u000a* [http://biblio.ugent.be/record/470088 Micheline Lesaffre: Music Information Retrieval: Conceptual Framework, Annotation and User behavior.]\u000a* [http://the.echonest.com/ The Echo Nest: a company specialising in MIR research and applications.]\u000a* [http://www.imagine-research.com/ Imagine Research : develops platform and software for MIR applications ]\u000a* [http://www.AudioContentAnalysis.org/ AudioContentAnalysis.org: MIR resources and matlab code ]\u000a\u000a==Example MIR applications==\u000a* [http://www.musipedia.org/ Musipedia \u2014 A melody search engine that offers several modes of searching, including whistling, tapping, piano keyboard, and Parsons code.]\u000a* [http://www.listengame.org/ The Listen Game \u2014 UCSD Computer Audition Lab MIR music ranking game]\u000a* [http://www.peachnote.com/ Peachnote \u2014 A melody search engine and n-gram viewer that searches through digitized music scores]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Music software]]
p145
sg4
S'88'
p146
sg6
VMusic information retrieval
p147
ssI218
(dp148
g2
V{{ infobox bibliographic database\u000a| image       = \u000a| caption     = \u000a| producer    =Thomson Reuters \u000a| country     =United States \u000a| history     = \u000a| languages   = \u000a| providers   =Web of Science, Dialog Bluesheets \u000a| cost        =Subscription \u000a| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio \u000a| depth       =Index, abstract, citation indexing, author \u000a| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances \u000a| temporal    =1975 to present \u000a| geospatial  =global \u000a| number      = \u000a| updates     = \u000a| p_title     = \u000a| p_dates     = \u000a| ISSN        = \u000a| web         = \u000a| titles      =  \u000a}}\u000a\u000aThe '''''Arts & Humanities Citation Index''''' ('''A&HCI'''), also known as '''''Arts & Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore the print counterpart is Current Contents.\u000a\u000aSubjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio. \u000a\u000aAvailable citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances. \u000a\u000aThis database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.\u000a\u000aAccording to Thomson Reuters, the ''Arts & Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>\u000a{{Cite web\u000a  | title =Arts & Humanities Search (File 255) \u000a  | publisher =Dialog bluesheets  \u000a  | date = \u000a  | url =http://library.dialog.com/bluesheets/html/bl0439.html \u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=Iowa>\u000aDescription of Arts & Humanities Search. \u000a{{Cite web\u000a  | title =e-Library catalog\u000a  | publisher =Iowas State University  \u000a  | year =2008 \u000a  | url =http://www.lib.iastate.edu/collections/db/artshm.html\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>\u000aDescription of Web of Science coverage.  \u000a{{Cite web\u000a  | title =e-Library catalog\u000a  | publisher =Iowas State University  \u000a  | year =2008 \u000a  | url =http://www.lib.iastate.edu/collections/db/websci.html\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=TR>\u000aSee the page entitled "Tech Specs" \u000a{{Cite web\u000a  | title =Database description\u000a  | publisher =Thomson Reuters  \u000a  | year = \u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref>\u000a==History==\u000aThe index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP & Science division.\u000a\u000a==See also==\u000a* [[Science Citation Index]]\u000a* [[Social Sciences Citation Index]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* {{Official|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.\u000a* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.\u000a\u000a{{Thomson Reuters}}\u000a[[Category:Citation indices]]\u000a[[Category:Thomson Reuters]]\u000a\u000a{{DEFAULTSORT:Arts And Humanities Citation Index}}
p149
sg4
S'218'
p150
sg6
VArts and Humanities Citation Index
p151
ssI93
(dp152
g2
V[[Category:Information retrieval]]\u000a[[Category:Music software|Search engines]]\u000a[[Category:Internet search engines]]\u000a[[Category:Online music and lyrics databases]]
p153
sg4
S'93'
p154
sg6
VCategory:Music search engines
p155
ssI223
(dp156
g2
V{{Infobox Bibliographic Database\u000a|title =SPIN  (Searchable Physics Information Notices)  \u000a|image = \u000a|caption = \u000a|producer =[[American Institute of Physics]] (AIP) \u000a|country =USA, Russia, Ukraine\u000a|history = \u000a|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] \u000a|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] \u000a|cost = \u000a|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology \u000a|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   \u000a|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia\u000a|temporal =1975 to the present  \u000a|geospatial =International \u000a|number =over 1.5 million \u000a|updates =Weekly \u000a|p_title =No print counterparts \u000a|p_dates = \u000a|ISSN =\u000a|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp \u000a|titles =  \u000a}}\u000a\u000a'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>\u000a\u000a==Journals==\u000aDelivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web\u000a  | title =What is the SPIN database? \u000a  | work =Information about SPIN \u000a  | publisher =[[American Institute of Physics]] \u000a  | date =July 2010 \u000a  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>\u000a\u000a==Sources==\u000aOverall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web\u000a  | title =SPIN Publication Coverage \u000a  | work =Complete list of publications covered and coverage years. \u000a  | publisher =American Institute of Physics \u000a  | date =July 2010 \u000a  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>  \u000a\u000a==Scope==\u000aSubject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web\u000a  | title =Indexes and Databases \u000a  | work =SPIN: Searchable Physics Information Notices\u000a  | publisher =Raymond H. Fogler Library, The University of Maine\u000a  | date =October 2010 \u000a  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>\u000a\u000a*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] \u000a*[[Atomic physics]] and [[Molecular physics]] \u000a*[[Biological physics]] and [[Medical physics]] \u000a*[[Classical physics]] and [[Quantum physics]] \u000a*[[Condensed matter physics]] \u000a*[[Elementary particle physics]] \u000a*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] \u000a*[[Geophysics]], [[Astronomy]], [[Astrophysics]] \u000a*[[Materials science]] \u000a*[[Nuclear physics]] \u000a*[[Plasma physics]] \u000a*[[Physical chemistry]]\u000a\u000a==See also==\u000a*[[List of academic databases and search engines]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.\u000a*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.\u000a*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.\u000a\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Bibliographic indexes]]\u000a[[Category:Citation indices]]\u000a[[Category:Scientific databases]]
p157
sg4
S'223'
p158
sg6
VSPIN bibliographic database
p159
ssI98
(dp160
g2
VIn [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find labeling that summarize the topic of each cluster and distinguish the clusters from each other.\u000a\u000a==Differential cluster labeling==\u000aDifferential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>\u000a\u000a===Pointwise mutual information===\u000a\u000a{{Main|Pointwise mutual information}}\u000a\u000aIn the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:\u000a\u000a<math>I(X, Y) = \u005csum_{x\u005cin X}{ \u005csum_{y\u005cin Y} {p(x, y)log_2\u005cleft(\u005cfrac{p(x, y)}{p_1(x)p_2(y)}\u005cright)}}</math>\u000a\u000awhere ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p<sub>1</sub>(x)'' is the probability distribution of X, and ''p<sub>2</sub>(y)'' is the probability distribution of Y.\u000a\u000aIn the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>I(C, T) = \u005csum_{c\u005cin {0, 1}}{ \u005csum_{t\u005cin {0, 1}} {p(C = c, T = t)log_2\u005cleft(\u005cfrac{p(C = c, T = t)}{p(C = c)p(T = t)}\u005cright)}}</math>\u000a\u000aIn this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.\u000a\u000a===Chi-Squared Selection===\u000a{{Main|Pearson's chi-squared test}}\u000aThe Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin A}{\u005csum_{b \u005cin B}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000awhere ''O<sub>a,b</sub>'' is the ''observed'' frequency of a and b co-occurring, and ''E<sub>a,b</sub>'' is the ''expected'' frequency of co-occurrence.\u000a\u000aIn the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin {0,1}}{\u005csum_{b \u005cin {0,1}}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000aFor example, ''O<sub>1,0</sub>'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E<sub>1,0</sub>'' is the expected number of documents that are in a particular cluster but don't contain a certain term.\u000aOur initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>\u000a\u000a''E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)''\u000a\u000awhere N is the total number of documents in the collection.\u000a\u000a==Cluster-Internal Labeling==\u000aCluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.\u000aCluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.\u000a\u000a===Centroid Labels===\u000a{{Main|Vector space model}}\u000aA frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.\u000aOne downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.\u000a\u000a===Contextualized centroid labels===\u000aA simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection. <ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters\u2019 contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>\u000aIn this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)\u000aIn a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\u005ctilde{t}_{i}</math> and <math>\u005ctilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.\u000a\u000a===Title labels===\u000aAn alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.\u000a\u000a===External knowledge labels===\u000aCluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.\u000a\u000a==External links==\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]\u000a* [http://erulemaking.ucsur.pitt.edu/doc/papers/dgo06-labeling.pdf Automatically Labeling Hierarchical Clusters]\u000a\u000a==References==\u000a<references/>\u000a\u000a{{DEFAULTSORT:Cluster Labeling}}\u000a[[Category:Information retrieval]]
p161
sg4
S'98'
p162
sg6
VCluster labeling
p163
ssI228
(dp164
g2
V{{Multiple issues|\u000a{{orphan|date=March 2010}}\u000a{{advert|date=August 2010}}\u000a}}\u000a\u000aThe '''Latin American Bibliography''' refers to the set of [[databases]] and information services on [[academic journals]] from [[Latin America]] and the [[Caribbean]] created by the [[National Autonomous University of Mexico]] (UNAM) in the decade of the seventies.{{Clarify|date=August 2009}}\u000a\u000aNowadays, the Latin-American Bibliography is composed by the following databases: CLASE (''Latin-American Citations in [[Social Sciences]] and [[Humanities]]''); PERIODICA (''Index of Latin-American Journals in [[Science]]''); [[Latindex]] (''Regional Co-operative Information System for Scholarly Journals from [[Latin America]], the Caribbean, Spain and Portugal'').\u000a\u000aThese databases were created by a group of information professionals, who identified the need to register, preserve and give access to the Latin-American knowledge published in the main academic [[Academic journal|journals]] of the region. Within UNAM, the fostering institution of these information products was the Science and Humanities Information Center (CICH) created in 1971.\u000a\u000aFor the size of its collection of Latin-American journals, for the quantity of compiled records and for the duration and consistency of the project, the Latin-American Bibliography produced in the UNAM constitutes one of the most valuable resources for scholars and experts specializing in Latin-American affairs.{{Citation needed|date=August 2009}}\u000a\u000a==Products==\u000a\u000aThree databases are available through the web site of UNAM\u2019s General Directorate for Libraries [http://dgb.unam.mx General Directorate for Libraries]:\u000a\u000a'''CLASE''' (''Latin-American Citations in Social Sciences and Humanities''). Bibliographical database, with more than 280,000 records, of which nearly 14,000 provide abstracts and links to the full text of the documents. It includes more than 1,400 journals specializing in Social Sciences, Humanities and Arts, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/clase.html CLASE website]\u000a\u000a'''PERIODICA''' (Index of Latin-American Journals in Science). Bibliographical database with more than 315,000 records, of which near 60,000 provide abstracts and links to the full text of the documents. The database indexes more than 1,500 journals specializing in Science and Technology, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/periodica.html PERIODICA website]\u000a\u000a'''[[Latindex]]''' (Regional Co-operative Information System for Scholarly Journals from Latin America, the Caribbean, Spain and Portugal). This initiative provides relevant information and data of the scholarly journals edited in the [[Iberoamerica]]n region. Three databases are produced through the collaborative work of the member institutions: '''Directory''',: with more than 17,000 records; '''Catalogue''', with more than 3,500 selected journals that fulfill international quality criteria and an '''Index of Electronic Journals''', offering nearly 3,000 links to available resources in full text. Direct link: [http://www.latindex.org Latindex website]\u000a\u000aCurrently, the Department of Latin-American Bibliography contributes to the production of two other Latin-American information products:\u000a\u000a'''ASFA''' (''Aquatic Sciences and Fisheries Abstracts''). Bibliographical international database on Aquatic Sciences and [[Fisheries]], covering subject areas such as [[technology]] and [[Public administration|administration]] of the marine environments and its resources (salt and sweet waters), including its socioeconomic and juridical aspects. It offers abstracts of articles published in approximately 7,000 periodic publications, besides thesis, monographs and other not conventional literature. The contribution relative to the Mexican journals is produced in the Department of Latin-American Bibliography from 1981. Link: [http://www.fao.org/fishery/asfa ASFA website]\u000a\u000a'''[[SciELO]] Mexico''' (''Scientific Electronic Library Online''). Open access electronic journals collection that includes a selection of the most recognized academic publications of the country in all areas of knowledge, previously selected accordingly to the most accepted criteria related to content and editorial standards. Currently it offers the full text of more than 2,500 articles from 28 academic Mexican journals. Direct link: [http://www.scielo.org.mx/scielo.php Scielo México website]\u000a\u000aOver the time, other databases were produced by the Department of Latin-American Bibliography during its more than 30 years of existence, namely:\u000a\u000a'''BLAT''' (''Latin-American Bibliography I and II''), with information compiled from international sources, mainly documents from Latin-American origin (produced by Latin American authors and institutions) or those in which their object of study was related to the region. The database ceased in 1997. Another one was '''MEXINV''', as a subset of CLASE, offered bibliographical records of documents relative only to [[Mexico]]. This database ceased in the decade of the nineties.\u000a\u000a===Institution===\u000a\u000aCurrently, the databases described above are produced by the Department of Latin-American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). The original databases (BLAT, CLASE, PERIODICA, MEXINV and Latindex) were created by the Science and Humanities Information Center (CICH). Since the incorporation of the CICH to UNAM\u2019s General Directorate for Libraries in 1997, this institution acts as Responsible Editor.\u000a\u000a==References==\u000a\u000a*Alonso Gamboa, José Octavio. Servicios, productos, docencia e investigación en información: la experiencia del Centro de Información Científica y Humanística de la Universidad Nacional Autónoma de México. Ciencias de la Información, vol. 24, no. 4, diciembre, 1993. p.&nbsp;201-208. URL: [http://www.bibliociencias.cu/gsdl/cgi-bin/library?e=d-000-00---0revistas--00-0-0--0prompt-10---4------0-1l--1-es-50---20-about---00031-001-1-0utfZz-8-00&cl=CL2.772&d=HASH01caacf727585263378aa110&x=1]\u000a\u000a*Alonso Gamboa, José Octavio. Accesso a revistas latinoamericanas en Internet. Una opción a través de las bases de datos Clase y Periódica. Ciencia da Informação, vol. 27, no. 1, Janeiro-abril, 1998, p.&nbsp;90-95. URL: http://www.scielo.br/pdf/ci/v27n1/12.pdf\u000a\u000a*Alonso Gamboa, José Octavio y Felipe Rafael Reyna Espinosa. Compilación de datos bibliométricos regionales usando las bases de datos CLASE y PERIÓDICA. Revista Interamericana de Bibliotecología, 2005. Vol. 28, no. 1, enero-junio: 63-78. URL: http://bibliotecologia.udea.edu.co/revinbi/Numeros/2801/doc3_28.html\u000a\u000a*Russell, Jane M.; Madera-Jaramillo, María J.; Hernández- García, Yoscelina y Ainsworth, Shirley. Mexican collaboration networks in the international and regional arenas. En: Kretschmer, H. & Havemann, F. (Eds.): Proceedings of WIS 2008, Berlin. Fourth International Conference on Webometrics, Informetrics and Scientometrics & Ninth COLLNET Meeting, Humboldt-Universität zu Berlin, Institute for Library and Information Science (IBI). URL: http://www.collnet.de/Berlin-2008/RussellWIS2008mcn.pdf\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Scientific databases]]\u000a[[Category:Citation indices]]
p165
sg4
S'228'
p166
sg6
VLatin American Bibliography
p167
ssI103
(dp168
g2
V'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are either:<nowiki>[3]</nowiki>\u000a* a) unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)\u000a* b) unsure about the ways to achieve their goals (either the technology or the process)\u000a* c) or even unsure about their goals in the first place.\u000a\u000aConsequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.\u000a\u000a==History==\u000aExploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?" Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.\u000a\u000aIn the last few years, a series of workshops have been held at various related and key events. In 2005, the [http://research.microsoft.com/~ryenw/xsi/index.html Exploratory Search Interfaces workshop] focused on beginning to define some of the key challenges in the field. Since then a series of other workshops have been held at related conferences: [http://research.microsoft.com/~ryenw/eess/index.html Evaluating Exploratory Search] at [http://www.sigir2006.org SIGIR06] and [http://research.microsoft.com/~ryenw/esi/index.html Exploratory Search and HCI] at [http://www.chi2007.org CHI07] (in order to meet with the experts in [[human\u2013computer interaction]]).\u000a\u000aIn March 2008, an [http://www.sciencedirect.com/science/journal/03064573 ''Information Processing and Management'' special issue]<nowiki>[2]</nowiki> has focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.\u000a\u000aIn June 2008, the [[National Science Foundation]] sponsored an [http://www.ils.unc.edu/ISSS_workshop/ invitational workshop] to identify a research agenda for exploratory search and similar fields for the coming years.\u000a\u000a==Research challenges==\u000a\u000a===Important scenarios===\u000aWith the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by [http://mspace.fm mSpace] states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.\u000a\u000a===Designing new interfaces===\u000aWith one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the user, so that they can choose from a list instead of guess a possible keyword query.\u000a\u000aMany of the [[human\u2013computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.\u000a\u000aComputational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). Facilitating exploratory search by model-based navigational cues. In Proceedings of the ACM International conference on Intelligent User Interface. 199-208.  http://portal.acm.org/citation.cfm?id=1719970.1719998</ref>\u000a\u000a===Evaluating interfaces===\u000aAs the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.\u000a\u000a===Models of exploratory search behavior===\u000aThere has been recent attempts to develop process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation\u000a  | doi = 10.1145/1460563.1460600\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | title = The Microstructures of Social Tagging: A Rational Model\u000a  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work. \u000a  | pages = 66\u201372\u000a  | date = April 2008\u000a  | url = http://portal.acm.org/citation.cfm?id=1460600\u000a  | isbn = 978-1-60558-007-4 }}\u000a</ref>\u000a.<ref>{{Citation\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | title = A Semantic Imitation Model of Social Tagging\u000a  | journal = Proceedings of the IEEE conference on Social Computing\u000a  | pages = 66\u201372\u000a  | date = Aug 2009\u000a  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>\u000a{{Citation\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | last2 = Pirolli  | first2 = Peter\u000a  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web\u000a  | journal = Human-Computer Interaction\u000a  | pages = 335\u2013412\u000a  | year = 2007\u000a  | url = http://portal.acm.org/citation.cfm?id=1466608\u000a  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based\u000amodel of Web navigation and its application to Web usability analysis. In S. Mc-\u000aDonald, Y. Waern, & G. Cockton (Eds.), People and computers XIV\u2014Usability or else!\u000aNew York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications\u000afor information architecture. Human Computer Interaction, 19, 225\u2013271.</ref>\u000aRecent development in exploratory search is often concentrated in predicting user's search intents in interaction with the user.<ref>\u000a{{Citation\u000a  | last1 = Ruotsalo  | first1 = Tuukka\u000a  | last2 = Athukorala  | first2 = Kumaripaba\u000a  | last3 = Glowacka  | first3 = Dorota\u000a  | last4 = Konuyshkova  | first4 = Ksenia\u000a  | last5 = Oulasvrita  | first5 = Antti\u000a  | last6 = Kaipiainen  | first6 = Samuli\u000a  | last7 = Kaski  | first7 = Samuel\u000a  | last8 = Jacucci  | first8 = Giulio\u000a  | title = Supporting exploratory search tasks with interactive user modeling\u000a  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T\u000a  | year = 2013}}\u000a</ref>\u000aSuch predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs\u000a<ref>\u000a{{Citation\u000a  | last1 = Ruotsalo  | first1 = Tuukka\u000a  | last2 = Peltonen  | first2 = Jaakko\u000a  | last3 = Eugster | first3 = Manuel J.A.\u000a  | last4 = Glowacka  | first4 = Dorota\u000a  | last5 = Konuyshkova  | first5 = Ksenia\u000a  | last6 = Athukorala  | first6 = Kumaripaba\u000a  | last7 = Kosunen | first7 = Ilkka    \u000a  | last8 = Reijonen  | first8 = Aki\u000a  | last9 = Myllymäki | first9 = Petri\u000a  | last10 = Kaski  | first10 = Samuel\u000a  | last11 = Jacucci  | first11 = Giulio\u000a  | title = Directing Exploratory Search with Interactive Intent Modeling\u000a  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM\u000a  | year = 2013}}\u000a</ref>\u000a.<ref>\u000a{{Citation\u000a  | last1 = Glowacka  | first1 = Dorota\u000a  | last2 = Ruotsalo  | first2 = Tuukka\u000a  | last3 = Konuyshkova  | first3 = Ksenia\u000a  | last4 = Athukorala  | first4 = Kumaripaba\u000a  | last5 = Kaski  | first5 = Samuel\u000a  | last6 = Jacucci  | first6 = Giulio\u000a  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords\u000a  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI\u000a  | url = http://dl.acm.org/citation.cfm?id=2449413\u000a  | pages = 117\u2013128 \u000a  | year = 2013}}\u000a</ref>\u000a\u000a==Major figures==\u000a\u000aKey figures, including experts from both [[information seeking]] and [[human\u2013computer interaction]], are:\u000a*[http://research.microsoft.com/~ryenw Ryen White]\u000a*[http://ils.unc.edu/~march Gary Marchionini]\u000a*[http://comminfo.rutgers.edu/~belkin/belkin.html Nicholas Belkin]\u000a*[http://users.ecs.soton.ac.uk/mc m.c. schraefel]\u000a*[[Marcia Bates]]\u000a\u000a==References==\u000a<References/>\u000a#White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;36\u201339.\u000a#Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&nbsp;433\u2013436\u000a#Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.\u000a#P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009\u000a\u000a{{DEFAULTSORT:Exploratory Search}}\u000a[[Category:Human\u2013computer interaction]]\u000a[[Category:Information retrieval]]\u000a[[Category:Information science]]
p169
sg4
S'103'
p170
sg6
VExploratory search
p171
ssI233
(dp172
g2
V'''Tunebot''' is a music search engine developed by the Interactive Audio Lab at [[Northwestern University]]. Users can search the database by humming or singing a melody into a microphone, playing the melody on a virtual keyboard, or by typing some of the lyrics. This allows users to finally identify that song that was stuck in their head.\u000a\u000a==Searching Techniques==\u000a\u000aTunebot is a [[Query by humming]] system. It compares a sung query to a database of musical themes by using the intervals between each note. This allows a user to sing in a different key than the target recording and still produce a match. The intervals are also unquantized to allow for other tunings besides the standard A=440Hz, since not many people in the world have [[perfect pitch]].\u000a\u000aIn addition to note intervals, Tunebot compares a query with potential targets by using rhythmic ratios between notes. Since ratios between note lengths are used, the tempo of the performance does not affect the rhythmic similarity measure. \u000a\u000aQueries and targets are then matched by a weighted string alignment algorithm between the note intervals and rhythmic ratios.\u000a<!--Note segmentation, then to pitches and then use Pitch intervals (instead of melodic contour - measured frequency at given times). Pitch intervals are relative (unquantized) to adjust for singing in the wrong key or wrong tempo. Faster and more reliable search.\u000a\u000aModel singer error: gaussian distribution because wider interval and lower intervals seem to be more prone to singer error. Combination of gaussians with 5 parameters to tweak: pitch weight, rhythm weight, sensitivity to distance for pitch, sensisitivity to distance for rhythm, octave decay\u000a\u000aDo we use rhythmic ratios? (LIR)\u000a\u000aGenetic algorithm to tune system parameters-->\u000a\u000a==The Database==\u000aThe database consists of unaccompanied melodies sung by contributors (a capella). Contributors log into the website and sing their examples to the system. Each of these recordings is associated with a corresponding song on [[Amazon.com|Amazon]]. A sung query is compared to these examples. A capella sung examples are used as search keys because it is much easier to compare one unaccompanied vocal (the sung query) to another (an example search key) than it is to compare an unaccompanied vocal to a full band recording, which may contain guitar, drums, other singers, sound effects, etc.\u000a\u000a==Distinguishing Features==\u000a\u000aTunebot learns from user input, and it improve its results as each user submits more queries. Since no human can sing perfectly in tune every time they sing, the search engine must take that into account. By choosing a song from a list of ranked results, users tell Tunebot which song was correct. Tunebot then pairs that song with the user's query, analyzes the differences, and runs a [[Genetic Algorithm]]. This process tweaks the parameters that control how the system compares the user's query to the targets. For instance, if a user has no sense of rhythm, that factor of the comparison is lowered for future queries.\u000a\u000a==References==\u000a\u000a* B. Pardo. [http://127.0.0.1/publications/pardo-IEEE-signal-processing-mag-06.pdf Finding Structure in Audio for Music Information Retrieval.] IEEE Signal Processing Magazine. vol. 49 (8), pp. 49-52, 2006\u000a* D. Little, D. Raffensperger, B. Pardo. [http://music.cs.northwestern.edu/files/ISMIR%202007%20v2.pdf A Query by Humming System that Learns from Experience.] Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27, 2007.\u000a* D. Little, D. Raffensperger and B. Pardo.[http://www.eecs.northwestern.edu/docs/techreports/2007_TR/NWU-EECS-07-03.pdf Online Training of a Music Search Engine.] Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007\u000a\u000a==External links==\u000a*[http://tunebot.cs.northwestern.edu Tunebot @ Northwestern]\u000a\u000a\u000a[[Category:Music search engines]]\u000a[[Category:Acoustic fingerprinting]]
p173
sg4
S'233'
p174
sg6
VTunebot
p175
ssI108
(dp176
g2
V{{Use mdy dates|date=September 2011}}\u000a''This article is about the U.S. National Science Foundation Office of Cyberinfrastructure .\u000a\u000aOn September 28, 2007, the U.S. [[National Science Foundation]] Office of Cyberinfrastructure announced a request for proposals with the name '''Sustainable Digital Data Preservation and Access Network Partner (DataNet)'''.<ref name="datanetprogram">{{cite web\u000a|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref>  The lead paragraph of its synopsis describes the program as:\u000a\u000a<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>\u000a\u000aThe introduction in the solicitation<ref name="datanetsolicitation">{{cite web\u000a|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref> goes on to say:\u000a\u000a<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF\u2019s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which \u201cscience and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.\u201d The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>\u000a\u000aThe initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener et al |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury et al |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. \u000a\u000aFor the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] et al |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] et al |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] et al |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a==External links==\u000a* [http://www.dataone.org DataONE]\u000a* [http://dataconservancy.org/ Data Conservancy]\u000a* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]\u000a* [http://datafed.org/ DataNet Federation Consortium]\u000a* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] \u000a \u000a\u000a[[Category:National Science Foundation]]\u000a[[Category:Science and technology in the United States]]\u000a[[Category:Information retrieval]]\u000a[[Category:Digital library projects]]
p177
sg4
S'108'
p178
sg6
VDatanet
p179
ssI238
(dp180
g2
V{{multiple issues|\u000a{{advert|date=August 2013}}\u000a{{cleanup|reason=Syntax, capitals|date=August 2013}}\u000a{{fanpov|date=August 2013}}\u000a{{Orphan|date=August 2013}}\u000a}}\u000a{{Infobox Website\u000a|name=MusicRadar(\u97f3\u4e50\u96f7\u8fbe)\u000a|logo=\u000a|screenshot=\u000a|caption=\u000a|url=http://www.doreso.com/\u000a|commercial=Yes\u000a|type=[[Music]] [[website]]\u000a|registration=Optional\u000a|owner=Shanghai Yinlong Information Technology Co., LTD\u000a|author=Shanghai Yinlong Information Technology Co., LTD\u000a|launch date=January 2013\u000a|current status=\u000a|revenue=}}\u000a\u000a'''Music radar''' is a sound-to-sound music search engine, which allows users to obtain more detailed information of music/songs by singing/humming or by recording original music. It is available on [[App Store (iOS)|App Store]]<ref>https://itunes.apple.com/cn/app/yin-le-lei-da/id635262613</ref> for [[iPhone]] and [[Google Play]]<ref>https://play.google.com/store/apps/details?id=com.voicedragon.musicclient.googleplay</ref> for [[Android (operating system)|Android]] mobiles. Music radar was launched by Shanghai Yinlong Information Technology Co., LTD in Jan. 2013.<ref>http://www.doreso.com/</ref>\u000a\u000a==Features==\u000aThe app (MusicRadar) currently has three ways of searching music: by identifying recorded original music fragment; by humming or singing the melody using microphone; and by direct input of the name of song or singer. Users could share their searching results on [[Facebook]], [[Twitter]] or other SNS website.\u000a\u000a==History==\u000aThe music radar team got the 1st place on Query by Singing/Humming (QBSH) task, Music Information Retrieval Evaluation eXchange (MIREX) 2012.<ref>http://www.music-ir.org/mirex/wiki/2012:Main_Page</ref> The app was launched for business intention at the end of January 2013, supporting query by singing/humming & audio fingerprinting. After two months, the app has reached its first one million user milestone in April, 2013. In May 2013, Music radar announces that it has integrated deep learning techniques in its query by singing/humming module to promote the recognize rate and reduce the user\u2019s waiting time. In July 2013, Music radar released China's first cloud based music recognizing openAPI to public.<ref>[http://roll.sohu.com/20130731/n383076514.shtml 2013\u5e747\u6708\uff0c\u97f3\u4e50\u96f7\u8fbe\u53d1\u5e03\u4e86\u56fd\u5185\u7b2c\u4e00\u4e2a\u201c\u97f3\u9891\u68c0\u7d22\u5f00\u653e\u4e91\u5e73\u53f0\u201d\uff0c\u63d0\u4f9b\u5f00\u653e\u7684\u97f3\u9891\u68c0\u7d22API\u3002]</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a\u000a[[Category:Acoustic fingerprinting]]\u000a[[Category:Music search engines]]\u000a[[Category:IOS software]]\u000a[[Category:Android (operating system) software]]
p181
sg4
S'238'
p182
sg6
VMusicRadar (service)
p183
ssI113
(dp184
g2
V{{essay-like|date=January 2015}}\u000a{{original research|date=January 2015}}\u000a'''Personalized search''' refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user\u2019s query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM (CACM)|year=2002|volume=45|issue=9|pages=50\u201355|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>\u000a\u000a==History==\u000a\u000aGoogle introduced Personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search set up for not just those who have a Google account but everyone as well. There is not very much information on how exactly Google personalizes their searches, however, it is believed that they use user language, location, and web history.<ref>http://personalization.ccs.neu.edu/paper.pdf</ref>\u000a\u000aEarly search engines, like [[Yahoo!]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by [[Google]], has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref>{{citation | last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to an from sites; the more links a site has, the higher it is placed on the page.<ref>{{cite AV media|last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results  indicating that those near the top are more relevant to a user's wants than those below.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref>\u000a\u000aWhile many [[search engines]] take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237\u2013251|doi=10.1002/asi.20477}}</ref> But user supplied information can be hard to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287\u2013296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581\u2013590}}</ref><ref>{{cite journal|last=Shen|first=X.|coauthors=Tan, B. and Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824\u2013831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675\u2013684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415\u2013422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>\u000a\u000aThere are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. The more you keep going the same site through a search result from Google, it believes that you like that page. So when you do certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if you're signed out, Google may personalize your results because it keeps a 180 day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>\u000a\u000aIn order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University set out to answer this question. By comparing an aggregate set of searches from logged in users against a control group, the research team found that 11.7% of results show differences due to personalization, however this varies widely by search query and result ranking position.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the IP address of the searching users. It should also be noted that results with high degress of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if you searched for "used car sales", Google may churn out results of local car dealerships in your area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000aWhen measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when you perform a search and follow it with a subsequent search, the results of the second search is influenced by the first search. An interesting point to note is that the top ranked URLs are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization, based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000a==The Filter Bubble==\u000a{{Main|Filter bubble}}\u000a\u000aSeveral concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by biasing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome." As a result people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aThe methods of personalization, and how useful it is to \u201cpromote\u201d certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the Filter Bubble happens. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal Of Pattern Recognition & Artificial Intelligence |year=2007|pages=183\u2013205}}</ref>\u000a\u000aAn area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information. This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref>http://{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|accessdate= 27 April 2014}}</ref>\u000a\u000aMany search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|coauthors=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEE transaction on knowledge and data engineering|year=2010|volume=22|issue=7|pages=969\u2013982|doi=10.1109/tkde.2009.144}}</ref>\u000a\u000aThe feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aSome have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>\u000a\u000a==The Case of Google==\u000a{{Main|Google Personalized Search}}\u000a\u000aThe perfect example of search personalization is [[Google]]. Google is not just a search engine, but a corporation that is entering every facet of our lives. Personalization with Google has gone far beyond just search. There are a host of new applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20\u201331}}</ref> that keeps track of all the information directly under one\u2019s name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the [[New York Times]]. The high level of personalization that was available with Google played a significant part in helping remain the world\u2019s most favorite search engine.\u000a\u000aOne large example of Google\u2019s ability to personalized search is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed as interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information you want. The concern, however, is that the very important information can be held back because it does not match with the criteria that the program sets for the particular user. This can create the \u201c[[filter bubble]]\u201d as described earlier.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aAn interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff, is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.\u000aGoogle\u2019s popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although many people would say that having Google personalize your search results based on what you searched previously would be a good thing, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>\u000aWith the power from this information, Google has chosen to bully its way into other sectors it owned such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.\u000a\u000aUsing Search Personalization, Google has doubled its video market share to about eighty percent. The legal definition of a monopoly is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.\u000a\u000aThe analytical firm Experian Hitwise stated that since two thousand and seven, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from thirty one percent in two thousand and seven to ten percent in two thousand and ten. Even Yahoo Images has gone from twelve percent to seven percent. It becomes very apparent that the decline of these companies has come because of Google\u2019s increase in market share from forty three percent in two thousand and seven to about fifty five percent in two thousand and nine.\u000a\u000aIt might be easy to say that all of this has come from Google being more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google\u2019s bullying because their numbers went from one point three million unique visitors to eleven point nine unique visitors in one month. That kind of growth can only come with the change of a process.\u000a\u000aIn the end, there are two things in common theme with all of these graphs. The first is that Google\u2019s market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around two thousand and seven, which is around the time that Google began to use its \u201cUniversal Search\u201d method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>\u000a\u000a==Benefits==\u000a\u000aOne of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human\u2019s capability of processing information has not expanded much.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show positive correlation between personalized search and the quality of consumers\u2019 decisions.\u000a\u000aThe first study was conducted by Kristin Diehl from University of South Carolina. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that \u2018consumers make worse choices because lower search costs cause them to consider inferior options.\u2019 It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> The study by Gerald Haubl from University of Alberta and Benedict G.C. Dellaert from Maastricht University mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers\u2019 decision quality and reduced the number of products inspected.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref>\u000a\u000a==Models==\u000a\u000aPersonalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>Coyle, M., & Smyth, B. (2007). Information recovery and discovery in collaborative web search. Advances in Information Retrieval (pp. 356\u2013367).</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.\u000a\u000aThe first model available is based on the users\u2019 historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.\u000a\u000aThere is another way to personalize search results. In Bracha Shapira and Boaz Zabar\u2019s \u201cPersonalized Search: Integrating Collaboration and Social Networks\u201d, Shapira and Zabar focused on a model that utilizes a recommendation system.<ref>Shapira, B., & Zabar, B. (2011). Personalized search: Integrating collaboration and social networks. Journal Of The American Society For Information Science & Technology, 62(1), 146-160. doi:10.1002/asi.21446</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.\u000a\u000a==Disadvantages==\u000a\u000aWhile there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users\u2019 search engine results to material that aligns with the users\u2019 interests and history. It limits the users\u2019 ability to become exposed to material that would be relevant to the user\u2019s search query but due to the fact that some of this material differs from the user\u2019s interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. \u201cObjectivity matters little when you know what you are looking for, but its lack is problematic when you do not\u201d.<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426\u2013445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref>  One of the main functions of the internet is the collection and sharing of information. This is the criticism of search personalization. It limits a core function of the web. It helps prevent users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user\u2019s search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue. The user\u2019s search results will reflect that. The user not be displayed both sides of the issue if the user\u2019s interests lean to one side or another. The user may be missing out on information that could be important. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users even though each user entered the same search query. \u201cWhen I further distilled the results, I saw that only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on\u201d.<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|volume=35.6}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.\u000a\u000aAnother disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling your internet interests and histories to other companies. This raises a privacy issue. The issue is if people are content with companies gather and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.\u000a\u000a==Sites that use Personalized Search==\u000a\u000aE. Pariser author of the Filter Bubble explains how there are differences that search personalization has on both Facebook and Google. Facebook implements personalization when it comes to the amount of things we share and also what pages we \u201clike\u201d. It also takes into consideration our social interactions, whose profile we visit the most, who we message or chat with are all indicators that are used when Facebook uses personalization. Rather than what we share being an indicator of what is filtered out, but Google takes into consideration what we \u201cclick\u201d to filter out what comes up in our searches. In addition Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and we share what other people want to see. Even while tagging photographs, Facebook uses personalization and recognition that will automatically assign a name to face for you without you having to tag them. In terms of Google we are provided similar websites and resources based on what we initially click on. This doesn't just affect Google and Facebook. There are even other websites that use the filter tactic to better adhere to user preferences. For example, Netflix also judges from the users search history to suggest movies that they may be interested in for the future. There are cites like Amazon and personal shopping cites also use other peoples history in order to serve their interests better. Twitter also uses personalization by \u201csuggesting\u201d other people to follow. In addition, based on who we \u201cfollow\u201d and who we \u201ctweet\u201d and \u201cretweet\u201d at Twitter filters out to peoples best interest for us.  Mark Zuckerberg, founder of Facebook, believed that we only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn\u2019t true. Although personalized search may seem helpful it is not a very accurate representation of who we are as people. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles in order to make themselves look better. Search personalization is not an ideal representation of any person. There are so many cites used for different purposes and that does not make up one person\u2019s identity at all that, but are in fact false representations of ourselves.<ref>http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf</ref>\u000a\u000a==Personalized Search and Online Shopping==\u000a\u000aSearch engines, such as Google and Yahoo!, utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual\u2019s web clicks, search engines can use personalized search to put forth advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in brick-and-mortar stores. These types of products and services are called long tail items.<ref>Badke, William. \u201cPersonalization and Information Literacy\u201d. Online, 47. Feb. 2012.</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.\u000a\u000aAside from aiding consumers and businesses in finding one-another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref>Inside Google. "Traffic Report: How Google Is Squeezing Out Competitors and Muscling Into New Markets." Consumer Watchdog. http://www.consumerwatchdog.org, 2 June 2010. Web.</ref> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word \u201cshoes\u201d using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer\u2019s queries.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a{{DEFAULTSORT:Personalized search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]\u000a[[Category:Internet terminology]]\u000a[[Category:Personalized search|*]]
p185
sg4
S'113'
p186
sg6
VPersonalized search
p187
ssI118
(dp188
g2
V{{refimprove|date=July 2014}}\u000a{{Infobox company\u000a| name     = Masterseek\u000a| logo     = [[Image:masterseek logo.png|260px]]\u000a| type     = [[Private company|Private]]\u000a| traded_as        = \u000a| foundation       = [[Denmark]] (1999)\u000a| founder          = [[Rasmus Refer]]\u000a| location_city    = [[New York City]]\u000a| location_country = {{nowrap|United States}}\u000a| area_served      = Worldwide\u000a| key_people       = Rasmus Refer <small>(Co-Founder, [[Chief executive officer|CEO]])</small><br />Jørgen Trygved <small>(Co-Founder)</small><br />Qasim Raza <small>([[Chief technology officer|CTO]])</small><br />Robert Perz <small>(COO, 2005-08)</small>\u000a| industry         = Internet<br>Computer software\u000a| products         = [[Business-to-business|B2B]] [[Search Engine]]\u000a| revenue          = \u000a| operating_income =\u000a| net_income       = \u000a| assets           = \u000a| num_employees    =\u000a| subsid           = [[Accoona]]\u000a| homepage         = {{URL|http://www.masterseek.com/}}\u000a| intl = yes\u000a}}\u000a\u000a'''Masterseek Corp.''' is a [[Business-to-business|B2B]] (Business to Business) [[search engine]] founded in [[Denmark]] in 1999.<ref name="hartzer"/> It currently hosts over 83 million worldwide company profiles from 75 countries,<ref name="usti"/> and business subscribers are given complete control over their corporate profiles.<ref name="hartzer"/> According to the amount of listed profiles, they are the largest B2B search engine worldwide.<ref name="seochat"/>\u000a\u000a==Founding==\u000a<!-- Deleted image removed: [[File: Rasmus refer.png|thumb|150px|left|[[Rasmus Refer]]]] -->\u000aMasterseek was founded in Denmark by [[Rasmus Refer]] in 1999.<ref name="hartzer"/> Their Denmark headquarters is located at Bredgade 29, DK-1260 Kbh. K, and they also have a current headquarters in [[New York City]], at 82 [[Wall Street]].<ref name="csc"/>\u000a\u000aAccording to its executives, Masterseek utilizes a business model based on an annual business subscription fee of USD $149, in return for which subscribers receive full editing control over their corporate profile, content and advertising, and control over widgets and embedded video, among other factors.<ref name="betaversion"/>\u000a\u000a==Finances==\u000aAs of June 2008, accountancy firm Horwart International had approximated the raw market value of the Masterseek company at $150 million.<ref name="investors"/> The company remains privately owned, but also in June 2008, it sold 10% of its authorized stocks to a range of foreign investors.<ref name="investors"/> The company announced on January 31, 2009 that they company was again offering a limited number of shares for sale in order to raise $4\u20136 million in order to gain a listing on the [[Swedish people|Swedish]] marketplace AktieTorget. Founder Refer also announced there were plans for an [[Initial Public Offering|IPO]].<ref name="ipo"/> By October 2009, they had signed with the Swedish-based company Thenberg & Kinde Fondkommission AB for financing.<ref name="seochat"/>\u000a\u000aIn the Danish company register the name Masterseek is coming up in three bankruptcies and one compulsory dissolution.<ref name="businessdk-cheats" />\u000a\u000a==Statistics==\u000aIn June 2008, the company stated it had 50 million company profiles, from over 75 countries, and handled 90,000 B2B searches daily.<ref name="hartzer"/><ref name="ipo"/><ref name="strengthen"/> The company stated they had 82 million profiles on March 21, 2011, with an average of 300,000 new profiles added monthly.<ref name="betaversion"/>\u000a\u000a==Acquiring Accoona==\u000a[[File:accoona logo.png|right|220px]]\u000aOn October 30, 2008, it was announced that Masterseek had acquired the B2B search engine [[Accoona]].<ref name="hartzer"/><ref name="paidcontent"/> \u000a"When [Business Insider] first heard about the money-losing Jersey City-based startup filing for IPO last year, [their] impulse was to run away screaming."<ref name="accoonnaddead"/> The search engine had been fairly successful in the United States and [[China]],<ref name="search"/> where it had an exclusive partnership with ''[[China Daily News]]''.<ref name="accoonnaddead"/> On August 3, 2006, ''[[Time Magazine|TIME]]'' had dubbed Accoona one of its "50 best websites," illustrating how the search engine used [[artificial intelligence]] to "understand" the meaning of keyword queries.<ref name="coolest"/> Accoona had run into difficulties and gone defunct by early October 2008, withdrawing its [[Initial Public Offering|IPO]].<ref name="accoonnaddead"/>\u000a\u000aAfter Masterseek bought the remaining search engine codes, domain name, and assets,<ref name="hartzer"/> Accoona was integrated with Masterseek, and re-launched in the USA and China. It was launched in Europe in January 2009.<ref name="search"/> Accoona information was also integrated into the Masterseek search engine.<ref name="hartzer"/>\u000a\u000a==Technology==\u000aThe Masterseek search engine relies on web crawlers that automatically collect and sort company details from the internet.<ref name="strengthen"/> Searches can look up company profiles, contact information, and descriptions of products and services. Searches can be global, national, regional, or involved local markets. Hits are listed by relevance according to search terms. There are different search options, including a specific product search, company searches, and people searches. Results can be displayed in most languages.<ref name="seochat"/> The search engine also offers MasterRank, a point system for ranking corporate websites.<ref name="Bussinessweek"/>\u000a\u000a===Beta version===\u000aMasterseek released a new [[Beta Version]] of its search engine on March 25, 2011. Before then, it was accessible to business owners, managers, and other professionals, but the Beta Version made searching the site free and open to the public.<ref name="betaversion"/>\u000a\u000a== Warnings against Masterseek Corp. ==\u000aIn December 2009 the Swedish [[Financial Supervisory Authority (Sweden)|Financial Supervisory Authority]] issued a warning against Masterseek Corp. to warn investors <ref name="fise" /><ref name="businessdk" />\u000a\u000aIn February 2010 the  issued a warning against Masterseek and related companies Bark Group and Blogger Wave.<ref name="shareholdersdk" />\u000a\u000a==Sponsorships==\u000aOn July 5, 2007, Masterseek announced they were cosponsors to [[Team CSC]], Denmark's cycling team, beginning with the team's involvement in the [[Tour de France]]. The Masterseek name began to be displayed on the team's apparel that week, with the Tour's start in [[London]].<ref name="csc"/>\u000a\u000a==Management==\u000a*[[Rasmus Refer]] - Founder, Director, President of Technology\u000a*Qasim Raza - Chief Technology Officer<ref name="Bussinessweek"/>\u000a\u000a==See also==\u000a*[[Accoona]]\u000a*[[Business-to-business|B2B]]\u000a*[[Search engines]]\u000a\u000a== References ==\u000a{{reflist|2| refs =\u000a\u000a<ref name="shareholdersdk">{{cite news\u000a|url=http://www.shareholders.dk/art/templates/pressemeddelelse.aspx?articleid=512&zoneid=29\u000a|title=Danish Shareholders Association warns against Bark Group, Blogger Wave and Masterseek\u000a|publisher=[http://shareholders.dk/ Dansk Aktionærforening]\u000a|accessdate=2014-07-27}}</ref>\u000a\u000a<ref name="businessdk">{{cite news\u000a|url=http://www.business.dk/digital/it-firma-snyder-investorer\u000a|title=IT-firma snyder investorer (IT firm cheating investors)\u000a|publisher=[http://business.dk/ Business.dk]\u000a|accessdate=2014-07-03}}</ref>\u000a\u000a<ref name="businessdk-cheats">{{cite news\u000a|url=http://bizzen.blogs.business.dk/2010/02/09/plattenslagere-skamrider-danske-varem%C3%A6rker-som-carlsberg-danisco-og-coop/\u000a|title=Plattenslagere skamrider danske varemærker som Carlsberg, Danisco og Coop (Cheats shame rides Danish brands)\u000a|publisher=[http://business.dk/ Business.dk]\u000a|accessdate=2014-07-25}}</ref>\u000a\u000a<ref name="fise">{{cite news\u000a|url=http://www.fi.se/Folder-EN/Startpage/Register/Investor-alerts/Warning-list/Warning-against-Masterseek-Corp/\u000a|title=Warning against Masterseek Corp.\u000a|publisher=Finansinspektionen\u000a|accessdate=2014-07-03}}</ref>\u000a\u000a<ref name="Bussinessweek">{{cite news\u000a|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=29327873\u000a|title=Masterseek Corp.\u000a|last=\u000a|first=\u000a|date=\u000a|publisher=''[[Businessweek]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="coolest">{{cite news\u000a|url=http://www.time.com/time/business/article/0,8599,1222614,00.html\u000a|title=50 Coolest Websites: 2006\u000a|last=Buechner\u000a|first=Maryanne\u000a|date=August 3, 2006\u000a|publisher=''[[Time Magazine|TIME]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="strengthen">{{cite news\u000a|url=http://www.reuters.com/article/2008/06/26/idUS149456+26-Jun-2008+MW20080626\u000a|title=Global Business Search Engine to Strengthen Its Advertising Network for B2B Search\u000a|last=\u000a|first=\u000a|date=June 26, 2008\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="investors">{{cite news\u000a|url=http://www.reuters.com/article/2008/06/27/idUS85249+27-Jun-2008+MW20080627\u000a|title=Search Engine is Looking for Strategic Investors\u000a|last=\u000a|first=Masterseek\u000a|date=June 27, 2008\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="accoonnaddead">{{cite news\u000a|url=http://www.businessinsider.com/2008/10/dead-search-engine-accoona-officially-dead\u000a|title=Dead Search Engine Accoona Officially Dead\u000a|last=Krangel\u000a|first=Eric\u000a|date=October 3, 2008\u000a|publisher=''[[Business Insider]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="hartzer">{{cite news\u000a|url=https://www.billhartzer.com/pages/b2b-search-engine-accoona-acquired-by-masterseek/\u000a|title=B2B Search Engine Accoona Acquired by Masterseek\u000a|last=Hartzer\u000a|first=Bill\u000a|date=November 5, 2008\u000a|publisher=BillHartzer.com: Search Engine Marketing\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="search">{{cite news\u000a|url=http://blog.searchenginewatch.com/081105-115108\u000a|title=Accoona Acquired by Masterseek\u000a|last=Johnson\u000a|first=Nathania \u000a|date=November 5, 2008\u000a|publisher=''[[Search Engine Watch]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="paidcontent">{{cite news\u000a|url=http://paidcontent.org/article/419-almost-dead-search-engine-accoona-bought-by-denmarks-masterseek/\u000a|title=Almost-Dead Search Engine Accoona Bought by Denmark's Masterseek\u000a|last=\u000a|first=\u000a|date=November 2008\u000a|publisher=''Paid Content''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="ipo">{{cite news\u000a|url=http://www.reuters.com/article/2009/01/31/idUS85625+31-Jan-2009+MW20090131\u000a|title=Masterseek.com Is Planning for an IPO\u000a|last=\u000a|first=Masterseek\u000a|date=January 31, 2009\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="seochat">{{cite news\u000a|url=http://www.seochat.com/c/a/Search-Engine-News/Masterseek-a-Global-Business-Search-Engine/\u000a|title=Masterseek: a Global Business Search Engine\u000a|last=Morgan\u000a|first=KC\u000a|date=October 27, 2009\u000a|publisher=''SEOchat''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="betaversion">{{cite news\u000a|url=http://www.wiredprnews.com/2011/03/21/masterseek-expeced-to-release-beta-version-of-search-engine-continue-dominance-in-business-to-business-search_2011032117895.html\u000a|title=Masterseek Expected to Release Beta Version of Search Engine\u000a|last=\u000a|first=\u000a|date=March 21, 2011\u000a|publisher=WirePRNews\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="usti">{{cite news\u000a|url=http://usbusinesstimes.com/internet/3424-business-search-powerhouse-masterseek-partners-with-cutting-edge-job-database-simply-hired.html\u000a|title=Masterseek.com Partners with Simply Hired\u000a|last=\u000a|first=admin\u000a|date=April 16, 2011\u000a|publisher=''US Business Times''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="csc">{{cite news\u000a|url=http://www.global-business-profiles.com/masterseek-cosponsors-team-csc/\u000a|title=Masterseek Cosponsors Team Csc\u000a|last=\u000a|first=\u000a|date=May 4, 2011\u000a|publisher=Global Business Profiles\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a}}\u000a\u000a==External links==\u000a*{{Official website|http://www.masterseek.com/}}\u000a*[http://twitter.com/#!/masterseek_tw1 Masterseek] on [[Twitter]]\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Web service providers]]\u000a[[Category:Internet properties established in 1999]]\u000a[[Category:Business services companies established in 1999]]\u000a[[Category:Information retrieval]]\u000a[[Category:Online companies]]\u000a[[Category:Software companies established in 1999]]\u000a\u000a[[da:Dansk Aktionærforening|Danish Shareholders Association]]
p189
sg4
S'118'
p190
sg6
VMasterseek
p191
ssI123
(dp192
g2
V{{COI|date=July 2014}}\u000a{{Original research|date=July 2014}}\u000a{{Use dmy dates|date=February 2012}}\u000a'''Multimedia Information Retrieval''' (MMIR or MIR) is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:\u000a\u000a# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.\u000a# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])\u000a# Methods for the [[categorization]] of media descriptions into classes.\u000a\u000a== Feature Extraction Methods ==\u000a\u000aFeature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 2.</ref>{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:\u000a\u000a* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel Frequency Cepstral Coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. " Visual Information Retrieval ", Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.\u000a* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim , N Moreau, T Sikora. " MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.\u000a\u000a== Merging and Filtering Methods ==\u000a\u000aMultimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). " Principles of Visual Information Retrieval ", Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions - as they frequently occur in motion description - have to be normalized to a fixed length first.\u000a\u000aFrequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.\u000a\u000a== Categorization Methods ==\u000a\u000aGenerally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011,p. 125.</ref>{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[Hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[Dynamic Time Warping]] - a semantically related method - is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:\u000a\u000a* Metric approaches ([[Cluster Analysis]], [[Vector Space Model]], [[Minkowski]] Distances, Dynamic Alignment)\u000a* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-Means, [[Self-Organizing Map]])\u000a* Risk Minimization (Support Vector Regression, [[Support Vector Machine]], [[Linear Discriminant Analysis]])\u000a* Density-based Methods (Bayes Nets, [[Markov Processes]], Mixture Models)\u000a* Neural Networks ([[Perceptron]], Associative Memories, Spiking Nets)\u000a* Heuristics ([[Decision Trees]], Random Forests, etc.)\u000a\u000aThe selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.\u000a\u000a== Open Problems ==\u000a\u000aThe quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. " Frontiers of Media Understanding ", atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.\u000a\u000a== Related Areas ==\u000a\u000aMMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. " Professional Media Understanding ", atpress, 2012.</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:\u000a\u000a* [[Bioinformatics|Bioinformation Analysis]]\u000a* [[Biosignal|Biosignal Processing]]\u000a* [[Content-based image retrieval|Content-based Image and Video Retrieval]]\u000a* [[Facial recognition system|Face Recognition]]\u000a* [[Music information retrieval|Audio and Music Classification]]\u000a* [[Speech Recognition]]\u000a* [[Technical analysis|Technical Chart Analysis]]\u000a* [[Information retrieval|Text Information Retrieval]]\u000a\u000aThe Journal of Multimedia Information Retrieval<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also <ref>H Eidenberger. " Handbook of Multimedia Information Retrieval ", atpress, 2012.</ref> for a complete overview over this research discipline.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a<!--\u000a\u000a<ref>AUTH. "TITLE", PUB, YEAR.</ref>\u000a<ref>AUTH. "[LINK TITLE]", MEDIA, PRODDATE. Retrieved UPDATE.</ref>\u000a\u000a-->\u000a\u000a\u000a\u000a[[Category:Information retrieval]]
p193
sg4
S'123'
p194
sg6
VMultimedia Information Retrieval
p195
ss.
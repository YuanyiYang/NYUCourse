(dp0
I1
(lp1
VProximity search (text)
p2
aVIn [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.\u000a\u000aFor example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.\u000a\u000a== Rationale ==\u000aThe basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.\u000a\u000aCommercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].\u000a\u000a== Boolean syntax and operators ==\u000aNote that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".\u000a\u000a== Usage in commercial search engines ==\u000aIn regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).\u000a\u000a[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]] and [[Altavista]]:\u000a* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.<ref>[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009</ref>\u000a* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is <tt>(keyword1 NEAR/n keyword2)</tt> where n is the number of words.<ref>[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009</ref>\u000a* [[Yandex]] uses the syntax <tt>keyword1 /n keyword2</tt> to search for two keywords separated by at most <math>n - 1</math> words, and supports a few other variations of this syntax.<ref>[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)</ref>\u000a* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.<ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)</ref><ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)</ref> The syntax is <tt>keyword1 NEAR keyword2</tt>.\u000a* Google supports AROUND(#).<ref>[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search's Little Known AROUND Operator"]</ref>\u000a\u000aOrdered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,<ref>[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]</ref> and an in Yahoo! Search this matches exactly one word.<ref>[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]</ref>  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)\u000a\u000aTo emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".\u000a\u000a== See also ==\u000a* [[Compound term processing]]\u000a* [[Edit distance]]\u000a* [[Information retrieval]]\u000a* [[Search engine]]\u000a* [[Search engine indexing]] - how texts are indexed to support proximity search\u000a* [[Semantic proximity]]\u000a\u000a== Notes ==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search algorithms]]
p3
asI132
(lp4
VSearch engine technology
p5
aV{{multiple issues|\u000a{{Refimprove|date=May 2014}}\u000a{{Tone|article|date=January 2013}}\u000a}}\u000aA search engine is a type of computer software used to search data in the form of text or a database for specified information.<ref>{{cite web|title=Define search engine|url=http://www.webopedia.com/TERM/S/search_engine.html|accessdate=1 June 2014}}</ref>\u000a\u000aSearch engines normally consist of spiders (also known as bots) which roam the web searching for links and keywords. They send collected data back to the indexing software which categorizes and adds the links to databases with their related keywords. When you specify a search term the engine does not scan the whole web but extracts related links from the database.\u000a\u000a==History of Search Technology==\u000a\u000a{{Empty section|date=July 2014}}\u000a\u000a== The Memex ==\u000a\u000aThe concept of hypertext and a memory extension originates from an article that was published in [[The Atlantic Monthly]] in July 1945 written by [[Vannevar Bush]], titled [[As We May Think]].  Within this article Vannevar urged scientists to work together to help build a body of knowledge for all mankind. He then proposed the idea of a virtually limitless, fast, reliable, extensible, associative memory storage and retrieval system. He named this device a [[memex]].<ref>{{cite journal|last1=Yeo|first1=Richard|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|page=21|doi=10.1017/S0269889706001128}}</ref>\u000a\u000aBush regarded the notion of \u201cassociative indexing\u201d as his key conceptual contri- bution. As he explained, this was \u201ca provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex. The process of tying two items together is the important thing.\u201d This \u201clinking\u201d (as we now say) constituted a \u201ctrail\u201d of documents that could be named, coded, and found again. Moreover, after the original two items were coupled, \u201cnumerous items\u201d could be \u201cjoined together to form a trail\u201d; they could be \u201creviewed in turn, rapidly or slowly, by deflecting a lever like that used for turning the pages of a book. It is exactly as though the physical items had been gathered together from widely separated sources and bound together to form a new book\u201d<ref>{{cite journal|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|pages=21\u201347|doi=10.1017/S0269889706001128|accessdate=1 June 2014|postscript=The example Bush gives is a quest to find information on the relative merits of the Turkish short bow and the English long bow in the crusades}}</ref>\u000a\u000aAll of the documents used in the memex would be in the form of microfilm copy acquired as such or, in the case of personal records, transformed to microfilm by the machine itself. Memex would also employ new retrieval techniques based on a new kind of associative indexing the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another to create personal "trails" through linked documents. The new procedures, that Bush anticipated facilitating information storage and retrieval would lead to the development of wholly new forms of encyclopedia.\u000a\u000aThe most important mechanism, conceived by Bush and considered as closed to the modern hypertext systems is the associative trail. It would be a way to create a new linear sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and side trails.\u000aThe essential feature of the memex [is] the process of tying two items together\u2026 When the user is building a trail, he names it in his code book, and taps it out on his keyboard. Before him are the two items to be joined, projected onto adjacent viewing positions. At the bottom of each there are a number of blank code spaces, and a pointer is set to indicate one of these on each item. The user taps a single key, and the items are permanently joined\u2026 Thereafter, at any time, when one of these items is in view, the other can be instantly recalled merely by tapping a button below the corresponding code space.\u000a\u000aIn the article of Bush is not described any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set. Instead, when the user made an entry, such as a new or annotated manuscript, or image, he was expected to index and describe it in his personal code book. Later on, by consulting his code book, the user could retrace annotated and generated entries.\u000a\u000aIn 1965 Bush took part in the project INTREX of MIT, for developing technology for mechanization the processing of information for library use. In his 1967 essay titled "Memex Revisited", he pointed out that the development of the digital computer, the transistor, the video, and other similar devices had heightened the feasibility of such mechanization, but costs would delay its achievements. He was right again.\u000a\u000aTed Nelson, who later did pioneering work with first practical hypertext system and coined the term "hypertext" in the 1960s, credited Bush as his main influence.<ref>{{cite web|title=The MEMEX of Vannevar Bush|url=http://history-computer.com/Internet/Dreamers/Bush.html}}</ref>\u000a\u000a== SMART ==\u000a\u000aGerard Salton, who died on August 28 of 1995, was the father of modern search technology. His teams at Harvard and Cornell developed the SMART informational retrieval system. Salton\u2019s Magic Automatic Retriever of Text included important concepts like the vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), term discrimination values, and relevancy feedback mechanisms.\u000a\u000aHe authored a 56 page book called A Theory of Indexing which explained many of his tests upon which search is still largely based.\u000a\u000a== String Search Engines ==\u000a\u000aIn 1987 an article was published detailing the development of a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-\u03bcm n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a content addressable memory (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.<ref>{{cite journal|last=Yamada|first=H.|author2=Hirata, M. |author3=Nagai, H. |author4= Takahashi, K. |title=A high-speed string-search engine|journal=IEEE Journal of Solid-State Circuits|date=Oct 1987|volume=22|issue=5|pages=829\u2013834|doi=10.1109/JSSC.1987.1052819|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1052819&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1052819|accessdate=30 May 2014|publisher=IEEE}}</ref>\u000a\u000a<!-- Potential source for article expansion:  http://ieeexplore.ieee.org/search/searchresult.jsp?queryText%3Dsearch-engine&sortType=asc_p_Publication_Year&pageNumber=1&resultAction=SORT -->\u000a\u000a== Web Search Engines ==\u000a\u000a=== Archie ===\u000a\u000aThe first web search engines was Archie, created in 1990<ref name="intelligent-technologies">{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=87|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program "archives," but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. For more information on where Archie is today, see:\u000ahttp://www.bunyip.com/products/archie/\u000a\u000aThe primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\u000a\u000aInitially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, "anonymous" FTP sites became repositories for files, allowing all users to post and retrieve them.\u000a\u000aEven with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\u000a\u000aArchie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.<ref name="wileyhistory">{{cite web|title=A History of Search Engines|url=http://www.wiley.com/legacy/compbooks/sonnenreich/history.html|publisher=Wiley|accessdate=1 June 2014}}</ref>\u000a\u000a=== Veronica ===\u000a\u000aIn 1993, the University of Nevada System Computing Services group developed Veronica.<ref name="intelligent-technologies"/> It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.<ref name="wileyhistory"/>\u000a\u000a=== The Lone Wanderer ===\u000a\u000aThe World Wide Web Wanderer, developed by Matthew Gray in 1993<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=86|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\u000a\u000aMatthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\u000a\u000aIn response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\u000a\u000aALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot doesn't run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they don't submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.<ref name="wileyhistory"/>\u000a\u000a=== Excite ===\u000a\u000aExcite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\u000aTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.<ref name="wileyhistory"/>\u000a\u000aExcite was the first serious commercial search engine which launched in 1995.<ref>{{cite web|title=The Major Search Engines|url=http://www.pccua.edu/kholland/major_search_engines.htm|accessdate=1 June 2014|date=21 January 2014}}</ref> It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\u000a\u000a=== Yahoo! ===\u000a\u000aIn April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\u000a\u000aAs the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\u000a\u000aThe Wanderer captured only URLs, which made it difficult to find things that weren\u2019t explicitly described by their URL. Because URLs are rather cryptic to begin with, this didn\u2019t help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\u000a\u000a=== Lycos ===\u000a\u000aAt Carnegie Mellon University during the July of 1994, Michael Mauldin, on leave from CMU,developed the Lycos search engine.\u000a\u000a== Types of Web Search Engines ==\u000a\u000aSearch engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=85|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref>\u000a\u000a# Finding and selecting full or partial content based on the keywords provided.\u000a# Maintaining index of the content and referencing to the location they find\u000a# Allowing users to look for words or combinations of words found in that index.\u000a\u000aThe process begins when a user enters a query statement into the system through the interface provided.\u000a\u000a{| class="wikitable"\u000a|-\u000a! Type\u000a! Example\u000a! Description\u000a|-\u000a| Conventional\u000a| librarycatalog\u000a| Search by keyword, title, author, etc.\u000a|-\u000a| Text-based\u000a| Lexis-Nexis,Google,Yahoo!\u000a| Search by keywords. Limited search using queries in natural language.\u000a|-\u000a| Multimedia\u000a| QBIC, WebSeek, SaFe\u000a| Search by visual appearance (shapes, colors,..)\u000a|-\u000a| Q/A\u000a| [[Stack Exchange]], NSIR\u000a| Search in (restricted) natural language\u000a|-\u000a| Clustering Systems\u000a| Vivisimo, Clusty\u000a|\u000a|-\u000a| Research Systems\u000a| Lemur, Nutch\u000a|\u000a|}\u000a\u000aThere are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\u000a\u000aCrawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\u000a\u000aHuman-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\u000a\u000aIn both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created \u2014you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index hasn't been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\u000a\u000aSo why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\u000a\u000aOne of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\u000a\u000aAnother common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered "important" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\u000a\u000aModern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. [[Google]]), database or structured data search engines (e.g. [[Dieselpoint]]), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and [[Yahoo!]], utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\u000a\u000a==Search engine categories==\u000a\u000a===Web search engines===\u000aSearch engines that are expressly designed for searching web pages, documents, and images were developed to facilitate searching through a large, nebulous blob of unstructured resources. They are engineered to follow a multi-stage process: crawling the infinite stockpile of pages and documents to skim the figurative foam from their contents, indexing the foam/buzzwords in a sort of semi-structured form (database or something), and at last, resolving user entries/queries to return mostly relevant results and links to those skimmed documents or pages from the inventory.\u000a\u000a====Crawl====\u000aIn the case of a wholly textual search, the first step in classifying web pages is to find an \u2018index item\u2019 that might relate expressly to the \u2018search term.\u2019 In the past, search engines began with a small list of URLs as a so-called seed list, fetched the content, and parsed the links on those pages for relevant information, which subsequently provided new links. The process was highly cyclical and continued until enough pages were found for the searcher\u2019s use.\u000aThese days, a continuous crawl method is employed as opposed to an incidental discovery based on a seed list. The crawl method is an extension of aforementioned discovery method. Except there is no seed list, because the system never stops worming.\u000a\u000aMost search engines use sophisticated scheduling algorithms to \u201cdecide\u201d when to revisit a particular page, to appeal to its relevance. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of chance, popularity, and overall quality of site. The speed of the web server running the page as well as resource constraints like amount of hardware or bandwidth also figure in.\u000a\u000a====Link map====\u000aThe pages that are discovered by web crawls are often distributed and fed into another computer that creates a veritable map of resources uncovered. The bunchy clustermass looks a little like a graph, on which the different pages are represented as small nodes that are connected by  links between the pages. \u000aThe excess of data is stored in multiple data structures that permit quick access to said data by certain algorithms that compute the popularity score of pages on the web based on how many links point to a certain web page, which is how people can access any number of resources concerned with diagnosing psychosis. Another example would be the accessibility/rank of web pages containing information on Mohamed Morsi versus the very best attractions to visit in Cairo after simply entering \u2018Egypt\u2019 as a search term. One such algorithm, [[PageRank]], proposed by Google founders Larry Page and Sergey Brin, is well known and has attracted a lot of attention because it highlights repeat mundanity of web searches courtesy of students that don\u2019t know how to properly research subjects on Google.\u000aThe idea of doing link analysis to compute a popularity rank is older than PageRank. Other variants of the same idea are currently in use \u2013 grade schoolers do the same sort of computations in picking kickball teams. But in all seriousness, these ideas can be categorized into three main categories: rank of individual pages and nature of web site content. Search engines often differentiate between internal links and external links, because web masters and mistresses are not strangers to shameless self-promotion. Link map data structures typically store the anchor text embedded in the links as well, because anchor text can often provide a \u201cvery good quality\u201d summary of a web page\u2019s content.\u000a\u000a===Database Search Engines===\u000aSearching for text-based content in databases presents a few special challenges from which a number of specialized search engines flourish. Databases can be slow when solving complex queries (with multiple logical or string matching arguments). Databases allow pseudo-logical queries which full-text searches do not use. There is no crawling necessary for a database since the data is already structured. However, it is often necessary to index the data in a more economized form to allow a more expeditious search.\u000a\u000a===Mixed Search Engines===\u000aSometimes, data searched contains both database content and web pages or documents. Search engine technology has developed to respond to both sets of requirements. Most mixed search engines are large Web search engines, like Google. They search both through structured and unstructured data sources. Take for example, the word \u2018ball.\u2019 In its simplest terms, it returns more than 40 variations on Wikipedia alone. Did you mean a ball, as in the social gathering/dance? A soccer ball? The ball of the foot? Pages and documents are crawled and indexed in a separate index. Databases are indexed also from various sources. Search results are then generated for users by querying these multiple indices in parallel and compounding the results according to \u201crules.\u201d\u000a\u000a<!-- \u000aWorking on article, loosely pasting in snippets of information to use in improving \u000aarticle later, leaving all this in comments while I work on it\u000a\u000aLOTS OF WORK TO DO\u000a\u000aPotential sections to research into..\u000a\u000a== Models of Information Retrieval ==\u000a=== Boolean Model ===\u000a=== Vector Model ===\u000a\u000a== Document Preprocessing ==\u000a# Tokenization ===\u000a# Stemming ===\u000a# The Porter Algorithm\u000a# Storing, indexing, and searching text\u000a#Inverted indexes\u000a\u000a== Word Distributions ==\u000aThe Zipf distribution\u000aThe Benford distribution\u000aHeap's law. TF*IDF. Vector space similarity and ranking.\u000a\u000a== Retrieval evaluation ==\u000a Precision and Recall. F-measure. Reference collections. The TREC conferences.\u000a\u000a== Automated indexing/labeling ==\u000a. Compression and coding. Optimal codes.\u000a\u000a== String matching ==\u000a. Approximate matching.\u000a\u000a== Query expansion ==. Relevance feedback.\u000a\u000a== Text classification ==\u000a. Naive Bayes. Feature selection. Decision trees.\u000a\u000aLinear classifiers. k-nearest neighbors. Perceptron. Kernel methods. Maximum-margin classifiers. Support vector machines. Semi-supervised learning.\u000aLexical semantics and Wordnet.\u000aLatent semantic indexing. Singular value decomposition. Vector space clustering. k-means clustering. EM clustering.\u000aRandom graph models. Properties of random graphs: clustering coefficient, betweenness, diameter, giant connected component, degree distribution.\u000aSocial network analysis. Small worlds and scale-free networks. Power law distributions. Centrality.\u000aGraph-based methods. Harmonic functions. Random walks. PageRank. Hubs and authorities. Bipartite graphs. HITS. Models of the Web.\u000a\u000aCrawling the web. Webometrics. Measuring the size of the web. The Bow-tie-method.\u000aHypertext retrieval. Web-based IR. Document closures. Focused crawling.\u000aQuestion answering\u000aBurstiness. Self-triggerability\u000aInformation extraction\u000aAdversarial IR. Human behavior on the web. Text summarization\u000a\u000a== Search Engine Parts ==\u000a\u000aThere are three main parts to every search engine: Spider, Index, and Web Interface.\u000a\u000a=== Spider === \u000a   \u000aA spider crawls the web. It follows links and scans web pages. All search engines have periods of deep crawl and quick crawl. During a deep crawl, the spider follows all links it can find and scans web pages in their entirety. During a quick crawl, the spider does not follow all links and may not scan pages in their entirety.\u000a\u000aThe job of the spider is to discover new pages and to collect copies of those pages, which are then analyzed in the index.\u000a\u000a==== Crawl Rate ====\u000a\u000aPages that are considered important get crawled frequently. The crawl rate depends directly on link popularity and domain authority.\u000a\u000aIf many links point to a website, it may be an important site, so it makes sense to crawl it more often than a site with fewer links. This is also a money-saving issue. If search engines were to crawl all sites at an equal rate, it would take more time overall and cost more as a result.\u000a\u000a=== Index ===\u000a\u000aThe index is the place where search engines keep basic copies of web pages and sort search results. When you a do a search, search engines do not search the web; they show results from their index. The number of pages in the index does not represent the entire web, but the number of pages that the spider has discovered, scanned and saved.\u000a\u000aThe index is the place where search engineers apply algorithms, and it is the place where rankings are partially determined. Search engineers may choose to apply an algorithm to the entire index, or only to a portion of it.\u000a\u000a==== Datacenters and Different Indexes ====\u000a\u000aSearch engines have multiple datacenters around the world. When you enter a search term, your query is directed to the closest datacenter.\u000a\u000aDifferent datacenters may have slightly different indexes, especially during an update. As a result, search results may differ depending on your location.\u000a\u000a== History ==\u000a\u000a=== Meta Tags ===\u000a\u000aMeta tags were designed to help search engines sort web pages. Pages included keywords in meta tags telling search engines about the contents of each page. For a short time meta tags worked and helped search engines serve relevant results, but over time marketers learned they could easily rank by stuffing those tags with keywords.\u000a\u000aAs a result, search engine optimization in those days became about cramming "loans, loans, loans, loans, loans" into the meta tag. Search engines got spammed beyond being of any use, and many faced an exodus of users as a result.\u000a\u000aYahoo started as web directory in 1994 and outsourced their search until 2004. Google launched in 1996 and did not have a successful business model until 2001. Microsoft did not come on the search engine scene until 2003.\u000aor more information on search engine history, you may want to investigate Search Engine History, a site entirely devoted to this topic. It also touches on the history of search engine optimization. Additionally, Web Master World has an excellent thread that covers the history of SEO.\u000a\u000aWeb Interface\u000a\u000aWhen you search using a web interface (like Google.com), in many cases results are already presorted to a certain extent. The degree to which results are presorted depends on the complexity of the algorithm. If the time to apply an algorithm to the index is considerable, then that algorithm is applied in advance. On the other hand, some algorithms are applied at the time when the search query is requested.\u000a\u000aSearch queries go through analysis to determine the possible intent behind the query. Google is currently leading in this area.\u000a\u000aStop Words\u000a\u000a"Stop words" are words that are frequently used in the English language. Those words include a, the, all, also, but, down, full, much etc. They are words that are used by everyone regardless of the topic. Generally, search engines ignore "stop" words and will usually correct your search to exclude them. For example, when you search for "cat and dog" search engines will exclude "and" and only search for "cat" "dog."\u000a\u000aGoogle does use stop words to an extent.\u000a\u000aKeyword Density\u000a\u000aKeyword density is a measure of how often a word appears on the page in relation to other words. It is an over-hyped measurement that doesn\u2019t help in search rankings. Search engines use far more than keyword density for on-page analysis. Their technology includes the location of terms on the page, word proximity and natural language processing.\u000a\u000aGoogle has purchased Applied Semantics for its AdSense Network, but may also be using this technology for on-page analysis. Additionally, please keep in mind that one of Google\u2019s current projects involves scanning thousands of books, from which it may learn more about natural language patterns.\u000a\u000aLocation of Terms on The Page\u000a\u000aBy analyzing how terms are located in relation to each other on the page, search engines can determine partial relevancy of the page. The closer terms are to each other, the more relevant a page is.\u000a\u000aIn many cases, keywords appear separately from each other throughout the page. This is considered normal in most cases, but be sure to include a term together at least once in the title, heading or paragraph.\u000a\u000aLink Analysis\u000a\u000aLink analysis is at the core of all search engine relevancy. Apart from Page Rank and general link popularity, Google looks at: link anchor text, the page from which the link comes, age of the link, location of the link, title of the page from which the link comes, authority of the linking page and more.\u000a\u000aLinks are the biggest quality indicators that search engines have at the moment. Before search engines existed, and before the web was commercialized it was much harder to find information. All you had to rely on was links. There were few if any spammers, and people who found interesting sites shared those sites with others by placing a link. Also, the first web pages and servers were universities and colleges; this is why Google is biased toward .edu domains \u2013 they were the first on the scene, and usually contain quality content and resources.\u000a\u000aAs the web became commercial and Google\u2019s Page Rank well known, links became a form of advertising, where a link could be bought or artificially made by spammers. This is the reason for Google\u2019s bias toward older links and links from trusted domains.\u000a\u000aYahoo put less weight on link analysis than Google, while Ask.com is more about "authoritative hubs." Ask.com generally has a harder time ranking documents unless there\u2019s a community around a topic.\u000a\u000aSize and Length of the Page\u000a\u000aThere\u2019s no "best" page copy length for ranking on search results. Search engines have specifically addressed this issue, and both long content and short content have equal chances to rank.\u000a\u000aBehavioral Feedback\u000a\u000aAll major search engines such as Google, Yahoo, Live and Ask collect user feedback about web pages. They look at search queries, prior search queries, time interval between those queries and semantic relationships in order to learn more about intent. They also track click through rates for different listings. If, for example, users click on a listing and then go back right away, search engines may remove that listing and artificially lower its position for one or more keywords.\u000a\u000aThis brings up the fact that user experience is becoming an important part of SEO. As search engines collect more data, they are constantly learning to interpret it. As they get better at it, retaining users on your pages for a certain time period (maybe a benchmark for an industry) may become an important factor in the SEO game.\u000a\u000aBehavior feedback is currently used in personalized search.\u000a<ref>{{cite web|url=http://www.seochat.com/c/a/search-engine-news/the-history-of-search-and-search-technology/|accessdate=1 June 2014}}</ref>\u000a-->\u000a\u000a==See also==\u000a*[[Database search engine]]\u000a*[[Enterprise search]]\u000a*[[Search engine]]\u000a*[[Disambiguation]]\u000a*[[Search engine indexing]]\u000a*[[Web crawler]]\u000a*[[Structured Search]]\u000a\u000a==External links==\u000a* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\u000a* [http://www.urbandictionary.com/define.php?term=Searchency Searchency]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Search Engine Technology}}\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p6
asI134
(lp7
VCategory:Vector space model
p8
aV[[Category:Information retrieval]]
p9
asI8
(lp10
VNational Centre for Text Mining
p11
aVThe '''National Centre for Text Mining''' (NaCTeM)\u000a<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community. \u000a\u000aThe [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest - examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].\u000a\u000aThe Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[information extraction]], [[natural language processing]] and parallel and distributed data mining systems in biomedical and clinical applications.\u000a\u000a==Services==\u000a[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them. <ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117\u2013132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref> \u000a\u000a[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[Disambiguation|disambiguates]] them.<ref name="pmid17050571">{{cite journal| author=Okazaki N, Ananiadou S| title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089\u201395 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }} </ref>\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts.\u000a\u000a[http://refine1-nactem.mc.man.ac.uk/facta/ ''' Facta+'''] is a MEDLINE search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal| author=Tsuruoka Y, Tsujii J, Ananiadou S| title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559\u201360 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }} </ref>\u000a\u000a[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system based on MEDLINE.\u000a\u000a[https://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].\u000a\u000a==Resources==\u000a\u000a[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] a large-scale terminological resource for the biomedical domain\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+corpus '''GENIA'''] a collection of reference materials for the development of biomedical text mining systems\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* http://www.nactem.ac.uk\u000a\u000a[[Category:Computational linguistics]]\u000a[[Category:Computer science organizations]]\u000a[[Category:Information retrieval]]\u000a[[Category:Linguistics organizations]]\u000a[[Category:School of Computer Science, University of Manchester]]
p12
asI137
(lp13
VPolySpot
p14
aV{{Infobox company\u000a| name = PolySpot\u000a| logo = [[File:PolySpot-Logo.jpg|300px]]\u000a| type = [[Privately held company|Private]]\u000a| foundation = [[Paris]] (2001)\u000a| location = [[Paris]], [[London]]\u000a| key_people = Guy Mounier, CEO\u000a| industry = [[Information technology]] <br/> [[Unified Information Access]] <br/> [[Search Engine]]\u000a| products = PolySpot Infowarehouse<br/>PolySpot Information At Work<br/>PolySpot Enterprise Search\u000a| slogan = Open Search Solutions\u000a| homepage = [http://www.polyspot.com/en/ www.polyspot.com]\u000a}}\u000a'''PolySpot''' is a subsidiary of CustomerMatrix, an [[enterprise search]] [[ISV|software company]].\u000a\u000aCreated in 2001, PolySpot has its headquarters in Paris, France. It also has offices in the United Kingdom.\u000a\u000aIn 2011, PolySpot raised EUR 2.5m from [[Newfund]].<ref>{{cite web|url=http://finance.yahoo.com/news/PolySpot-Raises-2-Million-prnews-4194799492.html| title=Yahoo News: PolySpot Raises 2 Million}}</ref> and True Global Ventures.\u000a\u000aIn 2013, CustomerMatrix (US) acquired PolySpot. In December 2013, PolySpot reduced its shareholders equity from 507,209 EUR to 206,120 EUR.\u000a\u000a== Functionalities ==\u000a\u000aPolySpot's infrastructure provide a unified information access to all data, through which users can instantly interact with all available information resources, both inside and outside the company, and regardless of whether or how the data are structured.\u000a\u000aPolySpot's indexing capabilities are based on the [[Apache Software Foundation|Apache]] [[Lucene]] [[Solr]] Java-based open-source projects.\u000a\u000a==References==\u000a{{Portal|Software}}\u000a{{Reflist|colwidth=30em}}\u000a\u000a==External links==\u000a*[http://www.polyspot.com/en/ Website]\u000a*[http://www.customermatrix.com/ CustomerMatrix Website]\u000a*[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=38687243 Company profile in BusinessWeek]\u000a*[http://www.arnoldit.com/search-wizards-speak/polyspot-2.html PolySpot in ArnoldIT]\u000a*[http://arnoldit.com/wordpress/2010/01/13/polyspot-lands-crdit-agricole-sa/ PolySpot lands Credit Agricole in ArnoldIT]\u000a\u000a{{DEFAULTSORT:Polyspot}}\u000a[[Category:Searching]]\u000a[[Category:Search engine software|*Enterprise search vendors]]\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p15
asI23
(lp16
VDynatext
p17
aV{{primary sources|date=October 2011}}\u000a'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.\u000a\u000aDynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.\u000a\u000aDynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996.\u000a\u000aDynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as the origin of the notion of [[well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].\u000a\u000a[[Inso]] corporation went out of business in 2002. \u000a\u000a==References==\u000a*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{markup-languages-stub}}
p18
asI12
(lp19
VBASE (search engine)
p20
aV{{multiple issues|\u000a{{notability|Web|date=February 2012}}\u000a{{refimprove|date=June 2009}}\u000a{{primary sources|date=February 2012}}\u000a{{one source|date=February 2012}}\u000a{{no footnotes|date=February 2012}}\u000a}}\u000a\u000a'''BASE''' ('''Bielefeld Academic Search Engine''') is a multi-disciplinary [[search engine]] to scholarly internet resources, created by [[Bielefeld University]] Library in [[Bielefeld]], [[Germany]]. It is based on search technology provided by [[Fast Search & Transfer]] (FAST), a [[Norway|Norwegian]] company. It [[Web harvesting|harvests]] OAI metadata from scientific [[Digital repository|digital repositories]] that implement the [[Open Archives Initiative Protocol for Metadata Harvesting]] (OAI-PMH), and are [[Index (search engine)|indexed]] using FAST's software. In addition to OAI [[metadata]], the library indexes selected web sites and local data collections, all of which can be searched via a single search interface.\u000a\u000aIt allows those who use the search engine to search metadata, when available, as well as conducting [[full text search]]es. It contrasts with commercial search engines in multiple ways, including in the types and kinds of resources it searches and the information it offers about the results it finds. Where available, [[Bibliographic database|bibliographic data]] is provided, and the results may be sorted by multiple fields, such as by author or year of publication.\u000a\u000a== See also ==\u000a* [[List of academic databases and search engines]]\u000a\u000a==External links==\u000a* [http://www.base-search.net/ BASE search]\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Open access (publishing)]]\u000a[[Category:Bibliographic databases]]\u000a\u000a\u000a{{software-stub}}
p21
asI13
(lp22
VMooers' law
p23
aV{{For|the observation regarding integrated circuits|Moore's law}}\u000a{{Refimprove|date=September 2011}}\u000a\u000a'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.\u000a\u000a{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=http://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers'%20law&f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}\u000a\u000a==Original interpretation==\u000a\u000aMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the users personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>\u000a\u000a==Out-of-context interpretation==\u000a\u000aThe more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:\u000a\u000a{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}\u000a{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}\u000a\u000aIn this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=pp 607\u2013609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}\u000a\u000a==External links==\u000a* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.\u000a* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.\u000a* [http://www.phillyimc.org/en/gasoline-7-17-moors-law-kent-moors-authority Another empirical observation with a similar-sounding name is Moors' law], named for Kent Moors of Duquesne University, which states crude oil prices double every five years. \u000a[[Category:Empirical laws]]\u000a[[Category:Library science]]\u000a[[Category:Information retrieval]]
p24
asI15
(lp25
VLatent semantic mapping
p26
aV'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.\u000a\u000aLSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.\u000a\u000a[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>\u000a\u000a== See also ==\u000a* [[Latent semantic analysis]]\u000a\u000a== Notes ==\u000a{{reflist}}\u000a\u000a== References ==\u000a* {{cite journal\u000a | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf\u000a | title=Latent semantic mapping [information retrieval]\u000a | author=Bellegarda, J.R.\u000a | date=2005\u000a}}\u000a* {{cite conference\u000a | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp\u000a | title=Latent semantic mapping: Principles and applications\u000a | author=J. Bellegarda\u000a | booktitle=ICASSP 2006\u000a | date=2006\u000a}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a\u000a\u000a{{semantics-stub}}\u000a{{compu-stub}}
p27
asI24
(lp28
VPoison words
p29
aV{{Missing information|Examples of poison words|date=September 2008}}\u000a{{Unreferenced|date=December 2007}}\u000a\u000a'''Poison words''', or '''forbidden words''', is the name given to words or phrases that trigger suspicion, mistrust and loss of respect, or are of inappropriate character for a given web site in its consideration for a [[search engine]].\u000a\u000aThere is no definite list of poison words which all natural language processing tools incorporate. \u000a\u000aThis is different from harmless but useless words that are called [[Stop words]].\u000a\u000aAdult (obscene) words can put a web page in an adult category where it is filtered out by various filters at search engines, so this is one set of poison words. But some consider any words that lower your ranking in a search engine as poison words. Some people consider any words that encourage ads to pervade a whole site and displace much higher earning ads as poison words.\u000a\u000a== See also ==\u000a\u000a* [[Bayesian poisoning]]\u000a* [[Natural language processing]]\u000a* [[Text mining]]\u000a* [[Index (search engine)|Search engine indexing]]\u000a\u000a== External links ==\u000a\u000a\u000a{{SearchEngineOptimization}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p30
asI147
(lp31
VConcept search
p32
aVA '''[[concept]] search''' (or conceptual search) is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query.\u000a\u000a__TOC__\u000a\u000a==Why Concept Search?==\u000aConcept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.\u000a\u000aPolysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>\u000a\u000aIn addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.\u000a\u000aA concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.\u000a\u000a==Approaches to Concept Search==\u000aIn general, information retrieval research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>\u000a\u000aEfforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:\u000a\u000a* Auxiliary structures\u000a* Local [[co-occurrence]] statistics\u000a* Transform techniques (particularly [[matrix decomposition]]s)\u000a\u000a===Auxiliary Structures===\u000aA variety of techniques based on Artificial Intelligence (AI) and [[Natural language processing|Natural Language Processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as [[WordNet]], can be efficiently implemented by reusing retrieval models and data structures of classical [[Information Retrieval]].<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.\u000a\u000aHandcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>\u000a\u000a===Local Co-occurrence Statistics===\u000aInformation retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example,  5 sentences or  50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.\u000a\u000aThis approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only  of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>\u000a\u000a===Transform Techniques===\u000aSome of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>\u000a\u000a* [[Independent component analysis]]\u000a* Semi-discrete decomposition\u000a* [[Non-negative matrix factorization]]\u000a* [[Singular value decomposition]]\u000a\u000aMatrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>\u000a\u000aIndependent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>\u000a\u000aSingular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[Latent semantic indexing|Latent Semantic Indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>\u000a\u000a==Uses of Concept Search==\u000a* '''[[eDiscovery]]''' - Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.\u000aDisability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>\u000a\u000a* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' - Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>\u000a\u000a* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' - Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.\u000a\u000a* '''Multimedia and Publishing''' - Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>\u000a\u000a* '''Digital Libraries and Archives''' - Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.\u000a\u000a* '''Genomic Information Retrieval (GIR)''' - Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.\u000a\u000a* '''Human Resources Staffing and Recruiting''' - Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.\u000a\u000a==Effective Concept Searching==\u000aThe effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:\u000a\u000a* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.\u000a\u000a* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''\u000a\u000a* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.\u000a\u000aAs with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Sprck Jones|Sprck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42\u201349</ref>\u000a\u000a==Relevance Feedback==\u000a[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schtze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.\u000a\u000aIn general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.\u000a\u000aRelevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>\u000a\u000a[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>\u000a\u000a==Guidelines for Evaluating a Concept Search Engine==\u000a# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.\u000a# Result items should be sorted and ranked by relevance.\u000a# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.\u000a# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.\u000a# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.\u000a# Combined queries using concepts, keywords, and metadata should be allowed.\u000a# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.\u000a# Query-ready indexes should be created relatively quickly.\u000a# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.\u000a# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.\u000a\u000a==Search Engine Conferences and Forums==\u000aFormalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>\u000a\u000aIn 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.\u000a\u000aPrecision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>\u000a\u000aAlthough the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven\u2019t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>\u000a\u000a==See also==\u000a* [[approximate string matching]]\u000a* [[Compound term processing]]\u000a* [[Concept mining]]\u000a* [[Computational linguistics]]\u000a* [[Information extraction]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent semantic analysis]]\u000a* [[Semantic network]]\u000a* [[Semantic search]]\u000a* [[Semantic Web]]\u000a* [[Statistical semantics]]\u000a* [[Text mining]]\u000a* [[Word Sense Disambiguation]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]\u000a* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]\u000a* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]\u000a* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]\u000a\u000a[[Category:Information retrieval]]
p33
asI21
(lp34
VKey Word in Context
p35
aV'''KWIC''' is an acronym for '''Key Word In Context''', the most common format for [[concordance (publishing)|concordance]] lines. The term KWIC was first coined by [[Hans Peter Luhn]].<ref>Manning, C. D., Schtze, H.: "Foundations of Statistical Natural Language Processing", p.35. The MIT Press, 1999</ref> The system was based on a concept called ''keyword in titles'' which was first proposed for Manchester libraries in 1864 by [[Andrea Crestadoro]].<ref name="index">{{cite book|title=Advanced Indexing and Abstracting Practices|url=http://books.google.co.uk/books?id=nIUkl7bLzYUC&pg=PA41&dq=Andrea+Crestadoro#v=onepage&q=Andrea%20Crestadoro&f=false}}</ref>\u000a\u000aA '''KWIC''' index is formed by sorting and aligning the words within an article title to allow each word (except the [[stop words]]) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized [[full text search]] became common.\u000a\u000aFor example, a search query including all of the words in the title statement of this article ("KWIC is an acronym for Key Word In Context, the most common format for concordance lines") and the [[Wikipedia:Slogans|Wikipedia slogan]] in English ("the free encyclopedia"), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).\u000a\u000a{| nowrap\u000a|-\u000a|align=right|KWIC is an\u000a|'''acronym''' for Key Word In Context, ...\u000a|page 1\u000a|-\u000a|align=right|... Key Word In Context, the most \u000a|'''common''' format for concordance lines.\u000a|page 1\u000a|-\u000a|align=right|... the most common format for \u000a|'''concordance''' lines.\u000a|page 1\u000a|-\u000a|align=right|... is an acronym for Key Word In \u000a|'''Context''', the most common format ...\u000a|page 1\u000a|-\u000a|align=right|Wikipedia, The Free \u000a|'''Encyclopedia'''\u000a|page 0\u000a|-\u000a|align=right|... In Context, the most common \u000a|'''format''' for concordance lines.\u000a|page 1\u000a|-\u000a|align=right|Wikipedia, The \u000a|'''Free''' Encyclopedia\u000a|page 0\u000a|-\u000a|align=right|KWIC is an acronym for \u000a|'''Key''' Word In Context, the most ...\u000a|page 1\u000a|-\u000a|&nbsp;\u000a|'''KWIC''' is an acronym for Key Word ...\u000a|page 1\u000a|-\u000a|align=right|... common format for concordance \u000a|'''lines'''.\u000a|page 1\u000a|-\u000a|align=right|... for Key Word In Context, the \u000a|'''most''' common format for concordance ...\u000a|page 1\u000a|-\u000a|&nbsp;\u000a|'''Wikipedia''', The Free Encyclopedia\u000a|page 0\u000a|-\u000a|align=right|KWIC is an acronym for Key\u000a|'''Word''' In Context, the most common ...\u000a|page 1\u000a|}\u000a\u000aA KWIC index is a special case of a '''permuted index'''. This term refers to the fact that it indexes all [[cyclic permutation]]s of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of [[Manual page (Unix)|manual pages]], often ended with a '''permuted index''' section, allowing the reader to easily find a section by any word from its heading. This practice, also known as '''KWOC''' (\u201c'''Key Word Out of Context'''\u201d), is no longer common.\u000a\u000a==References in Literature==\u000a\u000a''Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.''\u000a\u000a* [[David Parnas|David L. Parnas]] uses a KWIC Index as an example on how to perform modular design in his paper [http://portal.acm.org/citation.cfm?id=361623&coll=ACM&dl=ACM&CFID=9516243&CFTOKEN=98251202 ''On the Criteria To Be Used in Decomposing Systems into Modules''], available as an [http://www.acm.org/classics/may96/ ACM Classic Paper]\u000a* Christopher D. Manning and Hinrich Schtze describe a KWIC index and computer concordancing in section 1.4.5 of their book ''Foundations of Statistical Natural Language Processing''\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==See also==\u000a* <tt>[[Ptx (Unix)|ptx]]</tt>, a Unix command-line utility producing a [[permuted index]]\u000a*[[Concordancer]]\u000a*[[Concordance (publishing)]]\u000a*[[Burrows\u2013Wheeler transform]]\u000a*[[Hans Peter Luhn]]\u000a*[[Suffix tree]]\u000a\u000a[[Category:Indexing]]\u000a[[Category:Information retrieval]]\u000a[[Category:Reference]]\u000a[[Category:Searching]]
p36
asI151
(lp37
VLatent semantic indexing
p38
aV'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.<ref>Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36\u201340.</ref>\u000a\u000aLSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzcri]]<ref>{{ cite book\u000a | author = Benzcri, J.-P.\u000a | publisher=Dunod |location= Paris, France\u000a | year = 1973\u000a | title = L'Analyse des Donnes. Volume II. L'Analyse des Correspondences\u000a }}</ref> in the early 1970s, to a [[contingency table]] built from word counts in documents.\u000a\u000aCalled Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s.   The method, also called [[latent semantic analysis]] (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\u2019t share a specific word or words with the search criteria.\u000a\u000a__TOC__\u000a\u000a== Benefits of LSI ==\u000a\u000aLSI overcomes two of the most problematic constraints of Boolean keyword queries:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of [[vocabulary mismatch|mismatches in the vocabulary]] used by the authors of documents and the users of information retrieval systems.<ref>{{cite doi|10.1145/32206.32212}}</ref><ref>{{cite doi|10.1145/1871437.1871474}}</ref>   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.\u000a\u000aLSI is also used to perform automated document categorization.  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.<ref>Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns & S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45\u201351.</ref>    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.<ref>{{cite doi|10.1145/288627.288651}}</ref>   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.\u000a\u000aDynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.\u000a\u000aBecause it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.\u000a\u000aLSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.<ref>Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC</ref>   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.<ref>{{cite doi|10.1093/bioinformatics/bth464}}</ref>\u000a\u000aLSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).<ref>{{cite doi|10.1007/11427995_68}}</ref>   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.\u000a\u000aText does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.\u000a\u000aLSI has proven to be a useful solution to a number of conceptual matching problems.<ref>Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59\u201365.</ref><ref>Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161\u2013167.</ref>  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.<ref>Graesser, A., and Karnavat, A., Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures, Proceedings of CogSci 2000, pp. 184\u2013189.</ref>\u000a\u000a== LSI timeline ==\u000a\u000a'''Mid-1960s''' \u2013 Factor analysis technique first described and tested (H. Borko and M. Bernick)\u000a\u000a'''1988''' \u2013 Seminal paper on LSI technique published (Deerwester et al.)\u000a\u000a'''1989''' \u2013 Original patent granted (Deerwester et al.)\u000a\u000a'''1992''' \u2013 First use of LSI to assign articles to reviewers<ref>Dumais, S., and Nielsen, J., Automating the Assignment of Submitted Manuscripts to Reviewers, Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval, 1992, pp. 233\u2013244.</ref>  (Dumais and Nielsen)\u000a\u000a'''1994''' \u2013 Patent granted for the cross-lingual application of LSI (Landauer et al.)\u000a\u000a'''1995''' \u2013 First use of LSI for grading essays (Foltz, et al., Landauer et al.)\u000a\u000a'''1999''' \u2013 First implementation of LSI technology for intelligence community for analyzing unstructured text (SAIC).\u000a\u000a'''2002''' \u2013 LSI-based product offering to intelligence-based government agencies (SAIC)\u000a\u000a'''2005''' \u2013 First vertical-specific application \u2013 publishing \u2013 EDB (EBSCO, [[Content Analyst Company]])\u000a\u000a== Mathematics of LSI ==\u000a\u000aLSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a '''Singular Value Decomposition''' on the matrix, and using the matrix to identify the concepts contained in the text.\u000a\u000a=== Term-document matrix ===\u000a\u000aLSI begins by constructing a term-document matrix, <math>A</math>, to identify the occurrences of the <math>m</math> unique terms within a collection of <math>n</math> documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, <math>a_{ij}</math>, initially representing the number of times the associated term appears in the indicated document, <math>\u005cmathrm{tf_{ij}}</math>.  This matrix is usually very large and very sparse.\u000a\u000aOnce a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, <math>a_{ij}</math> of <math>A</math>, to be the product of a local term weight, <math>l_{ij}</math>, which describes the relative frequency of a term in a document, and a global weight, <math>g_i</math>, which describes the relative frequency of the term within the entire collection of documents.\u000a\u000aSome common local weighting functions <ref>\u000aBerry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).</ref> are defined in the following table.\u000a\u000a{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\u000a|-\u000a|  style="width:22%" | '''Binary''' ||\u000a| <math>l_{ij} = 1</math> if the term exists in the document, or else <math>0</math>\u000a|-\u000a|  style="width:22%" | '''TermFrequency''' ||\u000a| <math>l_{ij} = \u005cmathrm{tf}_{ij}</math>, the number of occurrences of term <math>i</math> in document <math>j</math>\u000a|-\u000a|  style="width:22%" | '''Log''' ||\u000a| <math>l_{ij} = \u005clog(\u005cmathrm{tf}_{ij} + 1)</math>\u000a|-\u000a|  style="width:22%" | '''Augnorm''' ||\u000a| <math>l_{ij} = \u005cfrac{\u005cBig(\u005cfrac{\u005cmathrm{tf}_{ij}}{\u005cmax_i(\u005cmathrm{tf}_{ij})}\u005cBig) + 1}{2}</math>\u000a|}\u000a\u000aSome common global weighting functions are defined in the following table.\u000a\u000a{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\u000a|-\u000a| style="width:22%" | '''Binary''' ||\u000a| <math>g_i = 1</math>\u000a|-\u000a| style="width:22%" | '''Normal''' ||\u000a| <math>g_i = \u005cfrac{1}{\u005csqrt{\u005csum_j \u005cmathrm{tf}_{ij}^2}}</math>\u000a|-\u000a| style="width:22%" | '''GfIdf''' ||\u000a| <math>g_i = \u005cmathrm{gf}_i / \u005cmathrm{df}_i</math>, where <math>\u005cmathrm{gf}_i</math> is the total number of times term <math>i</math> occurs in the whole collection, and <math>\u005cmathrm{df}_i</math> is the number of documents in which term <math>i</math> occurs.\u000a|-\u000a| style="width:22%" | '''Idf''' ||\u000a| <math>g_i = \u005clog_2 \u005cfrac{n}{1+ \u005cmathrm{df}_i}</math>\u000a|-\u000a| style="width:22%" | '''Entropy''' ||\u000a| <math>g_i = 1 + \u005csum_j \u005cfrac{p_{ij} \u005clog p_{ij}}{\u005clog n}</math>, where <math>p_{ij} = \u005cfrac{\u005cmathrm{tf}_{ij}}{\u005cmathrm{gf}_i}</math>\u000a|}\u000a\u000aEmpirical studies with LSI report that the Log Entropy weighting functions work well, in practice, with many data sets.<ref>Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.</ref>  In other words, each entry <math>a_{ij}</math> of <math>A</math> is computed as:\u000a\u000a:<math>g_i = 1 + \u005csum_j \u005cfrac{p_{ij} \u005clog p_{ij}}{\u005clog n}</math>\u000a\u000a:<math>a_{ij} = g_i \u005c \u005clog (\u005cmathrm{tf}_{ij} + 1)</math>\u000a\u000a=== Rank-reduced singular value decomposition ===\u000a\u000aA rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.<ref>Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573\u2013595.</ref>   It computes the term and document vector spaces by approximating the single term-frequency matrix, <math>A</math>, into three other matrices\u2014 an '''''m''''' by '''''r'''''  term-concept vector matrix <math>T</math>, an '''''r''''' by '''''r''''' singular values matrix <math>S</math>, and a '''''n''''' by '''''r''''' concept-document vector matrix, <math>D</math>, which satisfy the following relations:\u000a\u000a<math>A \u005capprox TSD^T</math>\u000a\u000a<math>T^T T = I_r \u005cquad D^T D = I_r </math>\u000a\u000a<math>S_{1,1} \u005cgeq S_{2,2} \u005cgeq \u005cldots \u005cgeq  S_{r,r} > 0 \u005cquad S_{i,j} = 0 \u005c; \u005ctext{where} \u005c; i \u005cneq j</math>\u000a\u000aIn the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''\u2014a measure of its unique dimensions '''\u2264 min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.\u000a\u000aThe LSI modification to a standard SVD is to reduce the rank or truncate the singular value matrix '''S''' to size '''''k'''''  '''''r''''', typically on the order of a '''''k''''' in the range of 100 to 300 dimensions, effectively reducing the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:\u000a\u000a:::::::'''A \u2248 A''<sub>k''</sub> = T''<sub>k''</sub> S''<sub>k''</sub> D''<sub>k''</sub><sup>T</sup>'''\u000a\u000aEfficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.\u000a\u000aNote that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.\u000a\u000a== Querying and augmenting LSI vector spaces ==\u000a\u000aThe computed '''T''<sub>k''</sub>''' and '''D''<sub>k''</sub>''' matrices define the term and document vector spaces, which with the computed singular values, '''S''<sub>k''</sub>''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.\u000a\u000aThe same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D<sup>T</sup>''' equation into the equivalent '''D = A<sup>T</sup> T S<sup>\u22121</sup>''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S<sup>\u22121</sup>'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.\u000a\u000aA drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.\u000a\u000aThe process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in <ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20\u201330 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref>) be used.\u000a\u000a== Additional uses of LSI ==\u000a\u000aIt is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.\u000a\u000aLSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.</ref>   Below are some other ways in which LSI is being used:\u000a\u000a* Information discovery<ref>Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189\u2013223.</ref>  (eDiscovery, Government/Intelligence community, Publishing)\u000a* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)<ref>Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.</ref>\u000a* Text summarization<ref>Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903\u2013907.</ref>  (eDiscovery, Publishing)\u000a* Relationship discovery<ref>Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374\u2013380.</ref>  (Government, Intelligence community, Social Networking)\u000a* Automatic generation of link charts of individuals and organizations<ref>Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23\u201324, 2006, Springer, LNCS vol. 3975, pp. 674\u2013675.</ref>  (Government, Intelligence community)\u000a* Matching technical papers and grants with reviewers<ref>Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220\u2013230.</ref>  (Government)\u000a* Online customer support<ref>Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.</ref>  (Customer Management)\u000a* Determining document authorship<ref>Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43\u201348.</ref>  (Education)\u000a* Automatic keyword annotation of images<ref>Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275\u2013278.</ref>\u000a* Understanding software source code<ref>Maletic, J., and Marcus, A., Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding, Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence, Vancouver, British Columbia, November 13\u201315, 2000, pp. 46\u201353.</ref>  (Software Engineering)\u000a* Filtering [[Spam (electronic)|spam]]<ref>Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460\u2013464.</ref>  (System Administration)\u000a* Information visualization<ref>Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Science, 101, 2004, pp. 5214\u20135219.</ref>\u000a* [[Automated essay scoring|Essay scoring]]<ref>Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.</ref>  (Education)\u000a* [[Literature-based discovery]]<ref>Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674\u2013685.</ref>\u000a\u000aLSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.<ref>There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.</ref>\u000a\u000a== Challenges to LSI ==\u000a\u000aEarly challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.<ref>Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.</ref>  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are not uncommon in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.<ref name="rehurek2011">{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim \u0158eh\u016f\u0159ek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289\u2013300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 }}</ref>\u000a\u000aAnother challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).<ref>Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153\u2013162.</ref>   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.<ref>Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.</ref>\u000a\u000aChecking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.<ref>Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).</ref><ref>Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).</ref><ref>Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.</ref>\u000a\u000a==See also==\u000a* [[Latent semantic analysis]]\u000a* [[Latent Semantic Structure Indexing]]\u000a* [[Principal component analysis]]\u000a* [[Correspondence analysis]]\u000a* [[Probabilistic latent semantic analysis]]\u000a\u000a{{Natural Language Processing}}\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== Further reading ==\u000a*{{cite book|authors=Berry, M. W., Browne M.|title=Understanding Search Engines: Mathematical Modeling and Text Retrieval|location=Philadelphia|publisher=Society for Industrial and Applied Mathematics|year=2005|isbn=978-0898715811|url=http://www.mblazquez.es/blog-ccdoc-recuperacion/documentos/book_understanding-search-engines.pdf}}\u000a*{{cite book|editors=Berry, M. W.|title=Survey of Text Mining: Clustering, Classification, and Retrieval|location=New York|publisher=Springer|year=2004|url=https://perso.uclouvain.be/vincent.blondel/publications/08-textmining.pdf|isbn=978-0387955636}}\u000a*{{cite book|authors=Landauer, T., et al.|title=Handbook of Latent Semantic Analysis|publisher=Lawrence Erlbaum Associates|year=2007|isbn= 978-0805854183|url=http://books.google.de/books/about/Handbook_of_latent_semantic_analysis.html?id=jgVWCuFXePEC&redir_esc=y}}\u000a*{{cite book|authors=Manning, C. D., Schutze H.|title=Foundations of Statistical Natural Language Processing|location=Cambridge, MA|publisher=The MIT Press|year=1999|url=http://nlp.stanford.edu/fsnlp/promo/contents.ps|isbn=9780262133609 }} [http://nlp.stanford.edu/fsnlp/ Companion webpage]\u000a\u000a==External links==\u000a* [http://www.cs.utk.edu/~lsi/ Michael Berry\u2019s site]\u000a* [http://radimrehurek.com/gensim Gensim] contains a scalable Python+[[NumPy]] implementation of LSI, even for datasets larger than the available RAM.\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.\u000a* [http://www.youtube.com/watch?v=QGd06MTRMHs Stanford University Andrew Ng Video on LSI]\u000a\u000a{{DEFAULTSORT:Latent semantic indexing}}\u000a[[Category:Information retrieval]]\u000a[[Category:Semantic Web]]
p39
asI152
(lp40
VURL redirection
p41
aV{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}\u000a\u000a{{refimprove|date=July 2014}}\u000a'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, '''domain redirection''' or '''domain forwarding''' is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org].\u000aURL redirection can be used for [[URL shortening]], to prevent [[link rot|broken links]] when web pages are moved, to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]], to guide navigation into and out of a website, for privacy protection, and for less innocuous purposes such as [[phishing]] attacks.\u000a\u000a== Purposes ==\u000aThere are several reasons to use URL redirection :\u000a\u000a=== Similar domain names ===\u000aA user might mis-type a URL\u2014for example, "example.com" and "exmaple.com". Organizations often register these "mis-spelled" domains and re-direct them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.\u000a\u000a=== Moving pages to a new domain ===\u000aWeb pages may be redirected to a new domain for three reasons:\u000a* a site might desire, or need, to change its domain name;\u000a* an author might move his or her individual pages to a new domain;\u000a* two web sites might merge.\u000a\u000aWith URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers.\u000a\u000aThe same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.\u000a\u000a=== Logging outgoing links ===\u000aThe access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link.\u000a\u000aThis information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>\u000a{{cite journal\u000a  | title = Google revives redirect snoopery\u000a  | journal = blog.anta.net\u000a  | date = 2009-01-29\u000a  | url = http://blog.anta.net/2009/01/29/509/\u000a  | issn = 1797-1993\u000a  | archiveurl=http://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/\u000a  | archivedate=2011-08-17\u000a}}</ref>\u000a\u000aThe same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.\u000a\u000a=== Short aliases for long URLs ===\u000a{{Main|URL shortening}}\u000a\u000aWeb applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.\u000a\u000a=== Meaningful, persistent aliases for long or changing URLs ===\u000a{{See also|Permalink|PURL|Link rot}}\u000a\u000aSometimes the URL of a page changes even though the content stays the same. Therefore URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.\u000a\u000a=== Post/Redirect/Get ===\u000a{{Main|Post/Redirect/Get}}\u000a\u000aPost/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).\u000a\u000a=== Manipulating search engines ===\u000aRedirect techniques are used to fool search engines.  For example, one page could show popular search terms to search engines but redirect the visitors to a different target page.  There are also cases where redirects have been used to "steal" the page rank of one popular page and use it for a different page, They will also redirect using searches with search engines as searches, usually involving the 302 [[List of HTTP status codes|HTTP status code]] of "moved temporarily."<ref>{{cite web|url=http://www.pandia.com/sw-2004/40-hijack.html |title=Google's serious hijack problem \u2013 Spammers hijack web site listings in Google |date=September 13, 2004 |publisher=Pandia.com |archiveurl=http://web.archive.org/web/20130605153457/http://www.pandia.com/sw-2004/40-hijack.html |archivedate=2013-06-05}}</ref><ref>[http://www.loriswebs.com/hijacking_web_pages.html "Stop Scrapers From Hijacking your Web Pages"]. Lori's Web Design.com. Retrieved 2013-12-18.</ref>\u000a\u000aSearch engine providers have noticed the problem and are working on appropriate actions.{{Citation needed|date=August 2009}}\u000a\u000aAs a result, today, such manipulations usually result in less rather than more site exposure.\u000a\u000a=== Manipulating visitors ===\u000aURL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.{{Citation needed|date=January 2010}} Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.\u000a\u000a=== Removing <code>referer</code> information ===\u000aWhen a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.\u000a\u000a== Techniques ==\u000aSeveral different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.\u000a\u000a=== Manual redirect ===\u000aThe simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:\u000a\u000a<source lang="html4strict">\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis method is often used as a fall-back&nbsp;\u2014 if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.\u000a\u000a=== HTTP status codes 3xx ===\u000aIn the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page.  The different codes describe the reason for the redirect, which allows for the correct subsequent action (such as changing links in the case of code 301, a permanent change of address).\u000a\u000aHTTP/1.1 defines [http://tools.ietf.org/html/rfc7231#section-6.4 several status codes] for redirection:\u000a* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)\u000a* [[HTTP 301|301 moved permanently]]\u000a* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)\u000a* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)\u000a* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)\u000a\u000aAll of these status codes require that the URL of the redirect target be given in the Location: header of the HTTP response.  The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.\u000a\u000a(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).\u000a\u000aAn [[HTTP]] response with the 301 "moved permanently" redirect looks like this:\u000a\u000a<source lang="html4strict">\u000aHTTP/1.1 301 Moved Permanently\u000aLocation: http://www.example.org/\u000aContent-Type: text/html\u000aContent-Length: 174\u000a\u000a<html>\u000a<head>\u000a<title>Moved</title>\u000a</head>\u000a<body>\u000a<h1>Moved</h1>\u000a<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000a==== Using server-side scripting for redirection ====\u000aWeb authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:\u000a\u000a<source lang="php">\u000aheader('HTTP/1.1 301 Moved Permanently');\u000aheader('Location: http://www.example.com/');\u000aexit();\u000a</source>\u000a\u000a(More headers may be required to prevent caching<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=http://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref>).\u000a\u000aThe programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code>\u000a\u000aHTTP/1.1 [http://tools.ietf.org/html/rfc7231#section-7.1.2 allows for] either a relative URI reference or an absolute URI reference. If the URI reference is relative the client computes the required absolute URI reference according to [http://tools.ietf.org/html/rfc3986#section-5 the rules defined in RFC 3986].\u000a\u000a==== Apache mod_rewrite ====\u000aThe [[Apache HTTP Server]]'s [http://httpd.apache.org/docs/current/mod/mod_alias.html mod_alias] extension can be used to redirect certain requests.  Typical configuration directives look like:\u000a\u000a<source lang="apache">\u000aRedirect permanent /oldpage.html http://www.example.com/newpage.html\u000aRedirect 301 /oldpage.html http://www.example.com/newpage.html\u000a</source>\u000a</blockquote>\u000a\u000aFor more flexible URL rewriting and redirection, Apache [http://httpd.apache.org/docs/current/mod/mod_rewrite.html mod_rewrite] can be used.  E.g. to redirect a requests to a canonical domain name:\u000a<source lang="apache">\u000aRewriteEngine on\u000aRewriteCond %{HTTP_HOST} ^([^.:]+\u005c.)*oldsite\u005c.example\u005c.com\u005c.?(:[0-9]*)?$ [NC]\u000aRewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]\u000a</source>\u000a\u000aSuch configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>.htaccess</code> file.\u000a\u000a==== nginx rewrite ====\u000a[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su \u2014 Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a\u000aFor example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a<source lang="pcre">\u000a	location /DragonFly {\u000a		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;\u000a	}\u000a	location /d {\u000a		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";\u000a		set	$ds	"&section=";\u000a		rewrite	^/./([^/]+)\u005c.([1-9])$		$db$1$ds$2	redirect;\u000a	}\u000a</source>\u000a\u000a=== Refresh Meta tag and HTTP refresh header ===\u000a[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.  See\u000a* [http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]\u000a* [http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]\u000a\u000aA timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.</ref>\u000a\u000aThis is an example of a simple HTML document that uses this technique:\u000a<source lang="html4strict">\u000a<html>\u000a<head>\u000a<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />\u000a</head>\u000a<body>\u000a<p>Please follow <a href="http://www.example.com/">this link</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000aThis technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.\u000a\u000aThe same effect can be achieved with an HTTP <code>refresh</code> header:\u000a<source lang="html4strict">\u000aHTTP/1.1 200 ok\u000aRefresh: 0; url=http://www.example.com/\u000aContent-type: text/html\u000aContent-length: 78\u000a\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis response is easier to generate by CGI programs because one does not need to change the default status code.\u000a\u000aHere is a simple CGI program that effects this redirect:\u000a<source lang="perl">\u000a#!/usr/bin/perl\u000aprint "Refresh: 0; url=http://www.example.com/\u005cr\u005cn";\u000aprint "Content-type: text/html\u005cr\u005cn";\u000aprint "\u005cr\u005cn";\u000aprint "Please follow <a href=\u005c"http://www.example.com/\u005c">this link</a>!"\u000a</source>\u000a\u000aNote: Usually, the HTTP server adds the status line and the Content-length header automatically.\u000a\u000aThe [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].\u000a\u000a=== JavaScript redirects ===\u000a[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location='http://www.example.com/'\u000a</syntaxhighlight>\u000aNormally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://online-marketing-technologies.com/tools/javascript-redirection-generator.html|title=Advanced JavaScript Redirections|publisher=Online Marketing Technologies}}</ref>\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location.replace('http://www.example.com/')\u000a</syntaxhighlight>\u000aHowever, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.\u000a\u000a=== Frame redirects ===\u000aA slightly different effect can be achieved by creating a single HTML [[Iframe|frame]] that contains the target page:\u000a<source lang="html4strict">\u000a<frameset rows="100%">\u000a  <frame src="http://www.example.com/">\u000a  <noframes>\u000a    <body>Please follow <a href="http://www.example.com/">link</a>.</body>\u000a  </noframes>\u000a</frameset>\u000a</source>\u000a\u000aOne main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar.\u000a\u000aThis ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>\u000a\u000aThe same effect can be done with an inline frame:\u000a<source lang="html4strict">\u000a<iframe height="100%" width="100%" src="http://www.example.com/">\u000aPlease follow <a href="http://www.example.com/">link</a>.\u000a</iframe>\u000a</source>\u000a\u000a=== Redirect chains ===\u000aOne redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (note the domain name) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] and then to the correct URL: http://en.wikipedia.org/wiki/URL_redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''rewriting'' the URL as much as possible on the server before returning it to the browser as a redirect.\u000a\u000a=== Redirect loops ===\u000aSometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.\u000a\u000a[http://tools.ietf.org/html/rfc7231#section-6.4 HTTP/1.1] states:\u000a<blockquote>\u000aA client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).\u000a\u000aNote: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.\u000a</blockquote>\u000aNote that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...\u000a\u000a== Services ==\u000aThere exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.\u000a\u000a=== URL redirection services ===\u000aA '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]].\u000a\u000aHyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.\u000a\u000aRecently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs.\u000a\u000aA major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.\u000a\u000a==== History ====\u000aThe first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined.\u000a\u000aWith the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.\u000a\u000a=== Referrer masking ===\u000aRedirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.)\u000a\u000aThis type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .\u000a\u000aHere is a simplistic example of such a service, written in [[PHP]].\u000a<source lang="php">\u000a<?php\u000a$url = htmlspecialchars($_GET['url']);\u000aheader( 'Refresh: 0; url=http://'.$url );\u000a?>\u000a<!-- Fallback using meta refresh. -->\u000a<html>\u000a <head>\u000a  <title>Redirecting...</title>\u000a  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">\u000a </head>\u000a <body>\u000a Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.\u000a </body>\u000a</html>\u000a</source>\u000a\u000aThe above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.\u000a\u000a==Security Issues==\u000aURL redirection can be abused by attackers for [[Phishing]] attacks, such as [[Open Redirect]] and [[Covert Redirect]].\u000a\u000a"An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a"Covert Redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It is disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a== See also ==\u000a* [[Link rot]]\u000a* [[Canonical meta tag]]\u000a* [[Domain masking]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]\u000a* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)\u000a* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification\u000a* [http://www.dancatts.com/articles/htaccess-301-redirects-for-moved-pages.php 301 Redirects for moved pages using .htaccess]\u000a* [http://911-need-code-help.blogspot.com/2011/03/redirecting-visitors-to-preferred.html Redirecting your visitors to your preferred domain] using 301 permanent redirects&nbsp;\u2014 rationale and mod_rewrite/PHP/ASP.NET implementations\u000a\u000a{{Spamming}}\u000a\u000a{{Use dmy dates|date=November 2010}}\u000a\u000a{{DEFAULTSORT:Url Redirection}}\u000a[[Category:Uniform resource locator]]\u000a[[Category:Black hat search engine optimization]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p42
asI25
(lp43
VPrecision and recall
p44
aV[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]\u000aIn [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]]. Suppose a program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.\u000a\u000aIn [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.\u000a\u000aIn simple terms, high '''precision''' means that an algorithm returned substantially more relevant results than irrelevant, while high '''recall''' means that an algorithm returned most of the relevant results.\u000a\u000a==Introduction==\u000aAs an example, in an [[information retrieval]] scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.\u000a\u000aIn a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of '''true positives''''' (i.e. the ''number of items correctly labeled as belonging to the positive class'') ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false positives]]''', which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives'' ''divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false negatives]]''', which are items which were not labeled as belonging to the positive class but should have been).\u000a\u000aIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\u000a\u000aIn a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).\u000a\u000aOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an obvious example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient\u2019s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\u000a\u000aUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s Informedness (DeltaP') and Markedness (DeltaP).<ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Factor  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37\u201363 |url=http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf}}</ref><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97\u2013119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2007"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2007"/>  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.\u000a\u000a== Definition (information retrieval context) ==\u000a\u000aIn [[information retrieval]] contexts, precision and recall are defined in terms of a set of '''retrieved documents''' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of '''relevant documents''' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]].\u000a\u000a===[[Positive predictive value | Precision]]===\u000a\u000aIn the field of [[information retrieval]], '''precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the find:\u000a\u000a:<math> \u005ctext{precision}=\u005cfrac{|\u005c{\u005ctext{relevant documents}\u005c}\u005ccap\u005c{\u005ctext{retrieved documents}\u005c}|}{|\u005c{\u005ctext{retrieved documents}\u005c}|} </math>\u000a\u000aPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called '''precision at n''' or '''P@n'''.\u000a\u000aFor example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.\u000a\u000aPrecision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.\u000a\u000aNote that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.\u000a\u000a===Recall===\u000a\u000aRecall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.\u000a\u000a:<math> \u005ctext{recall}=\u005cfrac{|\u005c{\u005ctext{relevant documents}\u005c}\u005ccap\u005c{\u005ctext{retrieved documents}\u005c}|}{|\u005c{\u005ctext{relevant documents}\u005c}|} </math>\u000a\u000aFor example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned\u000a\u000aIn binary classification, recall is called [[Sensitivity_and_specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.\u000a\u000aIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\u000a\u000a== Definition (classification context) ==\u000a{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"\u000a|+ Terminology and derivations<br \u000a/>from a confusion matrix\u000a|- valign=top\u000a|\u000a; true positive (TP)\u000a:eqv. with hit\u000a; true negative (TN)\u000a:eqv. with correct rejection\u000a; false positive (FP)\u000a:eqv. with [[false alarm]], [[Type I error]]\u000a; false negative (FN)\u000a:eqv. with miss, [[Type II error]]\u000a--------------------------------------------------------\u000a; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)\u000a:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]\u000a:<math>\u005cmathit{TPR} = \u005cmathit{TP} / P = \u005cmathit{TP} / (\u005cmathit{TP}+\u005cmathit{FN})</math>\u000a; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate\u000a:<math>\u005cmathit{SPC} = \u005cmathit{TN} / N = \u005cmathit{TN} / (\u005cmathit{FP} + \u005cmathit{TN}) </math>\u000a; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)\u000a:<math>\u005cmathit{PPV} = \u005cmathit{TP} / (\u005cmathit{TP} + \u005cmathit{FP})</math>\u000a; [[negative predictive value]] (NPV)\u000a:<math>\u005cmathit{NPV} = \u005cmathit{TN} / (\u005cmathit{TN} + \u005cmathit{FN})</math>\u000a; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)\u000a:<math>\u005cmathit{FPR} = \u005cmathit{FP} / N = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TN})</math>\u000a; [[false discovery rate]] (FDR)\u000a:<math>\u005cmathit{FDR} = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TP}) = 1 - \u005cmathit{PPV} </math>\u000a; [[false negative rate]] (FNR)\u000a:<math>\u005cmathit{FNR} = \u005cmathit{FN} / (\u005cmathit{FN} + \u005cmathit{TP}) = 1 - \u005cmathit{TPR} </math>\u000a------------------------------------------------\u000a; [[accuracy]] (ACC)\u000a:<math>\u005cmathit{ACC} = (\u005cmathit{TP} + \u005cmathit{TN}) / (P + N)</math>\u000a;[[F1 score]]\u000a: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]\u000a:<math>\u005cmathit{F1} = 2 \u005cmathit{TP} / (2 \u005cmathit{TP} + \u005cmathit{FP} + \u005cmathit{FN})</math>\u000a; [[Matthews correlation coefficient]] (MCC)\u000a:<math> \u005cfrac{ TP \u005ctimes TN - FP \u005ctimes FN } {\u005csqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\u000a</math>\u000a;\u000a<span style="font-size:90%;">''Source: Fawcett (2006).''<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>\u000a|}\u000a\u000aFor classification tasks, the terms '''true positives''', '''true negatives''', '''false positives''', and '''false negatives''' (see also [[Type I and type II errors]]) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation''). \u000a\u000aLet us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 22 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:\u000a\u000a{{DiagnosticTesting_Diagram}}\u000a\u000a<!--\u000a{| border="0" align="center" style="text-align: center; background: #FFFFFF;"\u000a|+\u000a!\u000a! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)\u000a|-\u000a!\u000a|-----\u000a|+\u000a! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)\u000a| '''tp''' <br> (true positive) <br/> Correct result\u000a| '''fp''' <br> (false positive) <br/> Unexpected result\u000a|-bgcolor="#EFEFEF"\u000a| '''fn''' <br> (false negative) <br/> Missing result\u000a| '''tn''' <br> (true negative) <br/> Correct absence of result\u000a|+\u000a|}\u000a\u000a-->\u000a\u000a\u000a\u000aPrecision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>\u000a\u000a: <math>\u005ctext{Precision}=\u005cfrac{tp}{tp+fp} \u005c, </math>\u000a\u000a: <math>\u005ctext{Recall}=\u005cfrac{tp}{tp+fn} \u005c, </math>\u000a\u000aRecall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy_and_precision#In_binary_classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity_(tests)#Specificity|specificity]].\u000a\u000a: <math>\u005ctext{True negative rate}=\u005cfrac{tn}{tn+fp} \u005c, </math>\u000a\u000a: <math>\u005ctext{Accuracy}=\u005cfrac{tp+tn}{tp+tn+fp+fn} \u005c, </math>\u000a\u000a== Probabilistic interpretation ==\u000a\u000aIt is possible to interpret precision and recall not as ratios but as probabilities:\u000a\u000a* '''Precision''' is the probability that a (randomly selected) retrieved document is relevant.\u000a\u000a* '''Recall''' is the probability that a (randomly selected) relevant document is retrieved in a search.\u000a\u000aNote that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by '''randomly selected retrieved document''', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected. \u000a\u000aNote that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation). \u000a\u000aAnother interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.\u000a\u000a== F-measure ==\u000a{{main|F1 score}}\u000aA measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:\u000a\u000a: <math>F = 2 \u005ccdot \u005cfrac{\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall}}{ \u005cmathrm{precision} + \u005cmathrm{recall}}</math>\u000a\u000aThere are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. <ref>{{cite journal|last=POWERS|first=D.M.W.|title=EVALUATION: FROM PRECISION, RECALL AND F-MEASURE TO ROC, INFORMEDNESS, MARKEDNESS & CORRELATION|journal=Journal of Machine Learning Technologies|date=February 27, 2011|volume=2|issue=1|pages=37-63|url=http://www.bioinfo.in/contents.php?id=51}}</ref> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\u000a\u000aIt is a special case of the general <math>F_\u005cbeta</math> measure (for non-negative real values of&nbsp;<math>\u005cbeta</math>):\u000a\u000a:<math>F_\u005cbeta = (1 + \u005cbeta^2) \u005ccdot \u005cfrac{\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall} }{ \u005cbeta^2 \u005ccdot \u005cmathrm{precision} + \u005cmathrm{recall}}</math>\u000a\u000aTwo other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.\u000a\u000aThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\u005cbeta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\u005cbeta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \u005cfrac{1}{\u005cfrac{\u005calpha}{P} + \u005cfrac{1-\u005calpha}{R}}</math>.  Their relationship is <math>F_\u005cbeta = 1 - E</math> where <math>\u005calpha=\u005cfrac{1}{1 + \u005cbeta^2}</math>.\u000a\u000a==Limitations as goals==\u000aThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zaj\u0105c. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref> \u000a\u000aFor [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']</ref>\u000a:''"[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).''\u000a:* ''Search results don't have to be very good.''\u000a:* ''Recall?    Not important (as long as you get at least some good hits).''\u000a:* ''Precision? Not important (as long as at least some of the hits on the first page you return are good)."''\u000a\u000a==See also==\u000a* [[Binary classification]]\u000a* [[Information retrieval]]\u000a* [[Receiver operating characteristic]]\u000a* [[Relevance]]\u000a* [[Sensitivity and specificity]]\u000a* [[Type I and type II errors]], where ''false positives'' and ''false negatives'' are defined\u000a* [[Uncertainty coefficient]], aka Proficiency\u000a\u000a== Sources ==\u000a<references>\u000a* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X\u000a* Hjrland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237\u000a* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''\u000a* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4\u000a</references>\u000a\u000a== External links ==\u000a* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval \u2013 C. J. van Rijsbergen 1979]\u000a* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Information science]]\u000a[[Category:Bioinformatics]]\u000a[[Category:Summary statistics for contingency tables]]\u000a\u000a[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]
p45
asI154
(lp46
VAUTINDEX
p47
aV{{multiple issues|\u000a{{COI|date=September 2014}}\u000a{{notability|Products|date=September 2014}}\u000a}}\u000a\u000a'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.<ref>Ripplinger, Brbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen</ref><ref>Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6\u20138 October 2009, Portugal</ref><ref>Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19\u201320 November</ref>\u000a\u000a'''AUTINDEX''' resulting from research in [[information extraction]] <ref>Paul Schmidt, Thomas Bhr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen fr die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt</ref><ref>Ursula Deriu, Jrn Lehmann & Paul Schmidt, 2009: \u201aErstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie\u2019. In: Proceedings Knowtech, Frankfurt</ref> is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrcken, Germany.\u000a\u000a'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX <ref>[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.</ref>), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch <ref>[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.</ref> and WISSMER,<ref>[//www.wissmer.info/index.php/de/]. Project Wissmer.</ref> see also the reference to IAI-Webite.<ref>[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.</ref>\u000a\u000aThe basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.<ref>Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR \u2013 The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.\u20138. Oktober 2009, Portugal. 2009, S. 259 - 262.</ref> Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.<br> \u000aAUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems <ref>[//www.wti-frankfurt.de]. WTI Information system.</ref> and in document management and content management environments. <br> \u000a \u000aTogether with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] <ref>Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.</ref> software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].<ref>[//www.wissmer.info]. Electro mobility information system.</ref>\u000a\u000a==See also==\u000a\u000a* [[Information retrieval]]\u000a* [[Linguistics]]\u000a* [[Knowledge Management]]\u000a* [[Natural Language Processing]]\u000a* [[Semantics]]\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a== Publications ==\u000a* Ripplinger, Brbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.\u000a* Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6\u20138 October 2009, Portugal.\u000a* Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19\u201320 November.\u000a* Paul Schmidt, Thomas Bhr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen fr die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.\u000a* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR \u2013 The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.\u20138. Oktober 2009, Portugal. 2009, S. 259 - 262.\u000a* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.\u201320. November 2009.\u000a* Rsener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD fr Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.\u000a* Svenja Siedle: ''Suchst du noch oder weit du schon? Inhaltserschlieung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''\u000a* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum fr Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)\u000a* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.\u000a\u000a== External links ==\u000a* http://www.iai-sb.de/ Institute for Applied Information Sciences\u000a\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p48
asI155
(lp49
VHashtag
p50
aV[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the usage of a #timetoact hashtag at a 2014 conference]]\u000a\u000aA '''hashtag''' is a word or an unspaced phrase prefixed with the [[Number sign|hash character (or number sign), <code>#</code>]], to form a label.<ref>http://www.merriam-webster.com/dictionary/hashtag</ref> It is a type of [[Tag (metadata)|metadata tag]]. Words or phrases in messages on [[microblogging]] and [[social networking service]]s such as [[Facebook]], [[Google+]], [[Instagram]], [[Twitter]], or [[VK (social network)|VK]] may be tagged by entering # before them,<ref>{{cite web|url=http://support.twitter.com/articles/49309# |title=Using hashtags on Twitter|publisher=support.twitter.com |accessdate=2013-11-25}}</ref> either as they appear in a sentence, e.g., "New artists announced for #SXSW2014 Music Festival"<ref>{{cite web|url=https://dev.twitter.com/media/hashtags |title=Best Practices for Hashtags &#124; Twitter Developers |publisher=Dev.twitter.com |date=2011-07-19 |accessdate=2013-11-12}}</ref> or appended to it. The term hashtag can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary - Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000aA hashtag allows grouping of similarly tagged messages, and also allows an electronic search to return all messages that contain it.\u000a\u000aDue to its widespread use, 'hashtag' was added to the ''[[Oxford English Dictionary]]'' in June 2014.<ref>{{cite web |title='Hashtag' added to the OED \u2013 but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=13 June 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000a==Origin==\u000aThe [[number sign]] was often used in [[information technology]] to highlight a special meaning. In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=2011-08-03 |accessdate=2014-08-25}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W.Kernighan &  d.Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the number sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Techonology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=http://books.google.co.uk/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=7 November 2014|isbn=9781863975858}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=9780195157048|pages=33, 260|url=http://books.google.co.uk/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>\u000a\u000aThe number sign then appeared and was used within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=3 June 2014}}</ref>\u000a\u000aThe use of the number sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=2014-08-29}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitter\u2019s Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter: \u000a{{quote |1=how do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/#!/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}\u000aInternationally, the hashtag became a practice of writing style for Twitter posts during the [[2009\u20132010 Iranian election protests]], as both English- and [[Persian language|Persian]]-language hashtags became useful for Twitter users inside and outside Iran.{{cite web|url=http://www.dw.de/%D8%AD%DA%A9%D8%A7%DB%8C%D8%AA-%D9%87%D8%B4%D8%AA%DA%AF%DB%8C-%DA%A9%D9%87-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86-%D8%A2%D8%BA%D8%A7%D8%B2-%DA%A9%D8%B1%D8%AF%D9%86%D8%AF/g-18012627|title = dw |date= 2009}}\u000a\u000aThe first use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=2013-09-19}}</ref> on 26 August 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.\u000a\u000aBeginning July 2, 2009,{{citation needed|date=November 2013}} Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending_topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=2014-12-03}}</ref>\u000a\u000a==Style==\u000aOn microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").\u000a\u000aThe quantity of hashtags used in a post or tweet is just as important as the type of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks \u201craising the ire of the community.\u201d<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=22 February 2014}}</ref>\u000a\u000aAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=22 February 2014}}</ref>{{failed verification|date=August 2014}}\u000a \u000a[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=http://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=2013-09-24 |accessdate=2014-08-25}}</ref>\u000a\u000a==Function==\u000a[[File:Seguir hashtags.png|300px|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag "#science".]]\u000aHashtags are mostly used as unmoderated ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion using the hashtag. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users, and neither can they be "retired" from public usage, meaning that hashtags can be used in theoretical perpetuity depending upon the longevity of the word or set of characters in a written language. They also do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes as espoused by those who make use of them.\u000a\u000aHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using "#cakefestival" rather than simply "#cake". However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\u000a\u000aHashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.\u000a\u000aHashtags can be used on the social network [[Instagram]], by posting pictures and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic like #photography #iPhone #iphoneography and therefore do not fulfil a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = 7 November 2013|publisher=BBC.co.uk |accessdate=2013-11-25}}</ref> The censorship and ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram\u2019s content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>\u000aOlszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship". Visual Communication Quarterly, 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref> \u000a\u000a\u000aHashtags are also used informally to express context around a given message, with no intent to actually categorize the message for later searching, sharing, or other reasons.  This can help express humor, excitement, sadness or other contextual cues, for example "It's Monday!! #excited #sarcasm"\u000a\u000a==Use outside of social networking websites==\u000aThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]; in the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = Oct 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = Oct 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts which can result from search terms or hashtags.{{citation needed|date=September 2014}}\u000a\u000a==Websites that support hashtags==\u000a{{Cleanup-list|section|date=May 2014}}\u000a{{columns-list|2|\u000a<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\u000a* [[App.net]]\u000a* [[Diaspora (software)|Diaspora software]] and [[Diaspora (social network)|social network]]\u000a* [[DeviantART]]\u000a* [[Facebook]]\u000a* [[Flickr]]\u000a* [[FriendFeed]]\u000a* [[Gawker Media]] websites\u000a* [[GNU Social]]\u000a* [[Google+]]\u000a* [[Instagram]]\u000a* [[Kickstarter]]\u000a* [[Orkut]]<ref>{{cite web|url = http://en.blog.orkut.com/2012/02/hashtags-in-orkut-communities.html|title = Hashtags in Orkut communities|date = February 6, 2012 <!-- , 6:11 PM --> |publisher = Orkut|author = Marco Wisniewski}}</ref>\u000a* [[Sina Weibo]]\u000a* [[SoundCloud]]\u000a* [[Tout (company)|Tout]]\u000a* [[tsu]]\u000a* [[Tumblr]]\u000a* [[Twitter]]\u000a* [[Vine (software)|Vine]]\u000a* [[VK (social network)|VK]]\u000a}}\u000a\u000a==Usage==\u000a\u000a===Mass broadcast media===\u000a\u000aSince 2010, television series on various television channels promote themselves through "branded" hashtag [[digital on-screen graphic|bugs]].<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = Apr 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> This is used as a means of promoting a [[backchannel]] of online side-discussion before, during and after an episode broadcast. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref> (for example, a motion picture trailer).\u000a\u000aWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |publisher=Ibtimes.com |date=2013-03-22 |accessdate=2013-09-19}}</ref>\u000a\u000aThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter\u2019s Hashtag Pages Could Be The New AOL Keywords \u2014 But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>\u000a\u000a===Purchasing===\u000a\u000aSince February 2013 there is a collaboration between the social networking site Twitter and [[American Express]] that makes it possible to buy discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = 12 February 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = 2013-11-25}}</ref> American Express members can sync their card with Twitter and use the offers by tweeting and look for a response in a tweet with the confirmation from American Express.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=2013-11-25}}</ref>\u000a\u000a===Event promotion===\u000a\u000a[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]\u000aOrganized real-world events have also made use of hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other on both Twitter and, in many cases, in real life during events.\u000a\u000aCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\u000a\u000aPolitical protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.\u000a\u000a===Consumer complaints===\u000aHashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a corporate social media hashtag is used to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so customers could share positive experiences about the restaurant chain. The marketing effort was cancelled after just two hours when McDonald's received numerous complaint tweets rather than the positive stories they were expecting.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = 17 May 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = 2012-06-12}}</ref>\u000a\u000a===Sentiment analysis===\u000aThe use of hashtags also reveals things about the sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]]<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>\u2014a difficult [[Artificial Intelligence|AI]] problem.{{citation needed|date=September 2014}}\u000a\u000a==In popular culture==\u000aDuring the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]] [[Jack Layton]] referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably "#fail").<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = Apr 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = Apr 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>\u000a\u000aThe term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of ''[[Houston Press]]'', uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = Jul 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]] and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]]<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>\u000a\u000aOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].<ref>\u000a{{cite web \u000a| title = Twitter / nickbilton: My first byline on A1 of the ... \u000a| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1 \u000a| accessdate = 2013-09-14 \u000a }}\u000a</ref>\u000a\u000a"Hashtag [[Heel (professional wrestling)|heel]]" is a moniker used by [[WWE]] wrestler [[Dolph Ziggler]].\u000a\u000a[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags - social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>\u000a\u000aIn May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>\u000a\u000aIn September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiance Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>\u000a\u000a===Adaptations===\u000aIn 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=June 11, 2010 8:05 am |url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=2010-06-11 |accessdate=2014-08-25}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil.<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=2014-06-10 |accessdate=2014-08-25}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=2014-06-10 |accessdate=2014-08-25}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.\u000a\u000aIn July 2012, Twitter adapted the hashtag style to make company [[ticker symbol]]s preceded by the [[dollar sign]] clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols - Jul. 31, 2012 |publisher=Money.cnn.com |date=2012-07-31 |accessdate=2013-11-12}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=2012-07-30 |accessdate=2013-11-12}}</ref> This is intended to allow users to search posts discussing companies and their stocks.\u000a\u000aIn August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=http://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=1 August 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013 it was seen on TV used by [[Jimmy Fallon]], and on ''[[The Colbert Report]]'' among other places.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags\u2014and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref>\u000a\u000a==References==\u000a{{Reflist|colwidth=30em}}\u000a{{commons category|Hashtags}}\u000a\u000a{{Microblogging}}\u000a{{Online social networking}}\u000a{{Web syndication}}\u000a\u000a[[Category:Hashtags| ]]\u000a[[Category:Collective intelligence]]\u000a[[Category:Computer jargon]]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Metadata]]\u000a[[Category:Reference]]\u000a[[Category:Web 2.0]]\u000a[[Category:Social media]]\u000a[[Category:2010s slang]]
p51
asI158
(lp52
VCategory:Deep Web
p53
aV{{cat main|Deep Web}}\u000a\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:World Wide Web]]
p54
asI160
(lp55
VComprehensive Model of Information Seeking
p56
aVThe '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].\u000a\u000aThe CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. \u000aScience Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.\u000a\u000a==Design==\u000a\u000a[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]\u000aThe CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person\u2019s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer\u2019s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>\u000a\u000a==Antecdents==\u000aThe CMIS antecedents\u2014demographics, personal experience, salience, and beliefs\u2014are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>\u000a\u000a==Information Carrier Characteristics==\u000a\u000aThe information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.<ref name="auto"/>\u000a\u000a==Information Seeking Actions==\u000a\u000aThere are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>\u000a\u000a==Stages in the CMIS==\u000a\u000aA key concept from the CMIS is the notion of \u201cstages,\u201d or \u201ccancer involvement\u201d.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.\u000a\u000aThe first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.\u000a \u000aThe second stage is ''Purposive-Placid''. This is characterized by the question, \u201cWhat can I do to prevent cancer?\u201d Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.\u000a\u000aThe third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.\u000a\u000aThe fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a\u000a\u000a[[Category:Communication]]\u000a[[Category:Information retrieval]]\u000a[[Category:Health sciences]]
p57
asI162
(lp58
VVector space model
p59
aV'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].\u000a\u000a==Definitions==\u000a\u000aDocuments and queries are represented as vectors.\u000a\u000a:<math>d_j = ( w_{1,j} ,w_{2,j} , \u005cdotsc ,w_{t,j} )</math>\u000a:<math>q = ( w_{1,q} ,w_{2,q} , \u005cdotsc ,w_{n,q} )</math>\u000a\u000aEach [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).\u000a\u000aThe definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).\u000a\u000aVector operations can be used to compare documents with queries.\u000a\u000a==Applications==\u000a\u000a[[Image:vector space model.jpg|right|250px]]\u000a\u000a[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.\u000a\u000aIn practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:\u000a\u000a:<math>\u000a\u005ccos{\u005ctheta} = \u005cfrac{\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|}\u000a</math>\u000a\u000aWhere <math>\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}</math> is the intersection (i.e. the [[dot product]]) of the document (d<sub>2</sub> in the figure to the right) and the query (q in the figure) vectors, <math>\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c|</math> is the norm of vector d<sub>2</sub>, and <math>\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c|</math> is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:\u000a\u000a:<math>\u000a\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c| = \u005csqrt{\u005csum_{i=1}^n q_i^2}\u000a</math>\u000a\u000aAs all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.\u000a\u000a==Example: tf-idf weights==\u000a\u000aIn the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang <ref>[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975</ref> the term-specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is <math>\u005cmathbf{v}_d = [w_{1,d}, w_{2,d}, \u005cldots, w_{N,d}]^T</math>, where\u000a\u000a:<math>\u000aw_{t,d} = \u005cmathrm{tf}_{t,d} \u005ccdot \u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}\u000a</math>\u000a\u000aand\u000a* <math>\u005cmathrm{tf}_{t,d}</math> is term frequency of term ''t'' in document ''d'' (a local parameter)\u000a* <math>\u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}</math> is inverse document frequency (a global parameter). <math>|D|</math> is the total number of documents in the document set; <math>|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|</math> is the number of documents containing the term ''t''.\u000a\u000aUsing the cosine the similarity between document ''d<sub>j</sub>'' and query ''q'' can be calculated as:\u000a\u000a:<math>\u005cmathrm{sim}(d_j,q) = \u005cfrac{\u005cmathbf{d_j} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_j} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|} = \u005cfrac{\u005csum _{i=1}^N w_{i,j}w_{i,q}}{\u005csqrt{\u005csum _{i=1}^N w_{i,j}^2}\u005csqrt{\u005csum _{i=1}^N w_{i,q}^2}}</math>\u000a\u000a==Advantages==\u000a\u000aThe vector space model has the following advantages over the [[Standard Boolean model]]:\u000a\u000a#Simple model based on linear algebra\u000a#Term weights not binary\u000a#Allows computing a continuous degree of similarity between queries and documents\u000a#Allows ranking documents according to their possible relevance\u000a#Allows partial matching\u000a\u000a==Limitations==\u000a\u000aThe vector space model has the following limitations:\u000a\u000a#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])\u000a#Search keywords must precisely match document terms; word [[substring]]s might result in a "[[false positive]] match"\u000a#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "[[false negative]] match".\u000a#The order in which the terms appear in the document is lost in the vector space representation.\u000a#Theoretically assumes terms are statistically independent. \u000a#Weighting is intuitive but not very formal. \u000a\u000aMany of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].\u000a\u000a==Models based on and extending the vector space model==\u000a\u000aModels based on and extending the vector space model include:\u000a* [[Generalized vector space model]]\u000a* [[Latent semantic analysis]]\u000a* [[Term Discrimination]]\u000a* [[Rocchio Classification]]\u000a* [[random_indexing|Random Indexing]]\u000a\u000a==Software that implements the vector space model==\u000a\u000aThe following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.\u000a\u000a===Free open source software===\u000a\u000a* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.\u000a* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.\u000a* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tf\u2013idf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].\u000a* [[Weka (machine learning)|Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.\u000a* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.\u000a* [http://senseclusters.sourceforge.net SenseClusters], an open source package, written in Perl, that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.\u000a* [https://github.com/fozziethebeat/S-Space/wiki S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].\u000a* [http://www.cs.uni.edu/~okane/source/ISR/ Vector Space Model Software Workbench] Collection of 50 source code programs for education.\u000a\u000a==Further reading==\u000a\u000a* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613\u2013620. ''(Article in which a vector space model was presented)''\u000a* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''\u000a* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]\u000a* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the "k-Nearest Neighbor" search]\u000a\u000a==See also==\u000a*[[Bag-of-words model]]\u000a*[[Nearest neighbor search]]\u000a*[[Compound term processing]]\u000a*[[Inverted index]]\u000a*[[w-shingling]]\u000a*[[Eigenvalues and eigenvectors]]\u000a*[[Conceptual Spaces]].\u000a\u000a==References==\u000a<references/>\u000a\u000a[[Category:Vector space model|*]]
p60
asI163
(lp61
VExplicit semantic analysis
p62
aVIn [[natural language processing]] and [[information retrieval]], '''explicit semantic analysis''' ('''ESA''') is a [[Vector space model|vectorial]] representation of text (individual words or entire documents) that uses a document corpus as a [[knowledge base]]. Specifically, in ESA, a word is represented as a column vector in the [[tf*idf|tf\u2013idf]] matrix of the text corpus and a document (string of words) is represented as the [[centroid]] of the vectors representing its words. Typically, the text corpus is [[Wikipedia]], though other corpora including the [[Open Directory Project]] have been used.<ref name="infosys">{{cite journal |authors=Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich |year=2011 |title=Concept-Based Information Retrieval using Explicit Semantic Analysis |url=http://www.cs.technion.ac.il/~gabr/publications/papers/Egozi2011CBI.pdf|format=pdf|accessdate=January 3, 2015|journal=ACM Transactions on Information Systems |volume=29 |issue=2}}</ref>\u000a\u000aESA was designed by [[Evgeniy Gabrilovich]] and Shaul Markovitch as a means of improving [[document classification|text categorization]]<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Overcoming the brittleness bottleneck using Wikipedia: enhancing text categorization with encyclopedic knowledge |conference=Proc. 21st National Conference on Artificial Intelligence (AAAI) |pages=1301\u20131306 |year=2006 |url=http://www.aaai.org/Papers/AAAI/2006/AAAI06-204.pdf}}</ref>\u000aand has been used by this pair of researchers to compute what they refer to as "[[Semantics|semantic]] relatedness" by means of [[cosine similarity]] between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.\u000aThe name "explicit semantic analysis" contrasts with [[latent semantic analysis]] (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis |conference=Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI) |pages=1606\u20131611 |year=2007 |url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf}}</ref><ref name="infosys"/>\u000a\u000aESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically [[Orthogonality|orthogonal]] concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of [[information retrieval]] systems when it is based not on Wikipedia, but on the [[Reuters]] corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".<ref>Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2009c.pdf The ESA retrieval model revisited]. Proceedings of the 32nd International ACM Conference on Research and Development in Information Retrieval (SIGIR), pp. 670-671, 2009.</ref>\u000aTo explain this observation, links have been shown between ESA and the [[generalized vector space model]].<ref>Thomas Gottron, Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011o.pdf Insights into explicit semantic analysis]. Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.</ref>\u000aGabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".<ref name="infosys" />\u000a\u000a'''Cross-language explicit semantic analysis''' ('''CL-ESA''') is a multilingual generalization of ESA.<ref>Martin Potthast, Benno Stein, and Maik Anderka. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2008b.pdf A Wikipedia-based multilingual retrieval model]. Proceedings of the 30th European Conference on IR Research (ECIR), pp. 522-530, 2008.</ref>\u000aCL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.\u000a\u000a== See also ==\u000a* [[Topic model]]\u000a\u000a== External links ==\u000a* [http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html Explicit semantic analysis] on Evgeniy Gabrilovich's homepage; has links to implementations\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a[[Category:Natural language processing]]\u000a[[Category:Vector space model]]
p63
asI36
(lp64
VMean reciprocal rank
p65
aV{{Refimprove|date=June 2007}}\u000a'''Mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:<ref>{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&ndash;82}}</ref>\u000a\u000a:<math> \u005ctext{MRR} = \u005cfrac{1}{|Q|} \u005csum_{i=1}^{|Q|} \u005cfrac{1}{\u005ctext{rank}_i}. \u005c!</math>\u000a\u000aThe reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.\u000a\u000a== Example ==\u000aFor example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:\u000a\u000a{| class="wikitable"\u000a|-\u000a! Query\u000a! Results\u000a! Correct response\u000a! Rank\u000a! Reciprocal rank\u000a|-\u000a| cat\u000a| catten, cati, '''cats'''\u000a| cats\u000a| 3\u000a| 1/3\u000a|-\u000a| torus\u000a| torii, '''tori''', toruses\u000a| tori\u000a| 2\u000a| 1/2\u000a|-\u000a| virus\u000a| '''viruses''', virii, viri\u000a| viruses\u000a| 1\u000a| 1\u000a|}\u000a\u000aGiven those three samples, we could calculate the mean reciprocal rank as (1/3&nbsp;+&nbsp;1/2&nbsp;+&nbsp;1)/3 = 11/18 or about 0.61.\u000a\u000aThis basic definition does not specify what to do if...\u000a# none of the proposed results are correct (use reciprocal rank 0), or if\u000a# there are multiple correct answers in the list. Consider using [[Information_retrieval#Mean_average_precision|mean average precision (MAP)]].\u000a\u000aSee also [[Information retrieval]] and [[Question answering]].<ref>{{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC | author=D. R. Radev, H. Qi, H. Wu, W. Fan |year=2002 }}</ref>\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Summary statistics]]\u000a[[Category:Information retrieval]]
p66
asI37
(lp67
VConcept Searching Limited
p68
aV{{Infobox company |\u000a  name   = Concept Searching Limited |\u000a  logo = [[Image:conceptSearching.jpg]] |\u000a  company_slogan = "Retrieval Just Got Smarter" |\u000a  type   =  [[Privately held company|Private]] |\u000a  foundation     = 2002|\u000a  location       = [[UK]], [[USA]] |\u000a  area_served    = Global |\u000a  industry       = [[Information retrieval]] |\u000a  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |\u000a  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]\u000a}}\u000a\u000a'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].\u000a\u000a==History==\u000aConcept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>\u000a\u000aCompound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>\u000a\u000aThe company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>\u000a\u000aConcept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>\u000a\u000a== Awards ==\u000a* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-Companies-That-Matter-in-Knowledge-Management-94933.aspx |title=KMWorld Magazine}}</ref>\u000a* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2014-98792.aspx |title=Trend-Setting Products}}</ref>\u000a\u000a==See also==\u000a* [[Compound term processing]]\u000a* [[Enterprise search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Concept Search]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.conceptsearching.com/ Company Website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Privately held companies of the United Kingdom]]
p69
asI38
(lp70
VGlobrix
p71
aV'''Globrix''' was a UK [[real estate]] [[Web search engine|search engine]] that was launched in January 2008. It was launched as a joint venture with [[News International]], publishers of ''[[The Sunday Times]]'', ''[[The Sun (newspaper)|The Sun]]'', ''[[The Times]]'', ''[[The News of the World]]'' and ''[[Thelondonpaper]]''.<ref>[http://www.nma.co.uk/news/news-international-invests-in-property-site-globrix/35492.article News International invests in property site, Globrix - NMA article]</ref>\u000a\u000a[[Estate agent]]s and [[letting agent]]s could list their properties for free. This competed with traditional paid-listings sites such as [[Rightmove]] (originally a joint venture between four of the UK's largest property agents, now a [[public limited company]]), [[Zoopla|Propertyfinder]] (also partly backed by News International) and [[Primelocation]] (owned by [[Daily Mail and General Trust]]). Unlike most property websites, Globrix directed users to agent websites rather than hosting the property details and capturing the lead on Globrix itself. Globrix gathered its property listings in three different ways; crawling agent websites, taking data feeds and by agents manually uploading via the Globrix extranet. Because Globrix was 'free to list', Globrix was able to gain substantial market coverage and claimed to list more properties than any other UK property website. Unlike websites like [[Gumtree]] and [[Oodle]], private sellers and landlords were not allowed to list their properties on the site.\u000a\u000aThe website charged property professionals and property related services companies for geo-targeted [[Web banner|banner ads]]. There were also premium services available to estate and letting agents (such as [[Search Engine Optimization]] consultancy, branded email alerts and increased traffic) and [[Google Ads]] were displayed in unsold advertising positions on the right hand side of search results.\u000a\u000a==Functionality==\u000a\u000aThe basic property search functionality is kept simple with just one text box on the homepage. Users can search for property by location (e.g. city, town, full postcode, partial postcode or, unusually for property portals, street name), places of interest (e.g. schools, stations, landmarks) or by key features (e.g. swimming pool, garden, double glazing, helipad).\u000a\u000aSearch results can then be refined further by changing the price parameters, number of bedrooms and bathrooms, property type (e.g. detached, bungalow, flat), outside space, nearby stations and schools and property features (e.g. wooden floors, sea view). Registered users are able to search by additional parameters such as price change.\u000a\u000aAs an alternative to the regular 'list view' of property results, users can also opt to see the search results plotted on [[Bing Maps]] (previously they used [[Google map]]) to allow users to look for property by location. (Some users are unimpressed with the lack of precision of the inferior Bing offering, which often manages to put the marker in a field, compared to the accuracy and ease of use of Googlemaps).  Users are able to drag and zoom the map, with relevant properties automatically placed in view. It is also possible for users to draw a catchment area directly onto the map of where they would like to search.\u000a\u000a==Data==\u000a\u000aGlobrix data was sometimes used by the national media to illustrate stories on house prices,<ref>House prices drop 100,000 in two weeks in race to sell before Christmas - Daily Mail [http://www.dailymail.co.uk/news/article-1089563/House-prices-drop-100-000-WEEKS-race-sell-Christmas.html]</ref> the economy, area trends, consumer confidence<ref>[http://news.bbc.co.uk/1/hi/business/7737507.stm House sales rise as prices fall - BBC News]</ref> and the property market.<ref>[http://www.telegraph.co.uk/finance/personalfinance/borrowing/mortgages/3268208/Housing-market-stagnates-as-buyers-disappear.html Housing market stagnates as buyers disappear - Daily Telegraph]</ref>\u000a\u000a==Awards==\u000a\u000aIn 2008, Globrix was awarded 'Best Property Portal UK' which is awarded by one of the group's own newspapers, the [[The Daily Mail]].<ref>[http://www.residentialpropertyawards.net/index.php/International/Winners/Winners-of-2008.html Daily Mail Property Awards 2008]</ref> Globrix also won 'Estate Agency Service Firm of the Year' at The Negotiator Awards.<ref>[http://negotiator-magazine.co.uk/events/awards/categories-and-finalists/agency-service-firm-of-the-year/ The Negotiator Awards 2008]</ref>\u000a\u000a==Founders==\u000a\u000aGlobrix was founded by Dan Lee and Ian Parry, both ex employees of UK-based search company [[Autonomy Corporation|Autonomy]] and the Norwegian search company [[Fast Search & Transfer|FAST]].\u000a\u000a==Merged with Zoopla==\u000a\u000aIn December 2012 Globrix merged with [[Zoopla]].<ref name="Estate Agent Today">{{cite web|title=Zoopla acquires Globrix as it steps up battle against Rightmove|url=http://www.estateagenttoday.co.uk/news_features/Zoopla-acquires-Globrix-as-it-steps-up-battle-against-Rightmove|work=Estate Agent Today|accessdate=8 September 2013}}</ref>\u000a\u000a== References ==\u000a<references/>\u000a\u000a==External links==\u000a* [http://www.globrix.com/ Globrix homepage]\u000a* [http://www.ft.com/cms/s/0/bc401968-824e-11dc-8a8f-0000779fd2ac.html News International invests in search engine] - Financial Times\u000a* [http://www.independent.co.uk/news/business/analysis-and-features/home-search-sites-have-a-new-kid-on-the-block-786342.html Home search sites have a new kid on the block] - The Independent\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Online real estate companies]]
p72
asI169
(lp73
VPtx (Unix)
p74
aV{{Unreferenced stub|auto=yes|date=December 2009}}\u000a{{Lowercase|title=ptx}}\u000a'''ptx''' is a [[Unix]] utility, named for the ''permuted index'' which can perform the function of the Keyword in Context ([[Key Word in Context|KWIC]]) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, thesauruses, or web sites to aid in locating entries of interest.\u000a\u000a==See also==\u000a* [[Concordancer]]\u000a\u000a[[Category:Searching]]\u000a[[Category:Unix text processing utilities]]\u000a\u000a\u000a{{Unix-stub}}
p75
asI42
(lp76
VGLIMPSE
p77
aV{{Other uses|Glimpse (disambiguation)}}\u000a{{Infobox software\u000a| name = Glimpse\u000a| logo = \u000a| screenshot =\u000a| caption =\u000a| developer = [[Internet WorkShop]]\u000a| status = \u000a| latest release version = 4.18.6 (source) / 4.18.5 (binary) \u000a| latest release date = {{release date|2012|06|09}}\u000a| operating system = [[Cross-platform]]\u000a| programming language = [[C (programming language)|C]]\u000a| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]\u000a| license = \u000a| website = {{URL|http://webglimpse.net/}}\u000a}}\u000a'''GLIMPSE''' is a text indexing and [[text retrieval|retrieval]] [[software]] program originally developed at the [[University of Arizona]] by [[Udi Manber]], [[Sun Wu]], and [[Burra Gopal]].  It was released under the ISC [[open source]] license in September 2014.\u000a\u000aGLIMPSE stands for GLobal IMPlicit SEarch. While many text indexing schemes create quite large indexes (usually around 50% of the size of the original text), a GLIMPSE-created index is only 2-4% of the size of the original text.\u000a\u000aGLIMPSE uses and takes a great deal of inspiration from [[Agrep]], which was also developed at the University of Arizona, but GLIMPSE uses a high level index whereas Agrep parses all the text each time.\u000a\u000aThe basic algorithm is similar to other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After one or more of these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a single workstation. In addition, the smaller index can be created more quickly than a full index.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://webglimpse.net/ Glimpse and WebGlimpse home page]\u000a*[http://webglimpse.net/pubs/glimpse.pdf Original Glimpse paper] (PDF)\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Free search engine software]]\u000a[[Category:Search engine software]]
p78
asI43
(lp79
VSimRank
p80
aV'''SimRank''' is a general [[Semantic similarity|similarity measure]], based on a simple and intuitive [[Graph theory|graph-theoretic model]].\u000aSimRank is applicable in any [[Domain model|domain]] with object-to-object [[Relation (mathematics)|relationships]], that measures similarity of the structural context in which objects occur, based on their relationships with other objects.\u000aEffectively, SimRank is a measure that says "'''two objects are considered to be similar if they are referenced by similar objects'''."\u000a\u000a== Introduction ==\u000a\u000aMany [[Application software|applications]] require a measure of "similarity" between objects.\u000aOne obvious example is the "find-similar-document" query,\u000aon traditional text corpora or the [[World Wide Web|World-Wide Web]].\u000aMore generally, a similarity measure can be used to [[Cluster analysis|cluster objects]], such as for [[collaborative filtering]] in a [[recommender system]], in which \u201csimilar\u201d users and items are grouped based on the users\u2019 preferences.\u000a\u000aVarious aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain.\u000aIn a [[Text corpus|document corpus]], matching text may be used, and for collaborative filtering, similar users may be identified by common preferences.\u000aSimRank is a general approach that exploits the object-to-object relationships found in many domains of interest.\u000aOn the [[World Wide Web|Web]], for example, two pages are related if there are [[hyperlink]]s between them.\u000aA similar approach can be applied to scientific papers and their citations, or to any other document corpus with [[cross-reference]] information.\u000aIn the case of recommender systems, a user\u2019s preference for an item constitutes a relationship between the user and the item.\u000aSuch domains are naturally modeled as [[Graph (mathematics)|graphs]], with [[Vertex (graph theory)|nodes]] representing objects and [[Edge (graph theory)#Graph|edges]] representing relationships.\u000a\u000aThe intuition behind the SimRank algorithm is that, in many domains, '''similar objects are referenced by similar objects'''.\u000aMore precisely, objects <math>a</math> and <math>b</math> are considered to be similar if they are pointed from objects <math>c</math> and <math>d</math>, respectively, and <math>c</math> and <math>d</math> are themselves similar.\u000aThe [[Recursion (computer science)#Recursive programming|base case]] is that objects are maximally similar to themselves\u000a.<ref name=jeh_widom>G. Jeh and J. Widom. SimRank: A Measure of Structural-Context Similarity. In [[SIGKDD|KDD'02]]: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543. [[Association for Computing Machinery|ACM Press]], 2002. [http://www-cs-students.stanford.edu/~glenj/simrank.pdf]</ref>\u000a\u000aIt is important to note that SimRank is a general algorithm that determines only the similarity of structural context.\u000aSimRank applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships.\u000aObviously, similarity of other domain-specific aspects are important as well; these can \u2014 and should be combined with relational structural-context similarity for an overall similarity measure.\u000aFor example, for [[Web page]]s SimRank can be combined with traditional textual similarity; the same idea applies to scientific papers or other document corpora.\u000aFor recommendation systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level).\u000aAgain, these similarities can be combined with the similarity scores that are computed based on preference patterns, in order to produce an overall similarity measure.\u000a\u000a== Basic SimRank equation ==\u000a\u000aFor a node <math>v</math> in a directed graph, we denote by <math>I(v)</math> and <math>O(v)</math> the set of in-neighbors and out-neighbors of <math>v</math>, respectively.\u000aIndividual in-neighbors are denoted as <math>I_i(v)</math>, for <math>1 \u005cle i \u005cle \u005cleft|I(v)\u005cright|</math>, and individual\u000aout-neighbors are denoted as <math>O_i(v)</math>, for <math>1 \u005cle i \u005cle \u005cleft|O(v)\u005cright|</math>.\u000a\u000aLet us denote the similarity between objects <math>a</math> and <math>b</math> by <math>s(a, b) \u005cin [0, 1]</math>. \u000aFollowing the earlier motivation, a recursive equation is written for <math>s(a, b)</math>.\u000aIf <math>a = b</math> then <math>s(a, b)</math> is defined to be <math>1</math>.\u000aOtherwise,\u000a:<math>s(a, b) = \u005cfrac{C}{\u005cleft|I(a)\u005cright| \u005cleft|I(b)\u005cright|}\u000a \u005csum_{i=1}^{\u005cleft|I(a)\u005cright|}\u005csum_{j=1}^{\u005cleft|I(b)\u005cright|}\u000a s(I_i(a), I_j(b))</math>\u000awhere <math>C</math> is a constant between <math>0</math> and <math>1</math>.\u000aA slight technicality here is that either <math>a</math> or <math>b</math> may not have any in-neighbors.\u000aSince there is no way to infer any similarity between <math>a</math> and <math>b</math> in this case, similarity is set to <math>s(a, b) = 0</math>, so the summation in the above equation is defined to be <math>0</math> when <math>I(a) = \u005cemptyset</math> or <math>I(b) = \u005cemptyset</math>.\u000a\u000a== Matrix representation of SimRank ==\u000a\u000aLet <math>\u005cmathbf{S}</math> be the similarity matrix whose entry <math>[\u005cmathbf{S}]_{a,b}</math> denotes the similarity score <math>s(a,b)</math>, and <math>\u005cmathbf{A}</math> be the column normalized adjacency matrix whose entry <math>[\u005cmathbf{A}]_{a,b}=\u005ctfrac{1}{|\u005cmathcal{I}(b)|}</math> if there is an edge from <math>a</math> to <math>b</math>, and 0 otherwise. Then, in matrix notations, SimRank can be formulated as\u000a\u000a:<math>\u000a   {{\u005cmathbf{S}}}= \u005cmax\u005c{C\u005ccdot (\u005cmathbf{A}^{T} \u005ccdot {{\u005cmathbf{S}}}\u005ccdot {{\u005cmathbf{A}}} ) , {{\u005cmathbf{I}}}\u005c},</math>\u000a\u000awhere <math>\u005cmathbf{I}</math> is an identity matrix.\u000a\u000a== Computing SimRank ==\u000a\u000aA solution to the SimRank equations for a graph <math>G</math> can be reached by [[Iterative method|iteration]] to a [[Fixed point (mathematics)|fixed-point]].\u000aLet <math>n</math> be the number of nodes in <math>G</math>.\u000aFor each iteration <math>k</math>, we can keep <math>n^2</math> entries <math>s_k(*, *)</math>, where <math>s_k(a, b)</math> gives the score between <math>a</math> and <math>b</math> on iteration <math>k</math>.\u000aWe successively compute <math>s_{k+1}(*, *)</math> based on <math>s_k(*, *)</math>.\u000aWe start with <math>s_0(*, *)</math> where each <math>s_0(a, b)</math> is a lower bound on the actual SimRank score <math>s(a, b)</math>:\u000a:<math> s_0(a, b) =\u000a \u005cbegin{cases}\u000a  1 \u005cmbox{  } , \u005cmbox{    } \u005cmbox{if } a = b  \u005cmbox{  } , \u005c\u005c\u000a  0 \u005cmbox{  } , \u005cmbox{    } \u005cmbox{if } a \u005cneq b \u005cmbox{  } .\u000a \u005cend{cases}</math>\u000a\u000aTo compute <math>s_{k+1}(a, b)</math> from <math>s_k(*, *)</math>, we use the basic SimRank equation to get:\u000a:<math>s_{k + 1}(a, b) = \u000a \u005cfrac{C}{\u005cleft|I(a)\u005cright| \u005cleft|I(b)\u005cright|}\u000a \u005csum_{i=1}^{\u005cleft|I(a)\u005cright|}\u005csum_{j=1}^{\u005cleft|I(b)\u005cright|}\u000a  s_k(I_i(a), I_j(b))</math>\u000afor <math>a \u005cne b</math>, and <math>s_{k+1}(a, b) = 1</math> for <math>a = b</math>.\u000aThat is, on each iteration <math>k + 1</math>, we update the similarity of <math>(a, b)</math> using the similarity scores of the neighbours of <math>(a, b)</math> from the previous iteration <math>k</math> according to the basic SimRank equation.\u000aThe values <math>s_k(*, *)</math> are [[Monotonic function|nondecreasing]] as <math>k</math> increases.\u000aIt was shown in <ref name="jeh_widom"/> that the values [[Limit of a sequence|converge]] to [[Limit of a sequence|limits]] satisfying the basic SimRank equation, the SimRank scores <math>s(*, *)</math>, i.e., for all <math>a, b \u005cin V</math>, <math>\u005clim_{k \u005cto \u005cinfty} s_k(a, b) = s(a, b)</math>.\u000a\u000aThe original SimRank proposal suggested choosing the decay factor <math>C = 0.8</math> and a fixed number <math>K = 5</math> of iterations to perform.\u000aHowever, the recent research <ref name="lizorkin">D. Lizorkin, P. Velikhov, M. Grinev and D. Turdakov. Accuracy Estimate and Optimization Techniques for\u000aSimRank Computation. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 422--433. [http://modis.ispras.ru/Lizorkin/Publications/simrank_accuracy.pdf]</ref> showed that the given values for <math>C</math> and <math>K</math> generally imply relatively low [[Accuracy and precision|accuracy]] of iteratively computed SimRank scores.\u000aFor guaranteeing more accurate computation results, the latter paper suggests either using a smaller decay factor (in particular, <math>C = 0.6</math>) or taking more iterations.\u000a\u000a== Partial Sums Memoization ==\u000a\u000aThe recent work of Lizorkin et al.<ref name="lizorkin"/> proposed three optimization techniques for speeding up the computation of SimRank:\u000a\u000a(1) Essential nodes selection may eliminate the computation of a fraction of node pairs with a-priori zero scores.\u000a\u000a(2) Partial sums memoization can effectively reduce repeated calculations of the similarity among different node pairs by caching part of similarity summations for later reuse.\u000a\u000a(3) A threshold setting on the similarity enables a further reduction in the number of node pairs to be computed. \u000a\u000aIn particular, the second observation of partial sums memoization plays a paramount role in greatly speeding up the computation of SimRank from <math>O(Kd^2n^2)</math> to <math>O(Kdn^2)</math>, where <math>K</math> is the number of iterations, <math>d</math> is average degree of a graph, and <math>n</math> is the number of nodes in a graph. The central idea of partial sums memoization consists of two steps:\u000a\u000aFirst, the partial sums over <math>{\u005cmathcal I}(a)</math> are memoized as\u000a:<math>\u000aPartial_{{\u005cmathcal I}(a)}^{s_{k}}(j)=\u005csum_{i\u005cin{\u005cmathcal I}(a)}s_{k}(i,j), \u005cqquad (\u005cforall j \u005cin {\u005cmathcal I}(b))\u000a</math>\u000aand then <math>s_{k+1} (a,b)</math> is iteratively computed from <math>Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j)</math> as\u000a:<math>\u000as_{k+1}( a,b )=\u005ctfrac{C}{| \u005cmathsf{\u005cmathcal{I}}( a ) | | \u005cmathsf{\u005cmathcal{I}}( b ) |}\u005csum_{j \u005cin \u005cmathsf{\u005cmathcal{I}}( b ) } Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j).\u000a</math>\u000aConsequently, the results of <math>Partial_{{\u005cmathcal I}(a)}^{s_{k}}(j)</math>, <math>\u005cforall j \u005cin {\u005cmathcal I}(b)</math>,\u000acan be reused later when we compute the similarities <math>s_{k+1}(a,*)</math> for a given vertex <math>a</math> as the first argument.\u000a\u000a== Further research on SimRank ==\u000a\u000a* Fogaras and Racz <ref name="fogaras_racz">D. Fogaras and B. Racz. Scaling link-based similarity search. In [[World Wide Web Conference|WWW '05]]: Proceedings of the 14th international conference on World Wide Web, pages 641--650, New York, NY, USA, 2005. [[Association for Computing Machinery|ACM]]. [http://www2005.org/docs/p641.pdf]</ref> suggested speeding up SimRank computation through [[Probability theory|probabilistic]] computation using the [[Monte Carlo method]].\u000a\u000a* Antonellis et al.<ref name="simrank_plusplus">I. Antonellis, H. Garcia-Molina and C.-C. Chang. Simrank++: Query Rewriting through Link Analysis of the Click Graph. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 408--421. [http://dbpubs.stanford.edu/pub/showDoc.Fulltext?lang=en&doc=2008-17&format=pdf&compression=&name=2008-17.pdf]</ref> extended SimRank equations to take into consideration (i) evidence factor for [[Graph (mathematics)#Properties of graphs|incident nodes]] and (ii) link weights.\u000a\u000a* Lizorkin et al.<ref name="lizorkin"/> proposed several [[Optimization (computer science)|optimization]] techniques for speeding up SimRank iterative computation.\u000a\u000a* Yu et al.<ref name="yu_icde13">W. Yu, X. Lin, W. Zhang. Towards Efficient SimRank Computation on Large Networks. In [[International Conference on Data Engineering|ICDE '13]]: Proceedings of the 29th IEEE International Conference on Data Engineering, pages 601--612. [http://www.cse.unsw.edu.au/~weirenyu/pubs/icde13.pdf]</ref> further improved SimRank computation via a fine-grained [[memoization]] method to share small common parts among different partial sums.\u000a\u000a== See also ==\u000a\u000a* [[PageRank]]\u000a\u000a== Citations ==\u000a{{reflist|colwidth=30em}}\u000a\u000a[[Category:Information retrieval]]
p81
asI27
(lp82
VDocument clustering
p83
aV{{disputed|date=March 2014}}\u000a{{inline|date=March 2014}}\u000a'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.\u000a\u000a==Overview==\u000aDocument clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.\u000a\u000aThe application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared offline applications.\u000a\u000aIn general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment \u2013 each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft \u2013 a document\u2019s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.\u000a\u000aOther algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.\u000a\u000aGiven a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.\u000a\u000a==Clustering in search engines==\u000aA [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by Enterprise Search engines such as [[Northern Light Group|Northern Light]] and [[Vivisimo]], consumer search engines such as [http://www.polymeta.com/ PolyMeta] and [http://www.helioid.com Helioid], or open source software such as [[Carrot2]].\u000a\u000aExamples:\u000a\u000a* Clustering divides the results of a search for "cell" into groups like "biology," "battery," and "prison."\u000a\u000a* [http://FirstGov.gov FirstGov.gov], the official Web portal for the U.S. government, uses document clustering to automatically organize its search results into categories.  For example, if a user submits \u201cimmigration\u201d, next to their list of results they will see categories for \u201cImmigration Reform\u201d, \u201cCitizenship and Immigration Services\u201d, \u201cEmployment\u201d, \u201cDepartment of Homeland Security\u201d, and more.\u000a\u000a== References ==\u000a{{reflist}}\u000aPublications:\u000a* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]\u000a* Claudio Carpineto, Stanislaw Osi\u0144ski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys (CSUR), Volume 41, Issue 3 (July 2009), Article No. 17, ISSN:0360-0300 \u000a* http://semanticsearchart.com/researchBest.html - comparison of several popular clustering algorithms, data and software to reproduce the result.\u000a* Tanmay Basu, C.A. Murthy , CUES: A New Hierarchical Approach for Document Clustering, 2013 [http://jprr.org  JPRR]\u000a\u000a==See Also==\u000a*[[Cluster Analysis]]\u000a*[[Fuzzy clustering]]\u000a[[Category:Information retrieval]]
p84
asI178
(lp85
VSearch-oriented architecture
p86
aV{{unreferenced|date=October 2007}}\u000aThe use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.\u000a\u000aIn a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.\u000a\u000aIn a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.\u000a\u000aThe benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. \u000a\u000a== Contrast with ==\u000a* [[Service-oriented architecture]] (SOA)\u000a* [[Service-Oriented Modeling]]\u000a\u000a== See also ==\u000a* [[Hibernate search]]\u000a \u000a[[Category:Software architecture]]\u000a[[Category:Data search engines]]\u000a[[Category:Searching]]
p87
asI52
(lp88
VCollaborative search engine
p89
aV{{Recommender systems}}\u000a'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].\u000a\u000a== Models of collaboration ==\u000a\u000aCollaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization\u000a<ref name=Golo2007>{{citation\u000a | title = Collaborative Exploratory Search\u000a | year = 2007\u000a | author = Golovchinsky Gene, Pickens Jeremy\u000a | journal = Proceedings of HCIR 2007 workshop\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf\u000a}}</ref> and depth of mediation \u000a,<ref name=Pickens2008>{{citation\u000a | title = Collaborative Exploratory Search\u000a | year = 2008\u000a | author = Pickens Jeremy, Golovchinsky Gene, Shah Chirag, Qvarfordt Pernilla, Back Maribeth\u000a | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval\u000a | pages = 315\u2013322\u000a | volume = \u000a | issue = \u000a | doi = 10.1145/1390334.1390389\u000a | isbn = \u000a 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389\u000a| chapter = Algorithmic mediation for collaborative exploratory search\u000a }}</ref> task vs. trait,<ref name=Morris2008>{{citation\u000a | contribution = Understanding Groups\u2019 Properties as a Means of Improving Collaborative Search Systems\u000a | year = 2008\u000a | author = Morris Meredith, Teevan Jaime\u000a | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf\u000a}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation\u000a | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval\u000a | year = 2008\u000a | author = Foley Colum\u000a | booktitle = PhD Thesis, Dublin City University\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf\u000a}}</ref>\u000a\u000a=== Explicit vs. implicit collaboration ===\u000a\u000aImplicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation\u000a | title = Collaborative Web Search\u000a | year = 2003\u000a | author = Barry Smyth, Evelyn Balfe, Peter Briggs, Maurice Coyle, Jill Freyne\u000a | journal = IJCAI\u000a | pages = 1417\u20131419\u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation\u000a | title = Community search assistant\u000a | year = 2001\u000a | author = Natalie S. Glance\u000a | journal = Workshop on AI for Web Search AAAI'02\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation\u000a | title = Discovering the Scope of Privacy Needs in Collaborative Search\u000a | year = 2008\u000a | author = Thorben Burghardt, Erik Buchmann, Klemens Bhm\u000a | journal = Web Intelligence (WI)\u000a | pages = \u000a 910| volume = \u000a | issue = \u000a | doi = 10.1109/WIIAT.2008.165\u000a | isbn = \u000a 978-0-7695-3496-1}}</ref> and the works of Longo et al.\u000a<ref name=Longo2009a>{{citation\u000a | title = Toward Social Search - From Explicit to Implicit Collaboration\u000a               to Predict Users' Interests\u000a | year = 2009\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = WEBIST 2009 - Proceedings of the Fifth International Conference\u000a               on Web Information Systems and Technologies, Lisbon, Portugal,\u000a               March 23\u201326, 2009\u000a | pages = 693\u2013696\u000a | volume = 1\u000a | issue = \u000a | doi = \u000a | isbn = 978-989-8111-81-4\u000a | url = \u000a}}</ref> \u000a<ref name=Longo2010>{{citation\u000a | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time\u000a | year = 2010\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = Transaction Computational Collective Intelligence II\u000a | pages = 46\u201369\u000a | volume = 2\u000a | issue = \u000a | doi = 10.1007/978-3-642-17155-0_3\u000a | isbn = \u000a 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/\u000a| series = Lecture Notes in Computer Science\u000a }}</ref> \u000a<ref name=Longo2009b>{{citation\u000a | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search\u000a | year = 2009\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = Computational Collective Intelligence. Semantic Web, Social\u000a               Networks and Multiagent Systems, First International Conference,\u000a               ICCCI 2009, Wroclaw, Poland, October 5\u20137, 2009. Proceedings\u000a | pages = 63\u201374\u000a | volume = 1\u000a | issue = \u000a | doi = \u000a | isbn = 978-3-642-04440-3\u000a | url = http://dl.acm.org/citation.cfm?id=1692026\u000a}}</ref> \u000aall represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.\u000a\u000aExplicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation\u000a | title = SearchTogether: An Interface for Collaborative Web Search\u000a | year = 2007\u000a | author = Meredith Ringel Morris, Eric Horvitz\u000a | journal = UIST\u000a| url = http://portal.acm.org/citation.cfm?id=1294211.1294215\u000a}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation\u000a | title = The Role of Communication in Collaborative Information Searching\u000a | year = 2008\u000a | author = Madhu C. Reddy, Bernhard J. Jansen, Rashmi Krishnappa\u000a | journal = ASTIS\u000a}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.\u000a\u000aHowever, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis, Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88\u201398| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.\u000a\u000a=== Community of practice  ===\u000a\u000aRecent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation\u000a | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries\u000a | year = 2002\u000a | author = Rohini U, Vamshi Ambati\u000a | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>\u000a\u000aCollaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation\u000a | title = Social Aspects of a Collaborative, Community-Based Search Network\u000a | editor4-first = Eelco\u000a | editor3-first = Pearl\u000a | editor2-first = Judy\u000a | editor1-first = Wolfgang\u000a | year = 2008\u000a | editor1-last = Nejdl\u000a | author = Maurice Coyle and Barry Smyth\u000a | journal = Adaptive Hypermedia and Adaptive Web-Based Systems\u000a | pages =  103\u2013112  \u000a | volume = 5149/2008\u000a | issue = \u000a \u000a | series = Volume| doi = 10.1007/978-3-540-70987-9\u000a | isbn = 978-3-540-70984-8\u000a | url = http://portal.acm.org/citation.cfm?id=1485050\u000a | editor2-last = Kay\u000a | editor4-last = Herder\u000a | editor3-last = Pu}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project Jumper 2.0.<ref name=Jumper2010>{{citation\u000a | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features\u000a | year = 2010\u000a | author = Jumper Networks Inc\u000a | journal = Press release\u000a | pages = \u000a | volume =\u000a | issue = \u000a | doi =\u000a | isbn =\u000a | url = http://www.trilexnet.com/labs/jumper}}</ref>\u000a\u000a=== Depth of mediation ===\u000a\u000aThis refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.\u000a\u000a=== Task vs. trait ===\u000a\u000aThis model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />\u000a\u000a== Privacy-aware collaborative search engines ==\u000a\u000aSearch terms and links clicked that are shared among users reveal their interests, habits, social\u000arelations and intentions.<ref name=EUArticle29>{{citation\u000a | title = Article 29 EU Data Protection Working Party\u000a | year = 2008\u000a | author = Data Protection Working Party\u000a | journal = EU\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. \u000a<ref name="Morris2007"/><ref name=Smith2005>{{citation\u000a | title = A Live-User Evaluation of Collaborative Web Search\u000a | year = 2005\u000a | author = Barry Smyth, Evelyn Balfe, Oisin Boydell, Keith Bradley, Peter Briggs, Maurice Coyle, Jill Freyne\u000a | journal = IJCAI\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref>\u000a<ref name=Smith2006>{{citation\u000a | title = Anonymous personalization in collaborative web search\u000a | year = 2005\u000a | author = Smyth,, Barry and Balfe,, Evelyn\u000a | journal = Inf. Retr.\u000a | pages = 165\u2013190\u000a | volume = 9\u000a | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = \u000a | url = \u000a}}</ref>\u000a<ref name=Jung2004>{{citation\u000a | title = Applying Collaborative Filtering for Efficient Document Search\u000a | year = 2004\u000a | author = Seikyung Jung, Juntae Kim, Herlocker, J.L.\u000a | journal = Inf. Retr.\u000a | pages = 640\u2013643\u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.\u000a\u000aAs CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation\u000a | title = Collaborative Search And User Privacy: How Can They Be Reconciled?\u000a | year = 2008\u000a | author = Thorben Burghardt, Erik Buchmann, Klemens Bhm, Chris Clifton\u000a | journal = CollaborateCom\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://dbis.ipd.uni-karlsruhe.de/1184.php\u000a}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.\u000a\u000a== References ==\u000a{{reflist|2}}\u000a{{Internet search}}\u000a\u000a[[Category:Information retrieval]]
p90
asI182
(lp91
VDesktop search
p92
aV{{multiple issues|\u000a{{Cleanup|date=October 2010}}\u000a{{technical|date=October 2014}}\u000a}}\u000a\u000a[[File:Puggle-search.png|thumb|Puggle Desktop Search]]\u000a[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]\u000a'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.\u000a\u000aOne of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.\u000a\u000aA variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.\u000a\u000aDesktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users needs to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] \u2014 the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.\u000a\u000aCompanies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index \u2014 but not preview \u2014 items they should not even know exist.{{Citation needed|date = November 2009}}\u000a\u000aHistorically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.\u000a\u000a== Technologies ==\u000aMost desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products, such as Lookeen,<ref>{{cite web|title=Real-Time Indexing and Lookeen 8|url=http://www.lookeen.net/2884/News/real-time-ndexing-and-lookeen-8/|publisher=Lookeen|accessdate=26 October 2014}}</ref> have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.\u000a\u000aDesktop search tools typically collect three types of information about files:\u000a* file and folder names\u000a* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]\u000a* file content (for supported types of documents only)\u000a\u000aTo search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.\u000a\u000aLong-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>[http://www.niallkennedy.com/blog/archives/2006/10/video-search.html "The current state of video search", by Niall Kennedy]</ref><ref>[http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html "The current state of audio search", by Niall Kennedy]</ref>\u000a\u000aThe sector attracted considerable attention from the struggle between Microsoft and Google.<ref>[http://news.bbc.co.uk/1/hi/technology/3952285.stm "Search wars hit desktop computers". (Oct 2004) BBC News]</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>[http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/ "Microsoft agrees to change Vista Desktop Search Tool" (Jun 2007)]</ref>\u000a\u000aAs of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more. <ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>  \u000a\u000aX1 makes one of the leading desktop search products on the market. X1 Search 8 is a software alternative to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more. <ref>[http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/ "X1 rises again with Desktop Search 8, Virtual Edition" (May 2013)]</ref>   \u000a\u000a==Platforms & their histories==\u000aThere are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.\u000a\u000a'''Windows'''\u000a\u000aToday's Windows Search replaced WDS (Windows Desktop Search). WDS, in turn, replaced Indexing Service. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.\u000a\u000aIn 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>http://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.\u000a\u000aWith the release of Windows Vista came Windows Search 3.1. Unlike it's predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced. Cutting back indexing times immensely. This brings us to the Windows Search 4.0 which is currently running on all PCs with Windows 7 and up.\u000a\u000a'''Mac OS'''\u000a\u000aMac OS was the first to fully implement Desktop Search, it allowed users to fully search all documents with in their Macintosh computer. This means file format types, meta-data on those file formats and the content within the files. Released in 1994 two years before Windows Search was released, AppleSearch already had content searching. The biggest issue that AppleSearch had large resource requirements "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>http://infomotions.com/musings/tricks/manuscript/1600-0001.html</ref> A Macintosh computer that had these specs cost around $1400 in today's dollars that's around $2050.<ref>http://stats.areppim.com/calc/calc_usdlrxdeflator.php</ref> On top of that, the software it self cost around $1400 for a single licenses.\u000a\u000aIn 1997, Sherlock was released alongside Mac OS 8.5. Sherlock, named after the famous fictional detective Sherlock Holmes, was integrated into Mac OS's file browser: Finder. Sherlock extended the desktop search to the world wide web. Allowing users to now search locally and externally. Adding the web to Sherlock was relatively easy as the plugins only needed to be written in a plain text file. Sherlock was included in every single Mac OS 8, 9 and 10 until 10.5.\u000a\u000aSpotlight was released in 2005, on Mac OSX 10.4, is a Selection-based search which means the user invokes a query using only the mouse. It allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage. Spotlight also uses a built-in Oxford American Dictionary and calculator to offer quick access to definitions and small calculations.<ref>http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html</ref> While Spotlight had a initially long start-up time (for first time set up). The entire hard disk was indexed, and as files are added to the hard disk, the index is constantly being updated in the background. This is done using minimal CPU & RAM resources, making searching relatively easy and quick.\u000a\u000a'''Linux'''\u000a\u000aFor Linux, we will primarily cover the Ubuntu distribution as it was and currently is still the most popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using Tracker<ref>http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/</ref> desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX based systems. Tracker, was released in late 2007 was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting, and meta-data matching, but support for searching through emails and messages (instant messages) was added. Years later, in 2014 Recoll<ref>http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING</ref> was added to Linux distributions, it works with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. The wonderful thing about Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example you could search for just file types or by content.<ref>http://archive09.linux.com/feature/114283</ref>\u000a\u000a==See also==\u000a*[[List of search engines#Desktop search engines|List of desktop search engines]]\u000a\u000a== References ==\u000a<!--* [http://ims.dei.unipd.it/members/agosti/teaching/2006-07/ir/ Maristella Agosti's website]-->\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a* ''[http://www.slate.com/id/2111643/ Keeper Finders]'', by Paul Boutin, ''[[Slate (magazine)|Slate]]'', December 31, 2004 &mdash; A comparison of Google, Ask Jeeves, HotBot, MSN and Copernic desktop search tools.\u000a* [http://www.goebelgroup.com/desktopmatrix.htm GoebelGroup.com's desktop search tools comparison chart] - Date of last update: 15 January 2007.\u000a* [http://labnol.blogspot.com/2004/10/detailed-comparison-of-desktop-search.html A detailed comparison of desktop search tools] - dated 2004.\u000a* [http://www.wikinfo.org/index.php/Comparison_of_desktop_search_software Comparison of desktop search software] - Date of last update: March 2008\u000a* [http://tbox.codeplex.com/ TBox] - DevTool, with ability to do fast search by text files\u000a\u000a{{Navigationbox Desktopsearch}}\u000a\u000a{{DEFAULTSORT:Desktop Search}}\u000a[[Category:Desktop search engines| ]]\u000a[[Category:Searching]]
p93
asI59
(lp94
VSearch-based application
p95
aV'''Search-based applications''' ('''SBA''') are [[software applications]] in which a [[Search engine|search engine platform]] is used as the core infrastructure for information access and reporting. SBAs use [[Semantic technology|semantic technologies]] to aggregate, normalize and classify [[Unstructured data|unstructured]], [[Semi-structured data|semi-structured]] and/or [[Structured data|structured content]] across multiple repositories, and employ [[Natural language processing|natural language technologies]] for accessing the aggregated information.\u000a\u000a== Pre-Conditions ==\u000a\u000aSearch based applications are fully packaged applications that:<ref>Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.</ref>\u000a* Are built on a search backbone to enable sub-second access to information in multiple formats and from multiple sources\u000a* Are delivered as a unified work environment to support a specific task or workflow, for example: eDiscovery, financial services regulatory compliance, fraud detection, voice of the customer, sales prospecting, pharmaceutical research, anti-terrorism intelligence, or customer support.\u000a* Integrate all the tools that are commonly needed for that specific task or workflow, including:\u000a** Multi-source information access\u000a** Authoring\u000a** Collaboration\u000a** Business process\u000a** Reporting and analysis\u000a** Alerting\u000a** Visualization\u000a* Provide pre-configured data integration with multiple repositories of information in multiple formats as appropriate for the application domain.\u000a* Integrate domain knowledge to support the particular task, including industry taxonomies and vocabularies, internal processes, workflow for the task, connectors to specialized collections of information, and decision heuristics typical of the field.\u000a* Provide a compelling user interface and interaction design that eliminates the need for users to \u201cpogo stick\u201d or continually jump from one application to another. This buffers the user from the complexity of operating separate applications and enables them to focus on getting work done.\u000a* Are quick to deploy, easy to customize or extend, and economical to administer\u000a\u000a== Practical Uses ==\u000a\u000aSBAs are used for a variety of purposes, including:\u000a\u000a* ''' Enterprise Business Applications:''' For example, [[Customer Relationship Management]] (CRM), [[Enterprise Resource Planning]] (ERP), [[Supply Chain Management]] (SCM), Compliance & Discovery, and [[Business Intelligence]] (BI)\u000a\u000a* ''' Web Applications:''' Typically, B2B, B2C and C2B applications that [[Mashup (digital)|mash-up]] data and functionality from diverse sources (databases, Web content, user-generated content, mapping data and functions, etc.)\u000a\u000aThe use of a search platform as the core infrastructure for software applications has been enabled largely by two search engine features:  1) Scalability 2) Ad hoc access to multiple heterogeneous sources from a single point of access.\u000a\u000aSearch based applications have proven popular and effective because they provide a dynamic, scalable access infrastructure that can be integrated with other features that information workers need:  task-specific, and easy to use work environments that integrate features that are usually designed to be used as separate applications, collaborative features, domain knowledge, and security.\u000a\u000aSearch engines are not a replacement for database systems; they are a complement. They have been optimally engineered to facilitate access to information, not to record and store transactions. In addition, the mathematical and statistical processors integrated to date into search engines remain relatively simple. At present, therefore, databases still provide a more effective structure for complex analytical functions.Search applications also focus on providing quality results considering search relevancy.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==Further Reading==\u000a* Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.\u000a* Butler Group [http://www.butlergroup.com/webinarIntroduction.asp?mcr=EXA190509&scr=EXA190509 Webinar on Search Based Applications] explaining SBA and how they work\u000a* Presentation on [http://www.informationbuilders.com/support/developers/presentations/?109 Search Based Applications] by   [[Information Builders]]\u000a* IDC Executive Brief [http://www.exalead.com/software/forms/download.php?resourceid=69 "The Information Advantage: Information Access in Tomorrow's Enterprise,"] October 2009, downloadable from the [[Exalead|Exalead.com]] website. Adapted from [http://www.idc.com/getdoc.jsp?containerId=217936 Hidden Costs of Information Work: A Progress Report] and [http://www.idc.com/getdoc.jsp?containerId=219883 Worldwide Search and Discovery Software 2009\u20132013 Forecast Update and 2008 Vendor Shares] by Susan Feldman, IDC.\u000a* IDC [http://www.kmworld.com/downloads/66062/Search_Market_Map_Chart.pdf Search and Discovery Software: 2009 Market Map]\u000a* KMWorld article [http://www.kmworld.com/Articles/Editorial/Feature/Search-based-applications-support-critical-decision-making-66062.aspx Search-based applications support critical decision making]\u000a* Kellblog post [http://www.kellblog.com/2010/02/11/idcs-definiton-of-search-based-applications/ IDC's Definition of Search-Based Applications]\u000a* Steve-Kearns' [http://www.basistech.com/knowledge-center/search/2010-05-building-multilingual-search-based-applications.pdf Building Multilingual Search Based Applications] presentation at Apache Lucene EuroCon 2010 conference\u000a* Information Today article [http://newsbreaks.infotoday.com/NewsBreaks/Attivio-Upgrades-Its-Active-Intelligence-Engine-67608.asp Attivio Upgrades Its Active Intelligence Engine]\u000a* [http://lucidworks.com/blog/debugging-search-application-relevance-issues/ Debugging Search Application Relevance Issues] by Grant Ingersoll. Accessed October 22, 2014.\u000a* [http://www.mind7.fr/en/information_intelligence.html Explanatory video on SBA's and Content Analysis]\u000a\u000a== See also ==\u000a{{col-begin}}\u000a{{col-2}}\u000a* [[Agile application]]\u000a* [[Agile development]]\u000a* [[Business Intelligence 2.0]] (BI 2.0)\u000a* [[Enterprise Search]]\u000a* [[Search oriented architecture]]\u000a* [[Software as a service]]\u000a* [[Lookeen]]\u000a* [[Lucene]]\u000a* [[Exalead]]\u000a{{col-end}}\u000a\u000a<!-- Categories -->\u000a[[Category:Enterprise application integration]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]\u000a[[Category:Internet terminology]]
p96
asI60
(lp97
VEuropean Summer School in Information Retrieval
p98
aVThe '''European Summer School in Information Retrieval''' (ESSIR) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.\u000aThe aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: \u201c''The term IR identifies the activities that a person \u2013 the user \u2013 has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''\u201d<ref>Agosti, M.: \u201cInformation Access using the Guide of User Requirements\u201d. In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>\u000a\u000aIR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[Computer Science]] to [[Information Science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].\u000a\u000aESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the [[Internet]].\u000a\u000aTwo books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval''\u000a,<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): \u201cLectures on Information Retrieval\u201c. Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11\u201315, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is ''Advanced Topics in Information Retrieval''.<ref>Melucci, M., and Baeza-Yates, R. (Eds): \u201cAdvanced Topics in Information Retrieval\u201c. The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>\u000a\u000a== ESSIR Editions ==\u000aESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by Nick Belkin, [[Rutgers University]], U.S.A., and Maristella Agosti, [[University of Padua]], Italy, for an Italian audience in 1989.\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Edition\u000a! Web Site\u000a! Location\u000a! Organiser(s)\u000a|-\u000a|  9th\u000a|  [http://www.ugr.es/~essir2013/ ESSIR 2013]\u000a|  Granada, Spain\u000a|  Juan M. Fernadez-Luna and Juan F. Huete\u000a|-\u000a|  8th\u000a|  [http://essir.uni-koblenz.de/ ESSIR 2011]\u000a|  Koblenz, Germany\u000a|  Sergej Sizov and Steffen Staab\u000a|-\u000a|  7th\u000a|  [http://essir2009.dei.unipd.it/ ESSIR 2009]\u000a|  Padua, Italy\u000a|  Massimo Melucci and Ricardo Baeza-Yates\u000a|-\u000a|  6th\u000a|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]\u000a|  Glasgow, Scotland, United Kingdom\u000a|  Iadh Ounis and Keith van Rijsbergen\u000a|-\u000a|  5th\u000a|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]\u000a|  Dublin, Ireland\u000a|  Alan Smeaton\u000a|-\u000a|  4th\u000a|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]\u000a|  Aussois (Savoie), France\u000a|  Catherine Berrut and Yves Chiaramella\u000a|-\u000a|  3rd\u000a|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]\u000a|  Varenna, Italy\u000a|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi\u000a|-\u000a|  2nd\u000a|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]\u000a|  Glasgow, United Kingdom\u000a|  Keith van Rijsbergen\u000a|-\u000a|  1st\u000a|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]\u000a|  Brixen, Italy\u000a|  Maristella Agosti\u000a|}\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]\u000a* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.dei.unipd.it/ Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.unipd.it/en/index.htm University of Padua, Italy]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Summer schools]]
p99
asI61
(lp100
VMatthews correlation coefficient
p101
aVThe '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]]. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 22 [[contingency table]]\u000a\u000a: <math>|\u005ctext{MCC}| = \u005csqrt{\u005cfrac{\u005cchi^2}{n}}</math>\u000a\u000awhere ''n'' is the total number of observations.\u000a\u000aWhile there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures{{Citation needed|reason=Source needed for being 'best'|date=December 2014}}. Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\u000a\u000aThe MCC can be calculated directly from the [[confusion matrix]] using the formula:\u000a\u000a: <math>\u000a\u005ctext{MCC} = \u005cfrac{ TP \u005ctimes TN - FP \u005ctimes FN } {\u005csqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\u000a</math>\u000a\u000aIn this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\u000a\u000aThe measure was introduced in 1975 by Matthews.<ref>{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442-451|doi=10.1016/0005-2795(75)90109-9}}</ref> The original formula equal to above was:\u000a: <math>\u000a\u005ctext{N} = TN + TP + FN + FP\u000a</math>\u000a: <math>\u000a\u005ctext{S} = \u005cfrac{ TP + FN } { N }\u000a</math>\u000a: <math>\u000a\u005ctext{P} = \u005cfrac{ TP + FP } { N }\u000a</math>\u000a: <math>\u000a\u005ctext{MCC} = \u005cfrac{ TP / N - S \u005ctimes P } {\u005csqrt{ P S  ( 1 - S)  ( 1 - P ) } }\u000a</math>\u000a\u000aAs a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[markedness]] (deltap) and informedness (deltap').<ref name="Perruchet2004">{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97\u2013119 |doi=10.1016/s0911-6044(03)00059-9}}</ref><ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37\u201363 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}</ref>\u000a\u000a== Confusion Matrix ==\u000a{{main|Confusion matrix}}\u000a\u000a{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"\u000a|+ Terminology and derivations<br \u000a/>from a confusion matrix\u000a|- valign=top\u000a|\u000a; true positive (TP)\u000a:eqv. with hit\u000a; true negative (TN)\u000a:eqv. with correct rejection\u000a; false positive (FP)\u000a:eqv. with [[false alarm]], [[Type I error]]\u000a; false negative (FN)\u000a:eqv. with miss, [[Type II error]]\u000a----\u000a; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)\u000a:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]\u000a:<math>\u005cmathit{TPR} = \u005cmathit{TP} / P = \u005cmathit{TP} / (\u005cmathit{TP}+\u005cmathit{FN})</math>\u000a; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate\u000a:<math>\u005cmathit{SPC} = \u005cmathit{TN} / N = \u005cmathit{TN} / (\u005cmathit{FP} + \u005cmathit{TN}) </math>\u000a; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)\u000a:<math>\u005cmathit{PPV} = \u005cmathit{TP} / (\u005cmathit{TP} + \u005cmathit{FP})</math>\u000a; [[negative predictive value]] (NPV)\u000a:<math>\u005cmathit{NPV} = \u005cmathit{TN} / (\u005cmathit{TN} + \u005cmathit{FN})</math>\u000a; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)\u000a:<math>\u005cmathit{FPR} = \u005cmathit{FP} / N = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TN})</math>\u000a; [[false discovery rate]] (FDR)\u000a:<math>\u005cmathit{FDR} = \u005cmathit{FP} / (\u005cmathit{FP} + \u005cmathit{TP}) = 1 - \u005cmathit{PPV} </math>\u000a; Miss Rate or [[Type I and type II errors#False positive and false negative rates|False Negative Rate]] (FNR)\u000a:<math>\u005cmathit{FNR} = \u005cmathit{FN} / (\u005cmathit{FN} + \u005cmathit{TP}) </math>\u000a----\u000a; [[accuracy]] (ACC)\u000a:<math>\u005cmathit{ACC} = (\u005cmathit{TP} + \u005cmathit{TN}) / (P + N)</math>\u000a;[[F1 score]]\u000a: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]\u000a:<math>\u005cmathit{F1} = 2 \u005cmathit{TP} / (2 \u005cmathit{TP} + \u005cmathit{FP} + \u005cmathit{FN})</math>\u000a; Matthews correlation coefficient (MCC)\u000a:<math> \u005cfrac{ TP \u005ctimes TN - FP \u005ctimes FN } {\u005csqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\u000a</math>\u000a\u000a;Informedness\u000a:<math>TPR + SPC - 1</math>\u000a;Markedness\u000a:<math>PPV + NPV - 1</math>\u000a;\u000a<span style="font-size:90%;">''Source: Fawcett (2006).''<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>\u000a|}\u000a\u000aLet us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 22 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:\u000a\u000a{{DiagnosticTesting_Diagram}}\u000a\u000a== See also ==\u000a* [[Phi coefficient]]\u000a* [[F1 score]]\u000a* [[Cramr's V (statistics)|Cramr's V]], a similar measure of association between nominal variables.\u000a* [[Cohen's kappa]]\u2659\u000a\u000a== References ==\u000a\u000a{{Reflist}}\u000a\u000a=== General References ===\u000a* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" ''Bioinformatics'' 2000, 16, 412&ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]\u000a* Matthews, B.W., Comparison of the predicted and observed secondary structure of T4 phage lysozyme" ''Biochim. Biophys. Acta'' 1975, 405, 442&ndash;451\u000a* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]\u000a\u000a{{DEFAULTSORT:Matthews Correlation Coefficient}}\u000a[[Category:Machine learning]]\u000a[[Category:Information retrieval]]\u000a[[Category:Statistical classification]]\u000a[[Category:Computational chemistry]]\u000a[[Category:Cheminformatics]]\u000a[[Category:Bioinformatics]]\u000a[[Category:Statistical ratios]]\u000a[[Category:Summary statistics for contingency tables]]
p102
asI62
(lp103
VIsearch
p104
aV{{for|the adware|Isearch (malware)}}\u000a\u000a'''Isearch''' is [[open-source software|open-source]] [[text retrieval]] software first developed in 1994 by Nassib Nassar as part of the Isite [[Z39.50]] information framework. The project started at the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) of the North Carolina supercomputing center MCNC and funded by the [[National Science Foundation]] to follow in the track of [[Wide Area Information Server|WAIS]] and develop prototype systems for distributed information networks encompassing Internet applications, library catalogs and other information resources.\u000a\u000aThe main features of Isearch include full text and field searching, relevance ranking, Boolean queries, and support for many document types such as HTML, mail folders, list digests, MEDLINE, BibTeX, SGML/XML, FGDC Metadata, NASA DIF, ANZLIC metadata, ISO 19115 metadata and many other resource types and document formats.\u000a\u000aIt was the first search engine to be designed from the ground up to support [[SGML]] and ISO [[Z39.50]] search and retrieval. It included many innovations including the "document type" model\u2014which is simply a (object oriented) method of associating each document with a class of functions providing a standard interface for accessing the document. It was one of the first engines (if not the first) to ever support XML.\u000a\u000aThe Isearch search/indexing text algorithms were based on [[Gaston Gonnet]]'s seminal work into PAT arrays and trees for text retrieval--- ideas that were developed for the New Oxford English Dictionary Project at the Univ. of Waterloo, and provided the seeds for [[Tim Bray]]'s PAT SGML engine that formed the basis of [[Open Text]]. One of the limiting factors, however, of the  Isearch design was that it was not well suited to handle the extremely large data sets that became popular in the mid to late 1990s. In many cases Isearch was adapted or modified to use different algorithms but usually retained the document type model and the architectural relationship with Isite.\u000a\u000aIsearch was widely adopted and used in hundreds of public search sites, including  many high profile projects such as the [http://patft1.uspto.gov/ U.S. Patent and Trademark Office (USPTO) patent search],[http://clearinghouse3.fgdc.gov/  the Federal Geographic Data Clearinghouse (FGDC)], the NASA Global Change Master Directory, the NASA EOS Guide System, the NASA Catalog Interoperability Project, the Astronomical pre-print service based at the Space Telescope Science Institute, The PCT Electronic Gazette at the World Intellectual Property Organization (WIPO), Linsearch (a search engine for Open Source Software designed by Miles Efron), the SAGE Project of the Special Collections Department at Emory University, Eco Companion Australasia (an environmental geospatial resources catalog), Australian National Genomic Information Service (ANGIS), the [[Open Directory Project]] and numerous governmental portals in the context of the Government Information Locator Service (GILS) [[United States Government Printing Office|GPO]] mandate (ended in 2005?).\u000a\u000aFrom 1994 to 1998 most of the development was centered around the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) in North Carolina (Engine core) and BSn in Germany (Doctypes). By 1998 much of the open-source Isearch core developers re-focused development into several spin-offs. In 1998 it became part of the Advanced Search Facility reference software platform funded by the U.S. Department of Commerce.\u000a\u000aA/WWW Enterprises now maintains the open source version for public usage, supported by paying government clients, such as the U.S. Patent and Trademark Office, NASA, and the FGDC who have provided support to enhance the functionality and reliability of the software. The software suite is considered a reference implementation of catalog service software.\u000a\u000aAs of 2010, the open source version of Isearch is still used on 250+ nodes of FGDC, and by ANZLIC in Australia and selected Geospatial OneStop contributors to facilitate harvesting by GOS, including NOAA, Census Bureau and the Tenn. Field Office of the US Fish and Wildlife Service, among others.\u000a\u000a==References==\u000a*[http://www.springerlink.com/content/g5e2wfd0lekygvut/ Application of Metadata Concepts to Discovery of Internet Resources]\u000a*[http://www.springerlink.com/content/b5chmkgx8akg4m2h/ An Operational Metadata Framework for Searching, Indexing, and Retrieving Distributed Geographic Information Services on the Internet]\u000a* The UNIX Web Server Book, Second Edition, by R. Douglas Matthews et al. (Ventana Press, 1997).\u000a* [http://www.webtechniques.com/archives/1997/05/nassar/  "Searching With Isearch". May 1997, Web Techniques]\u000a* [http://www.itl.nist.gov/fipspubs/fip192.htm FIPS-192: APPLICATION PROFILE FOR THE GOVERNMENT INFORMATION LOCATOR SERVICE (GILS)]\u000a* [http://www.uneca.org/awich/AWICH%20Workshop/YaoundeWorkshop/Clearinghouse%20Yaounde.pdf Clearinghouse and Metadata Concepts, Danel Behanu, U.N. Economic Commission for Africa,  2004]\u000a* [http://web.archive.org/web/19991006225226/http://www.whitehouse.gov/OMB/memoranda/m9805.html M-98-05 Guidance on the Government Information Locator Service] published by the [[Office of Management and Budget|OMB]]\u000a* [http://www.hpcwire.com/archives/3149.html 01/1995 Press Release: Patent Office Launch Internet AIDS Patent Library]\u000a\u000a==External links==\u000a*[http://www.fgdc.gov/dataandservices/isite U.S. Federal Geographic Data Committee Isite]\u000a*[http://isite.awcubed.com/ Isite/Isearch2 Documentation Site]\u000a*[ftp://ftp.awcubed.com/pub/Software Current Isearch download site]\u000a*[http://www.etymon.com/tr.html Etymon: Isearch]\u000a*[http://www.ibu.de/node/52 BSn/NONMONOTONIC Lab: IB Search Engine], embeddable search engine. A commercial spin-off from the Isearch project.\u000a\u000a===Comparisons===\u000a* [http://www.ukoln.ac.uk/metadata/roads/product-comparison/  Product Comparison: Information Gateway Software]\u000a* [http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf  A Comparison of Open Source Search Engines, Christian Middleton, Ricardo Baeza-Yates]\u000a* [http://www.infomotions.com/musings/opensource-indexers/ Comparing Open Source Indexers]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Free search engine software]]
p105
asI191
(lp106
VVariable neighborhood search
p107
aV'''Variable neighborhood search''' (VNS),<ref>{{cite journal |pages=367\u2013407 |last1 = Hansen  |first1 = P.|last2 = Mladenovic|first2 = N.|last3 = Perez|first3 = J.A.M.|title=Variable neighbourhood search: methods and applications\u000a|volume=175 |journal= Annals of Operations Research |year=2010 |doi=10.1007/s10479-009-0657-6}}</ref> proposed by [[Mladenovi\u0107, Hansen]], 1997,<ref name=".....">{{cite journal\u000a | author = Nenad Mladenovic, Pierre Hansen\u000a | year = 1997\u000a | title = Variable neighborhood search\u000a | journal = Computers and Operations Research\u000a | volume = 24\u000a | issue= 11\u000a | pages = 1097\u20131100\u000a | doi=10.1016/s0305-0548(97)00031-2\u000a }}\u000a</ref> is a [[metaheuristic]] method for solving a set of [[combinatorial optimization (mathematics)|combinatorial optimization]] and global optimization problems.\u000aIt explores distant neighborhoods of the current incumbent solution, and moves from there to a new one if and only if an improvement was made. The local search method is applied repeatedly to get from solutions in the neighborhood to local optima.\u000aVNS was designed for approximating solutions of discrete and continuous optimization problems and according to these, it is aimed for solving [[linear programming|linear program]] problems, [[linear programming|integer program]] problems, mixed integer program problems, [[nonlinear programming|nonlinear program]] problems, etc.\u000a\u000a== Introduction ==\u000aVNS systematically changes the neighborhood in two phases: firstly, descent to find a [[local optimum]] and finally, a perturbation phase to get out of the corresponding valley.\u000a\u000aApplications are rapidly increasing in number and pertain to many fields: [[location theory]], [[cluster analysis]], [[scheduling]], [[Vehicle routing problem|vehicle routing]], [[Network planning and design|network design]], lot-sizing, [[artificial intelligence]], engineering, pooling problems, biology, [[Phylogenetics|phylogeny]], [[wikt:reliability|reliability]], geometry, telecommunication design, etc.\u000a\u000aThere are several books important for understanding VNS, such as: ''Handbook of Metaheuristics'', 2010,<ref>{{cite journal |last1=Gendreau|  first1=M.|last2= Potvin|first2=J-Y.|title=Handbook of Metaheuristics|publisher =Springer|year=2010 }}</ref> Handbook of Metaheuristics, 2003<ref>{{cite journal|last1=Glover|  first1=F.|last2= Kochenberger|first2=G.A.|title=Handbook of Metaheuristics|publisher = Kluwer Academic Publishers |year=2003}}</ref> and Search methodologies, 2005.<ref>{{cite journal |last1=Burke|first1=EK.|last2= Kendall | first2=G.| title=Search methodologies. Introductory tutorials in optimization and decision support techniques |journal = Springer|year=2005}}</ref>\u000aEarlier work that motivated this approach can be found in\u000a# Davidson, W.C.,<ref>{{cite journal |last1=Davidson  |first1=W.C.|title=Variable metric algorithm for minimization  |journal= Argonne National Laboratory Report ANL-5990 |year=1959 }}</ref>\u000a# Fletcher, R., Powell, M.J.D.,<ref>{{cite journal |pages=163\u2013168 |last1=Fletcher |first1=R. |last2=Powell |first2=M.J.D. |title=Rapidly convergent descent method for minimization|volume=6 |journal=Comput.J. |year=1963 |doi=10.1093/comjnl/6.2.163}}</ref>\u000a# Mladenovic, N.<ref>{{cite journal |pages= 112 |last1=Mladenovic |first1=N. |title=A variable neighborhood algorithm\u2014a new metaheuristic for combinatorial optimization | journal=Abstracts of papers presented at Optimization Days, Montreal |year=1995 }}\u000a</ref> and 4. Brimberg, J., Mladenovic, N.<ref>{{cite journal |pages=1\u201312 |last1=Brimberg |first1=J. |last2 = Mladenovic |first2=N. |title=A variable neighborhood algorithm for solving the continuous location-allocation problem |volume=10 |journal=Stud. Locat. Anal. |year=1996}}</ref> Recent surveys on VNS  methodology as well as numerous applications can be found in 4OR, 2008.<ref>{{cite journal |pages=319\u2013360 |last1=Hansen |first1=P. |last2 = Mladenovic |first2=N. |last3= Perez| first3=J.A.M|title=Variable neighbourhood search: methods and applications|volume=6 |journal=4OR |year=2008 |doi=10.1007/s10288-008-0089-1}}</ref> and Annals of OR, 2010.\u000a\u000a== Basic description ==\u000aDefine one deterministic [[optimization problem]] with\u000a\u000a<math> \u005cmin {\u005c{f (x)|x \u005cin X, X \u005csubseteq S\u005c}} </math>, (1)\u000a\u000awhere ''S'', ''X'', ''x'', and ''f''  are the solution space, the feasible set, a feasible solution, and a real-valued [[mathematical optimization|objective function]], respectively. If ''S'' is a finite but large set, a combinatorial optimization problem is defined. If <math>{S = R^{n}}</math>, there is continuous optimization model.\u000a\u000aA solution <math>{x^* \u005cin X}</math> is optimal if\u000a\u000a<math> {f (x^{*}) \u005cleq f (x), \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>.\u000a\u000aExact algorithm for problem (1) is to be found an optimal solution ''x*'', with the validation of its optimal structure, or if it is unrealizable, in procedure have to be shown that there is no  achievable solution, i.e., <math>X =\u005cvarnothing</math>, or the solution is unbounded. CPU time has to be finite and short. For continuous optimization, it is reasonable to allow for some degree of tolerance, i.e., to stop when a feasible solution <math>x^{*}</math> has been found such that\u000a\u000a<math> {f (x^{*}) \u005cleq f (x) + \u005cepsilon, \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math> or\u000a<math> {(f (x^{*})- f (x))/ f (x^{*})  <  \u005cepsilon  , \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>\u000a\u000aSome heuristics speedily accept an approximate solution, or optimal solution but one with no validation of its optimality.\u000aSome of them have an incorrect certificate, i.e., the solution <math>x_h</math> obtained satisfies\u000a\u000a<math> {(f (x_{h})- f (x))/ f (x_{h})  \u005cleq  \u005cepsilon  , \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>\u000afor some <math>\u005cepsilon</math>, though this is rarely small.\u000a\u000aHeuristics are faced with the problem of local optima as a result of avoiding boundless computing time.\u000aA local optimum <math>x_L</math> of problem is such that\u000a\u000a<math> {f (x_{L}) \u005cleq f (x), \u005cqquad \u005cforall{x}\u005c, \u005cin N(x_{L}) \u005ccap X} </math>\u000a\u000awhere <math> N(x_{L})</math>  denotes a neighborhood of <math> x_{L} </math>\u000a\u000a== Description ==\u000aAccording to (Mladenovic, 1995), VNS is a metaheuristic which systematically performs the procedure of neighborhood change, both in descent to local minima and in escape from the valleys which contain them.\u000a\u000aVNS is built upon the following perceptions:\u000a\u000a# A local minimum with respect to one neighbourhood structure is not necessarily a local minimum for another neighbourhood structure.\u000a# A global minimum is a local minimum with respect to all possible neighborhood structures.\u000a# For many problems, local minima with respect to one or several neighborhoods are relatively close to each other.\u000a\u000aUnlike many other metaheuristics, the basic schemes of VNS and its extensions are simple and require few, and sometimes no parameters. Therefore, in addition to providing very good solutions, often in simpler ways than other methods, VNS gives insight into the reasons for such a performance, which, in turn, can lead to more efficient and sophisticated implementations.\u000a\u000aThere are several papers where it could be studied among recently mentioned, such as (Hansen and Mladenovic 1999, 2001a, 2003, 2005; Moreno-Prez et al.;<ref>{{cite journal||last1=Moreno-Prez|first1=JA.|last2=Hansen|first2=P. |last3=Mladenovic|first3=N.| title = Parallel variable neighborhood search|journal=Alba E (ed) Parallel metaheuristics: a new class of algorithms|year=2005}}</ref>)\u000a\u000a==[[Local search (optimization)|Local search]]==\u000a\u000aA local search heuristic is performed through choosing an initial solution x, discovering a direction of descent from x, within a neighbourhood N(x), and proceeding to the minimum of f(x) within N(x) in the same direction. If there is no direction of descent, the heuristic stops; otherwise, it is iterated. Usually the highest direction of descent, also related to as best improvement, is used. This set of rules is summarized in Algorithm 1, where we assume that an initial solution x is given. The output consists of a local minimum, also denoted by x, and its value. Observe that a neighbourhood structure N(x) is defined for all x \u2208 X. At each step, the neighbourhood N(x) of x is explored completely. As this may be timeconsuming, an alternative is to use the first descent heuristic. Vectors <math>x^i \u005cin N(x)</math> are then enumerated systematically and a move is made as soon as a direction for the descent is found. This is summarized in Algorithm 2.\u000a\u000aAlgorithm 1 Best improvement (highest descent) heuristic\u000a\u000aFunction BestImprovement(x)\u000a\u000a  1: repeat\u000a  2:     x' \u2190 x\u000a  3:     x\u2190argmin_{f (y)}, y\u2208N(x)\u000a  4: until ( f (x) \u2265 f (x'))\u000a  5: return x\u000a\u000aAlgorithm 2 First improvement (first descent) heuristic\u000a\u000aFunction FirstImprovement(x)\u000a\u000a  1: repeat\u000a  2:    x' \u2190 x; i\u21900\u000a  3:    repeat\u000a  4:       i\u2190i+1\u000a  5:       x\u2190argmin{ f (x), f (x^i)}, x^i  \u2208 N(x)\u000a  6:    until ( f (x) < f (x^i) or i = |N(x)|)\u000a  7: until ( f (x) \u2265 f (x'))\u000a  8: return x\u000a\u000aLet one denote <math> \u005cmathcal{ N}_k(k=1, . . . ,k_{max}) </math>, a finite set of pre-selected neighborhood structures, and with <math>\u005cmathcal{N}_k(x)</math> the set of solutions in the ''kth'' neighborhood of ''x''.\u000a\u000aOne will also use the notation <math>\u005cmathcal{N'}_k(x), k = 1, . . . , k'_{max} </math> when describing local descent. Neighborhoods <math>\u005cmathcal{N}_k(x)</math> or <math>\u005cmathcal{N'}_k(x)</math> may be induced from one or more [[metric (mathematics)|metric]] (or quasi-metric) functions introduced into a solution space ''S''.\u000aAn optimal solution <math>x_{opt}</math> (or [[maxima and minima|global minimum]]) is a feasible solution where a minimum of problem ( is reached. We call ''x' \u2208 X'' a local minimum of problem with respect to <math>\u005cmathcal{N}_k(x) </math>, if there is no solution <math> x \u005cin \u005cmathcal{N'}_k(x) \u005csubseteq X </math> such that <math>f (x) < f (x')</math>.\u000a\u000aIn order to solve problem by using several neighbourhoods, facts 1\u20133 can be used in three different ways: (i) deterministic; (ii) [[stochastic]]; (iii) both deterministic and stochastic. We first give in Algorithm 3 the steps of the neighbourhood change function which will be used later. Function NeighbourhoodChange() compares the new value f(x') with the incumbent value f(x) obtained in the neighbourhood k (line 1). If an improvement is obtained, k is returned to its initial value and the new incumbent updated (line 2). Otherwise, the next neighbourhood is considered (line 3).\u000a\u000aAlgorithm 3&nbsp;\u2013 Neighborhood change\u000a\u000aFunction NeighborhoodChange (x, x', k)\u000a\u000a<code>\u000a 1: if f (x') < f(x) then\u000a 2:    x \u2190 x' // Make a move\u000a 3:    k \u2190 1 // Initial neighborhood\u000a 4: else\u000a 5:    k \u2190 k+1 // Next neighborhood\u000a\u000a</code>\u000a\u000aWhen VNS does not render good solution, there are several steps which could be helped in process, such as comparing first and best improvement strategies in local search, reducing neighborhood, intensifying shaking, adopting VND, adopting FSS, and experimenting with parameter settings.\u000a\u000aThe Basic VNS (BVNS) method (Mladenovic and Hansen 1997) combines deterministic and stochastic changes of neighbourhood. Its steps are given in Algorithm 4. Often successive neighbourhoods <math> \u005cmathcal{N}_k</math> will be nested. Observe that point x' is generated at random in Step 4 in order to avoid cycling, which might occur if a deterministic rule were applied. In Step 5, the first improvement local search (Algorithm 2) is usually\u000aadopted. However, it can be replaced with best improvement (Algorithm 1).\u000a\u000aAlgorithm 4: Basic VNS\u000a\u000aFunction VNS (x, kmax, tmax );\u000a\u000a<code>\u000a\u000a 1: repeat\u000a 2:    k \u2190 1;\u000a 3:    repeat\u000a 4:       x' \u2190Shake(x, k) /* Shaking */;\u000a 5:       x'' \u2190 FirstImprovement(x' ) /* Local search */;\u000a 6:       NeighbourhoodChange(x, x', k) /* Change neighbourhood */;\u000a 7:    until k = k_max ;\u000a 8:    t \u2190CpuTime()\u000a 9: until t > t_max ;\u000a\u000a</code>\u000a\u000aThe basic VNS is a first improvement [[method of steepest descent|descent method]] with randomization. Without much additional effort, it can be transformed into a descent-ascent method: in NeighbourhoodChange() function, replace also x by x" with some probability, even if the solution is worse than the incumbent. It can also be changed into a best improvement method: make a move to the best neighbourhood k* among all k_max of them.\u000aAnother variant of the basic VNS can be to find a solution x' in the \u201cShaking\u201d step as the best among b (a parameter) randomly generated solutions from the ''k''th neighbourhood. There are two possible variants of this extension: (1) to perform only one local search from the best among b points; (2) to perform all b local searches and then choose the best. In paper (Fleszar and Hindi<ref>{{cite journal|last1=Fleszar|first1=K|last2=Hindi|first2=KS|title=Solving the resource-constrained project scheduling problem by a variable neighborhood search|journal=Eur J Oper Res|year=2004|volume=155|issue=2|pages=402\u2013413|doi=10.1016/s0377-2217(02)00884-6}}</ref>) could be found algorithm.\u000a\u000a== Extensions ==\u000a* VND<ref>{{cite journal|last1=Brimberg|first1=J.|last2=Hansen|first2=P.|last3=Mladenovic|first3=N.|last4=Taillard |first4=E. |title=Improvements and comparison of heuristics for solving the multisource Weber problem|journal=Oper. Res.|year=2000|volume=48 |pages=444\u2013460 |doi=10.1287/opre.48.3.444.12431}}</ref>\u000a:The variable neighborhood descent (VND) method is obtained if a change of neighborhoods is performed in a deterministic way. In the descriptions :of its algorithms, we assume that an initial solution x is given. Most local search heuristics in their descent phase use very few :neighbourhoods. The final solution should be a local minimum with respect to all <math>k_{max}</math> neighbourhoods; hence the chances to reach :a global one are larger when using VND than with a single neighbourhood structure.\u000a* RVNS<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Petrovic|first2=J.|last3=Kovacevic-Vujcic|first3=V.|last4=Cangalovic |first4=M. |title=Solving spread spectrum radar polyphase code design problem by tabu search and variable neighborhood search|journal=Eur. J. Oper. Res.|year=2003b|volume=151 |pages=389\u2013399 |doi=10.1016/s0377-2217(02)00833-0}}</ref>\u000a\u000a:The reduced VNS (RVNS) method is obtained if random points are selected from <math>\u005cmathcal{N}_k(x)</math> and no descent is made. Rather, the :values of these new points are compared with that of the incumbent and an update takes place in case of improvement. It is assumed that a :stopping condition has been chosen like the maximum [[CPU time]] allowed <math>t_{max}</math> or the maximum number of iterations :between two improvements.\u000a:To simplify the description of the algorithms it is used <math>t_{max}</math> below. Therefore, RVNS uses two parameters: <math>t_{max}</math> :and <math>k_{max}</math>. RVNS is useful in very large instances, for which local search is costly. It has been observed that the best value for :the parameter k_max is often 2. In addition, the maximum number of iterations between two improvements is usually used as a stopping condition. :RVNS is akin to a [[Monte-Carlo method]], but is more systematic.\u000a* Skewed VNS\u000a:The skewed VNS (SVNS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Jaumard|first2=B|last3=Mladenovic|first3=N|last4=Parreira |first4=A |title=Variable neighborhood search :for weighted maximum satisfiability problem|journal=Les Cahiers du GERAD G\u20132000\u201362, HEC Montral, Canada|year=2000}}</ref> addresses the :problem of exploring valleys far from the incumbent solution. Indeed, once the best solution in a large region has been found, it is necessary to :go some way to obtain an improved one. Solutions drawn at random in distant neighbourhoods may differ substantially from the incumbent and VNS :can then degenerate, to some extent, into the Multistart heuristic (in which descents are made iteratively from solutions generated at random, a :heuristic which is known not to be very efficient). Consequently, some compensation for distance from the incumbent must be made.\u000a* Variable Neighbourhood Decomposition Search\u000a:The variable neighbourhood decomposition search (VNDS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovic|first2=N|last3=Prez-Brito|first3=D |title=Variable neighborhood decomposition :search|journal=J Heuristics|year=2001|volume=7|issue=4|pages=335\u2013350}}</ref> extends the basic VNS into a two-level VNS scheme based upon :decomposition of the problem. For ease of presentation, but without loss of generality, it is assumed that the solution x represents the set of :some elements.\u000a* Parallel VNS\u000a:Several ways of parallelizing VNS have recently been proposed for solving the p-Median problem. In Garca-Lpez et al.:<ref>{{cite journal|last1=Garca-Lpez|first1=F|last2=Melin-Batista|first2=B|last3= Moreno-Prez|first3= JA|last4= |first4=JM :|title=The parallel :variable neighborhood search for the p-median problem|journal=J Heuristics|year=2002|volume=8|issue=3|pages=375\u2013388}}</ref>&nbsp; three of them :are tested: (i) parallelize local search; (ii) augment the number of solutions drawn from the current neighbourhood and make a :local search in :parallel from each of them and (iii) do the same as (ii) but update the information about the best solution found. Three Parallel :VNS strategies :are also suggested for solving the [[Travelling purchaser problem]] in Ochi et al.<ref>{{cite journal|last1=Ochi|first1=LS|last2=Silva|first2=MB|last3= Drummond|first3= L|title=Metaheuristics based on GRASP and VNS for solving traveling purchaser :problem|journal=MIC\u20192001, Porto|year=2001|pages=489\u2013494}}</ref>\u000a* Primal-dual VNS\u000a:For most modern heuristics, the difference in value between the optimal solution and the obtained one is completely unknown. Guaranteed :performance of the primal heuristic may be determined if a [[upper and lower bounds|lower bound]] on the objective function value is known. To :this end, the standard approach is to relax the integrality condition on the primal variables, based on a mathematical programming formulation of :the problem.\u000a:However, when the dimension of the problem is large, even the relaxed problem may be impossible to solve exactly by standard :commercial solvers. :Therefore, it seems a good idea to solve dual relaxed problems heuristically as well. It was obtained guaranteed bounds on :the primal heuristics :performance.  In Primal-dual VNS (PD-VNS) (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Brimberg|first2=J|last3=Uro\u0161evic|first3=D|last4=Mladenovic|first4=N|title=Primal-dual variable neighborhood search for the simple plant location problem|journal=INFORMS J Comput|year=2007a|volume=19|issue=4|pages=552\u2013564|doi=10.1287/ijoc.1060.0196}}</ref> one :possible general way to attain both the guaranteed bounds and the exact solution is proposed.\u000a* Variable Neighborhood Branching.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Mladenovic|first2=N.|last3=Urosevic|first3=D.|title=Variable neighborhood search and local branching|journal=Computers and Operations Research|year=2006|volume=33|pages=3034\u20133045|doi=10.1016/j.cor.2005.02.033}}</ref>\u000a:The mixed integer linear programming (MILP) problem consists of maximizing or minimizing a linear function, subject to equality or inequality :constraints, and integrality restrictions on some of the variables.\u000a* Variable Neighborhood Formulation Space Search .)<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Plastria|first2=F.|author2-link=Frank Plastria|last3=Urosevic|first3=D.|title=Reformulation descent applied to circle packing problems|journal=Computers and Operations Research|year=2006|volume=32|pages=2419\u20132434|doi=10.1016/j.cor.2004.03.010}}</ref>\u000a:FSS is method which is very useful because, one problem could be defined in addition formulations and moving through formulations is legitimate. :It is proved that local search works within formulations, implying a final solution when started from some initial solution in first formulation. :Local search systematically alternates between different formulations which was investigated for [[Circle packing in a circle|circle packing]] :problem (CPP) where [[stationary point]] for a [[nonlinear programming]] formulation of CPP in [[Cartesian coordinate system|Cartesian coordinates]] is not strictly a stationary point in [[Polar coordinate system|polar coordinates]].\u000a\u000a== Development ==\u000aIn order to make a simple version of VNS, here is the list of steps which should be made. Most of it is very similar with steps in other metaheuristics.\u000a# It is necessary to be involved in problem, give some examples and try to solve them\u000a# Study books, surveys and scientific papers\u000a# Try to test some benchmarks\u000a# Choose appropriate data structure for representing in memory\u000a# Find initial solution\u000a# Calculate objective function\u000a# Design a procedure for Shaking\u000a# Choose an local search heuristic with some moves as drop, add, swap, interchange, etc.\u000a# Compare VNS with other methods from the literature\u000a\u000a== Applications ==\u000aApplications of VNS, or of varieties of VNS are very abundant and numerous. Some fields where it could be found collections of scientific papers:\u000a* Industrial applications\u000a* Design problems in communication\u000a* Location problems\u000a* [[Data mining]]\u000a* [[Graph theory|Graph problems]]\u000a* [[Knapsack problem|Knapsack]] and packing problems\u000a* Mixed integer problems\u000a* Time tabling\u000a* [[Scheduling]]\u000a* [[Vehicle routing problem]]s\u000a* [[Arc routing]] and waste collection\u000a* Fleet sheet problems\u000a* Extended vehicle routing problems\u000a* Problems in biosciences and chemistry\u000a* Continuous optimization\u000a* Other optimization problems\u000a* Discovery science\u000a\u000a== Conclusion ==\u000aVNS implies several features which are presented in Hansen and Mladenovic<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovic|first2=N|title=Variable neighborhood search|journal=Glover F, Kochenberger G (eds) Handbook\u000aof Metaheuristics|year=2003|issue=Kluwer, Dordrecht|pages=145\u2013184}}</ref> and some are presented here:\u000a\u000a(i) Simplicity: VNS is simple a simple and clear which is universally applicable;\u000a\u000a(ii) Precision: VNS is formulated in precise mathematical definitions;\u000a\u000a(iii) Coherence: all actions of the heuristics for solving problems follow from the VNS principles;\u000a\u000a(iv) Effectiveness: VNS supplies optimal or near-optimal solutions for all or at least most realistic instances;\u000a\u000a(v) Efficiency: VNS takes a moderate computing time to generate optimal or near-optimal solutions;\u000a\u000a(vi) Robustness: the functioning of the VNS is coherent over a variety of instances;\u000a\u000a(vii) User friendliness: VNS has no parameters, so it is easy for understanding, expressing and using;\u000a\u000a(viii) Innovation: VNS is generating new types of application.\u000a\u000a(ix) Generality: VNS is inducing to good results for a wide variety of\u000aproblems;\u000a\u000a(x) Interactivity: VNS allows the user to incorporate his knowledge to improve the resolution process;\u000a\u000a(xi) Multiplicity: VNS is able to produce a certain near-optimal solutions from which the user can choose;\u000a\u000aInterest in VNS is growing quickly, evidenced by the increasing number of papers published each year on this topic (10 years ago, only a few; 5 years ago, about a dozen; and about 50 in 2007).\u000aMoreover, the 18th EURO mini-conference held in Tenerife in November 2005 was entirely devoted to VNS. It led to special issues of [[Institute of Mathematics and its Applications|IMA Journal of Management Mathematics]] in 2007, European Journal of Operational Research (http://www.journals.elsevier.com/european-journal-of-operational-research/), and Journal of Heuristics (http://www.springer.com/mathematics/applications/journal/10732/) in 2008.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://toledo.mi.sanu.ac.rs/~grujicic/vnsconference EURO Mini Conference XXVIII on Variable Neighbourhood Search]\u000a\u000a[[Category:Searching]]
p108
asI194
(lp109
VLocate (Unix)
p110
aV{{lowercase}}\u000a'''<code>locate</code>''', a [[Unix]] utility first created in 1983,<ref>\u000aRef: [[Usenix]] ''';login:''', Vol 8, No 1, February/March, 1983, p. 8.\u000a</ref>\u000aserves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by '''<code>updatedb</code>''' or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than <code>[[find]]</code>, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements (particularly on very large filesystems).\u000a\u000aThe GNU version forms a part of [[GNU Findutils]].\u000a\u000aSome versions can also index network filesystems.\u000a\u000a==mlocate==\u000amlocate is a locate/updatedb implementation.\u000a\u000a[https://fedorahosted.org/mlocate/ mlocate site]\u000a\u000a==References==\u000a<references/>\u000a\u000a==External links==\u000a* {{man|1|locate|FreeBSD}}\u000a* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]\u000a\u000aVariants:\u000a* {{wayback|url=http://slocate.trakker.ca/|title=slocate (Secure Locate)|date=20090204031919}}\u000a** {{man|1|slocate|die.net}}\u000a* [http://carolina.mff.cuni.cz/~trmac/blog/mlocate/ <code>mlocate</code>] - faster updates\u000a** {{man|1|locate|die.net|mlocate}}\u000a* [http://rlocate.sourceforge.net/ rlocate] - always up-to-date\u000a* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate\u000a* [http://www.locate32.net/ Locate32 for Windows] Windows analog of GNU locate with GUI, released under GNU license\u000a\u000a{{unix commands}}\u000a\u000a[[Category:GNU Project software]]\u000a[[Category:Unix file system-related software]]\u000a[[Category:Searching]]\u000a\u000a{{Unix-stub}}
p111
asI195
(lp112
VLookeen
p113
aV{{Infobox Software\u000a| name = Lookeen\u000a| screenshot =\u000a| caption =\u000a| developer = [[Axonic Informationssysteme GmbH]]\u000a| latest_release_version = 8.3.1.5156\u000a| latest_release_date = May 21, 2013\u000a| latest_preview_version =\u000a| latest_preview_date =\u000a| operating_system = [[Microsoft Windows]]\u000a| genre = [[Search Tool|Email search]]\u000a| company_type   = Private (venture-backed)\u000a| foundation     = 2006\u000a| location       = [[Karlsruhe]], [[Germany]]\u000a| key_people     = [[Martin Welker]], CEO<br>[[Peter Oehler]], COO\u000a| industry       = Email Applications\u000a| website = [http://www.lookeen.com www.lookeen.com]\u000a}}\u000a'''Lookeen''' is a business search [[Plug-in (computing)|add-on]] for [[Microsoft Outlook]], produced under shareware license. The program uses [[Apache Software Foundation|Apache]]'s search engine [[Lucene]] and helps searching for [[Computer file|files]], [[emails]], [[contacts]], [[Email attachment|attachements]] as well as [[desktop environment|desktop]] elements on [[personal computers]] as well as in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://email.about.com/od/outlookaddons/gr/lookeen.htm ''Lookeen 2010'']. Editor Review on about.com. Retrieved on August 22, 2014.</ref>\u000a==Using==\u000aLookeen is an add-on for Microsoft Outlook. The [[shareware]] program is developed according to the Microsoft company recommendation on the add-ons design. After installation the program automatically integrates into Microsoft Outlook workspace. After the indexing process, Lookeen easily allows to search whole [[Personal Storage Table|Outlook archives]] and the [[My Documents]] folder.<ref>[http://www.pcworld.com/article/233114/lookeen.html ''Lookeen'']. Editor Review on pcworld.com. Retrieved on August 22, 2014.</ref>\u000aIn contrast to the Microsoft Outlook [[native (computing)|native]] search engine, Lookeen indexes the complete [[folder (computing)|folder]] structure. Whereas the native Outlook search only allows searches within the presently used and active folder, Lookeen searches in complete Outlook archives for needed information. \u000a\u000a===Supported mailbox storages===\u000aLookeen supports the following types of mail accounts: [[POP3]], [[IMAP]], [[HTTP]] and [[Microsoft Exchange Server]]. Both uncached and [[cache (computing)|cached]] exchange server modes are supported.\u000a===Supported filetypes===\u000aThe following filetypes can be indexed and searched for with Lookeen (in alphabetical order]: [[.bmp]], [[.doc]], [[.docx]], [[.gif]], [[.htm]], [[.html]], [[.jpeg]], [[.jpg]], [[.msg]], [[.pdf]], [[.php]], [[.png]], [[.pps]], [[.ppsx]], [[.ppt]], [[.pptx]], [[.rtf]], [[.txt]], [[.tif]], [[.tiff]], [[.xls]], [[.xlsm]], [[.xlsx]], [[.xml]]. \u000a===Central indexing===\u000aLookeen 8 supports central indexing of shared resources (e.g. network files, public exchange folders). This shared index is created once and integrated by the clients via its URL. Goal is to reduce network- and server-traffic and reduce the index storage cost for local indexes.<ref>[http://www.techmynd.com/outlook-search-tool-lookeen-licenses-giveaway/ ''Excellent Outlook Search Tool \u2013 Lookeen'']. Editor Review on Techmynd.com. Retrieved on August 22, 2014.</ref>\u000a===Enterprise Roll-Out Support===\u000aLookeen 8 supports [[Group Policies]] for advanced software distributions in companies. Many options (e.g. index location, settings location, included sources, index intervals, license keys, etc.) can be defined by the administrator. That enables enterprises to use Lookeen in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://www.itwire.com/featured-news/54892-lookeen-8-accelerates-outlook-e-mail-search ''Lookeen 8 accelerates Outlook E-Mail-Search'']. Official Press Release on ITwire.com. Retrieved on August 22, 2014.</ref>\u000a\u000a==History==\u000aStructure and Design of the first version strongly resembled the e-mail search software Lookout as developed by the [[Silicon Valley]] [[Startup company|Startup]] [[Lookout Software LCC]]. In 2004, Microsoft bought Lookout for allegedly 6 Million US-Dollars in order to integrate the search technology into its [[Windows Desktop Search]].<ref>[http://www.microsoft.com/presspass/press/2004/jul04/07-16lookoutpr.mspx ''MSN Announces Investment in Search Technology'']. Press Release on Microsoft.com. Retrieved on August 12, 2014.</ref> Lookout continued being available as [[Freeware]], but was not compatible anymore with Microsoft Outlook with the Release of [[Microsoft Windows Vista]] in January 2007. \u000aIn 2007, the German IT company [[Axonic Informationssysteme GmbH]] started working on a follow-up software for Lookout and finally released Lookeen in January 2008 as a professional solution for file and e-mail searches.<ref>[http://unternehmen.wikia.com/wiki/Axonic ''Company history of the creators of Lookeen'']. Official company registry entry on unternehmens.wikia.com. Retrieved on August 22, 2014.</ref> Within eight months, Lookeen was then sold in more than 40 countries.\u000a\u000a==Lookeen Server Enterprise Search==\u000aIn Juli 2011, a corresponding [[enterprise search]] version has been released. The [[Lookeen Server]] supports global indexing functions taking privacy and data security concerns into account by totally centralizing control options.<ref>[http://www.lookeen-server.com/en/product/overview ''Overview: Lookeen Server'']. From lookeen-server.com. Retrieved on August 20, 2014.</ref> \u000a\u000a==See also==\u000a* [[Comparison of enterprise search software]]\u000a* [[List of enterprise search vendors]]\u000a* [[List of Search Engines]]\u000a\u000a==References==\u000a<references />\u000a== External links ==\u000a* [http://www.crunchbase.com/company/lookeen CrunchBase: Lookeen Profile]\u000a* [http://www.lookeen.net Lookeen homepage]\u000a* [http://www.lookeen-server.com Lookeen Server homepage]\u000a* [http://www.axonic.net Creators of Lookeen]\u000a\u000a[[Category:Shareware]]\u000a[[Category:Software]]\u000a[[Category:Microsoft Office-related software]]\u000a[[Category:Desktop search engines]]\u000a[[Category:Searching]]
p114
asI68
(lp115
VFaceted search
p116
aV{{mergeto|Faceted classification|date=January 2015}}\u000a'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>\u000a\u000aFacets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.\u000a\u000aWithin the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].{{fact|date=May 2014}}\u000a\u000a==Development==\u000a\u000aThe [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:\u000a<blockquote>\u000aThe web search world, since its very beginning, has offered two paradigms:\u000a*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.\u000a*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. \u000aOver the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>\u000a</blockquote>\u000a\u000a==Technology==\u000a\u000aVarious search engine software supports faceted classification.\u000a\u000a* [[Apache Lucene]] and derived software:\u000a**  [[Apache Solr]]\u000a** [[Swiftype]]\u000a** [[Elasticsearch]]\u000a* A number of major vendors listed at [[Comparison of enterprise search software#Faceted_Navigation]]\u000a* [[Dieselpoint]]\u000a* [[Endeca]]\u000a* iSeek, search engine for general web and education<ref>[http://www.iseek.com iSeek]</ref>\u000a* [[SpeedTrack]]<ref>http://www.speedtrack.com/technology</ref>\u000a* XSEARCH<ref>[http://www.weitkamper.com]</ref>\u000a\u000a==Mass market use==\u000a\u000aFaceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] such as [[Swiftype]] provide software for implementing faceted search applications.\u000a\u000aOnline retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.\u000a\u000a==Libraries and information science==\u000a\u000a\u000a\u000a\u000aIn 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[http://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>\u000a\u000aModern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system.\u000a\u000aInfoHarness<ref>{{cite journal|last1=Shklar|first1=Leon|last2=Thatte|first2=Satish|last3=Marcus|first3=Howard|last4=Sheth|first4=Amit|title=The "InfoHarness" Information Integration Platform|journal=Proceedings of the Second International Conference on the World Wide Web|date=1994|url=http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.43.4042|accessdate=5 January 2015}}</ref> <ref>{{cite journal|last1=Shklar|first1=Leon|last2=Sheth|first2=Amit|last3=Kashyap|first3=Vipul|last4=Shah|first4=Kshitij|title=InfoHarness: Use of automatically generated metadata for search and retrieval of heterogeneous information|journal=Advanced Information Systems Engineering|date=20 July 2005|doi=10.1007/3-540-59498-1_248|url=http://link.springer.com/chapter/10.1007/3-540-59498-1_248|accessdate=5 January 2015|ref=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.7442}}</ref> is one of the first Web System (developed in 1994) that provided faceted search over heterogeneous information artifacts such as Web pages, images, videos and documents. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.\u000a\u000a==See also==\u000a* [[Enterprise Search]]\u000a* [[Exploratory search]]\u000a* [[Faceted classification]]\u000a* [[Human\u2013computer information retrieval]]\u000a* [[Information Extraction]]\u000a* [[NoSQL]]\u000a* [[Trove (website)]]\u000a\u000a==References==\u000a<References/>\u000a\u000a{{DEFAULTSORT:Faceted Search}}\u000a[[Category:Information retrieval]]
p117
asI54
(lp118
VList of enterprise search vendors
p119
aV== Free and open source [[enterprise search]] software ==\u000a<!--\u000a################# READ THIS\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000aAlso, read the definition of enterprise search before adding a new search engine\u000a-->\u000a*[[Apache Solr]]\u000a*[[DataparkSearch]]\u000a*[[ElasticSearch]]\u000a*[[Htdig|ht://Dig]]\u000a*[[ApexKB]]\u000a*[[mnoGoSearch]]\u000a*[[OpenSearchServer]]\u000a*[[Searchdaimon]]\u000a*[[Sphinx_(search_engine)|Sphinx]]\u000a*[[Zettair]]\u000a\u000a== Vendors of open source enterprise search software ==\u000a* [[30 Digits]] - Implementation, consulting, support, and value-add components for [[Lucene]] and [[Solr]]\u000a* [[Apache Software Foundation]] - The foundation is the entity behind the [[Lucene]] family of products\u000a* [[LucidWorks]] (former Lucid Imagination) - Commercial support, training and services for [[Lucene]] and [[Solr]]\u000a* Customermatrix (acquired Polyspot, CRM development and products for [[Lucene]]) \u000a* [[Searchblox]] - Commercial product for [[Lucene]] and [[ElasticSearch]]\u000a* [[Sematext]] - Consulting, development and products for [[Lucene]], [[Solr]], [[Nutch]], and [[Hadoop]]\u000a* [[FlaxUK|Flax]] - Architecture, development and support for [[Lucene]], [[Solr]] and [[Xapian]]\u000a\u000a== Vendors of proprietary enterprise search software ==\u000a<!--\u000a################# READ THIS\u000a\u000aPlease do not add web links or companies which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a*[[AskMeNow]]\u000a*[[Attivio]]\u000a*[[Concept Searching Limited]]\u000a*[[Content Analyst Company|Content Analyst Company LLC]]\u000a*[[Coveo]]\u000a*[[Dassault Systmes]] (acquired [[Exalead]])\u000a*[[Denodo]]\u000a*[[Dieselpoint, Inc.]]\u000a*[[dtSearch Corp.]]\u000a*[[EMC Corp.]]\u000a*[[Exorbyte GmbH]]\u000a*[[Expert System S.p.A.]]\u000a*[[Exterro, Inc.]]\u000a*[[Fabasoft Mindbreeze|Fabasoft]]\u000a*[[Funnelback]]\u000a*[[Google Search Appliance]]\u000a*[[HP]] (acquired [[Autonomy Corporation]] which in turn acquired [[Verity]] K2 and Ultraseek)\u000a*[[IBM]] (acquired [[Vivisimo]], rebranded "[[Watson (computer)|Watson]]")\u000a*[[Inbenta]]\u000a*[[inter:gator Enterprise Search]]\u000a*[[ISYS Search Software]]\u000a*[[Lookeen]]\u000a*[[Mark Logic|MarkLogic]]\u000a*[[Microsoft]] (includes [[Microsoft Search Server]], [[Fast Search & Transfer]])\u000a*[[Fabasoft Mindbreeze|Mindbreeze]] \u000a*[[Neofonie]] (includes WeFind)\u000a*[[Omniture]] (acquired by [[Adobe Systems]])\u000a*[[Open Text Corporation]]\u000a*[[Oracle Corporation]] (includes [[Oracle_Corporation#Oracle_Secure_Enterprise_Search|Secure Enterprise Search]] and [[Endeca Technologies Inc.]])\u000a*[[Q-go]]\u000a*[[Q-Sensei]]\u000a*[[Recommind (software company)|Recommind]]\u000a*[[SAP AG|SAP]] (includes SAP NetWeaver Enterprise Search, Search Services in SAP NetWeaver AS ABAP, and Search and Classification TREX)\u000a*[[Silent Eight]]\u000a*[[Sinequa]]\u000a*[[SLI_Systems]]\u000a*[[Sophia Search Limited]]\u000a* [[Swiftype]]\u000a*[[TeraText]]\u000a*[[Thunderstone Software]]\u000a*[[X1 Technologies, Inc.]]\u000a*[[ZyLAB Technologies]]\u000a*[[ZL Technologies]]\u000a\u000a== External links ==\u000a* [http://www.dmoz.org/Computers/Software/Information_Retrieval/Fulltext/ DMOZ category Information Retrieval/Fulltext]\u000a{{Companies by industry}}\u000a\u000a{{DEFAULTSORT:Enterprise search vendors}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a[[Category:Search engine software|*Enterprise search vendors]]\u000a[[Category:Lists of software]]\u000a[[Category:Lists of companies by industry|Enterprise search vendors]]
p120
asI199
(lp121
VHamming distance
p122
aV{| align="right"\u000a|-\u000a| [[Image:Hamming distance 3 bit binary.svg|thumb|140px|3-bit binary [[cube]] for finding Hamming distance]]\u000a| [[Image:Hamming distance 3 bit binary example.svg|thumb|140px|Two example distances: 100\u2192011 has distance 3 (red path); 010\u2192111 has distance 2 (blue path)]]\u000a|-\u000a|colspan=2 | [[Image:Hamming distance 4 bit binary.svg|thumb|280px|4-bit binary [[tesseract]] for finding Hamming distance]]\u000a|-\u000a|colspan=2 | [[Image:Hamming distance 4 bit binary example.svg|thumb|280px|Two example distances: 0100\u21921001 has distance 3 (red path); 0110\u21921110 has distance 1 (blue path)]]\u000a|}\u000a\u000aIn [[information theory]], the '''Hamming distance''' between two [[String (computer science)|string]]s of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of ''substitutions'' required to change one string into the other, or the minimum number of ''errors'' that could have transformed one string into the other.\u000a\u000a==Examples==\u000aThe Hamming distance between:\u000a* "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.\u000a* "'''</span>k<span style="color:#0082ff">a</span>r<span style="color:#0082ff">ol</span>in</span>'''" and "'''</span>k<span style="color:red;">e</span>r<span style="color:red;">st</span>in</span>'''" is 3.\u000a* '''10<span style="color:#0082ff">1</span>1<span style="color:#0082ff">1</span>01''' and '''10<span style="color:red;">0</span>1<span style="color:red;">0</span>01''' is 2.\u000a* '''2<span style="color:#0082ff">17</span>3<span style="color:#0082ff">8</span>96''' and '''2<span style="color:red;">23</span>3<span style="color:red;">7</span>96''' is 3.\u000a\u000a==Special properties==\u000aFor a fixed length ''n'', the Hamming distance is a [[Metric (mathematics)|metric]] on the vector space of the words of length n, as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by [[complete induction]] that it satisfies the [[triangle inequality]] as well. The Hamming distance between two words ''a'' and ''b'' can also be seen as the [[Hamming weight]] of ''a''&minus;''b'' for an appropriate choice of the &minus; operator.\u000a\u000aFor '''binary strings''' ''a'' and ''b'' the Hamming distance is equal to the number of ones ([[Hamming weight|population count]]) in ''a'' [[Exclusive or|XOR]] ''b''. The metric space of length-''n'' binary strings, with the Hamming distance, is known as the ''Hamming cube''; it is equivalent as a metric space to the set of distances between vertices in a [[hypercube graph]]. One can also view a binary string of length ''n'' as a vector in <math>R^n</math> by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an ''n''-dimensional [[hypercube]], and the Hamming distance of the strings is equivalent to the [[Manhattan distance]] between the vertices.\u000a\u000a==History and applications==\u000a\u000aThe Hamming distance is named after [[Richard Hamming]], who introduced it in his fundamental paper on [[Hamming code]]s ''Error detecting and error correcting codes'' in 1950.<ref>{{harvtxt|Hamming|1950}}.</ref> It is used in [[telecommunication]] to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the '''signal distance'''. Hamming weight analysis of bits is used in several disciplines including [[information theory]], [[coding theory]], and [[cryptography]]. However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the [[Levenshtein distance]] is more appropriate.\u000aFor ''q''-ary strings over an [[alphabet]] of size ''q''&nbsp;\u2265&nbsp;2 the Hamming distance is applied in case of orthogonal [[modulation]], while the [[Lee distance]] is used for phase modulation. If ''q''&nbsp;=&nbsp;2 or ''q''&nbsp;=&nbsp;3 both distances coincide.\u000a\u000aThe Hamming distance is also used in [[systematics]] as a measure of genetic distance.<ref name="pmid18351799">{{harvtxt|Pilcher|Wong|Pillai|2008}}.</ref>\u000a\u000aOn a grid such as a chessboard, the Hamming distance is the minimum number of moves it would take a [[Rook_(chess)|rook]] to move from one cell to the other.\u000a\u000a== Algorithm example ==\u000aThe [[Python (programming language)|Python]] function <code>hamming_distance()</code> computes the Hamming distance between\u000atwo strings (or other [[Iterator|iterable]] objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.\u000a{{-}}\u000a\u000a<syntaxhighlight lang="python">\u000adef hamming_distance(s1, s2):\u000a    """Return the Hamming distance between equal-length sequences"""\u000a    if len(s1) != len(s2):\u000a        raise ValueError("Undefined for sequences of unequal length")\u000a    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\u000a</syntaxhighlight>\u000a\u000aThe following [[C (programming language)|C]] function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the [[bitwise operation|bitwise]] [[exclusive or]] of the two inputs, and then finds the [[Hamming weight]] of the result (the number of nonzero bits) using an algorithm of {{harvtxt|Wegner|1960}} that repeatedly finds and clears the lowest-order nonzero bit.\u000a\u000a<syntaxhighlight lang="c">\u000aint hamming_distance(unsigned x, unsigned y)\u000a{\u000a    int       dist;\u000a    unsigned  val;\u000a\u000a    dist = 0;\u000a    val = x ^ y;    // XOR\u000a\u000a    // Count the number of bits set\u000a    while (val != 0)\u000a    {\u000a        // A bit is set, so increment the count and clear the bit\u000a        dist++;\u000a        val &= val - 1;\u000a    }\u000a\u000a    // Return the number of differing bits\u000a    return dist;\u000a}\u000a</syntaxhighlight>\u000a\u000a==See also==\u000a{{Portal|Mathematics}}\u000a* [[Closest string]]\u000a* [[Damerau\u2013Levenshtein distance]]\u000a* [[Euclidean distance]]\u000a* [[Mahalanobis distance]]\u000a* [[Jaccard index]]\u000a* [[String metric]]\u000a* [[Srensen similarity index]]\u000a* [[Word ladder]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a==References==\u000a*{{FS1037C}}\u000a*{{citation\u000a | last = Hamming | first = Richard W. | author-link = Richard W. Hamming\u000a | mr = 0035935\u000a | issue = 2\u000a | journal = [[Bell System Technical Journal]]\u000a | pages = 147\u2013160\u000a | title = Error detecting and error correcting codes\u000a | url = http://wayback.archive.org/web/20060525060427/http://www.caip.rutgers.edu/~bushnell/dsdwebsite/hamming.pdf\u000a | volume = 29\u000a | year = 1950\u000a | doi=10.1002/j.1538-7305.1950.tb00463.x}}.\u000a*{{citation\u000a | last1 = Pilcher | first1 = C. D.\u000a | last2 = Wong | first2 = J. K.\u000a | last3 = Pillai | first3 = S. K.\u000a | date = March 2008\u000a | doi = 10.1371/journal.pmed.0050069\u000a | issue = 3\u000a | journal = PLoS Med.\u000a | page = e69\u000a | pmid = 18351799\u000a | title = Inferring HIV transmission dynamics from phylogenetic sequence relationships\u000a | volume = 5\u000a | pmc = 2267810}}.\u000a*{{citation\u000a | last = Wegner | first = Peter | author-link = Peter Wegner\u000a | doi = 10.1145/367236.367286\u000a | issue = 5\u000a | journal = [[Communications of the ACM]]\u000a | page = 322\u000a | title = A technique for counting ones in a binary computer\u000a | volume = 3\u000a | year = 1960}}.\u000a\u000a[[Category:String similarity measures]]\u000a[[Category:Coding theory]]\u000a[[Category:Articles with example Python code]]\u000a[[Category:Articles with example C++ code]]\u000a[[Category:Metric geometry]]\u000a[[Category:Cubes]]
p123
asI72
(lp124
VSearch engine (computing)
p125
aV{{more footnotes|date=August 2014}}\u000a{{one source|date=August 2014}}\u000aA '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}\u000a\u000aThe most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].\u000a\u000a==How search engines work==\u000aSearch engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].\u000a\u000a[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]\u000a\u000aThe list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive_or|XOR]]) in a probabilistic context.\u000a\u000aTo provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[serp|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.\u000a\u000aOther types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.\u000a\u000a==See also==\u000a{{Portal|Computer Science}}\u000a{{div col|colwidth=30em}}\u000a*[[Automatic summarization]]\u000a*[[Bibliographic database]]\u000a*[[Desktop search]]\u000a*[[Emanuel Goldberg]] (inventor of early search engine)\u000a*[[Enterprise search]]\u000a*[[Federated search]]\u000a*[[Full text search]]\u000a*[[Human search engine]]\u000a*[[Image search]]\u000a*[[Index (search engine)]]\u000a*[[Inverted index]]\u000a*[[List of search engines]]\u000a*[[List of enterprise search vendors]]\u000a*[[Medical literature retrieval]]\u000a*[[Metasearch engine]]\u000a*[[Search engine optimization]]\u000a*[[Search suggest drop-down list]]\u000a*[[Selection-based search]]\u000a*[[Semantic search]]\u000a* [[Solver (computer science)]]\u000a*[[Spamdexing]]\u000a*[[SQL]]\u000a*[[Text mining]]\u000a*[[Vertical search]]\u000a*[[Video search engine]]\u000a*[[Web search engine]]\u000a{{div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Search Engine (Computing)}}\u000a[[Category:Information retrieval]]\u000a[[Category:Data search engines| Search engine]]
p126
asI201
(lp127
VLee distance
p128
aV{{No footnotes|date=July 2011}}\u000aIn [[coding theory]], the '''Lee distance''' is a [[distance]] between two [[String (computer science)|string]]s <math>x_1 x_2 \u005cdots x_n</math> and <math>y_1 y_2 \u005cdots y_n</math> of equal length ''n'' over the ''q''-ary [[alphabet]] {0,&nbsp;1,&nbsp;\u2026,&nbsp;''q''&nbsp;&minus;&nbsp;1} of size ''q''&nbsp;\u2265&nbsp;2.\u000aIt is a [[Metric (mathematics)|metric]], defined as\u000a\u000a: <math>\u005csum_{i=1}^n \u005cmin(|x_i-y_i|,q-|x_i-y_i|).</math>\u000a\u000aIf ''q''&nbsp;=&nbsp;2 the Lee distance coincides with the [[Hamming distance]].\u000a\u000aThe [[metric space]] induced by the Lee distance is a discrete analog of the [[Elliptic geometry|elliptic space]].\u000a\u000a==Example==\u000aIf ''q''&nbsp;=&nbsp;6, then the Lee distance between 3140 and 2543 is 1&nbsp;+&nbsp;2&nbsp;+&nbsp;0&nbsp;+&nbsp;3&nbsp;=&nbsp;6.\u000a\u000a==History and application==\u000aThe Lee distance is named after [[C. Y. Lee (mathematician)|C. Y. Lee]]. It is applied for phase [[modulation]] while the Hamming distance is used in case of orthogonal modulation.\u000a\u000a==References==\u000a* {{Citation|first=C. Y.|last=Lee|title=Some properties of nonbinary [[error-correcting codes]]|journal=[[IEEE Transactions on Information Theory|IRE Transactions on Information Theory]]|volume=4|year=1958|pages=77\u201382|issue=2|doi=10.1109/TIT.1958.1057446}}.\u000a* {{Citation|first=E. R.|last=Berlekamp|authorlink=Elwyn Berlekamp|title=Algebraic Coding Theory|publisher=McGraw-Hill|year=1968}}.\u000a* {{Citation|last1=Deza|first1=E.|first2=M.|last2=Deza|author2-link=Michel Deza|title=Dictionary of Distances|year=2006|publisher=Elsevier|isbn=0-444-52087-2}}.\u000a\u000a[[Category:Coding theory]]\u000a[[Category:String similarity measures]]
p129
asI75
(lp130
VLearning to rank
p131
aV'''Learning to rank'''<ref name="liu">{{citation\u000a|author=Tie-Yan Liu\u000a|title=Learning to Rank for Information Retrieval\u000a|series=Foundations and Trends in Information Retrieval: Vol. 3: No 3\u000a|year=2009\u000a|isbn=978-1-60198-244-5\u000a|doi=10.1561/1500000016\u000a|pages=225\u2013331\u000a|journal=Foundations and Trends in Information Retrieval\u000a|volume=3\u000a|issue=3\u000a}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]\u000a</ref> or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The\u000aMIT Press ISBN 9780262018258.</ref> Training data consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.\u000a\u000aLearning to rank is a relatively new research area which has emerged in the past decade.\u000a\u000a== Applications ==\u000a\u000a=== In information retrieval ===\u000a[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]\u000aRanking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], [[computational advertising]] (online ad placement).\u000a\u000aA possible architecture of a machine-learned search engine is shown in the figure to the right.\u000a\u000aTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),\u000a<!-- "assessor" is the more standard term, used e.g. by TREC conference -->\u000awho check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used \u2014 only the top few documents, retrieved by some existing ranking models are checked. <!--\u000a  TODO: write something about selection bias caused by pooling\u000a--> Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation\u000a | author=Joachims, T.\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf\u000a | title=Optimizing Search Engines using Clickthrough Data\u000a | year=2002\u000a}}</ref> ''query chains'',<ref>{{citation\u000a | author=Joachims T., Radlinski F.\u000a | title=Query Chains: Learning to Rank from Implicit Feedback\u000a | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf\u000a | year=2005\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a}}</ref> or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].\u000a\u000aTraining data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.\u000a\u000aTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation\u000a | author=B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt.\u000a | title=Early exit optimizations for additive machine learned ranking systems\u000a | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. (to appear)\u000a | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf\u000a}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation\u000a | author=Broder A., Carmel D., Herscovici M., Soffer A., Zien J.\u000a | title=Efficient query evaluation using a two-level retrieval process\u000a | journal=Proceedings of the twelfth international conference on Information and knowledge management\u000a | year=2003\u000a | pages=426\u2013434\u000a | isbn=1-58113-723-0\u000a | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf\u000a }}</ref> [[Okapi BM25|BM25]]. This phase is called ''top-<math>k</math> document retrieval'' and many good heuristics were proposed in the literature to accelerate it, such as using document's static quality score and tiered indexes.<ref name="manning-q-eval">{{citation\u000a | author=Manning C.,  Raghavan P. and Schtze H.\u000a | title=Introduction to Information Retrieval\u000a | publisher=Cambridge University Press\u000a | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\u000a\u000a=== In other areas ===\u000aLearning to rank algorithms have been applied in areas other than information retrieval:\u000a* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation\u000a | author=Kevin K. Duh\u000a | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data\u000a | year=2009\u000a | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf\u000a}}</ref>\u000a* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />\u000a* In [[proteomics]] for the identification of frequent top scoring peptides.<ref name="Hen09">{{citation\u000a | author=Henneges C., Hinselmann G., Jung S., Madlung J., Schtz W., Nordheim A., Zell A.\u000a | title=Ranking Methods for the Prediction of Frequent Top Scoring Peptides from Proteomics Data\u000a | year=2009\u000a | url=http://www.omicsonline.com/ArchiveJPB/2009/May/01/JPB2.226.pdf\u000a}}</ref>\u000a* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.</ref>\u000a\u000a== Feature vectors ==\u000aFor convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such approach is sometimes called ''bag of features'' and is analogous to [[bag of words]] and [[vector space model]] used in information retrieval for representation of documents.\u000a\u000aComponents of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):\u000a* ''Query-independent'' or ''static'' features \u2014 those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>\u000a{{cite conference\u000a | first=M. |last=Richardson\u000a | coauthors=Prakash, A. and Brill, E.\u000a | title=Beyond PageRank: Machine Learning for Static Ranking\u000a | booktitle=Proceedings of the 15th International World Wide Web Conference\u000a | pages=707\u2013715\u000a | publisher=\u000a | year=2006\u000a | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf\u000a | accessdate=\u000a }}</ref>\u000a* ''Query-dependent'' or ''dynamic'' features \u2014 those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.\u000a* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''\u000a\u000aSome examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>\u000a* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;\u000a* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;\u000a* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.\u000a\u000aSelecting and designing good features is an important area in machine learning, which is called [[feature engineering]].\u000a\u000a== Evaluation measures ==\u000aThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\u000a\u000aExamples of ranking quality measures:\u000a* [[Mean average precision]] (MAP);\u000a* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];\u000a* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;\u000a* [[Mean reciprocal rank]];\u000a* [[Kendall's tau]]\u000a* [[Spearman's rank correlation coefficient|Spearman's Rho]]\u000a\u000aDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.\u000a\u000aRecently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:\u000a* [[Expected reciprocal rank]] (ERR);<ref>{{citation\u000a|author=Olivier Chapelle, Donald Metzler, Ya Zhang, Pierre Grinspan\u000a|title=Expected Reciprocal Rank for Graded Relevance\u000a|url=http://research.yahoo.com/files/err.pdf\u000a|journal=CIKM\u000a|year=2009\u000a|pages=\u000a}}</ref>\u000a* [[Yandex]]'s pfound.<ref>{{citation\u000a|author=Gulin A., Karpovich P., Raskovalov D., Segalovich I.\u000a|title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods\u000a|url=http://romip.ru/romip2009/15_yandex.pdf\u000a|journal=Proceedings of ROMIP'2009\u000a|year=2009\u000a|pages=163\u2013168\u000a}} (in Russian)</ref>\u000aBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\u000a\u000a== Approaches ==\u000a{{Expand section|date=December 2009}}\u000aTie-Yan Liu of [[Microsoft Research Asia]] in his paper "Learning to Rank for Information Retrieval"<ref name="liu" /> and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and [[loss function]]:\u000a\u000a=== Pointwise approach ===\u000aIn this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem \u2014 given a single query-document pair, predict its score.\u000a\u000aA number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.\u000a\u000a=== Pairwise approach ===\u000aIn this case learning-to-rank problem is approximated by a classification problem \u2014 learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.\u000a\u000a=== Listwise approach ===\u000aThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.\u000a\u000a=== List of methods ===\u000aA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\u000a:{|class="wikitable sortable"\u000a! Year || Name || Type || Notes\u000a|-\u000a| 1989 || OPRF <ref name="Fuhr1989">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=ACM Transactions on Information Systems\u000a | title=Optimum polynomial retrieval functions based on the probability ranking principle\u000a | volume=7\u000a | number=3\u000a | pages=183\u2013204 \u000a | year=1989\u000a | doi=10.1145/65943.65944\u000a}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)\u000a|-\u000a| 1992 || SLR <ref name="Cooperetal1992">{{citation\u000a | author=Cooper, William S.; Gey, Frederic C.; Dabney, Daniel P.\u000a | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval \u000a | title=Probabilistic retrieval based on staged logistic regression\u000a | pages=198\u2013210 \u000a | year=1992\u000a | doi=10.1145/133160.133199\u000a}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression\u000a|-\u000a| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.\u000a|-\u000a| 2002 || Pranking<ref>{{cite paper | id = {{citeseerx|10.1.1.20.378}} | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.\u000a|-\u000a| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || <span style="display:none">3</span> pairwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.\u000a|-\u000a| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || \u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||\u000a|-\u000a| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || RankGP<ref>{{cite paper | id = {{citeseerx|10.1.1.90.220}} | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||\u000aRegularized least-squares based ranking. The work is extended in\u000a<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio|coauthors=Tsivtsivadze, Evgeni, Airola, Antti, Jrvinen, Jouni, Boberg, Jorma|title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129\u2013165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.\u000a|-\u000a| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || <span style="display:none">3</span> listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.\u000a|-\u000a| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)\u000a|-\u000a| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. \u000a|-\u000a| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.\u000a|-\u000a| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.\u000a|-\u000a| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2010 || [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.\u000a|-\u000a| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.\u000a|-\u000a| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || \u000a|-\u000a| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.\u000a|}\u000a\u000aNote: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\u000a\u000a== History ==\u000a[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=Computer Journal\u000a | title=Probabilistic Models in Information Retrieval\u000a | volume=35\u000a | number=3\u000a | pages=243\u2013255\u000a | year=1992\u000a | doi=10.1093/comjnl/35.3.243\u000a}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author=Manning C.,  Raghavan P. and Schtze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\u000a\u000aSeveral conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\u000a\u000a=== Practical usage by search engines ===\u000aCommercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>\u000a\u000a[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.\u000a\u000aIn November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary MatrixNet algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]</ref> based on their own search engine's production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>\u000a\u000aAs of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web\u000a  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html\u000a  | archiveurl = http://www.webcitation.org/5sq8irWNM\u000a  | archivedate = 2010-09-18\u000a  | title = Are Machine-Learned Models Prone to Catastrophic Errors?\u000a  | date = 2008-05-24\u000a  | last = Rajaraman\u000a  | first = Anand\u000a  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web\u000a  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing\u000a  | archiveurl = http://www.webcitation.org/5sq7DX3Pj\u000a  | archivedate = 2010-09-15\u000a  | title = Cuil Blog: So how is Bing doing?\u000a  | date = 2009-06-26\u000a  | last = Costello\u000a  | first = Tom}}</ref>\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a; Competitions and public datasets\u000a* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]\u000a* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]\u000a* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]\u000a* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]\u000a\u000a; Open Source code\u000a* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]\u000a* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]\u000a* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]\u000a[[Category:Ranking functions]]
p132
asI204
(lp133
VJaccard index
p134
aVThe '''Jaccard index''', also known as the '''Jaccard similarity coefficient''' (originally coined ''coefficient de communaut'' by [[Paul Jaccard]]), is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the size of the [[Union (set theory)|union]] of the sample sets:\u000a\u000a:<math> J(A,B) = {{|A \u005ccap B|}\u005cover{|A \u005ccup B|}}.</math>\u000a\u000a(If ''A'' and ''B'' are both empty, we define ''J''(''A'',''B'')&nbsp;=&nbsp;1.) Clearly, \u000a:<math> 0\u005cle J(A,B)\u005cle 1.</math>\u000a\u000aThe [[MinHash]] min-wise independent permutations [[locality sensitive hashing]] scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a [[hash function]].\u000a\u000aThe '''Jaccard distance''', which measures ''dis''similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:\u000a\u000a:<math> d_J(A,B) = 1 - J(A,B) = { { |A \u005ccup B| - |A \u005ccap B| } \u005cover |A \u005ccup B| }.</math>\u000a\u000aAn alternate interpretation of the Jaccard distance is as the ratio of the size of the [[symmetric difference]] <math>A \u005ctriangle B = (A \u005ccup B) - (A \u005ccap B)</math> to the union. \u000a\u000aThis distance is a [[Distance function|metric]] on the collection of all finite sets.<ref name="lipkus">{{citation |last=Lipkus |first=Alan H\u000a|title=A proof of the triangle inequality for the Tanimoto distance\u000a|journal=J Math Chem |volume=26 |number=1-3 |year=1999 |pages=263\u2013265 }}</ref><ref>{{citation |last1=Levandowsky |first1=Michael |last2=Winter |first2=David |title=Distance between sets|journal=Nature |volume=234 |number=5 |year=1971 |pages=34\u201335 |doi=10.1038/234034a0}}</ref>\u000a\u000aThere is also a version of the Jaccard distance for [[measure (mathematics)|measures]], including [[probability measure]]s. If <math>\u005cmu</math> is a measure on a [[measurable space]] <math>X</math>, then we define the Jaccard coefficient by <math>J_\u005cmu(A,B) = {{\u005cmu(A \u005ccap B)} \u005cover {\u005cmu(A \u005ccup B)}}</math>, and the Jaccard distance by <math>d_\u005cmu(A,B) = 1 - J_\u005cmu(A,B) = {{\u005cmu(A \u005ctriangle B)} \u005cover {\u005cmu(A \u005ccup B)}}</math>. Care must be taken if <math>\u005cmu(A \u005ccup B) = 0</math> or <math>\u005cinfty</math>, since these formulas are not well defined in that case.\u000a\u000a== Similarity of asymmetric binary attributes ==\u000aGiven two objects, ''A'' and ''B'', each with ''n'' [[binary numeral system|binary]] attributes, the Jaccard coefficient is a useful measure of the overlap that ''A'' and ''B'' share with their attributes.  Each attribute of ''A'' and ''B'' can either be 0 or 1.  The total number of each combination of attributes for both ''A'' and ''B'' are specified as follows:\u000a:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.\u000a:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.\u000a:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.\u000a:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.\u000aEach attribute must fall into one of these four categories, meaning that\u000a:<math>M_{11} + M_{01} + M_{10} + M_{00} = n.</math>\u000a\u000aThe Jaccard similarity coefficient, ''J'', is given as\u000a:<math>J = {M_{11} \u005cover M_{01} + M_{10} + M_{11}}.</math>\u000a\u000aThe Jaccard distance, ''d''<sub>''J''</sub>, is given as\u000a:<math>d_J = {M_{01} + M_{10} \u005cover M_{01} + M_{10} + M_{11}}.</math>\u000a\u000a== Generalized Jaccard similarity and distance ==\u000a\u000aIf <math>\u005cmathbf{x} = (x_1, x_2, \u005cldots, x_n)</math> and <math>\u005cmathbf{y} = (y_1, y_2, \u005cldots, y_n)</math> are two vectors with all real <math>x_i, y_i \u005cgeq 0</math>, then their Jaccard similarity coefficient is defined as\u000a:<math>J(\u005cmathbf{x}, \u005cmathbf{y}) = \u005cfrac{\u005csum_i \u005cmin(x_i, y_i)}{\u005csum_i \u005cmax(x_i, y_i)},</math>\u000aand Jaccard distance\u000a:<math>d_J(\u005cmathbf{x}, \u005cmathbf{y}) = 1 - J(\u005cmathbf{x}, \u005cmathbf{y}).</math>\u000a\u000aWith even more generality, if <math>f</math> and <math>g</math> are two non-negative measurable functions on a measurable space <math>X</math> with measure <math>\u005cmu</math>, then we can define\u000a:<math>J(f, g) = \u005cfrac{\u005cint\u005cmin(f, g) d\u005cmu}{\u005cint \u005cmax(f, g)  d\u005cmu},</math>\u000awhere <math>\u005cmax</math> and <math>\u005cmin</math> are pointwise operators. Then Jaccard distance is\u000a:<math>d_J(f, g) = 1 - J(f, g).</math>\u000a\u000aThen, for example, for two measurable sets <math>A, B \u005csubseteq X</math>, we have <math>J_\u005cmu(A,B) = J(\u005cchi_A, \u005cchi_B),</math> where <math>\u005cchi_A</math> and <math>\u005cchi_B</math> are the characteristic functions of the corresponding set.\u000a\u000a== Tanimoto similarity and distance ==\u000a\u000a<!-- [[Tanimoto score]] redirects here, please change that redirect if you change this section title -->\u000a\u000aVarious forms of functions described as  Tanimoto similarity  and Tanimoto distance occur  in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources<ref>For example {{cite book |first=Huihuan |last=Qian |first2=Xinyu |last2=Wu |first3=Yangsheng |last3=Xu |title=Intelligent Surveillance Systems |publisher=Springer |year=2011 |page=161 |isbn=978-94-007-1137-2 }}</ref> cite an  unavailable IBM Technical Report<ref>{{cite journal |last=Tanimoto |first=T. |title=An Elementary Mathematical theory of Classification and Prediction |journal=Internal IBM Technical Report |date=17 Nov 1957 |issue=8? |volume=1957 }}</ref> as the seminal reference.\u000a\u000aIn "A Computer Program for Classifying Plants", published in October 1960,<ref>{{cite journal |first=David J. |last=Rogers |first2=Taffee T. |last2=Tanimoto |title=A Computer Program for Classifying Plants |journal=[[Science (journal)|Science]] |volume=132 |issue=3434 |pages=1115\u20131118 |year=1960 |doi=10.1126/science.132.3434.1115 }}</ref> a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is  the most authoritative  source for the meaning of the terms "Tanimoto similarity" and "Tanimoto Distance". The similarity ratio is equivalent to Jaccard similarity, but the distance function is ''not'' the same as Jaccard distance.\u000a\u000a=== Tanimoto's definitions of similarity and distance ===\u000a\u000aIn that paper, a "similarity ratio" is  given over [[Bit array|bitmaps]], where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (i.e. nonzero) in either sample.\u000a\u000aPresented in mathematical terms, if samples ''X'' and ''Y'' are bitmaps, <math>X_i</math> is the ''i''th bit of ''X'', and <math> \u005cland , \u005clor </math> are [[bitwise operation|bitwise]] ''[[logical conjunction|and]]'', ''[[logical disjunction|or]]'' operators respectively, then the similarity ratio <math>T_s</math> is\u000a\u000a: <math> T_s(X,Y) =  \u005cfrac{\u005csum_i ( X_i \u005cland Y_i)}{\u005csum_i ( X_i \u005clor Y_i)}</math>\u000a\u000aIf each sample is modelled instead as a set of attributes, this value is  equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.\u000a\u000aTanimoto goes on to define a "distance coefficient" based on this ratio, defined for bitmaps with non-zero similarity:\u000a\u000a: <math>T_d(X,Y) = -\u005clog_2 ( T_s(X,Y) ) </math>\u000a\u000aThis coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is  easy to construct an example which disproves the property of [[Triangle inequality#Metric space|triangle inequality]].\u000a\u000a=== Other definitions of Tanimoto distance ===\u000a\u000aTanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance <math> 1 - T_s</math>. This function is a proper distance metric. "Tanimoto Distance" is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.\u000a\u000aIf Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as\u000a\u000a: <math>\u000af(A,B) =\u005cfrac{ A \u005ccdot B}{\u005cvert A\u005cvert^2 +\u005cvert B\u005cvert^2 -  A \u005ccdot B }\u000a</math>\u000a\u000awhere the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then <math>A \u005ccdot B = \u005csum_i A_iB_i = \u005csum_i ( A_i \u005cland B_i)</math> and <math>{\u005cvert A\u005cvert}^2 = \u005csum_i A_i^2 = \u005csum_i A_i </math>.\u000a\u000aThis is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of <math> T_s </math> do not necessarily extend to <math>f</math>. In particular, the difference function <math>1 - f</math> does not preserve [[triangle inequality]], and is not therefore a proper distance metric, whereas <math>1 - T_s </math> is.\u000a\u000aThere is a real danger that the combination of "Tanimoto Distance" being defined using this formula, along with the statement "Tanimoto Distance is a proper distance metric" will lead to the false conclusion that the function <math>1 - f</math> is in fact a distance metric over vectors or multisets in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.\u000a\u000aLipkus<ref name="lipkus" /> uses a definition of Tanimoto similarity which is equivalent to <math>f</math>, and refers to Tanimoto distance as the function <math> 1 - f</math>. It is however made clear within the paper that the context is restricted by the use of a (positive) weighting vector <math>W</math> such that, for any vector ''A'' being considered, <math> A_i \u005cin \u005c{0,W_i\u005c} </math>. Under these circumstances, the  function  is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a metric space under this function.\u000a\u000a== See also ==\u000a* [[Srensen similarity index]]\u000a* [[simple matching coefficient]]\u000a* [[Mountford's index of similarity]]\u000a* [[Most frequent k characters]]\u000a* [[Hamming distance]]\u000a* [[Dice's coefficient]], which is equivalent: <math>J=D/(2-D)</math> and <math>D=2J/(1+J)</math>\u000a* [[Tversky index]]\u000a* [[Correlation]]\u000a* [[Mutual information]], a normalized [[Mutual information#Metric|metricated]] variant of which is an entropic Jaccard distance.\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a{{More footnotes|date=March 2011}}\u000a\u000a== References ==\u000a*{{citation|first1=Pang-Ning|last1=Tan|first2=Michael|last2=Steinbach|first3=Vipin|last3=Kumar|title=Introduction to Data Mining|year=2005|isbn=0-321-32136-7}}.\u000a*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1901|title=tude comparative de la distribution florale dans une portion des Alpes et des Jura|journal=Bulletin de la Socit Vaudoise des Sciences Naturelles|volume=37|pages=547\u2013579}}.\u000a*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1912|title=The distribution of the flora in the alpine zone|journal=New Phytologist|volume=11|pages=37\u201350|doi=10.1111/j.1469-8137.1912.tb05611.x}}.\u000a\u000a== External links ==\u000a* [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap2_data.pdf Introduction to Data Mining lecture notes from Tan, Steinbach, Kumar]\u000a* [http://sourceforge.net/projects/simmetrics/ SimMetrics a sourceforge implementation of Jaccard index and many other similarity metrics]\u000a* [http://www.idea-miner.de/cgi-bin/INT_Tools/ver_vergleich_0_1/cmp_menu2.cgi Web based tool for comparing texts using Jaccard coefficient]\u000a* [http://www.gettingcirrius.com/2011/01/calculating-similarity-part-2-jaccard.html Tutorial on how to calculate different similarities]\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/JaccardMetric.scala Jaccard] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a{{DEFAULTSORT:Jaccard Index}}\u000a[[Category:Index numbers]]\u000a[[Category:Measure theory]]\u000a[[Category:Clustering criteria]]\u000a[[Category:String similarity measures]]
p135
asI34
(lp136
VInformation Retrieval Facility
p137
aV{{Advert|date=May 2012}}\u000a\u000a[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]\u000a\u000aThe '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.\u000a\u000aThe IRF had members in the following categories:\u000a\u000a* Researchers in [[information retrieval]] (IR) or related scientific areas\u000a* Industrial/corporate information management professionals\u000a* Patent authorities and governmental institutions\u000a* Students of one of the above\u000a\u000a==The Scientific Board==\u000a'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]\u000a\u000a'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],\u000aDirector of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]\u000a\u000a'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]\u000a\u000a'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]\u000a\u000a'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]\u000a\u000a'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]\u000a\u000a'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]\u000a\u000a'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]\u000a\u000a'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]\u000a\u000a'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]\u000a\u000a'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]\u000a\u000a'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]\u000a\u000a'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]\u000a\u000a'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',\u000a[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]\u000a\u000a==Scientific Goals==\u000a\u000a* Modelling innovative and specialised information retrieval systems for global patent document collections.\u000a* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.<\u000a* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.\u000a* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.\u000a* Ability to create different views of patent data depending on the focus of the information need.\u000a* Defining standardised methods for benchmarking the information retrieval process in patent document collections.\u000a* Ability to handle text and non-text parts of a patent in a coherent manner.\u000a* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.\u000a* Integrating the temporal dimension of patent documents in retrieval strategies.\u000a* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.\u000a* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.\u000a* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.\u000a* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.\u000a* Investigating and experimenting with computing architectures for very high-capacity information management.\u000a* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.\u000a* Discovering and investigating novel use cases and business applications deriving from intellectual property information.\u000a* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.\u000a* Development and integration of different information access methods.\u000a* Research on effective methods for interactive information retrieval.\u000a\u000a==Semantic Supercomputing==\u000aCurrent technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:\u000a\u000a* multi-node clusters (currently 80 cores, up to 1024)\u000a* highest speed interconnect technology\u000a* single system image with large compound memory (currently 320 GB, up to 4 TB)\u000a* fully integrated configurable computing (currently 4 FPGA cores, up to 256)\u000a\u000aThe combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.\u000a\u000a==The World Patent Corpus==\u000aThe IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. \u000aThe entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What\u2019s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.\u000a\u000a== Research Collections ==\u000aThe IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.\u000a\u000aThe MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.\u000a\u000aThe ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.\u000a\u000a==External links==\u000a* [http://www.ir-facility.org/ Official site: ir-facility.org]\u000a* [http://youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] \u000a* [http://youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]\u000a\u000a==References==\u000a* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]\u000a* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]\u000a\u000a[[Category:Organizations established in 2006]]\u000a[[Category:Computer science organizations]]\u000a[[Category:Information retrieval]]\u000a[[Category:Education in Vienna]]
p138
asI206
(lp139
VEdit distance
p140
aVIn [[computer science]], '''edit distance''' is a way of quantifying how dissimilar two [[String (computing)|strings]] (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in [[natural language processing]], where automatic [[Spell checker|spelling correction]] can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In [[bioinformatics]], it can be used to quantify the similarity of [[macromolecule]]s such as [[DNA]], which can be viewed as strings of the letters A, C, G and T.\u000a\u000aSeveral definitions of edit distance exist, using different sets of string operations. One of the most common variants is called [[Levenshtein distance]], named after the Soviet Russian computer scientist [[Vladimir Levenshtein]]. In this version, the allowed operations are the removal or insertion of a single character, or the substitution of one character for another. Levenshtein distance may also simply be called "edit distance", although several variants exist.<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}}\u000a\u000a==Formal definition and properties==\u000aGiven two strings {{mvar|a}} and {{mvar|b}} on an alphabet {{mvar|\u03a3}} (e.g. the set of [[ASCII]] characters, the set of [[byte]]s [0..255], etc.), the edit distance d({{mvar|a}}, {{mvar|b}}) is the minimum-weight series of edit operations that transforms {{mvar|a}} into {{mvar|b}}. One of the simplest sets of edit operations is that defined by Levenshtein in 1966:<ref name="slp"/>\u000a\u000a:'''Insertion''' of a single symbol. If {{mvar|a}} = {{mvar|u}}{{mvar|v}}, then inserting the symbol {{mvar|x}} produces {{mvar|u}}{{mvar|x}}{{mvar|v}}. This can also be denoted \u03b5\u2192{{mvar|x}}, using \u03b5 to denote the empty string.\u000a:'''Deletion''' of a single symbol changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|v}} ({{mvar|x}}\u2192\u03b5).\u000a:'''Substitution''' of a single symbol {{mvar|x}} for a symbol {{mvar|y}} \u2260 {{mvar|x}} changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|y}}{{mvar|v}} ({{mvar|x}}\u2192{{mvar|y}}).\u000a\u000aIn Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum ''number'' of operations required to transform {{mvar|a}} to {{mvar|b}}. A more general definition associates non-negative weight functions {{mvar|w}}<sub>ins</sub>({{mvar|x}}), {{mvar|w}}<sub>del</sub>({{mvar|x}}) and {{mvar|w}}<sub>sub</sub>({{mvar|x}}&nbsp;{{mvar|y}}) with the operations.<ref name="slp">{{cite book |author1=Daniel Jurafsky |author2=James H. Martin |title=Speech and Language Processing |publisher=Pearson Education International |pages=107\u2013111}}</ref>\u000a\u000aAdditional primitive operations have been suggested. A common mistake when typing text is '''transposition''' of two adjacent characters commonly occur, formally characterized by an operation that changes {{mvar|u}}{{mvar|x}}{{mvar|y}}{{mvar|v}} into {{mvar|u}}{{mvar|y}}{{mvar|x}}{{mvar|v}} where {{mvar|x}}, {{mvar|y}} \u2208 {{mvar|\u03a3}}.<ref name="ukkonen83">{{cite conference |author=Esko Ukkonen |title=On approximate string matching |conference=Foundations of Computation Theory |year=1983 |pages=487\u2013495 |publisher=Springer}}</ref><ref name="ssm"/>\u000aFor the task of correcting [[Optical character recognition|OCR]] output, '''merge''' and '''split''' operations have been used which replace a single character into a pair of them or vice-versa.<ref name="ssm">{{cite journal |first1=Klaus U. |last1=Schulz |first2=Stoyan |last2=Mihov |year=2002 |id={{citeseerx|10.1.1.16.652}} |title=Fast string correction with Levenshtein automata |journal=International Journal of Document Analysis and Recognition |volume=5 |issue=1 |pages=67\u201385 |doi=10.1007/s10032-002-0082-8}}</ref>\u000a\u000aOther variants of edit distance are obtained by restricting the set of operations. [[Longest common subsequence]] (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.<ref name="navarro"/>{{rp|37}} Similarly, by only allowing substitutions (again at unit cost), [[Hamming distance]] is obtained; this must be restricted to equal-length strings.<ref name="navarro"/>\u000a[[Jaro\u2013Winkler distance]] can be obtained from an edit distance where only transpositions are allowed.\u000a\u000a===Example===\u000aThe [[Levenshtein distance]] between "kitten" and "sitting" is 3. The minimal edit script that transforms the former into the latter is:\u000a\u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a\u000aLCS distance (insertions and deletions only) gives a different distance and minimal edit script:\u000a\u000a# delete '''k''' at 0\u000a# insert '''s''' at 0\u000a# delete '''e''' at 4\u000a# insert '''i''' at 4\u000a# insert '''g''' at 6\u000a\u000afor a total cost/distance of 5 operations.\u000a\u000a===Properties===\u000aEdit distance with non-negative cost satisfies the axioms of a [[Metric (mathematics)|metric]], giving rise to a [[metric space]] of strings, when the following conditions are met:<ref name="navarro"/>{{rp|37}}\u000a\u000a* Every edit operation has positive cost;\u000a* for every operation, there is an inverse operation with equal cost.\u000a\u000aWith these properties, the metric axioms are satisfied as follows:\u000a\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|a}}) = 0, since each string can be trivially transformed to itself using exactly zero operations.\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) > 0 when {{mvar|a}} \u2260 {{mvar|b}}, since this would require at least one operation at non-zero cost.\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|b}}, {{mvar|a}}) by equality of the cost of each operation and its inverse.\u000a:Triangle inequality: {{mvar|d}}({{mvar|a}}, {{mvar|c}}) \u2264 {{mvar|d}}({{mvar|a}}, {{mvar|b}}) + {{mvar|d}}({{mvar|b}}, {{mvar|c}}).<ref>{{cite conference |author1=Lei Chen |author2=Raymond Ng |title=On the marriage of L\u209a-norms and edit distance |conference=Proc. 30th Int'l Conf. on Very Large Databases (VLDB) |volume=30 |year=2004}}</ref>\u000a\u000aLevenshtein distance and LCS distance with unit cost satisfy the above conditions, and therefore the metric axioms. Variants of edit distance that are not proper metrics have also been considered in the literature.<ref name="navarro"/>\u000a\u000aOther useful properties of unit-cost edit distances include:\u000a\u000a* LCS distance is bounded above by the sum of lengths of a pair of strings.<ref name="navarro"/>{{rp|37}}\u000a* LCS distance is an upper bound on Levenshtein distance.\u000a* For strings of the same length, Hamming distance is an upper bound on Levenshtein distance.<ref name="navarro"/>\u000a\u000aRegardless of cost/weights, the following property holds of all edit distances:\u000a\u000a* When {{mvar|a}} and {{mvar|b}} share a common prefix, this prefix has no effect on the distance. Formally, when {{mvar|a}} = {{mvar|uv}} and {{mvar|b}} = {{mvar|uw}}, then {{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|v}}, {{mvar|w}}).<ref name="ssm"/> This allows speeding up many computations involving edit distance and edit scripts, since common prefixes and suffixes can be skipped in linear time.\u000a\u000a==Computation==\u000a===Basic algorithm===\u000a{{main|Wagner\u2013Fischer algorithm}}\u000aUsing Levenshtein's original operations, the edit distance between <math>a = a_1\u005cldots a_n</math> and <math>b = b_1\u005cldots b_m</math> is given by <math>d_{mn}</math>, defined by the recurrence<ref name="slp"/>\u000a\u000a:<math>d_{i0} = \u005csum_{k=1}^{i} w_\u005cmathrm{del}(b_{k}), \u005cquad  for\u005c; 1 \u005cleq i \u005cleq m</math>\u000a:<math>d_{0j} = \u005csum_{k=1}^{j} w_\u005cmathrm{ins}(a_{k}), \u005cquad  for\u005c; 1 \u005cleq j \u005cleq n</math>\u000a:<math>d_{ij} = \u005cbegin{cases} d_{i-1, j-1} & \u005cquad a_{j} = b_{i}\u005c\u005c \u005cmin \u005cbegin{cases} d_{i-1, j} + w_\u005cmathrm{del}(b_{i})\u005c\u005c d_{i,j-1} + w_\u005cmathrm{ins}(a_{j}) \u005c\u005c d_{i-1,j-1} + w_\u005cmathrm{sub}(a_{j}, b_{i}) \u005cend{cases} & \u005cquad a_{j} \u005cneq b_{i}\u005cend{cases} , \u005cquad  for\u005c; 1 \u005cleq i \u005cleq m, 1 \u005cleq j \u005cleq n.</math>\u000a\u000aThis algorithm can be generalized to handle transpositions by adding another term in the recursive clause's minimization.<ref name="ukkonen83"/>\u000a\u000aThe straightforward, [[Recursion (computer science)|recursive]] way of evaluating this recurrence takes [[exponential time]]. Therefore, it is usually computed using a [[dynamic programming]] algorithm that is commonly credited to [[Wagner\u2013Fischer algorithm|Wagner and Fischer]],<ref>{{cite journal |author1=R. Wagner |author2=M. Fischer |title=The string-to-string correction problem |journal=J. ACM |volume=21 |year=1974 |pages=168\u2013178 |doi=10.1145/321796.321811}}</ref> although it has a history of multiple invention.<ref name="slp"/><ref name="ukkonen83"/>\u000aAfter completion of the Wagner\u2013Fischer algorithm, a minimal sequence of edit operations can be read off as a backtrace of the operations used during the dynamic programming algorithm starting at <math>d_{mn}</math>.\u000a\u000aThis algorithm has a [[time complexity]] of \u0398({{mvar|m}}{{mvar|n}}). When the full dynamic programming table is constructed, its [[space complexity]] is also \u0398({{mvar|m}}{{mvar|n}}); this can be improved to \u0398(min({{mvar|m}},{{mvar|n}})) by observing that at any instant, the algorithm only requires two rows (or two columns) in memory. However, this optimization makes it impossible to read off the minimal series of edit operations.<ref name="ukkonen83"/> A linear-space solution to this problem is offered by [[Hirschberg's algorithm]].<ref>{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=1-849-96720-2}}</ref>{{rp|634}}\u000a\u000a===Improved algorithms===\u000aImproving on the Wagner\u2013Fisher algorithm described above, [[Esko Ukkonen|Ukkonen]] describes several variants,<ref>{{cite journal |title=Algorithms for approximate string matching |journal=Information and Control |volume=64 |issue=1\u20133 |year=1985 |url=http://www.cs.helsinki.fi/u/ukkonen/InfCont85.PDF}}</ref> one of which takes two strings and a maximum edit distance {{mvar|s}}, and returns min({{mvar|s}}, {{mvar|d}}). It achieves this by only computing and storing a part of the dynamic programming table around its diagonal. This algorithm takes time O({{mvar|s}}min({{mvar|m}},{{mvar|n}})), where {{mvar|m}} and {{mvar|n}} are the lengths of the strings. Space complexity is O({{mvar|s}}) or O({{mvar|s}}), depending on whether the edit sequence needs to be read off.<ref name="ukkonen83"/>\u000a\u000a==Applications==\u000aEdit distance finds applications in [[computational biology]] and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and [[approximate string matching]], where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.\u000a\u000aVarious algorithms exist that solve problems beside the computation of distance between a pair of strings, to solve related types of problems.\u000a\u000a* [[Hirschberg's algorithm]] computes the optimal [[Sequence alignment|alignment]] of two strings, where optimality is defined as minimizing edit distance.\u000a* [[Approximate string matching]] can be formulated in terms of edit distance. Ukkonen's 1985 algorithm takes a string {{mvar|p}}, called the pattern, and a constant {{mvar|k}}; it then builds a [[deterministic finite state automaton]] that finds, in an arbitrary string {{mvar|s}}, a substring whose edit distance to {{mvar|p}} is at most {{mvar|k}}<ref>{{cite journal |author=Esko Ukkonen |title=Finding approximate patterns in strings |journal=J. Algorithms |volume=6 |pages=132\u2013137 |year=1985 |doi=10.1016/0196-6774(85)90023-9}}</ref> (cf. the [[Aho\u2013Corasick string matching algorithm|Aho\u2013Corasick algorithm]], which similarly constructs an automaton to search for any of a number of patterns, but without allowing edit operations). A similar algorithm for approximate string matching is the [[bitap algorithm]], also defined in terms of edit distance.\u000a* [[Levenshtein automaton|Levenshtein automata]] are finite-state machines that recognize a set of strings within bounded edit distance of a fixed reference string.<ref name="ssm"/>\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a[[Category:String similarity measures]]
p141
asI141
(lp142
VHuman\u2013computer information retrieval
p143
aV'''Human\u2013computer information retrieval''' ('''HCIR''') is the study of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. The fields of [[human\u2013computer interaction]] (HCI) and information retrieval (IR) have both developed innovative techniques to address the challenge of navigating complex information spaces, but their insights have often failed to cross disciplinary borders. Human\u2013computer information retrieval has emerged in academic research and industry practice to bring together research in the fields of IR and HCI, in order to create new kinds of search systems that depend on continuous human control of the search process.\u000a\u000a== History ==\u000a\u000aThis term ''human\u2013computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.<ref name=march2006>Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science. Available online at http://www.asis.org/Bulletin/Jun-06/marchionini.html.</ref> Marchionini\u2019s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."\u000a\u000aIn 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human\u2013computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] \u2013 changes that were only embryonic in the late 1990s.\u000a\u000aA few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].\u000a\u000a== What is HCIR? ==\u000a\u000aHCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."<ref name=ingwer1992>Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham. Available online at http://vip.db.dk/pi/iri/index.htm.</ref>\u000a\u000aA key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.<ref>{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}</ref>\u000a\u000aMost modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document\u2019s [[relevance]] to the query.<ref>Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. </ref> In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information_retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).\u000a\u000aBecause of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models \u2013 one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]\u2019s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information_retrieval#Precision|precision]] and [[Information_retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.<ref name=koene1996>Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13\u201318, 1996). M. J. Tauber, Ed. CHI \u201896. ACM Press, New York, NY, 205-212. Available online at http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm.\u000a</ref> Other HCIR research, such as Pia Borlund\u2019s IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.<ref name=borlund2003>Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152. Available online at http://informationr.net/ir/8-3/paper152.html.</ref>\u000a\u000a== Goals ==\u000a\u000aMarchionini put forth the following goals towards a system where the user has more control in determining relevant results.<ref name=march2006/>\u000a\u000aSystems should\u000a*no longer only deliver the relevant documents, but must also provide semantic information along with those documents\u000a*increase user responsibility as well as control; that is, information systems require human intellectual effort\u000a*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases\u000a*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services\u000a*support the entire [[information life cycle]] (from creation to preservation) rather than only the dissemination or use phase\u000a*support tuning by end users and especially by information professionals who add value to information resources\u000a*be engaging and fun to use\u000a\u000aIn short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process \u2014 the [[information professional]] \u2014 to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).\u000a\u000a== Techniques ==\u000a\u000aThe techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.\u000a\u000aMany [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user\u2019s hands.\u000a\u000a[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.<ref>Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.</ref>\u000a\u000a[[Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.\u000a\u000a[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.<ref>Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.</ref>\u000a\u000aSummarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].\u000a\u000a[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].\u000a\u000a== References ==\u000a\u000a<References/>\u000a\u000a==External links==\u000a*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}\u000a*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}\u000a\u000a{{DEFAULTSORT:Human-computer information retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Human\u2013computer interaction]]
p144
asI208
(lp145
VJaro\u2013Winkler distance
p146
aV{{About|the measure|other uses|Jaro (disambiguation){{!}}Jaro}}\u000a\u000a{{Original research|date=May 2013}}\u000a{{Notability|date=May 2013}}\u000a\u000aIn [[computer science]] and [[statistics]], the '''Jaro\u2013Winkler distance''' (Winkler, 1990) is a measure of similarity between two [[String (computer science)|strings]].  It is a variant of the '''Jaro distance''' metric (Jaro, 1989, 1995), a type of string [[edit distance]], and was developed in the area of [[record linkage]] (duplicate detection) (Winkler, 1990). The higher the Jaro\u2013Winkler distance for two strings is, the more similar the strings are.  The Jaro\u2013Winkler distance metric is designed and best suited for short strings such as person names.  The score is normalized such that 0 equates to no similarity and 1 is an exact match.\u000a\u000a== Definition ==\u000a\u000aThe Jaro distance <math>d_j</math> of two given strings <math>s_1</math> and <math>s_2</math> is\u000a\u000a: <math>d_j = \u005cleft\u005c{\u000a\u000a\u005cbegin{array}{l l}\u000a  0 & \u005ctext{if }m = 0\u005c\u005c\u000a  \u005cfrac{1}{3}\u005cleft(\u005cfrac{m}{|s_1|} + \u005cfrac{m}{|s_2|} + \u005cfrac{m-t}{m}\u005cright) & \u005ctext{otherwise} \u005cend{array} \u005cright.</math>\u000a\u000aWhere:\u000a\u000a* <math>m</math> is the number of ''matching characters'' (see below);\u000a* <math>t</math> is half the number of ''transpositions'' (see below).\u000a\u000aTwo characters from <math>s_1</math> and <math>s_2</math> respectively, are considered ''matching'' only if they are the same and not farther than <math>\u005cleft\u005clfloor\u005cfrac{\u005cmax(|s_1|,|s_2|)}{2}\u005cright\u005crfloor-1</math>.\u000a\u000aEach character of <math>s_1</math> is compared with all its matching\u000acharacters in <math>s_2</math>. The number of matching (but different sequence order) characters\u000adivided by 2 defines the number of ''transpositions''.\u000aFor example, in comparing CRATE with TRACE, only 'R'   'A'   'E'  are the matching characters, i.e. m=3. Although 'C', 'T' appear in both strings, they are farther than 1, i.e., floor(5/2)-1=1. Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.\u000a\u000aJaro\u2013Winkler distance uses a [[prefix]] scale <math>p</math> which gives more favourable ratings to strings that match from the beginning for a set prefix length <math>\u005cell</math>.  Given two strings <math>s_1</math> and <math>s_2</math>, their Jaro\u2013Winkler distance <math>d_w</math> is:\u000a\u000a: <math>d_w = d_j + (\u005cell p (1 - d_j))</math>\u000a\u000awhere:\u000a\u000a* <math>d_j</math> is the Jaro distance for strings <math>s_1</math> and <math>s_2</math>\u000a* <math>\u005cell</math> is the length of common prefix at the start of the string up to a maximum of 4 characters\u000a* <math>p</math> is a constant [[scaling factor]] for how much the score is adjusted upwards for having common prefixes.  <math>p</math> should not exceed 0.25, otherwise the distance can become larger than 1.  The standard value for this constant in Winkler's work is <math>p = 0.1</math>\u000a\u000aAlthough often referred to as a ''distance metric'', the Jaro\u2013Winkler distance is actually not a [[metric (mathematics)|metric]] in the mathematical sense of that term because it does not obey the [[triangle inequality]] [http://richardminerich.com/tag/jaro-winkler/].\u000a\u000aIn some implementations of Jaro-Winkler, the prefix bonus <math>\u005cell p (1 - d_j)</math> is only added when the compared strings have a Jaro distance above a set "boost threshold" <math>b_t</math>. The boost threshold in Winkler's implementation was 0.7.\u000a\u000a: <math>d_w = \u005cleft\u005c{\u000a\u000a\u005cbegin{array}{l l}\u000a  d_j & \u005ctext{if }d_j < b_t\u005c\u005c\u000a  d_j + (\u005cell p (1 - d_j)) & \u005ctext{otherwise} \u005cend{array} \u005cright.</math>\u000a\u000a== Example ==\u000a\u000a''Note that Winkler's "reference" C code differs in at least two ways from published accounts of the Jaro\u2013Winkler metric. First is his use of a typo table (adjwt) and also some optional additional tolerance for long strings.''\u000a\u000aGiven the strings <math>s_1</math> ''MARTHA'' and <math>s_2 </math> ''MARHTA'' we find:\u000a\u000a* <math>m = 6</math>\u000a* <math>|s_1| = 6</math>\u000a* <math>|s_2| = 6</math>\u000a* There are mismatched characters T/H and H/T leading to <math>t = \u005cfrac{2}{2} = 1</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a<math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{6}{6} + \u005cfrac{6}{6} + \u005cfrac{6-1}{6}\u005cright) = 0.944</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 3</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.944 + (3 * 0.1 (1 - 0.944)) = 0.961</math>\u000a\u000aGiven the strings <math>s_1</math> ''DWAYNE'' and <math>s_2</math> ''DUANE'' we find:\u000a\u000a* <math>m = 4</math>\u000a* <math>|s_1| = 6</math>\u000a* <math>|s_2| = 5</math>\u000a* <math>t = 0</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a: <math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{4}{6} + \u005cfrac{4}{5} + \u005cfrac{4-0}{4}\u005cright) = 0.822</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 1</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.822 + (1 * 0.1 (1 - 0.822)) = 0.84</math>\u000a\u000aGiven the strings <math>s_1</math> ''DIXON'' and <math>s_2</math> ''DICKSONX'' we find:\u000a\u000a{{elucidate|date=March 2013}}\u000a\u000a{| class="wikitable"\u000a|-\u000a|\u000a| D\u000a| I\u000a| X\u000a| O\u000a| N\u000a|-\u000a| D\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| 0\u000a|-\u000a| I\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| C\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| K\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| S\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| O\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a| <span style="background: #ffcc33">0\u000a|-\u000a| N\u000a| 0\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a| <span style="background: #ffcc33">1\u000a|-\u000a| X\u000a| 0\u000a| 0\u000a| 0\u000a| 0\u000a| <span style="background: #ffcc33">0\u000a|}\u000a\u000a* <math>m = 4</math>  Note that the two ''X''s are not considered matches because they are outside the match window of 3.\u000a\u000a* <math>|s_1| = 5</math>\u000a* <math>|s_2| = 8</math>\u000a* <math>t = 0</math>\u000a\u000aWe find a Jaro score of:\u000a\u000a: <math>d_j = \u005cfrac{1}{3}\u005cleft(\u005cfrac{4}{5} + \u005cfrac{4}{8} + \u005cfrac{4-0}{4}\u005cright) = 0.767</math>\u000a\u000aTo find the Jaro\u2013Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\u000a\u000a* <math>\u005cell = 2</math>\u000a\u000aThus:\u000a\u000a: <math>d_w = 0.767 + (2 * 0.1 (1 - 0.767)) = 0.814</math>\u000a\u000a== See also ==\u000a\u000a* [[Levenshtein distance]]\u000a* [[Record linkage]]\u000a* [[Census]]\u000a\u000a== References ==\u000a\u000a* {{cite journal | last=Cohen |first=W. W. |last2=Ravikumar |first2=P. |last3=Fienberg |first3=S. E. |year=2003 |title=A comparison of string distance metrics for name-matching tasks |journal=KDD Workshop on Data Cleaning and Object Consolidation |volume=3 |pages=73-8 |url=https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf}}\u000a* {{cite journal | author = [[Matthew A. Jaro|Jaro, M. A.]] | title = Advances in record linkage methodology as applied to the 1985 census of Tampa Florida | journal = Journal of the American Statistical Association | year = 1989 | volume = 84 | issue = 406 |pages=414\u201320| url = | doi = 10.1080/01621459.1989.10478785 }}\u000a* {{cite journal |author=Jaro, M. A. |title=Probabilistic linkage of large public health data file  |journal= Statistics in Medicine |year=1995 |volume=14 |issue=5\u20137 |pages=491\u20138  |pmid=7792443 |doi=10.1002/sim.4780140510}}\u000a* {{cite journal\u000a\u000a  | author = [[William E. Winkler|Winkler, W. E.]]\u000a  | title = String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage\u000a  | journal = Proceedings of the Section on Survey Research Methods\u000a  | publisher = American Statistical Association\u000a  | pages = 354\u2013359\u000a  | year = 1990\u000a  | url = http://www.amstat.org/sections/srms/Proceedings/papers/1990_056.pdf }}\u000a\u000a* {{cite journal | author = [[William E. Winkler|Winkler, W. E.]] | title = Overview of Record Linkage and Current Research Directions | journal = Research Report Series, RRS | year = 2006 | volume = | issue = | url = http://www.census.gov/srd/papers/pdf/rrs2006-02.pdf}}\u000a\u000a== External links ==\u000a\u000a* [http://web.archive.org/web/20100227020019/http://www.census.gov/geo/msb/stand/strcmp.c strcmp.c - Original C Implementation by the author of the algorithm]\u000a\u000a{{DEFAULTSORT:Jaro-Winkler distance}}\u000a\u000a[[Category:String similarity measures]]
p147
asI209
(lp148
VString kernel
p149
aVIn [[machine learning]] and [[data mining]], a '''string kernel''' is a [[Positive-definite kernel|kernel function]] that operates on [[String (computer science)|strings]], i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings ''a'' and ''b'' are, the higher the value of a string kernel ''K''(''a'', ''b'') will be.\u000a\u000aUsing string kernels with [[Kernel trick|kernelized]] learning algorithms such as [[support vector machine]]s allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued [[feature vector]]s.<ref name="Lodhi"/> String kernels are used in domains where sequence data are to be [[Cluster analysis|clustered]] or [[statistical classification|classified]], e.g. in [[text mining]] and [[bioinformatics|gene analysis]].<ref>\u000a{{Citation\u000a  | title = The spectrum kernel: A string kernel for SVM protein classification\u000a  | last = Leslie\u000a  | first = C.\u000a  | last2 = Eskin\u000a  | first2 = E.\u000a  | last3 = Noble\u000a  | first3 = W.S.\u000a  | booktitle = Proceedings of the Pacific Symposium on Biocomputing\u000a  | volume = 7\u000a  | pages = 566\u2013575\u000a  | year = 2002\u000a}}</ref>\u000a\u000a==Informal introduction==\u000a\u000aSuppose one wants to compare some text passages automatically and indicate their relative similarity.\u000aFor many applications, it might be sufficient to find some keywords which match exactly.\u000aOne example where exact matching is not always enough is found in [[Spam (electronic)|spam]] detection.<ref>\u000a{{Citation\u000a  | title = Improved Online Support Vector Machines Spam Filtering Using String Kernels\u000a  | last = Amayri\u000a  | first = O.\u000a}}</ref>\u000aAnother would be in computational gene analysis, where [[Homology (biology)|homologous]] [[genes]] have [[mutated]], resulting in common subsequences along with deleted, inserted or replaced symbols.\u000a<!--- TODO insert a picture here --->\u000a\u000a==Motivation==\u000a\u000aSince several well-proven data clustering, classification and information retrieval\u000a<!--- and other ... see manifold learning --->\u000amethods (for example support vector machines) are designed to work on vectors\u000a(i.e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data.\u000a\u000aThe string kernel method is to be contrasted with earlier approaches for text classification where feature vectors only indicated\u000athe presence or absence of a word.\u000aNot only does it improve on these approaches, but it is an example for a whole class of kernels adapted to data structures, which\u000abegan to appear at the turn of the 21st century. A survey of such methods has been compiled by Grtner.<ref>\u000a{{Citation\u000a  | last = Grtner\u000a  | first = T.\u000a  | title = A survey of kernels for structured data\u000a  | journal = CM SIGKDD Explorations Newsletter\u000a  | publisher = [[Association for Computing Machinery|ACM]]\u000a  | year = 2003\u000a  | volume = 5\u000a  | number = 1\u000a  | page = 58}}\u000a</ref>\u000a\u000a==Definition==\u000a\u000aA [[Kernel trick|kernel]] on a domain <math>D</math> is a function <math>K: D \u005ctimes D \u005crightarrow \u005cmathbb{R}</math>\u000asatisfying some conditions (being [[symmetric]] in the arguments, [[continuous function|continuous]] and [[Positive-semidefinite function|positive semidefinite]] in a certain sense).\u000a\u000a[[Mercer's theorem]] asserts that <math>K</math> can then be expressed as <math>K(x,y)=\u005cvarphi(x)\u005ccdot \u005cvarphi(y)</math> with <math>\u005cvarphi</math> mapping the arguments into an [[inner product space]].\u000a\u000aWe can now reproduce the definition of a '''string subsequence kernel'''<ref name="Lodhi">{{Cite journal\u000a  | last = Lodhi\u000a  | first = Huma\u000a  | last2 = Saunders\u000a  | first2 = Craig\u000a  | last3 = Shawe-Taylor\u000a  | first3 = John\u000a  | last4 = Cristianini\u000a  | first4 = Nello\u000a  | last5 = Watkins\u000a  | first5 = Chris\u000a  | title = Text classification using string kernels\u000a  | journal = [[Journal of Machine Learning Research]]\u000a  | year = 2002\u000a  | pages = 419\u2013444}}</ref>\u000aon strings over an [[Alphabet (computer science)|alphabet]] <math>\u005cSigma</math>. Coordinate-wise, the mapping is defined as follows:\u000a\u000a:<math>\u005cvarphi_u :\u000a\u005cleft\u005c{\u000a\u005cbegin{array}{l}\u000a\u005cSigma^n \u005crightarrow \u005cmathbb{R}^{\u005cSigma^n} \u005c\u005c\u000a s \u005cmapsto \u005csum_{\u005cmathbf{i} : u=s_{\u005cmathbf{i}}} \u005clambda^{l(\u005cmathbf{i})}\u000a\u005cend{array}\u000a\u005cright.\u000a</math>\u000a\u000aThe <math>\u005cmathbf{i}</math> are [[multiindices]] and <math>u</math> is a string of length <math>n</math>:\u000asubsequences can occur in a non-contiguous manner, but gaps are penalized.\u000aThe parameter <math>\u005clambda</math> may be set to any value between <math>0</math> (gaps are not allowed) and <math>1</math>\u000a(even widely-spread "occurrences" are weighted the same as appearances as a contiguous substring).\u000a\u000a<!--- TODO put an example here !!! --->\u000a\u000aFor several relevant algorithms, data enters into the algorithm only in expressions involving an inner product of feature vectors,\u000ahence the name [[kernel methods]]. A desirable consequence of this is that one does not need to explicitly calculate the transformation <math>\u005cphi(x)</math>, only the inner product via the kernel, which may be a lot quicker, especially when [[approximation|approximated]].<ref name=Lodhi/>\u000a<!--- ==Efficitent Computation== --->\u000a<!--- == See also == --->\u000a\u000a==References==\u000a<!--- cite "alignment kernels", precursor --->\u000a{{Reflist}}\u000a\u000a[[Category:Algorithms on strings]]\u000a[[Category:Kernel methods for machine learning]]\u000a[[Category:Natural language processing]]\u000a[[Category:String similarity measures]]
p150
asI82
(lp151
VUncertain inference
p152
aV'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481\u2013485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.\u000a\u000a==Definitions==\u000aRijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:\u000a\u000a<math>P(d \u005cto q)</math>\u000a\u000aA user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.\u000aIn many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \u005cto q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\u000aSince ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \u005cto q</math> is uncertain. This will affect the plausibility of a given query.\u000a\u000aBy doing this it accomplishes two things:\u000a* Separate the processes of revising probabilities from the logic\u000a* Separate the treatment of relevance from the treatment of requests\u000a\u000a[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.\u000a\u000aUncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.\u000a\u000a==Example==\u000aIf we have a query of the form:\u000a\u000a<math>q = A \u005cwedge B \u005cwedge C</math>\u000a\u000awhere A, B and C are query assertions, then for a document D we want the probability:\u000a\u000a<math>P (D \u005cto (A \u005cwedge B \u005cwedge C))</math>\u000a\u000aIf we transform this into the [[conditional probability]] <math>P ((A \u005cwedge B \u005cwedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.\u000a\u000a==Further work==\u000aCroft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author=W. B. Croft | coauthors=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.\u000a\u000a[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.\u000a\u000a[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.\u000a\u000a== See also ==\u000a* [[Fuzzy logic]]\u000a* [[Probabilistic logic]]\u000a* [[Plausible reasoning]]\u000a* [[Imprecise probability]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]\u000a[[Category:Inference]]
p153
asI35
(lp154
VRelevance (information retrieval)
p155
aV{{Other uses|Relevance}}\u000a\u000aIn [[information science]] and [[information retrieval]], '''relevance''' denotes how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.\u000a\u000a== History ==\u000a\u000aThe concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.\u000a\u000aThe formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810\u2010832.</ref>\u000a\u000aSince 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".\u000a\u000aRecently, Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> showed a connection between the [[Binary Independence Model|relevance probability]] and the [[vocabulary mismatch]] problem in retrieval, which could lead to at least 50-300% gains in retrieval accuracy.<ref>Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\u000a\u000a== Evaluation ==\u000a\u000aThe information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.\u000a\u000aIn order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.\u000a\u000aIn contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).\u000a\u000a== Clustering and relevance ==\u000a\u000aThe [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:\u000a* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, \u201cA model of cluster searching based on classification,\u201d Information Systems, vol. 5, pp. 189\u2013195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, \u201cUsing interdocument similarity information in document retrieval systems,\u201d Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3\u201311, 1986.</ref>\u000a* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, \u201cCluster-based retrieval using language models,\u201d in SIGIR \u201904: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186\u2013193, ACM Press, 2004.</ref>    It is important to ensure that clusters \u2013 either in isolation or combination \u2013 successfully model the set of possible relevant documents.\u000a\u000aA second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, \u201cThe cluster hypothesis revisited,\u201d in SIGIR \u201985: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188\u2013196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,\u000a* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>\u000a* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, \u201cA study of relevance propagation for web search,\u201d in SIGIR \u201905: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408\u2013415, ACM Press, 2005.</ref> methods\u000a* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, \u201cDocument expansion for speech retrieval,\u201d in SIGIR \u201999: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34\u201341, ACM Press, 1999.</ref>\u000a* score regularization<ref name=diazreg>F. Diaz, \u201cRegularizing query-based retrieval scores,\u201d Information Retrieval, vol. 10, pp. 531\u2013562, December 2007.</ref>\u000aLocal methods require an accurate and appropriate document similarity measure.\u000a\u000a==Epistemological issues==\u000a{{Section OR|date=May 2014}}\u000aAre users best at evaluating the relevance of a given document, or is it better to use experts?\u000aMost research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase "relevance" output. An alternative strategy would be to use journal [[impact factor]] to rank output and thus base relevance on expert evaluations. Other strategies, such as including diversity of the search results, may be used as well. The important thing to recognize is, however, that relevance is fundamentally a question of [[epistemology]], not [[psychology]]. (Peoples' psychology reflects certain epistemological influences).\u000a\u000a==References==\u000a {{reflist}}\u000a\u000a==Additional reading==\u000a*Hjrland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.\u000a\u000a*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])\u000a\u000a[[Category:Information retrieval]]
p156
asI213
(lp157
VHellinger distance
p158
aVIn [[probability theory|probability]] and [[mathematical statistics|statistics]], the '''Hellinger distance''' (also called [[Bhattacharyya distance]] as this was originally introduced by [[Anil Kumar Bhattacharya]]) is used to quantify the similarity between two [[probability distributions]]. It is a type of [[f-divergence|''f''-divergence]].  The Hellinger distance is defined in terms of the [[Hellinger integral]], which was introduced by [[Ernst Hellinger]] in 1909.<ref>{{SpringerEOM|title=Hellinger distance|id=h/h046890|first=M.S. |last=Nikulin}}</ref><ref>{{Citation \u000a| last = Hellinger \u000a| first = Ernst\u000a| author-link = Ernst Hellinger\u000a| title = Neue Begrndung der Theorie quadratischer Formen von unendlichvielen Vernderlichen \u000a| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 \u000a| year = 1909 \u000a| journal = [[Journal fr die reine und angewandte Mathematik]]\u000a| language = German\u000a| volume = 136 \u000a| pages = 210\u2013271\u000a| jfm = 40.0393.01\u000a| doi=10.1515/crll.1909.136.210\u000a}}</ref>\u000a\u000a==Definition==\u000a\u000a===Measure theory===\u000aTo define the Hellinger distance in terms of [[measure theory]], let ''P'' and ''Q'' denote two [[probability measure]]s that are [[absolute continuity|absolutely continuous]] with respect to a third probability measure &lambda;.  The square of the Hellinger distance between ''P'' and ''Q'' is defined as the quantity\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cdisplaystyle \u005cint \u005cleft(\u005csqrt{\u005cfrac{dP}{d\u005clambda}} - \u005csqrt{\u005cfrac{dQ}{d\u005clambda}}\u005cright)^2 d\u005clambda. </math>\u000a\u000aHere, ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are the [[Radon\u2013Nikodym derivative]]s of ''P'' and ''Q'' respectively.  This definition does not depend on &lambda;, so the Hellinger distance between ''P'' and ''Q'' does not change if &lambda; is replaced with a different probability measure with respect to which both  ''P'' and ''Q'' are absolutely continuous.  For compactness, the above formula is often written as\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{dP} - \u005csqrt{dQ}\u005cright)^2. </math>\u000a\u000a===Probability theory using Lebesgue measure===\u000aTo define the Hellinger distance in terms of elementary probability theory, we take &lambda; to be [[Lebesgue measure]], so that ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are simply [[probability density function]]s.  If we denote the densities as ''f'' and ''g'', respectively, the squared Hellinger distance can be expressed as a standard calculus integral\u000a\u000a:<math>\u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{f(x)} - \u005csqrt{g(x)}\u005cright)^2 dx = 1 - \u005cint \u005csqrt{f(x) g(x)} \u005c, dx,</math>\u000a\u000awhere the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain must be one.\u000a\u000aThe Hellinger distance ''H''(''P'',&nbsp;''Q'') satisfies the property (derivable from the [[Cauchy-Schwarz inequality#L2|Cauchy-Schwarz inequality]])\u000a\u000a: <math>0\u005cle H(P,Q) \u005cle 1.</math>\u000a\u000a===Discrete distributions===\u000aFor two discrete probability distributions <math>P=(p_1 \u005cldots p_k)</math> and <math>Q=(q_1 \u005cldots q_k)</math>,\u000atheir Hellinger distance is defined as\u000a\u000a: <math>\u000a  H(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005csqrt{\u005csum_{i=1}^{k} (\u005csqrt{p_i} - \u005csqrt{q_i})^2},\u000a</math>\u000a\u000awhich is directly related to the [[Euclidean distance|Euclidean norm]] of the difference of the square root vectors, i.e.\u000a: <math>\u000aH(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005cbigl\u005c|\u005csqrt{P} - \u005csqrt{Q} \u005cbigr\u005c|_2 .\u000a</math>\u000a\u000a== Connection with the statistical distance ==\u000a\u000aThe Hellinger distance <math>H(P,Q)</math> and the [[total variation distance]] (or statistical distance) <math>\u005cdelta(P,Q)</math> are related as follows:<ref>[http://www.tcs.tifr.res.in/~prahladh/teaching/2011-12/comm/lectures/l12.pdf Harsha's lecture notes on communication complexity]</ref>\u000a\u000a: <math>\u000aH^2(P,Q) \u005cleq \u005cdelta(P,Q) \u005cleq \u005csqrt 2 H(P,Q)\u005c,.\u000a</math>\u000a\u000aThese inequalities follow immediately from the inequalities between the [[Lp space#The p-norm in finite dimensions|1-norm]] and the [[Lp space#The p-norm in finite dimensions|2-norm]].\u000a\u000a==Properties==\u000aThe maximum distance 1 is achieved when ''P'' assigns probability zero to every set to which ''Q'' assigns a positive probability, and vice versa.\u000a\u000aSometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.\u000a\u000aThe Hellinger distance is related to the [[Bhattacharyya distance|Bhattacharyya coefficient]] <math>BC(P,Q)</math> as it can be defined as\u000a\u000a: <math>H(P,Q) = \u005csqrt{1 - BC(P,Q)}.</math>\u000a\u000aHellinger distances are used in the theory of [[sequential analysis|sequential]] and [[asymptotic statistics]].<ref>Erik Torgerson (1991) ''Comparison of Statistical Experiments'', volume 36 of Encyclopedia of Mathematics. Cambridge University Press.\u000a</ref><ref>{{cite book\u000a  | author = Liese, Friedrich and Miescke, Klaus-J.\u000a  | title = Statistical Decision Theory: Estimation, Testing, and Selection\u000a  | year = 2008\u000a  | publisher = Springer\u000a  | isbn = 0-387-73193-8\u000a  }}\u000a</ref>\u000a\u000a==Examples==\u000aThe squared Hellinger distance between two [[normal distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_1,\u005csigma_1^2)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_2,\u005csigma_2^2)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005csqrt{\u005cfrac{2\u005csigma_1\u005csigma_2}{\u005csigma_1^2+\u005csigma_2^2}} \u005c,  e^{-\u005cfrac{1}{4}\u005cfrac{(\u005cmu_1-\u005cmu_2)^2}{\u005csigma_1^2+\u005csigma_2^2}}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[exponential distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Exp}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Exp}(\u005cbeta)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 \u005csqrt{\u005calpha \u005cbeta}}{\u005calpha + \u005cbeta}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[Weibull distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{W}(k,\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{W}(k,\u005cbeta)</math> (where <math> k </math> is a common shape parameter and <math> \u005calpha\u005c, , \u005cbeta </math> are the scale parameters respectively):\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 (\u005calpha \u005cbeta)^{k/2}}{\u005calpha^k + \u005cbeta^k}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Poisson distribution]]s with rate parameters <math>\u005calpha</math> and <math>\u005cbeta</math>, so that <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Poisson}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Poisson}(\u005cbeta)</math>, is:\u000a: <math>\u000a  H^2(P,Q) = 1-e^{-\u005cfrac{1}{2}(\u005csqrt{\u005calpha} - \u005csqrt{\u005cbeta})^2}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Beta distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005ctext{Beta}(a_1,b_1)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005ctext{Beta}(a_2, b_2)</math> is:\u000a: <math>\u000aH^{2}(P,Q)	=1-\u005cfrac{B\u005cleft(\u005cfrac{a_{1}+a_{2}}{2},\u005cfrac{b_{1}+b_{2}}{2}\u005cright)}{\u005csqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}\u000a  </math>\u000awhere <math>B</math> is the [[Beta function]].\u000a\u000a==See also==\u000a* [[Kullback Leibler divergence]]\u000a* [[Fisher information metric]]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==References==\u000a* {{cite book |author=Yang, Grace Lo; Le Cam, Lucien M. |title=Asymptotics in Statistics: Some Basic Concepts |publisher=Springer |location=Berlin |year=2000 |pages= |isbn=0-387-95036-2 |oclc= |doi=}}\u000a* {{cite book |author=Vaart, A. W. van der |title=Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) |publisher=Cambridge University Press |location=Cambridge, UK |year= |pages= |isbn=0-521-78450-6 |oclc= |doi=}}\u000a* {{cite book |author=Pollard, David E. |title=A user's guide to measure theoretic probability |publisher=Cambridge University Press |location=Cambridge, UK |year=2002 |pages= |isbn=0-521-00289-3 |oclc= |doi=}}\u000a\u000a[[Category:Probability theory]]\u000a[[Category:F-divergences]]\u000a[[Category:Statistical distance measures]]\u000a[[Category:String similarity measures]]
p159
asI86
(lp160
VGreenpilot
p161
aV{{COI|date=April 2010}}\u000aThe online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.\u000a\u000aThe project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Khler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewhlter Ort 2009).<ref>[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]</ref>\u000a\u000a==Objective==\u000aThe Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,<ref>[http://www.medpilot.de/ Medpilot portal]</ref> also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.\u000a\u000a==Technical Background==\u000aGreenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:\u000a\u000a* semantic search optimised for the fields of Medicine and Life Sciences\u000a* a contextual analysis of texts taking synonyms and compounds into account\u000a* multilingual and cross-language search\u000a* linking of lay and expert vocabulary\u000aThe search results are generated from a search index.\u000a\u000aAdditionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.\u000a\u000a==Contents==\u000aThe Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.\u000aThe following is a list of sources from November 2009:<ref>[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]</ref>\u000a\u000a===Library Catalogues===\u000a* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)\u000a* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)\u000a* Catalogue of the Bonn University Library\u000a* Library catalogues of scientifically relevant departments within the collective library network (GBV)\u000a* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)\u000a* Catalogue of the Johann Heinrich von Thnen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries\u000a* Catalogue of the Julius Khn-Institut, Federal Research Centre for Cultivated Plants\u000a* Catalogue of the Friedrich Lffler-Institut, Federal Research Institute for Animal Health\u000a* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food\u000a* Catalogue of the Federal Institute for Risk Assessment\u000a* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)\u000a* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)\u000a* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)\u000a* Catalogue of the special collection inshore and deep-sea fishery\u000a* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)\u000a* Catalogue of the German National Library of Economics (ZBW)\u000a\u000a===Bibliographic databases===\u000a* AGRIS (1975\u20132008), FAO ( Food and Agriculture Organization of the United Nations)\u000a* VITIS-VEA, Viticulture and Enology Abstracts\u000a* Medline (2004\u20132009)\u000a* UFORDAT, Environmental Research Database (UBA)\u000a* ULIDAT, Environmental Literature Database (UBA)\u000a* ELFIS, International Information System for the Agricultural Sciences and Technology\u000a\u000a===Relevant Internet Sources===\u000a* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture\u000a* Open Access journals with full text documents\u000a\u000a===Metasearch===\u000a* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.\u000a* ECONIS, Catalogue of the German National Library of Economics (ZBW).\u000a\u000a==Other Features==\u000a\u000a===Search and results page===\u000a* Search and advanced search\u000a* Context sensitive help function\u000a* [[Truncation]] and [[Boolean function]]s\u000a* Personalised refining of search results by filtering for a specific document type, language or database\u000a* [[Bookmark]]s\u000a\u000a===Document ordering===\u000a* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).\u000a\u000a===Personalisation===\u000a* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.\u000a\u000a==See also==\u000a*[[List of digital library projects]]\u000a*[[vascoda]]\u000a\u000a==References==\u000a<references />\u000a\u000a==External links==\u000a* [http://www.greenpilot.de Greenpilot website]\u000a* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]\u000a* [http://www.land-of-ideas.org Germany - Land of Ideas website]\u000a\u000a{{coord missing|Germany}}\u000a\u000a[[Category:Libraries in Germany]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]
p162
asI87
(lp163
VAnchor text
p164
aV{{Use dmy dates|date=February 2013}}\u000aThe '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, Web Search Engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|last=Bader Aljaber, Nicola Stokes, James Bailey and Jian Pei|url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]\u000a\u000a==Overview==\u000aAnchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:\u000a\u000a:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>\u000a\u000aThe anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.\u000a\u000a==Common misunderstanding of the concept==\u000a\u000aThis proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds [[significant]] [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}\u000a\u000a==Search engine algorithms==\u000aAnchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=\u000aHow the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>\u000a\u000a[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web\u000a|last=Fox\u000a|first=Vanessa\u000a|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html\u000a|title=Get a more complete picture about how other sites link to you\u000a|date=15 March 2007\u000a|publisher=Official Google Webmaster Central Blog\u000a|accessdate=2007-03-27\u000a| archiveurl= http://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\u000aIn the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.<ref>{{cite web\u000a|last=Cutts\u000a|first=Matt\u000a|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html\u000a|title=A quick word about Googlebombs\u000a|date=25 January 2007\u000a|publisher=Official Google Webmaster Central Blog\u000a|accessdate=2007-03-27\u000a| archiveurl= http://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\u000a\u000aIn April 2012, Google announced in its March "Penguin" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}</ref><ref>{{cite web|first=Simon|last=Dalley|accessdate=2012-04-04|date=4 April 2012|url=http://www.growtraffic.co.uk/google-changes-the-way-it-handles-anchor-text|title=Google Changes The way It Handles Anchor Text|publisher=Grow Traffic}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.\u000a.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=\u000aGoogle Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>\u000a\u000a==Anchor Text Terminology==\u000aThere are different classifications of anchor text that are used within the search engine optimization community such as the following:\u000a\u000a'''Exact Match:''' whenever an anchor is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.\u000a\u000a'''Branded:''' whenever a brand is used as the anchor. "[[Wikipedia]]" is a branded anchor text.\u000a\u000a'''Naked Link:''' whenever a URL is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.\u000a\u000a'''Generic:''' whenever a generic word or phrase is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.\u000a\u000a'''Images:''' whenever an image is linked, Google will use the "ALT" tag as the anchor text\u000a.<ref>{{cite web\u000a|last=Gotch\u000a|first=Nathan\u000a|url=http://www.gotchseo.com/anchor-text/\u000a|title=The Epic Guide to Anchor Text\u000a|date=26 October 2014}}</ref>\u000a\u000a==References==\u000a\u000a{{reflist|colwidth=30em}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Internet terminology]]\u000a[[Category:Search engine optimization]]\u000a[[Category:Hypertext]]
p165
asI79
(lp166
VQuery expansion
p167
aV'''Query expansion''' ('''QE''') is the process of reformulating a seed query to improve retrieval performance in [[information retrieval]] operations.<ref>{{cite journal\u000a | last = Vectomova | first = Olga |author2=Wang, Ying  | year = 2006\u000a | title = A study of the effect of term proximity on query expansion | journal = [[Journal of Information Science]]\u000a | volume = 32 | issue = 4 | pages = 324&ndash;333\u000a | doi = 10.1177/0165551506065787 | id =  | url = http://jis.sagepub.com/cgi/content/abstract/32/4/324\u000a | format = Abstract | accessdate = 2006-12-09\u000a }}</ref>\u000aIn the context of web [[search engine]]s, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of [[data]]) and expanding the search query to match additional documents.  Query expansion involves techniques such as:\u000a\u000a* Finding [[synonym]]s of words, and searching for the synonyms as well\u000a* Finding all the various [[Morphology (linguistics)|morphological]] forms of words by [[stemming]] each word in the [[search query]]\u000a* Fixing [[Typographical error|spelling errors]] and automatically searching for the corrected form or suggesting it in the results\u000a* Re-weighting the terms in the original query\u000a\u000aQuery expansion is a methodology studied in the field of [[computer science]], particularly within the realm of [[natural language processing]] and [[information retrieval]].\u000a\u000a== Precision and recall tradeoffs ==\u000a\u000aSearch engines invoke query expansion to increase the quality of user search results.  It is assumed that users do not always formulate search queries using the best terms. Best in this case may be because the database does not contain the user entered terms.  \u000a\u000aBy [[stemming]] a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total [[recall (information retrieval)|recall]]. This comes at the expense of reducing the [[precision (information retrieval)|precision]].  By expanding a search query to search for the synonyms of a user entered term, the recall is also increased at the expense of precision.  This is due to the nature of the equation of how precision is calculated, in that a larger recall implicitly causes a decrease in precision, given that factors of recall are part of the denominator. It is also inferred that a larger recall negatively impacts overall search result quality, given that many users do not want more results to comb through, regardless of the precision.\u000a\u000aThe goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.  At the same time, many of the current commercial search engines use word frequency ([[Tf-idf]]) to assist in ranking.  By ranking the occurrences of both the user entered words and synonyms and alternate morphological forms, documents with a higher density (high frequency and close proximity) tend to migrate higher up in the search results, leading to a higher quality of the search results near the top of the results, despite the larger recall.\u000a\u000aThis tradeoff is one of the defining problems in query expansion, regarding whether it is worthwhile to perform given the questionable effects on precision and recall. Critics{{Who|date=March 2009}} state one of the problems is that the dictionaries and [[thesauri]], and the stemming algorithm, are driven by human bias and while this is implicitly handled by the query expansion algorithm, this explicitly affects the results in a non-automated manner (similar to how statisticians can 'lie' with statistics){{Citation needed|date=July 2013}}. Other critics{{Who|date=March 2009}} point out potential for corporate influence on the dictionaries, promoting advertising of online web pages in the case of [[web search engine]]s. {{Citation needed|date=December 2007}}\u000a\u000a==See also==\u000a\u000a* [[Search engine]]\u000a* [[Search engine indexing]]\u000a* [[Information retrieval]]\u000a* [[Document retrieval]]\u000a* [[Linguistics]]\u000a* [[Natural language processing]]\u000a* [[Stemming]]\u000a* [[Morphology (linguistics)]]\u000a\u000a== Software libraries ==\u000a*[http://qtanalyzer.codeplex.com/ QueryTermAnalyzer] open-source, C#. Machine learning based query term weight and synonym analyzer for query expansion.\u000a*[http://lucene-qe.sourceforge.net/ LucQE] - open-source, Java.  Provides a framework along with several implementations that allow to perform query expansion with the use of Apache [[Lucene]].\u000a*[[Xapian]] is an open-source search library which includes support for query expansion\u000a\u000a== References ==\u000a\u000a* D. Abberley, D. Kirby, S. Renals, and T. Robinson, The THISL broadcast news  retrieval system. In ''Proc. ESCA ETRW Workshop Accessing Information in Spoken Audio'', (Cambridge), pp.&nbsp;14\u201319, 1999. Section on [http://homepages.inf.ed.ac.uk/srenals/pubs/1999/esca99-thisl/node6.html Query Expansion] - Concise, mathematical overview.\u000a* R. Navigli, P. Velardi. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42\u201349 - An analysis of query expansion methods relying on WordNet as the reference ontology.\u000a* Y. Qiu and H.P. Frei. [http://citeseer.ist.psu.edu/qiu93concept.html Concept Based Query Expansion]. In ''Proceedings of SIGIR-93, 16th ACM International Conference on Research and Development in Information Retrieval'', Pittsburgh, SIGIR Forum, ACM Press, June 1993 - Academic document on a specific method of query expansion\u000a* Efthimis N. Efthimiadis. [http://faculty.washington.edu/efthimis/pubs/Pubs/qe-arist/QE-arist.html Query Expansion]. In: Martha E. Williams (ed.), ''Annual Review of Information Systems and Technology (ARIST)'', v31, pp 121\u2013187, 1996 - An introduction for less-technical viewers.\u000a\u000a=== Notes ===\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Query Expansion}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p168
asI95
(lp169
VCross-language information retrieval
p170
aV{{refimprove|date=September 2014}}\u000a\u000a'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.  CLIR techniques can be classified into different categories based on different translation resources: \u000a* Dictionary-based CLIR techniques\u000a* Parallel corpora based CLIR techniques\u000a* Comparable corpora based CLIR techniques\u000a* Machine translator based CLIR techniques\u000a\u000aThe first workshop on CLIR was held in Zrich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).\u000a\u000aThe term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.\u000a\u000a==See also==\u000a*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)\u000a\u000a==References==\u000a<references />\u000a\u000a==External links==\u000a*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]\u000a\u000a{{DEFAULTSORT:Cross-Language Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a\u000a\u000a{{linguistics-stub}}
p171
asI224
(lp172
VCAB Direct (database)
p173
aV{{Redirect|Global Health|other uses|Global health}}\u000a{{italic title}}\u000a{{Infobox Bibliographic Database\u000a|title =CAB Abstracts \u000a|image = \u000a|caption = \u000a|producer =[[CABI (organisation)|CABI]]\u000a|country =United Kingdom \u000a|history =1973 to present \u000a|languages =Fifty languages, English abstracts \u000a|providers =Datastar, Dialog bluesheets, STN International, CAB Direct (CABI's own platform), Thomson-Reuters [[Web of Knowledge]], EBSCO, OvidSP, Dimdi \u000a|cost = \u000a|disciplines =applied life sciences - agriculture, environment, veterinary sciences, applied economics, food science and nutrition \u000a|depth =bibliographic, abstracting and indexing \u000a|formats =journal articles, abstracts, proceedings, books, book chapters, monographs, annual reports, handbooks, bulletins, newsletters, discussion papers, field notes, technical information, thesis papers  \u000a|temporal =1973-Present \u000a|geospatial =Global - international \u000a|number =6 million + \u000a|updates =\u000a|p_title = \u000a|p_dates = \u000a|ISSN =\u000a|web = \u000a|titles =http://www.cabi.org/default.aspx?site=170&page=1028  \u000a}}\u000a{{Infobox Bibliographic Database\u000a|title =Global Health bibliographic database \u000a|image = \u000a|caption = \u000a|producer = \u000a|country = \u000a|history = \u000a|languages = 50 languages (158 countries)\u000a|providers =CAB Direct, SilverPlatter, Web of Knowledge, EBSCO, OvidSP, Dialog, Dimdi \u000a|cost = \u000a|disciplines =international health research (medical and public)\u000a|depth =bibliographic, abstracting and indexing \u000a|formats =scientific journals, reports, books and conferences \u000a|temporal =1973 to present \u000a|geospatial =global-international \u000a|number =1.2 million scientific records \u000a|updates = \u000a|p_title = \u000a|p_dates = \u000a|ISSN =\u000a|web = \u000a|titles =  \u000a}}\u000a\u000a'''CAB Direct''' is a source of references for the ''[[life sciences|applied life sciences]]'' It incorporates two  bibliographic databases: '''''CAB Abstracts''''' and '''''Global Health'''''. CAB Direct is an access point for multiple [[bibliographic databases]] produced by ''CABI''. This database contains 8.8 million [[bibliographic record]]s, which includes  85,000 full text articles. It also includes noteworthy literature reviews. News articles and reports are also part of this combined database.<ref name=direct>{{cite web\u000a  | title =CAB Direct \u000a  | work = \u000a  | publisher =CABI  \u000a  | date =July 2010 \u000a  | url =http://www.cabdirect.org/ \u000a  | accessdate =2010-07-18}}</ref>\u000a\u000aIn the U.K., in 1947, the ''Imperial Agricultural Bureaux'' became the ''Commonwealth Agricultural Bureaux'' or ''CAB''. In 1986 the ''Commonwealth Agricultural Bureaux'' became ''[[CAB International]]'' or ''CABI''  <ref name=history-cabi>{{cite web\u000a  | title =Our history \u000a  | work =Bulleted history \u000a  | publisher =CABI  \u000a  | date =July 2010 \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1388 \u000a  | accessdate =2010-07-18}}</ref>\u000a\u000a==CAB Abstracts==\u000a'''CAB Abstracts''' is an applied life sciences bibliographic database emphasising [[agricultural]] literature, which is international in scope. It contains 6 million records, with coverage from 1973 to present day, adding 300,000 abstracts per year. Subject coverage includes [[agriculture]], [[environmental science|environment]], [[veterinary]] sciences, [[applied economics]], [[food science]] and nutrition. Database covers international issues in agriculture, [[forestry]], and allied disciplines in the life sciences. Indexed publications are from 150 countries in 50 languages, including English abstracts for most articles. Literature coverage includes journals, proceedings,  books, and a large collection of agricultural serials. Other non-journal formats are also indexed.<ref name=cabAb>\u000a{{cite web\u000a  | title =CAB Abstracts \u000a  | work = \u000a  | publisher =CABI  \u000a  | date =July 2010 \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=125 \u000a  | accessdate =2010-07-18}}</ref><ref name=TRcababse>{{cite web\u000a  | title =CAB Abstracts (Web of Knowledge) \u000a  | work = \u000a  | publisher =Thomson Reuters \u000a  | date =July 2010 \u000a  | url =http://science.thomsonreuters.com/training/cab/#overview \u000a  | accessdate =2010-07-18}}</ref><ref name=ovidCABabs>{{cite web\u000a  | title =CAB Abstracts \u000a  | work =Coverage is 1973-Present \u000a  | publisher =Ovid Technologies, Inc  \u000a  | date =December 2010 \u000a  | url =http://www.ovid.com/site/catalog/DataBase/31.jsp?top=2&mid=3&bottom=7&subsection=10  \u000a  | accessdate =2010-12-10}}</ref> \u000a===CAB Abstracts Archive===\u000a'''CAB Abstracts Archive''' is a searchable database produced by ''CABI''. It is created from 600 volumes of printed abstracts,  which are the collected and published [[scientific research]] from 1910 to 1972, and then digitized to form the archive. This archive database contains more than 1.8 million records which covers agriculture, [[veterinary]] science, nutrition and the environment. Subject coverage also includes [[biodiversity]], [[pest control]], [[environmental pollution]], [[animal disease]] (including [[zoonotic disease]]s), [[nutrition]], and [[food production]]. [[Natural resource management]] includes plant and [[animal breeding]]. CAB Abstracts Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].\u000a\u000aThe following print journals (digitized) comprise CAB Abstracts Archive:\u000a                                                \u000a:Animal Breeding Abstracts, Dairy Science Abstracts, Field Crop Abstracts, \u000a:Forestry Abstracts, Horticultural Science Abstracts, Nematological Abstracts, \u000a:Nutrition Abstracts and Reviews Series A: Human and Experimental, \u000a:Nutrition Abstracts and Reviews Series B: Livestock Feeds and Feeding,  \u000a:Plant Breeding Abstracts, Review of Agricultural Entomology, \u000a:Review of Medical and Veterinary Mycology, Review of Plant Pathology, \u000a:Review of Medical and Veterinary Entomology, Review of Plant Pathology, \u000a:Soils and Fertilizers, Tropical Veterinary Bulletin, Veterinary Bulletin  \u000a:and Weed Abstracts.\u000a\u000a===Weed Abstracts===\u000a'''''Weed Abstracts''''', derived from CAB Abstracts, is an abstracts database focused on [[scientific journal|published research]] regarding [[weed]]s and [[herbicides]]. This includes [[plant biology|weed biology]], encompassing [[research|research areas]] from [[genetics]] to [[ecology]], including [[parasitic]], [[poisonous]], [[allergenic]] and [[aquatic plant|aquatic]] weeds. Further coverage includes all topics related to [[weed control]], in both [[farming|crop]] and non-crop situations. Research on herbicides, includes formulations, [[herbicide resistance]] and the effects of [[herbicide residues]] in the environment. 10,000 records are add to this database per year. \u000a\u000a'''''Weed Abstracts''''' is updated weekly with summaries from notable English and foreign language journal articles, reports, conferences and books about weeds and herbicides. With the back-file, coverage is from 1990 to present day bringing the total of available research summaries to 130,000 records.<ref name=weedAb>{{cite web\u000a  | title =Weed Abstracts \u000a  | work = \u000a  | publisher =CABI  \u000a  | date =July 2010 \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2203\u000a  | accessdate =2010-07-20}}</ref>\u000a\u000a==Global Health database==\u000a'''''Global Health''''' is a bibliographic database which focuses on [[scientific literature|research literature]] in [[public health]] and [[Health science|medical health]] science sectors (including practice). Information (see infobox above) in indexed in more than 5000 [[academic journals]], and indexed from other sources such as reports, books and conferences.  Global Health contains over 1.2 million [[scientific]] records from 1973 to the present, with an addition of  90,000 indexed and abstracted records per year. Sources are abstracted from publications in 158 countries written in 50 languages. Any relevant non-English-language papers are translated into English. Proceedings, patents, thesis papers, electronic publications and relevant but difficult-to-find literature sources are also part of this database.<ref name=glbl-hlth-cabi>{{cite web\u000a  | title =Global Health overview \u000a  | work = \u000a  | publisher =CABI  \u000a  | date =July 2010 \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=328 \u000a  | accessdate =2010-07-18}}</ref><ref name=TRglblhlth>{{cite web\u000a  | title =Global Health (Web of Knowledge) \u000a  | work = \u000a  | publisher =Thomson Reuters \u000a  | date =July 2010 \u000a  | url =http://thomsonreuters.com/content/PDF/scientific/globalhealth_fs.pdf \u000a  | format =Free PDF download \u000a  | accessdate =2010-07-18}}</ref><ref name=ovidGH>{{cite web\u000a  | title =Global Health (Ovid) \u000a  | work = \u000a  | publisher =Ovid Technologies Inc. \u000a  | date =July 2010 \u000a  | url =http://www.ovid.com/site/catalog/DataBase/30.jsp?top=2&mid=3&bottom=7&subsection=10 \u000a  | accessdate =2010-07-18}}</ref> \u000a\u000a===Global Health Archive===\u000a'''''Global Health Archive''''' is a searchable database produced by CABI. It is created from 800,000 records, from six printed abstract journals,  which are collected published scientific research from 1910 to 1972, digitized to form the archive. Global Health Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].<ref name=ghArchive/>\u000a\u000aWhen combined with the ''Global Health'' database indexing coverage can be from 1910 to present day. Hence, coverage is made up of past [[epidemics]], from rates and patterns of disease [[Transmission (medicine)|transmission]], duration of [[pandemics]], timing of epidemiological peaks, [[geographic distribution]] of diseases, and [[World Health Organization|government preparedness]] and [[quarantine]] provisions.  The following can also be taken  into account:  effects on different age and [[social groups]], severity in developing vs. developed countries, [[symptoms]], causes of [[Human|mortality]] - such as secondary problems like [[pneumonia]] - and mortality rates.<ref name=ghArchive>{{cite web\u000a  | title =Global Health Archive \u000a  | work = \u000a  | publisher =CABI \u000a  | date =March 2010  \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2221 \u000a  | accessdate =2010-07-18}}</ref><ref name=ovidGHA>{{cite web\u000a  | title =Global Health Archive (Ovid)\u000a  | work = \u000a  | publisher =Ovid Technologies Inc. \u000a  | date =July 2010 \u000a  | url =http://www.ovid.com/site/catalog/DataBase/1748.jsp?top=2&mid=3&bottom=7&subsection=10 \u000a  | accessdate =2010-07-18}}</ref> \u000a\u000a====Journal and topic coverage====\u000aRecords for this database are derived from the following journals throughout certain years:<ref name=ghArchive/><ref name=ovidGHA/>\u000a\u000a:Tropical Diseases Bulletin (1912-83),\u000a:Abstracts on Hygiene and Communicable Diseases (1926-83), \u000a:Review of Veterinary and Medical Entomology (1913-72), \u000a:Review of Veterinary and Medical Mycology (1943-72) \u000a:Nutrition Abstracts and Reviews (1931-72), and Helminthological Abstracts (1932-72).\u000a\u000aSubject coverage includes [[Public health]], [[tropical disease|Tropical]] and [[Communicable disease]]s, Nutrition, [[Parasitology]], [[Entomology]], and [[Mycology]].\u000a\u000a===Tropical Diseases Bulletin===\u000a'''''Tropical Diseases Bulletin''''' is a bibliographic and abstracts database which focuses on research published regarding [[infectious disease]]s and [[public health]] in [[developing countries]] and the [[tropics]] and [[subtropics]]. This includes research areas from [[epidemiology]] to [[diagnosis]], [[therapy]] to [[disease prevention]], [[tropical medicine]], and related aspects of [[travel medicine]]. Published research coverage on [[patients]] and populations encompasses the health of marginalized populations: [[immigrant]]s, [[refugee]]s, and [[indigenous peoples]].<ref name=tropical/>\u000a\u000aBack-file coverage is from 1990 to present day, with an accessible base of 195,000 abstracts and the addition of 11,000 records per year. As a monthly journal '''''Tropical Diseases Bulletin''''' is also available in print. This print journal has author, subject and serials cited indexes.  Coverage of the print back-file is to 1912. A searchable, electronic database version of this journal is part of the ''Global Health Archive'' (see above).<ref name=tropical>\u000a{{cite web\u000a  | title =Tropical Diseases Bulletin\u000a  | work = \u000a  | publisher =CABI \u000a  | year =2010  \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2201\u000a  | accessdate =2010-07-18}}</ref>\u000a\u000a==Organic Research Database==\u000aThis indexing database focuses on scientific literature pertaining to all topics in  [[organic farming]], in both the [[temperate zone|temperate]] and [[tropical zone]]s. This includes [[sustainability|sustainability issues]] and [[soil science|soil fertility]]. Coverage is global; literature is obtained from 125 countries. The temporal coverage spans 30 years, 180,000 organic research abstracts, along with the addition of 8000 records per year. Linking to full text articles, guided searches, broad subject categorization along with subject refinement are also provided. The editorial advisory board of this database also commission reviews pertaining to organic farming.<ref name=organic>\u000a{{cite web\u000a  | title =Organic Research Database\u000a  | work =Description and bibliographic information \u000a  | publisher =CABI \u000a  | year =2011  \u000a  | url =http://www.cabi.org/organicresearch/default.aspx?site=154&page=932\u000a  | accessdate =2011-01-03}}</ref><ref name=usda>\u000a{{cite web\u000a  | title =Primary Research and Literature Databases\u000a  | work = focus on sustainable and alternative agricultural topics\u000a  | publisher =[[USDA]] - [[National Agriculture Library]] - [[AFSIC]] \u000a  | year =2011  \u000a  | url =http://afsic.nal.usda.gov/nal_display/index.php?info_center=2&tax_level=2&tax_subject=288&level3_id=0&level4_id=0&level5_id=0&topic_id=1597&&placement_default=0\u000a  | accessdate =2011-01-03}}</ref>\u000a\u000a==CABI full text repository==\u000a'''''CABI full text repository''''' is integrated into all ''CABI databases'' including CAB Abstracts, and Global Health. Both of these are online and print journals. Coverage includes 70,000 full text articles, through agreements with third party publishers. Eighty percent of the content is exclusive to CABI.<ref name=full-text/>  \u000a\u000aThe full text repository is made up of fifty percent journal articles, and equal percentage of conference (proceeding) papers, and other accessible literature is also included. Eighty percent of the articles are in English and coverage includes 56 countries. Also included in this database are relevant but hard to find materials which crosses disciplines consisting of [[agriculture]], [[health sciences|health]] and the [[life sciences]]. Main stream literature and hard to find materials of equal relevance are given equal access.<ref name=full-text>{{cite web\u000a  | title =CABI full text \u000a  | work = \u000a  | publisher =CABI \u000a  | date =March 2010  \u000a  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2227 \u000a  | accessdate =2010-07-18}}</ref>\u000a\u000a''CABI full text repository'' is indexed in other databases, which also serve as access points, consisting of ''Web of Knowledge (Thomson Reuters)'', ''CAB Direct'', ''OvidSP, Dialog, Dimdi, and EBSCOhost''.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Bibliographic database providers]]\u000a[[Category:Bibliographic indexes]]\u000a[[Category:Citation indices]]\u000a[[Category:Environmental science]]\u000a[[Category:Global health]]
p174
asI80
(lp175
VBinary Independence Model
p176
aV{{context|date=June 2012}}\u000aThe '''Binary Independence Model''' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.\u000a\u000a==Definitions==\u000aThe Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\u000aThe representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x<sub>1</sub>, ..., x<sub>m</sub>)'' where ''x<sub>t</sub>=1'' if term ''t'' is present in the document ''d'' and ''x<sub>t</sub>=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\u000a"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.\u000a\u000aThe probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:\u000a\u000a<math>P(R|x,q) = \u005cfrac{P(x|R,q)*P(R|q)}{P(x|q)}</math>\u000a\u000awhere ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.\u000aThe exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.\u000a\u000a''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\u000aSince a document is either relevant or nonrelevant to a query we have that:\u000a\u000a<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>\u000a\u000a=== Query Terms Weighting ===\u000aGiven a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the\u000aterms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \u005cfrac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight\u000aof term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Sprck Jones|Sprck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.\u000a\u000aThe Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Sprck Jones.<ref name="jones77"/>\u000a\u000a== See also ==\u000a\u000a* [[Bag of words model]]\u000a\u000a==Further reading==\u000a* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | coauthors=Prabhakar Raghavan & Hinrich Schtze | publisher=Cambridge University Press | year=2008}}\u000a* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | coauthors=Charles L. A. Clarke & Gordon V. Cormack | publisher=MIT Press | year=2010}}\u000a\u000a==References==\u000a{{Reflist|refs=\u000a<ref name="cyu76">{{cite doi | 10.1145/321921.321930 }}</ref>\u000a<ref name="jones77">{{cite doi | 10.1002/asi.4630270302 }}</ref> \u000a}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]
p177
asI226
(lp178
VSCImago Journal Rank
p179
aV'''SCImago Journal Rank''' (SJR indicator) is a measure of scientific influence of [[academic journal|scholarly journal]]s that accounts for both the number of [[citation]]s received by a journal and the importance or prestige of the journals where such citations come from. The SJR indicator is a variant of the [[centrality|eigenvector centrality measure]] used in network theory. Such measures establish the importance of a node in a network based on the principle that connections to high-scoring nodes contribute more to the score of the node. The SJR indicator, which is inspired by the [[PageRank]] algorithm, has been developed to be used in extremely large and heterogeneous journal citation networks. It is a size-independent indicator and its values order journals by their "average prestige per article" and can be used for journal comparisons in science evaluation processes.\u000a\u000aThe ''SJR indicator'' is a free journal metric which uses an algorithm similar to [[PageRank]] and provides an alternative to the [[impact factor]] (IF), which is based on data from the [[Science Citation Index]].<ref>{{cite journal | url = http://www.nature.com/news/2008/080102/full/451006a.html | title= Free journal-ranking tool enters citation market | journal = [[Nature (journal)|Nature]] | date= 2 January 2008 | volume= 451 | issue= 6 | doi= 10.1038/451006a | author = Declan Butler | pmid= 18172465 | pages= 6 |accessdate=14 May 2010}}</ref><ref>{{cite journal | url = http://www.fasebj.org/cgi/content/short/22/8/2623 | title = Comparison of SCImago journal rank indicator with journal impact factor | author= Matthew E. Falagas et al | doi = 10.1096/fj.08-107938 | journal = [[The FASEB Journal]] | year = 2008 | issue = 22 | pages = 2623\u20132628 | pmid = 18408168 | volume = 22 }}</ref> Average citations per document in a 2-year period, abbreviated as Cites per Doc. (2y), is another index that measures the scientific impact of an average article published in the journal. It is computed using the same formula that journal [[impact factor]] ([[Thomson Reuters]]).\u000a\u000a== Rationale ==\u000aIf scientific impact is considered related to the number of endorsements, in the form of citations, a journal receives, then prestige can be understood as a combination of the number of endorsements and the prestige or importance of the journals issuing them. The ''SJR indicator'' assigns different values to citations depending on the importance of the journals where they come from. This way, citations coming from highly important journals will be more valuable and hence will provide more prestige to the journals receiving them. The calculation of the ''SJR indicator'' is very similar to the ''[[Eigenfactor]] score'', with the former being based on the [[Scopus]] database and the latter on the ISI [[Web of Science]] database.<ref>{{cite web | title=SCImago Journal & Country Rank (SJR) as an alternative to Thomson Reuters's Impact Factor and EigenFactor | url=http://www.scimagojr.com/news.php?id=41 | date=21 Aug 2008 | accessdate=20 September 2012}}</ref>\u000a\u000a== Computation ==\u000aThe SJR indicator computation is carried out using an iterative [[algorithm]] that distributes prestige values among the journals until a steady-state solution is reached. The SJR algorithm begins by setting an identical amount of prestige to each journal, then using an iterative procedure, this prestige is redistributed in a process where journals transfer their achieved prestige to each other through citations. The process ends up when the difference between journal prestige values in consecutive iterations do not reach a minimum threshold value any more. The process is developed in two phases, (a) the computation of ''Prestige SJR'' (''PSJR'') for each journal: a size-dependent measure that reflects the whole journal prestige, and (b) the normalization of this measure to achieve a size-independent measure of prestige, the ''SJR indicator''.\u000a\u000a== See also ==\u000a* [[Journal Citation Reports]]\u000a* [[Citation index]]\u000a* [[Eigenfactor]]\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* {{official website|http://www.scimagojr.com/}}\u000a* [http://blogs.openaccesscentral.com/blogs/bmcblog/entry/scimago_a_new_source_of SCImago \u2013 a new source of journal metrics offering a wealth of free data on open access journals]\u000a* [http://www.earlham.edu/~peters/fos/2008/01/more-on-scimago-journal-rank-v-impact.html More on SCImago Journal Rank v. Impact Factors]\u000a\u000a{{DEFAULTSORT:Scimago journal rank}}\u000a[[Category:Citation indices]]\u000a[[Category:Academic publishing]]
p180
asI100
(lp181
VLegal information retrieval
p182
aV'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].  \u000a\u000a== Overview ==\u000a\u000aIn a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref> \u000a\u000aLegal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change. \u000a\u000aTechniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.\u000a\u000a== Problems ==\u000a\u000aApplication of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>\u000a\u000aLegal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref> \u000a\u000a#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.\u000a#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;\u000a#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;\u000a#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;\u000a\u000aIn addition, it also has the common meaning: \u000a<ol start="5">\u000a<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>\u000a</ol>\u000a\u000aThough the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. \u000a\u000aEven if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.\u000a\u000aAdditionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />\u000a\u000aOvercoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>\u000a\u000a== Techniques ==\u000a\u000a===Boolean searches===\u000a\u000a[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above. \u000a\u000aThe recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>\u000a\u000a===Manual classification===\u000a\u000aIn order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s \u201cNatural Language\u201d<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers<ref name=WL /> or Lexis' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>\u000a\u000aThese systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>\u000a\u000aThe major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>\u000a\u000a===Natural language processing===\u000a\u000aIn order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, \u201cSMILE,\u201d which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>\u000a\u000aDespite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>\u000a\u000a== Notes ==\u000a{{Reflist|2}}\u000a\u000a==References==\u000a{{Refbegin}}\u000a*{{cite journal\u000a|author     = Maxwell, K.T., and Schafer, B.\u000a|year       = 2008\u000a|title      = Concept and Context in Legal Information Retrieval\u000a|url        = http://portal.acm.org/citation.cfm?id=1564016\u000a|journal    = Frontiers in Artificial Intelligence and Applications\u000a|volume     = 189\u000a|pages      = 63\u201372\u000a|publisher  = IOS Press\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Jackson, P. et al.\u000a|year       = 1998\u000a|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation\u000a|url        = http://portal.acm.org/citation.cfm?id=288627.288642\u000a|journal    = Conference on Information and Knowledge Management\u000a|pages      = 60\u201367\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Blair, D.C., and Maron, M.E.\u000a|year       = 1985\u000a|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval\u000a|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997\u000a|journal    = Communications of the ACM\u000a|volume     = 28\u000a|issue      = 3 \u000a|pages      = 289\u2013299\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a|doi=10.1145/3166.3197\u000a}}\u000a*{{cite journal\u000a|author     = Peters, W. et al.\u000a|year       = 2007\u000a|title      = The structuring of legal knowledge in LOIS\u000a|url        = http://www.springerlink.com/content/d04l7h2507700g45/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 15\u000a|issue      = 2\u000a|pages      = 117\u2013135\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-007-9034-4\u000a}}\u000a*{{cite journal\u000a|author     = Saravanan, M. et al.\u000a|year       = 2007\u000a|title      = Improving legal information retrieval using an ontological framework \u000a|url        = http://www.springerlink.com/content/h66412k08h855626/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 17\u000a|issue      = 2\u000a|pages      = 101\u2013124\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-009-9075-y\u000a}}\u000a*{{cite journal\u000a|author     = Schweighofer, E. and Liebwald, D.\u000a|year       = 2007\u000a|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary\u000a|url        = http://www.springerlink.com/content/v62v7131x10413v0/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 15\u000a|issue      = 2\u000a|pages      = 103\u2013115\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-007-9029-1\u000a}}\u000a*{{cite journal\u000a|author     = Gelbart, D. and Smith, J.C.\u000a|year       = 1993\u000a|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management\u000a|url        = http://portal.acm.org/citation.cfm?id=158994\u000a|journal    = International Conference on Artificial Intelligence and Law\u000a|pages      = 142\u2013151\u000a|publisher  = ACM\u000a|accessdate = 2009-11-07\u000a}}\u000a*{{cite journal\u000a|author     = Ashley, K.D. and Bruninghaus, S.\u000a|year       = 2009\u000a|title      = Automatically classifying case texts and predicting outcomes\u000a|url        = http://www.springerlink.com/content/lhg8837331hgu024/\u000a|journal    = Artificial Intelligence and Law\u000a|volume     = 17\u000a|issue      = 2\u000a|pages      = 125\u2013165\u000a|publisher  = Springer Netherlands\u000a|accessdate = 2009-11-07\u000a|doi=10.1007/s10506-009-9077-9\u000a}}\u000a{{Refend}}\u000a\u000a{{DEFAULTSORT:Legal Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Legal research]]
p183
asI230
(lp184
VWeb of Science
p185
aV{{Merge from|Web of Knowledge|date=December 2014|discuss=Talk:Web of Science#Merge}}\u000a{{Infobox Bibliographic Database\u000a|title = Web of Science\u000a|image = [[File:Web of science next generation.png|thumb|350px|Web of Science]]\u000a|caption = \u000a|producer = Thomson Reuters \u000a|country = United States \u000a|history = \u000a|languages = \u000a|providers = Various institutions and commercial organizations \u000a|cost = \u000a|disciplines = Science, social science, arts, humanities (supports 256 disciplines) \u000a|depth = citation indexing,  author, topic title, subject keywords, abstract,  periodical title, author's address, publication year \u000a|formats = full text articles, reviews, editorials, chronologies, abstracts, proceedings (journals and book-based ), technical papers \u000a|temporal = 1900 to present \u000a|geospatial = \u000a|number = 90 million + \u000a|updates = \u000a|p_title = \u000a|p_dates = \u000a|ISSN =\u000a|web =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science \u000a|titles =http://thomsonreuters.com/content/science/pdf/Web_of_Science_factsheet.pdf\u000a}}\u000a'''Web of Science''' (WoS, previously known as [[Web of Knowledge]]) is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] that provides a comprehensive citation search. It gives access to multiple databases that reference cross-disciplinary research, which allows for in-depth exploration of specialized sub-fields within an [[academic discipline|academic or scientific discipline]].<ref>Drake, Miriam A. Encyclopedia of Library and Information Science. New York, N.Y.: Marcel Dekker, 2004.</ref>\u000a\u000a==Background==\u000aA citation index is built on the fact that citations in science serve as linkages between similar research items, and lead to matching or related scientific literature, such as [[academic journal|journal articles]], [[conference proceedings]], abstracts, etc. In addition, literature which shows the greatest impact in a particular field, or more than one discipline, can be easily located through a citation index. For example, a paper's influence can be determined by linking to all the papers that have cited it. In this way, current trends, patterns, and emerging fields of research can be assessed. [[Eugene Garfield]], the "father of citation indexing of academic literature,"<ref>Jacso, Peter. The impact of Eugene Garfield through the prizm of Web of Science. Annals of Library and Information Studies, Vol. 57, September 2010, P. 222. [http://nopr.niscair.res.in/bitstream/123456789/10235/4/ALIS%2057%283%29%20222-247.pdf PDF]</ref> who launched the [[Science Citation Index]] (SCI), which in turn led to the Web of Science,<ref>Garfield, Eugene, Blaise Cronin, and Helen Barsky Atkins. The Web of Knowledge: A Festschrift in Honor of Eugene Garfield. Medford, N.J.: Information Today, 2000.</ref> wrote: \u000a\u000a{{Quote|Citations are the formal, explicit linkages between papers that have particular points in common. A citation index is built around these linkages. It lists publications that have been cited and identifies the sources of the citations. Anyone conducting a literature search can find from one to dozens of additional papers on a subject just by knowing one that has been cited. And every paper that is found provides a list of new citations with which to continue the search.\u000aThe simplicity of citation indexing is one of its main strengths. <ref>Garfield, Garfield, Eugene. Citation indexing: Its theory and application in science, technology, and humanities. New York: Wiley, 1979, P. 1. [http://garfield.library.upenn.edu/ci/chapter1.PDF PDF]</ref>}}\u000a\u000a==Coverage==\u000a[[File:Web_of_Science_Core_Collection.png|thumb|200px|Accessing the Web of Science via the [[Web of Knowledge]]]]\u000aExpanding the coverage of Web of Science, in November 2009 Thomson Reuters introduced ''Century of Social Sciences''. This service contains files which trace social science research back to the beginning of the 20th century,<ref name=InfoToNov2009>"''Thomson Reuters introduces century of social sciences''". Information Today 26.10 (2009): 10. General OneFile. Web. 23 June 2010.  [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211794482&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0  Document URL].</ref><ref name=ComLibNov2009>Thomson Reuters introduces century of social sciences." Computers in Libraries 29.10 (2009): 47. General OneFile. Internet. 23 June 2010. [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211236981&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0 Document URL]</ref> and Web of Science now has indexing coverage from the year 1900 to the present.<ref name=oview>\u000a{{Cite web |title =Overview - Web of Science| publisher =Thomson Reuters| year = 2010\u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\u000a  | format =Overview of coverage gleaned from promotional language.  \u000a  | accessdate =2010-06-23}}</ref><ref name=UoOL>\u000a{{Cite web| last = Lee| first =Sul H.| title =Citation Indexing and ISI's Web of Science \u000a  | publisher =The University of Oklahoma Libraries| year =2010\u000a  | url =http://www.ou.edu/webhelp/librarydemos/isi/ | format =Discussion of finding literature manually. Description of [[citation index]]ing, and Web of Science.| accessdate =2010-06-23}}</ref> The multidisciplinary coverage of the Web of Science encompasses over 50,000 scholarly books, 12,000 journals and 160,000 conference proceedings<ref name="Web of Science">[http://wokinfo.com/citationconnection/realfacts/#regional Web of Science. Thomson Reuters, 2014]</ref> (as of September 3, 2014). The selection is made on the basis of [[impact factor|impact evaluations]] and comprise [[open-access journal]]s, spanning multiple [[academic discipline]]s. The coverage includes: the [[science]]s, [[social science]]s, [[the arts|arts]], and humanities, and goes across disciplines.<ref name=oview/><ref name=facts/> However, Web of Science does not index all journals, and its coverage in some fields is less complete than in others.\u000a\u000aFurthermore, as of September 3, 2014 the total file count of the Web of Science was 90 million records, which included over a billion cited references. This citation service on average indexes around 65 million items per year, and it is described as the largest accessible citation database.<ref name=facts>[http://wokinfo.com/citationconnection/  Bulleted fact sheet]. Thomson Reuters. 2014.</ref>\u000a\u000aTitles of foreign-language publications are translated into English and so cannot be found by searches in the original language.<ref name=harvard-search>{{Cite web\u000a  |title =Some Searching Conventions\u000a  | publisher =President and Fellows of Harvard College   | date = December 3, 2009   | url =http://hcl.harvard.edu/research/guides/citationindex/#some   | format =    | accessdate =2010-06-23}}</ref>\u000a\u000a==Citation databases==\u000aWeb of Science consist of seven online databases:<ref name=included/><ref name="Web of Science">[http://wokinfo.com/media/pdf/WoSFS_08_7050.pdf Jo Yong-Hak. Web of Science. Thomson Reuters, 2013]</ref>\u000a*[[Conference Proceedings Citation Index]] covers more than 160,000 conference titles in the Sciences starting from 1990 to the present day\u000a*[[Science Citation Index Expanded]] covers more than 8,500 notable journals encompassing 150 disciplines. Coverage is from the year 1900 to the present day.\u000a*[[Social Sciences Citation Index]] covers more than 3,000 journals in social science disciplines.  Range of coverage is from the year 1900 to the present day.\u000a*[[Arts & Humanities Citation Index]] covers more than  1,700 arts and humanities journals starting from 1975. In addition, 250 major scientific and social sciences journals are also covered. \u000a*[[Index Chemicus]] lists more than 2.6 million compounds. The time of coverage is from 1993 to present day.\u000a*[[Current Chemical Reactions]] indexes over one million reactions, and the range of coverage is from 1986 to present day. The '' INPI '' archives from 1840 to 1985 are also indexed in this database.\u000a*[[Book Citation Index]] covers more than 60,000 editorially selected books starting from 2005.\u000a\u000a=== Contents ===\u000aThe seven [[citation index|citation indices]] listed above contain references which have been cited by other articles. One may use them to undertake cited reference search, that is, locating articles that cite an earlier, or current publication. One may search citation databases by topic, by author, by source title, and by location. Two chemistry databases,  ''Index Chemicus'' and  ''Current Chemical Reactions'' allow for the creation of structure drawings, thus enabling users to locate [[chemical compound]]s and reactions. Institutions such as universities and research departments generally access the Web of Science through the [[Web of Knowledge]] platform. (An example of a typical search.<ref>[http://cires.colorado.edu/~jjose/P-Cited/DeCarlo06_ISI.pdf A typical Web of Science search example.]</ref>)\u000a\u000a===Abstracting and indexing===\u000aThe following  types of literature are indexed: scholarly books, [[peer review]]ed journals, original research articles, reviews, editorials, chronologies, abstracts, as well as other items. Disciplines included in this index are  [[agriculture]], [[biological sciences]], [[engineering]], medical and [[life sciences]], [[physics|physical]] and [[chemical sciences]], [[anthropology]], law, [[library science]]s, [[architecture]], dance, music, film, and theater. Seven citation databases encompasses coverage of the above disciplines.<ref name=UoOL/><ref name=included>\u000a{{Cite web |title =Coverage - Web of Science| publisher =Thomson Reuters| year = 2010\u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\u000a  | format =Overview of coverage gleaned from promotional language.  \u000a  | accessdate =2010-06-23}}</ref><ref name="Web of Science" />\u000a\u000a==Limitations in the use of citation analysis==\u000aAs with other scientific approaches, scientometrics and bibliometrics have their own limitations. Recently, a criticism was voiced pointing toward certain deficiencies of the journal impact factor (JIF) calculation process, based on Thomson Reuters Web of Science, such as: journal citation distributions usually are highly skewed towards established journals; journal impact factor properties are field-specific and can be easily manipulated by editors, or even by changing the editorial policies; this makes the entire process essentially nontransparent.<ref name="Declaration">[http://am.ascb.org/dora/ San Francisco Declaration on Research Assessment: Putting science into the assessment of research, December 16, 2012]</ref>\u000a\u000aRegarding the more objective journal metrics, there is a growing view that for greater accuracy it must be supplemented with an article-based assessment and peer-review.<ref name="Declaration" /> Thomson Reuters replied to criticism in general terms by stating that "no one metric can fully capture the complex contributions scholars make to their disciplines, and many forms of scholarly achievement should be considered."<ref>Thomson Reuters Statement Regarding the San Francisco Declaration on Research Assessment [http://researchanalytics.thomsonreuters.com/]</ref>\u000a\u000a== See also ==\u000a\u000a{{Div col|3}}\u000a*[[List of academic journal search engines]]\u000a*[[CSA (database company)|CSA databases]]\u000a*[[Dialog (online database)]]\u000a*[[Energy Citations Database]]\u000a*[[Energy Science and Technology Database]]\u000a*[[ETDEWEB]]\u000a*[[Geographic Names Information System]]\u000a*[[Materials Science Citation Index]]\u000a*[[PASCAL (database)|PASCAL database]]\u000a* [[PubMed Central]]\u000a* [[SciELO]]\u000a* [[VINITI Database RAS]]\u000a* [[Web development tools]]\u000a{{Div col end}}\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* [http://scientific.thomson.com/products/wos/ Web of Science]\u000a* [http://www.webofknowledge.com/ Web of Knowledge]\u000a* [http://web.archive.org/web/20110521161422/http://hcl.harvard.edu/research/guides/citationindex/ Searching the Citation Indexes (Web of Science)] Harvard College Library. 2010. (archive)\u000a* [http://video.mit.edu/watch/web-of-science-12339/ MIT Web of Science video tutorial]. 2008.\u000a\u000a{{Thomson Reuters}}\u000a{{DEFAULTSORT:Web Of Science}}\u000a[[Category:Bibliographic databases]]\u000a[[Category:Full text scholarly online databases]]\u000a[[Category:Thomson family]]\u000a[[Category:Thomson Reuters]]\u000a[[Category:Citation indices]]\u000a[[Category:Scholarly search services]]
p186
asI103
(lp187
VExploratory search
p188
aV'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are either:<nowiki>[3]</nowiki>\u000a* a) unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)\u000a* b) unsure about the ways to achieve their goals (either the technology or the process)\u000a* c) or even unsure about their goals in the first place.\u000a\u000aConsequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.\u000a\u000a==History==\u000aExploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?" Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.\u000a\u000aIn the last few years, a series of workshops have been held at various related and key events. In 2005, the [http://research.microsoft.com/~ryenw/xsi/index.html Exploratory Search Interfaces workshop] focused on beginning to define some of the key challenges in the field. Since then a series of other workshops have been held at related conferences: [http://research.microsoft.com/~ryenw/eess/index.html Evaluating Exploratory Search] at [http://www.sigir2006.org SIGIR06] and [http://research.microsoft.com/~ryenw/esi/index.html Exploratory Search and HCI] at [http://www.chi2007.org CHI07] (in order to meet with the experts in [[human\u2013computer interaction]]).\u000a\u000aIn March 2008, an [http://www.sciencedirect.com/science/journal/03064573 ''Information Processing and Management'' special issue]<nowiki>[2]</nowiki> has focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.\u000a\u000aIn June 2008, the [[National Science Foundation]] sponsored an [http://www.ils.unc.edu/ISSS_workshop/ invitational workshop] to identify a research agenda for exploratory search and similar fields for the coming years.\u000a\u000a==Research challenges==\u000a\u000a===Important scenarios===\u000aWith the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by [http://mspace.fm mSpace] states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.\u000a\u000a===Designing new interfaces===\u000aWith one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the user, so that they can choose from a list instead of guess a possible keyword query.\u000a\u000aMany of the [[human\u2013computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.\u000a\u000aComputational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). Facilitating exploratory search by model-based navigational cues. In Proceedings of the ACM International conference on Intelligent User Interface. 199-208.  http://portal.acm.org/citation.cfm?id=1719970.1719998</ref>\u000a\u000a===Evaluating interfaces===\u000aAs the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.\u000a\u000a===Models of exploratory search behavior===\u000aThere has been recent attempts to develop process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation\u000a  | doi = 10.1145/1460563.1460600\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | title = The Microstructures of Social Tagging: A Rational Model\u000a  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work. \u000a  | pages = 66\u201372\u000a  | date = April 2008\u000a  | url = http://portal.acm.org/citation.cfm?id=1460600\u000a  | isbn = 978-1-60558-007-4 }}\u000a</ref>\u000a.<ref>{{Citation\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | title = A Semantic Imitation Model of Social Tagging\u000a  | journal = Proceedings of the IEEE conference on Social Computing\u000a  | pages = 66\u201372\u000a  | date = Aug 2009\u000a  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>\u000a{{Citation\u000a  | last1 = Fu  | first1 = Wai-Tat\u000a  | last2 = Pirolli  | first2 = Peter\u000a  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web\u000a  | journal = Human-Computer Interaction\u000a  | pages = 335\u2013412\u000a  | year = 2007\u000a  | url = http://portal.acm.org/citation.cfm?id=1466608\u000a  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based\u000amodel of Web navigation and its application to Web usability analysis. In S. Mc-\u000aDonald, Y. Waern, & G. Cockton (Eds.), People and computers XIV\u2014Usability or else!\u000aNew York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications\u000afor information architecture. Human Computer Interaction, 19, 225\u2013271.</ref>\u000aRecent development in exploratory search is often concentrated in predicting user's search intents in interaction with the user.<ref>\u000a{{Citation\u000a  | last1 = Ruotsalo  | first1 = Tuukka\u000a  | last2 = Athukorala  | first2 = Kumaripaba\u000a  | last3 = Glowacka  | first3 = Dorota\u000a  | last4 = Konuyshkova  | first4 = Ksenia\u000a  | last5 = Oulasvrita  | first5 = Antti\u000a  | last6 = Kaipiainen  | first6 = Samuli\u000a  | last7 = Kaski  | first7 = Samuel\u000a  | last8 = Jacucci  | first8 = Giulio\u000a  | title = Supporting exploratory search tasks with interactive user modeling\u000a  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T\u000a  | year = 2013}}\u000a</ref>\u000aSuch predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs\u000a<ref>\u000a{{Citation\u000a  | last1 = Ruotsalo  | first1 = Tuukka\u000a  | last2 = Peltonen  | first2 = Jaakko\u000a  | last3 = Eugster | first3 = Manuel J.A.\u000a  | last4 = Glowacka  | first4 = Dorota\u000a  | last5 = Konuyshkova  | first5 = Ksenia\u000a  | last6 = Athukorala  | first6 = Kumaripaba\u000a  | last7 = Kosunen | first7 = Ilkka    \u000a  | last8 = Reijonen  | first8 = Aki\u000a  | last9 = Myllymki | first9 = Petri\u000a  | last10 = Kaski  | first10 = Samuel\u000a  | last11 = Jacucci  | first11 = Giulio\u000a  | title = Directing Exploratory Search with Interactive Intent Modeling\u000a  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM\u000a  | year = 2013}}\u000a</ref>\u000a.<ref>\u000a{{Citation\u000a  | last1 = Glowacka  | first1 = Dorota\u000a  | last2 = Ruotsalo  | first2 = Tuukka\u000a  | last3 = Konuyshkova  | first3 = Ksenia\u000a  | last4 = Athukorala  | first4 = Kumaripaba\u000a  | last5 = Kaski  | first5 = Samuel\u000a  | last6 = Jacucci  | first6 = Giulio\u000a  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords\u000a  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI\u000a  | url = http://dl.acm.org/citation.cfm?id=2449413\u000a  | pages = 117\u2013128 \u000a  | year = 2013}}\u000a</ref>\u000a\u000a==Major figures==\u000a\u000aKey figures, including experts from both [[information seeking]] and [[human\u2013computer interaction]], are:\u000a*[http://research.microsoft.com/~ryenw Ryen White]\u000a*[http://ils.unc.edu/~march Gary Marchionini]\u000a*[http://comminfo.rutgers.edu/~belkin/belkin.html Nicholas Belkin]\u000a*[http://users.ecs.soton.ac.uk/mc m.c. schraefel]\u000a*[[Marcia Bates]]\u000a\u000a==References==\u000a<References/>\u000a#White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;36\u201339.\u000a#Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&nbsp;433\u2013436\u000a#Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.\u000a#P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009\u000a\u000a{{DEFAULTSORT:Exploratory Search}}\u000a[[Category:Human\u2013computer interaction]]\u000a[[Category:Information retrieval]]\u000a[[Category:Information science]]
p189
asI233
(lp190
VTunebot
p191
aV'''Tunebot''' is a music search engine developed by the Interactive Audio Lab at [[Northwestern University]]. Users can search the database by humming or singing a melody into a microphone, playing the melody on a virtual keyboard, or by typing some of the lyrics. This allows users to finally identify that song that was stuck in their head.\u000a\u000a==Searching Techniques==\u000a\u000aTunebot is a [[Query by humming]] system. It compares a sung query to a database of musical themes by using the intervals between each note. This allows a user to sing in a different key than the target recording and still produce a match. The intervals are also unquantized to allow for other tunings besides the standard A=440Hz, since not many people in the world have [[perfect pitch]].\u000a\u000aIn addition to note intervals, Tunebot compares a query with potential targets by using rhythmic ratios between notes. Since ratios between note lengths are used, the tempo of the performance does not affect the rhythmic similarity measure. \u000a\u000aQueries and targets are then matched by a weighted string alignment algorithm between the note intervals and rhythmic ratios.\u000a<!--Note segmentation, then to pitches and then use Pitch intervals (instead of melodic contour - measured frequency at given times). Pitch intervals are relative (unquantized) to adjust for singing in the wrong key or wrong tempo. Faster and more reliable search.\u000a\u000aModel singer error: gaussian distribution because wider interval and lower intervals seem to be more prone to singer error. Combination of gaussians with 5 parameters to tweak: pitch weight, rhythm weight, sensitivity to distance for pitch, sensisitivity to distance for rhythm, octave decay\u000a\u000aDo we use rhythmic ratios? (LIR)\u000a\u000aGenetic algorithm to tune system parameters-->\u000a\u000a==The Database==\u000aThe database consists of unaccompanied melodies sung by contributors (a capella). Contributors log into the website and sing their examples to the system. Each of these recordings is associated with a corresponding song on [[Amazon.com|Amazon]]. A sung query is compared to these examples. A capella sung examples are used as search keys because it is much easier to compare one unaccompanied vocal (the sung query) to another (an example search key) than it is to compare an unaccompanied vocal to a full band recording, which may contain guitar, drums, other singers, sound effects, etc.\u000a\u000a==Distinguishing Features==\u000a\u000aTunebot learns from user input, and it improve its results as each user submits more queries. Since no human can sing perfectly in tune every time they sing, the search engine must take that into account. By choosing a song from a list of ranked results, users tell Tunebot which song was correct. Tunebot then pairs that song with the user's query, analyzes the differences, and runs a [[Genetic Algorithm]]. This process tweaks the parameters that control how the system compares the user's query to the targets. For instance, if a user has no sense of rhythm, that factor of the comparison is lowered for future queries.\u000a\u000a==References==\u000a\u000a* B. Pardo. [http://127.0.0.1/publications/pardo-IEEE-signal-processing-mag-06.pdf Finding Structure in Audio for Music Information Retrieval.] IEEE Signal Processing Magazine. vol. 49 (8), pp. 49-52, 2006\u000a* D. Little, D. Raffensperger, B. Pardo. [http://music.cs.northwestern.edu/files/ISMIR%202007%20v2.pdf A Query by Humming System that Learns from Experience.] Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27, 2007.\u000a* D. Little, D. Raffensperger and B. Pardo.[http://www.eecs.northwestern.edu/docs/techreports/2007_TR/NWU-EECS-07-03.pdf Online Training of a Music Search Engine.] Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007\u000a\u000a==External links==\u000a*[http://tunebot.cs.northwestern.edu Tunebot @ Northwestern]\u000a\u000a\u000a[[Category:Music search engines]]\u000a[[Category:Acoustic fingerprinting]]
p192
asI235
(lp193
VSensMe
p194
aV{{Refimprove|date=April 2010}}\u000a{{Infobox Software|\u000a|name                   = SensMe\u000a|logo                   = [[File:SensMelogo.png|center|64px|SensMe Logo]]\u000a|screenshot             = [[File:SensMe.jpg|200px|center]]\u000a|caption                = Screenshot of SensMe on Media Go\u000a|developer              = [[Sony Corporation]]\u000a|genre                  = Mood detection software for music data.\u000a|platform               = [[Sony Walkman]] MP3/4 players<br />[[Sony Ericsson]]<br />[[Media Go]]<br />[[PlayStation Portable]]\u000a|license                = [[Proprietary software|Proprietary]]\u000a|website                = [http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w910i?cc=gb&lc=en http://www.sonyericsson.com]\u000a}}\u000a\u000a'''SensMe''' is a [[Proprietary software|proprietary]] music mood and tempo detection system created by [[Sony|Sony Corporation]], and employed in numerous Sony branded products, most notably the [[Walkman]] MP3/MP4 players (E <ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6052&NewsAreaId=2 Your cool, colourful music partner Feature-packed WALKMAN E450 Video MP3 player from Sony with premium sound for young music fans (15 July 2010 )]</ref> and S series<ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6203&NewsAreaId=2 Sony introduces super-slim WALKMAN S750 (15 September 2010)]</ref>), [[Media Go]], [[PlayStation Portable]], and [[Sony Ericsson]] phone series, .\u000a\u000a==Technical specifications==\u000a\u000a''SensMe'' works by mapping music to a dual axis map based on the mood and tempo of music tracks.<ref>What is SensMe? http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w980</ref> Mood and tempo is determined by using the appropriate Sony compatible software which analyzes music tracks individually and computes the relevant track information. Analyzed tracks can then be plotted onto an intuitive dual axis map through which the music library on the device can be navigated, and playlists can be generated based on relative speed and mood. The horizontal axis is based on mood and the vertical axis is based on [[tempo]].\u000a\u000a==PlayStation Portable==\u000a\u000aSensMe was made available on the PlayStation Portable as of system software version 6.10.<ref name="update610" /> It can be downloaded via the [[XrossMediaBar|XMB]] or by using a computer.<ref name="pspdownload">SensMe PSP Download http://www.playstation.com/psp-app/sensme/en/</ref> The application features twelve channels by which music is categorized. These include Favorites, Newly Added, Dance, Extreme, Lounge, Emotional, Mellow, Upbeat, Relax, Energetic, Morning/Day/Night/Midnight, and Shuffle All.\u000a\u000a===PlayStation Portable Version History===\u000a{| class="wikitable"\u000a!width="180"|Version<br> Release date (UTC)\u000a!class="unsortable"|Description\u000a|-\u000a|align=center|'''1.50'''<br>March 31, 2010\u000a|\u000a* Music tracks transferred using a PlayStation 3 system or music management application other than Media Go are now also categorized into channels.\u000a* Users can now add music tracks to a block list so they do not play.\u000a* Users can now activate or deactivate the [Dynamic Normalizer] feature.\u000a|-\u000a|align=center|'''1.01'''<br>October 22, 2009\u000a|\u000a* Descriptions of some menu items in some languages have been revised.\u000a|-\u000a|align=center|'''1.00'''<br>October 1, 2009\u000a|\u000a* Initial release.\u000a|}\u000a\u000a==SensMe compatible products==\u000a* [[Walkman]]\u000a* [[Media Go]]\u000a* [[PlayStation Portable]]<ref name="update610">PSP Firmware Update (v6.10) http://blog.us.playstation.com/2009/09/psp-firmware-update-v6-10/</ref>\u000a\u000a[[File:Sony Ericsson W760i running SensMe.JPG|thumb|right|Screenshot of SensMe on a Sony Ericsson]]\u000a\u000a===Sony Ericsson handsets===\u000a*''[[Sony Ericsson Aino|Aino]]''\u000a*''[http://www.sony.co.uk/product/nws-s-series/nwz-s639f Sony NWZ-S639F Media Player]''\u000a*''[[Sony Ericsson Elm|elm]]''\u000a*''[[Sony Ericsson W380|W380]]''\u000a*''[[Sony Ericsson W518a|W508]]''\u000a*''[[Sony Ericsson W518a|W518a]]''\u000a*''[[Sony Ericsson W595|W595]]''\u000a*''[[Sony Ericsson W705|W705]]''\u000a*''[[Sony Ericsson W705|W715]]''\u000a*''[[Sony Ericsson W760|W760]]''\u000a*''[[Sony Ericsson W890i|W890i]]''\u000a*''[[Sony Ericsson W902|W902]]''\u000a*''[[Sony Ericsson W910|W910i]]''\u000a*''[[Sony Ericsson W980|W980]]''\u000a*''[[Sony Ericsson W995|W995]]''\u000a*''[[Sony Ericsson Xperia X10|Xperia X10]]''\u000a*''[[Sony Ericsson Xperia Neo|Xperia Neo]]''\u000a*''[[Sony Ericsson Xperia Play|Xperia Play]]''\u000a*''[[Sony Ericsson Xperia ray|Xperia Ray]]''\u000a===Sony handsets===\u000a*''[[Sony Xperia E|Xperia E]]''\u000a*''[[Sony Xperia M|Xperia M]]''\u000a*''[[Sony Xperia Sola|Xperia Sola]]''\u000a* [[Sony_Xperia_L|Xperia L]]\u000a*''[[Sony Xperia S|Xperia S]]''\u000a*''[[Sony Xperia P|Xperia P]]''\u000a*''[[Sony Xperia U|Xperia U]]''\u000a*''[[Sony Xperia T|Xperia T]]''\u000a*''[[Sony Xperia TX|Xperia TX]]''\u000a*''[[Sony Xperia TL|Xperia TL]]''\u000a*''[[Sony Xperia tipo|Xperia tipo]]''\u000a*''[[Sony Xperia T|Xperia Go]]''\u000a*''[[Sony Xperia V|Xperia V]]''\u000a*''[[Sony Xperia Z|Xperia Z]]''\u000a*''[[Sony Xperia Z1|Xperia Z1]]''\u000a*''[[Sony Xperia Z1 Compact|Xperia Z1 Compact]]''\u000a*''[[Sony Xperia Z Ultra|Xperia Z Ultra]]''\u000a*''[[Sony Xperia Z1f|Xperia Z1f/Z1s]]''\u000a*''[[Sony Xperia ZL|Xperia ZL]]''\u000a*''[[Sony Xperia ZL|Xperia SP]]''\u000a*''[[Sony Xperia Z2|Xperia Z2]]''\u000a*''[[Sony Xperia Z3|Xperia Z3]]''\u000a*''[[Sony Xperia Z3 Compact|Xperia Z3 Compact]]''\u000a*''[http://www.sonyericsson.com/cws/products/mobilephones/overview/zylo?cc=ph&lc=en#view=features_specifications Zylo (W20i)]''\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Sony software]]\u000a[[Category:Music search engines]]
p195
asI237
(lp196
VSoundHound
p197
aV{{distinguish|SoundCloud}}\u000a{{Infobox software\u000a| name                   = Soundcloud\u000a| title                  = \u000a| logo                   = [[File:SoundHound Mobile Icon.png|250px]]\u000a| logo caption           = SoundHound Mobile Icon\u000a| screenshot             = <!-- [[File: ]] -->\u000a| caption                = \u000a| collapsible            = \u000a| author                 = \u000a| developer              = SoundHound, Inc\u000a| released               = {{Start date|2009|01|29|df=yes}}\u000a| discontinued           = \u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| programming language   = \u000a| operating system       = \u000a| platform               = \u000a| size                   = \u000a| language               = \u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| status                 = \u000a| genre                  = \u000a| license                = \u000a| alexa                  = \u000a| website                = {{URL|//www.soundhound.com/}}\u000a}}\u000a'''SoundHound''' (known as '''Midomi''' until December 2009) is a [[mobile device]] service that allows users to identify music by [[Query by humming|humming]], singing or playing a recorded track. The service was launched by Melodis Corporation (now SoundHound Inc), under [[Chief Executive]] Keyvan Mohajer in 2007 and has received funding from Global Catalyst Partners, TransLink Capital and Walden Venture Capital.\u000a\u000a==Features==\u000aSoundHound is a music search engine available on the [[Apple App Store]],<ref name="Apple App Store" /> [[Google Play]],<ref name="Google Play" /> [[Windows Phone Store]], and on June 5, 2013, was available on the BlackBerry 10 platform.<ref name="Windows" /> It enables users to identify music recorded through their device's microphone.<ref name=CNETv3 /> It is also possible to speak or type the name of the artist, composer, song and piece.<ref name=CNETv3 /> Unlike competitor [[Shazam (service)|Shazam]], SoundHound can recognise tracks from singing, humming, speaking, or typing, as well as from a recording.<ref>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=2 October 2012|newspaper=CNET|date=16 July 2008}}</ref> Sound matching is achieved through the company's 'Sound2Sound' technology, which  can match even poorly-hummed performances to professional recordings.<ref name="CNETWVC" />\u000a\u000aThe app then returns the lyrics (if any), links to videos on YouTube, links to iTunes, ringtones, the ability to launch [[Pandora Radio]],<ref name="TNWHound">{{cite news | url=http://thenextweb.com/apps/2011/05/26/soundhounds-new-voice-app-hound-wants-to-change-the-way-we-search/ | title=SoundHound\u2019s new voice app "Hound" wants to change the way we search | date=26 May 2011 | accessdate=2 October 2012 | last=Boyd Myers | first=Courtney | newspaper=The Next Web}}</ref> as well as recommendations for other music.<ref>{{cite news|last=Dolcourt|first=Jessica|title=SoundHound for iPhone channels iTunes, recommends beats|url=http://reviews.cnet.com/8301-19512_7-20037798-233.html|accessdate=2 October 2012|newspaper=CNET|date=2 March 2011}}</ref> A feature called LiveLyrics displays a song's lyrics in time with the music, if they are available. Double-tapping on those lyrics moves the music to that point in the song.<ref>{{cite news|last=Cabebe|first=Jaymar|title=SoundHound adds LiveLyrics|url=http://download.cnet.com/8301-2007_4-20081243-12/soundhound-adds-livelyrics/|accessdate=3 October 2012|newspaper=CNET|date=20 July 2011}}</ref> It is also possible for users to play music from their iPhone's iPod library through the app. If lyrics are available for a song, it will show them as it plays.<ref name="CNETv3" />\u000a\u000aThere are three versions of the app: SoundHound, SoundHound Infinity and Hound. SoundHound is free but has banner ads, while SoundHound Infinity (styled SoundHound \u221e), priced at 4.99 in the UK or $6.99 in the US, is the premium offering and has the same functionality but without banner ads.<ref name="CNETFree" /> Hound only allows users to search for artists or songs by speaking into it. Similar to the SoundHound app, Hound then returns a song preview, lyrics, album art and videos as well as artist bios and tour dates.<ref name="TNWHound" />\u000a\u000a==History==\u000aMidomi, renamed SoundHound in December 2009 with the launch of version 3.0 of the mobile app,<ref name=CNETv3>{{cite news|last=Dolcourt|first=Jessica|title=Midomi 3.0 seeks song lyrics, knows what's hot|url=http://reviews.cnet.com/8301-19512_7-10408563-233.html|accessdate=2 October 2012|newspaper=CNET|date=3 December 2009}}</ref> was launched in [[beta]] in January 2007, as a [http://www.midomi.com website], with 2 million licensed tracks.<ref name="CNET1" /> The technology, dubbed Multimodal Adaptive Recognition System (MARS), considers pitch, tempo variation, speech content and pauses in order to recognise samples.<ref name=CNET1>{{cite news|last=Mills|first=Elinor|title=This Web site can name that tune|url=http://news.cnet.com/This-Web-site-can-name-that-tune/2100-1027_3-6153657.html|accessdate=2 October 2012|newspaper=CNET|date=26 January 2007}}</ref> The company behind the site, Melodis Corporation, was started in 2004 by [[Chief Executive]] Keyvan Mohajer, a [[PhD]] in sound-recognition from [[Stanford]].<ref name=CNET1 /> Melodis changed its name to SoundHound Inc in May 2010.<ref>{{cite press release|title=SoundHound Inc. Announces Name Change from Melodis Corporation|publisher=SoundHound Inc|date=20 May 2010|url=//www.soundhound.com/index.php?action=s.press_release&pr=15|accessdate=2 October 2012}}</ref>\u000a\u000aThe first version of the app was released on the [[Apple App Store]] in July 2008.<ref name="Apple App Store">{{cite news | url=http://news.cnet.com/8301-17938_105-9987892-1.html | title=Sing for search results with iPhone app | date=10 July 2008 | accessdate=2 October 2012 | last=Jackson | first=Holly | newspaper=CNET}}</ref> At the launch of [[Windows Marketplace for Mobile]] in October 2009, Midomi was one of the apps included in the store<ref name="Windows">{{cite news | url=http://reviews.cnet.com/8301-12261_7-10368174-10356022.html | title=Windows mobile app store, My Phone service officially opening | date=6 October 2009 | accessdate=2 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> and could be purchased for $4.99.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=2 October 2012|newspaper=CNET|date=7 October 2009}}</ref> It joined the [[Android OS|Android]] app store in June 2010.<ref name="Google Play">{{cite news | url=http://www.cnet.com/8301-19736_1-20007745-251.html | title=New SoundHound names that tune--for free (Android) | date=15 June 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> On January 2013, the [[BlackBerry]] version of the app was then available in [[BlackBerry World]] following the announcement and launch of [[BlackBerry 10]].<ref name="BlackBerry">{{cite web\u000a  |title=BlackBerry shows off some of its 70,000 new third-party apps, including Skype, Rdio, Kindle, and Whatsapp\u000a  |publisher=[[The Verge]]\u000a  |url=http://www.theverge.com/2013/1/30/3932042/blackberry-10-apps-announcement\u000a  |accessdate=2013-01-30}}</ref>\u000a\u000aA free version of the app was released in April 2010, with all the functionality of the premium version, while limiting the number of searches to five per month, and adding banners ads.<ref name="CNETFree">{{cite news | url=http://reviews.cnet.com/8301-19512_7-20003228-233.html | title=Sonic freebie: New, free SoundHound music-ID app for iPhone, iPad | date=27 April 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> The premium version was now renamed SoundHound Infinity.<ref name="CNETFree" /> A stripped-down version, Hound, was released in May 2011.<ref name="TNWHound" />\u000a\u000aIn January 2011, Apple revealed that SoundHound was the top paid iPad app  on its [[Apple App Store|App Store]] was SoundHound, while rival Shazam was fourth in the top ten list of free iPhone apps.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=2 October 2012|newspaper=CNET|date=19 January 2011}}</ref>\u000a\u000aIn June 2012, the firm announced that it had 80 million users while version 5.0 was released, with a new design and features that include an in-built player and integration with LiveLyrics.<ref name="TNW80m">{{cite news | url=http://thenextweb.com/apps/2012/06/07/shazam-competitor-soundhound-passes-80m-users-and-rolls-out-updated-mobile-apps/ | title=Shazam competitor SoundHound passes 80m users and rolls out updated mobile apps | work=The Next Web | date=7 June 2012 | accessdate=2 October 2012 | author=Sawers, Paul}}</ref>\u000a\u000aIn December 2013, the app passed 185 million users.<ref>{{cite news|title=SoundHound Reveals Its Top Songs of 2013|url=http://www.heraldonline.com/2013/12/16/5509030/soundhound-reveals-its-top-songs.html|accessdate=16 December 2013|newspaper=The Next Web|date=16 December 2013}}</ref>\u000a\u000aIn December 2013, the app launches iTunes Radio integration.<ref>{{cite news|title=SoundHound App Update Adds iTunes Radio Integration for iPad and iPhone Users. |url=http://www.padgadget.com/2013/12/20/soundhound-app-update-adds-itunes-radio-integration-for-ipad-and-iphone-users/|accessdate=11 February 2014|newspaper=PadGadget|date=20 December 2013}}</ref>\u000a\u000aIn September 2013, the app enables 170 million global users to sync, save, and transfer music search & discovery history across multiple devices.<ref>{{cite news|title=SoundHound adds cloud history sync on iOS and Android apps|url=http://www.intomobile.com/2013/09/25/soundhound-adds-cloud-history-sync-ios-and-android-apps/|accessdate=11 February 2014|newspaper=INTOMOBILE|date=20 September 2013}}</ref>\u000a\u000aIn January 2014, SoundHound and Hyundai Motor Group partnered to embed music search and discovery into select 2014 Hyundai & Kia models.<ref>{{cite news|title=Hyundai and Kia tap SoundHound to help you identify music in your car|url=http://www.engadget.com/2014/01/14/hyundai-kia-soundhound-music-tagging/|accessdate=11 February 2014|newspaper=Engadget|date= January 14, 2014}}</ref>\u000a\u000aIn January 2014, the app launched an "immersive second screen GRAMMYs experience".<ref>{{cite news|title=SoundHound's music search app turns its focus to the Grammys with real-time updates and more|url=http://www.engadget.com/2014/01/25/soundhound-grammys-2014/|accessdate=11 February 2014|newspaper=Engadget|date= January 24, 2014}}</ref>\u000a\u000aIn April 2014, the app passed 200 million users.<ref>//www.soundhound.com/index.php?action=s.press_release&pr=67</ref>\u000a\u000a===Funding===\u000aMelodis secured $7 million in a Series B funding round in October 2008, bringing total funds raised to $12 million. The round was led by TransLink Capital with the participation of JAIC America and [[Series A round|Series A]] investor Global Catalyst Partners.<ref>{{cite press release|title=Search and Sound Recognition Innovator MELODIS and Creator of Midomi Raises $7 Million in Series B Funding|publisher=Melodis Corporation|date=7 October 2008|url=//www.soundhound.com/index.php?action=s.press_release&pr=5|accessdate=2 October 2012}}</ref>\u000a\u000aIn 2009, Melodis attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups [[Pandora Radio|Pandora]] and [[SNOCAP|Snocap]].<ref name=CNETWVC>{{cite news|last=Needleman|first=Rafe|title=Midomi music search gets funding and opportunities|url=http://news.cnet.com/8301-19882_3-10298068-250.html|accessdate=2 October 2012|newspaper=CNET|date=28 July 2009}}</ref> The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.<ref>{{cite press release|title=Melodis, Sound Search Technology and Applications Innovator, Raises $4M Led by Walden Venture Capital and a Strategic Investor|publisher=Melodis Corporation|date=4 August 2009|url=//www.soundhound.com/index.php?action=s.press_release&pr=12|accessdate=2 October 2012}}</ref>\u000a\u000a==See also==\u000a*[[Query by humming]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==External links==\u000a*[http://midomi.com midomi.com]\u000a*[//www.soundhound.com/ SoundHound website]\u000a\u000a[[Category:Android (operating system) software]]\u000a[[Category:IOS software]]\u000a[[Category:Symbian software]]\u000a[[Category:Music search engines]]\u000a[[Category:Companies established in 2005]]\u000a[[Category:Companies based in California]]\u000a[[Category:BlackBerry software]]
p198
asI110
(lp199
VInternational Society for Music Information Retrieval
p200
aVThe '''International Society for Music Information Retrieval''' ('''ISMIR''') is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11 ISSN: 1082-9873.]</ref> which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>\u000a\u000a==Purpose==\u000aGiven the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.\u000a\u000aAs the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science and many others.\u000a\u000a==Annual Conference==\u000aSince its inception in 2000, ISMIR has been the world\u2019s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.\u000a* [http://ismir2012.ismir.net ISMIR 2012], 8\u201312 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2012'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2011.ismir.net ISMIR 2011], 24\u201328 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2011'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2010.ismir.net ISMIR 2010], 9\u201313 August 2010, Utrecht (Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2010'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2009.ismir.net ISMIR 2009], 26\u201330 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2009'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2008.ismir.net ISMIR 2008], 14\u201318 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2008'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2007.ismir.net ISMIR 2007], 23\u201330 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2007'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2006.ismir.net ISMIR 2006], 8\u201312 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2006'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2005.ismir.net ISMIR 2005], 11\u201315 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2005'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2004.ismir.net ISMIR 2004], 10\u201315 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2004'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2003.ismir.net ISMIR 2003], 26\u201330 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2003'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2002.ismir.net ISMIR 2002], 13\u201317 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2002'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2001.ismir.net ISMIR 2001], 15\u201317 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2001'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2000.ismir.net ISMIR 2000], 23\u201325 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='200'&page=0&order=Authors&order_type=ASC proceedings]\u000a\u000aThe [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences.\u000a\u000a==MIREX==\u000aThe Music Information Retrieval Evaluation eXchange (MIREX)<ref>[http://www.music-ir.org/mirex MIREX Wiki]</ref> is an annual evaluation campaign for Music Information Retrieval (MIR) algorithms, coupled to the ISMIR conference.\u000a\u000aMIR tasks evaluated at past MIREXs include:\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Train/Test_Tasks Audio Train/Test Tasks]\u000a**Audio Artist Identification\u000a**Audio Genre Classification\u000a**Audio Music Mood Classification\u000a**Audio Classical Composer Identification\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Genre_Classification Symbolic Genre Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Onset_Detection Audio Onset Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Key_Detection Audio Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Key_Detection Symbolic Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tag_Classification Audio Tag Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Cover_Song_Identification Audio Cover Song Identification]\u000a*[http://www.music-ir.org/mirex/wiki/Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following) Real-time Audio to Score Alignment (a.k.a Score Following)]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Singing/Humming Query by Singing/Humming]\u000a*[http://www.music-ir.org/mirex/wiki/Multiple_Fundamental_Frequency_Estimation_&_Tracking Multiple Fundamental Frequency Estimation & Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Chord_Estimation Audio Chord Estimation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Melody_Extraction Audio Melody Extraction]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Tapping Query by Tapping]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Beat_Tracking Audio Beat Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Music_Similarity_and_Retrieval Audio Music Similarity and Retrieval]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Melodic_Similarity Symbolic Melodic Similarity]\u000a*[http://www.music-ir.org/mirex/wiki/Structural_Segmentation Structural Segmentation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Drum_Detection Audio Drum Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tempo_Extraction Audio Tempo Extraction]\u000a\u000a==See also==\u000a* [[International Conference on Digital Audio Effects]]\u000a* [[Music Technology]]\u000a* [[Sound and music computing|Sound and Music Computing]]\u000a* [[Sound and Music Computing Conference]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a[[Category:Music technology]]\u000a[[Category:Multimedia]]\u000a[[Category:Information retrieval]]
p201
asI239
(lp202
VSongza
p203
aV{{use mdy dates|date=July 2014}}\u000a{{use American English|date=July 2014}}\u000a{{Infobox website\u000a|name           = Songza\u000a|logo           = [[File:Songza Logo.jpg|frameless|150px]]\u000a|screenshot     = \u000a|caption        = \u000a|url            = {{URL|songza.com}}\u000a|alexa          = {{Loss}} 9,279 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/songza.com |title= Songza.com Site Info | publisher= [[Alexa Internet]] |accessdate= April 1, 2014 }}</ref><!--Updated monthly by OKBot.-->\u000a|commercial     = \u000a|type           = Free [[internet radio]]\u000a|language       = [[English language|English]]\u000a|location       = [[Long Island City, New York|Long Island City]], [[Queens]], [[New York City]], [[New York]], United States\u000a|registration   = \u000a|owner          = [[Google Inc.]]\u000a|author         = [[Aza Raskin]] and Scott Robbin\u000a|launch date    = {{start date and age|2007|11|08|paren=yes}}\u000a|current status = Active\u000a|revenue        = \u000a|slogan         = Good music makes good times.<ref>{{cite web|url= http://songza.com |title= Songza.com Site Info | publisher= Songza Media, Inc. |accessdate= August 15, 2012 }}</ref>\u000a}}\u000a\u000a'''Songza''' is a free [[music streaming]] and [[Recommender system|recommendation]] service for Internet users in the United States and Canada. \u000a\u000aStating that its playlists are made by music experts, the service recommends various playlists based on time of day and mood or activity.<ref name="The New York Times">{{cite news| first= Ben| last= Sisaro| work = [[The New York Times]] |title= Pandora Faces Rivals for Ears and Ads| accessdate = June 20, 2012| url= http://www.nytimes.com/2012/06/21/business/songza-and-spotify-challenge-pandora-for-ears-and-ads.html?_r=3| date= June 20, 2012}}</ref><ref name=PandoDaily>{{cite web| first= Erin|last= Griffith| publisher= [[PandoDaily]]|title= Songza's Founders Realized They Weren't Thinking Radically Enough{{spaced ndash}} Here's How They Changed That| accessdate = August 15, 2012|url= http://pandodaily.com/2012/08/15/songzas-founders-realized-they-werent-thinking-radically-enough-heres-how-they-changed-that/}}</ref> Songza offers playlists for activities such as waking up, working out, commuting, concentrating, unwinding, entertaining, and sleeping.<ref name="The Washington Post" >{{cite news| first= Hayley| last= Tsukayama| work = [[The Washington Post]] |title=TechBits: Songza adapts the music to your mood| accessdate = June 23, 2012| url = http://www.washingtonpost.com/techbits-songza-adapts-the-music-to-your-mood/2012/06/23/gJQAYRzKyV_story.html| date= June 25, 2012}}</ref>  Users can vote songs up or down, and the service will adapt to the user's personal music preferences.<ref name="The Washington Post" /> Users can find playlists not just based on artists, songs, or genres, but also based on themes, interests, and eras, such as "[[List of 1990s one-hit wonders in the United States|90s One-Hit Wonders]]", or "Music of [[Fashion Week]]".<ref name=SongzaAbout>{{cite web| publisher= Songza|title= About Us| accessdate = March 25, 2011| url = http://songza.com/page/about/}}</ref>\u000a\u000aSongza is headquartered in the [[Long Island City]] neighborhood of the [[Queens]] [[borough (New York City)|borough]] of [[New York City]], [[New York]].<ref name="NY Daily News">{{cite news| first= Clare | last= Trapasso| work = [[Daily News (New York)|Daily News]] |title= Songza music service streams for success| accessdate = July 27, 2012| url= http://articles.nydailynews.com/2012-07-27/news/32874462_1_spotify-apps-music-download}}</ref>\u000a\u000a== History ==\u000a[[Amie Street]] acquired Songza, a product created by [[Aza Raskin]] and Scott Robbin, in October 2008.<ref>{{cite web| first= Kristen| last=Nicole| publisher= bub.blicio.us |title= Interview with Amie Street: Why Keep Acquisition of Songza a Secret?| accessdate = March 25, 2011| url = http://bub.blicio.us/interview-with-amie-street-why-keep-acquisition-of-songza-a-secret/}}</ref> In August 2010, Amie Street was sold to Amazon for an undisclosed amount.<ref>{{cite web| first= Michael | last= Arrington| publisher= [[TechCrunch]]|title= Amazon Acquires Amie Street, But Not in a Good Way| accessdate = September 8, 2010| url= http://techcrunch.com/2010/09/08/amazon-acquires-amie-street-but-not-in-a-good-way/}}</ref>  Shortly after this the co-founders{{spaced ndash}} CEO Elias Roman, COO Peter Asbill, CPO Elliott Breece and CCO Eric Davich{{spaced ndash}} refocused their efforts on Songza.<ref name="The New York Times" /><ref>{{cite web| publisher= [[Internships.com]]|title= 5 in 5! with Eric Davich, Chief Content Officer and Co-Founder of Songza| accessdate = August 6, 2012| url= http://www.internships.com/eyeoftheintern/applying-2/employers-applying-2/5-5-eric-davichchief-content-officer-cofounder-songza/?cid=SO_ST_TW_080612_5IN5_SONGZA}}</ref>  The team discontinued the original version and relaunched a new alpha version of Songza, keeping nothing of the original product but the name.<ref name=Upstart>{{cite news| first= Michael| last= del Castillo| work =  [[American City Business Journals|Upstart Business Journal]] |title= Downtime: The birth of Songza| accessdate = June 15, 2012| url= http://upstart.bizjournals.com/entrepreneurs/hot-shots/2012/06/15/songza-minigolfs-to-no-1-app.html?page=2}}</ref>\u000a\u000aOver the next year the founders experimented with various iterations, when the app originally launched in 2010 "it was like a pre-Turntable.fm.  A function called Social Radio allowed users to be DJs for their friends" stated PandoDaily.<ref name="PandoDaily" />  This version of the app allowed it to be social and crowdsourced; the problem with it was that the service as it stood was not sufficiently differentiated from other services on the market and the quality of the crowd sourced playlists was low.<ref name=PandoDaily/>  Following a year of testing various iterations of the alpha version of the app, Songza relaunched in beta on iPhone and Android apps on September 13, 2011, armed with a team of 25 expert music curators.<ref name="The New York Times" /><ref name="PandoDaily" /><ref name=TechCrunch>{{cite web| first= Rip| last= Empson| publisher= [[TechCrunch]]|title= Songza Raises Seven Figure Round; Launches Mobile, Sharable Music Collections in the Cloud| accessdate = September 13, 2011| url= http://techcrunch.com/2011/09/13/songza-raises-seven-figure-round-launches-mobile-sharable-music-collections-in-the-cloud/}}</ref><ref>[http://www.ad60.com/2011/09/19/songza-launches-iphone-android-apps-digitize-mix-tape/ "Songza launches iPhone and Android apps to digitize the mix tape"].</ref>\u000a\u000aIn March 2012, Songza released its Music Concierge feature, on iPhone and the web.<ref name="The New York Times" /><ref name = TechCrunch>{{cite web| first= Jordan| last= Crook| publisher= [[TechCrunch.com]]|title= Songza, the Music Streaming Service That Does All Work for You, Launches an iPad App| accessdate = June 7, 2012| url= http://techcrunch.com/2012/06/07/songza-the-music-streaming-service-that-does-all-work-for-you-launches-an-ipad-app/}}</ref>  The concierge presents users with up to six situations based on time of day, with filters for whatever mood they might be in.  For example, on a Wednesday morning a user might be presented with situations for "Waking Up", "Singing in the Shower", "Working Out" and so on.  This feature was rolled out to iPad on June 7, 2012; during the first ten days following the iPad app launch, Songza saw over 1.15 million downloads.<ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza Hits 1.15 Million iOS Downloads in 10 Days| accessdate = June 18, 2012| url= http://www.pcmag.com/article2/0,2817,2405952,00.asp}}</ref>\u000a\u000aOn June 12, 2012, Songza was listed as the top free app on iTunes for the iPad and the number two free app for the iPhone.<ref>{{cite web| first= Glenn| last= Peoples| publisher= [[Billboard.biz]]|title= Songza Reaches One Million iOs Downloads in Ten Days, But Is It the Next Big Thing?| accessdate = June 19, 2012| url= http://www.billboard.biz/bbbiz/others/songza-reaches-one-million-ios-downloads-1007360352.story}}</ref>  Concierge was released on Android on July 10, 2012, and for Android tablets on August 14, 2012.<ref>{{cite web| first= Andrew| last= Kameka| publisher= Androinica.com |title= Songza re-ups with expert Music Concierge playlists, lockscreen controls, and new Holo-like design| accessdate = July 10, 2012| url= http://androinica.com/2012/07/songza-android-app/}}</ref><ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza App Now Available on Android Tablets| accessdate = August 14, 2012| url= http://www.pcmag.com/article2/0,2817,2408435,00.asp}}</ref>  The app expanded to Canada on August 7, 2012, and became the number-one overall free app in Canada on August 13, 2012.<ref name=PandoDaily/><ref>{{cite web| first= Anand| last= Ram| publisher= o.canada.com |title= Songza's Elias Roman wants to provide the music for every mood | accessdate = August 7, 2012| url= http://o.canada.com/2012/08/05/songzas-elias-roman-wants-to-provide-the-music-for-every-mood/}}</ref> Within the week of Microsoft's Build developer event in June 2013, Songza snuck in its official Windows 8 App.<ref>[http://www.wpcentral.com/songza-sneaks-windows-store-wins-our-hearts]. WP Central. June 27, 2013.</ref>\u000a\u000aSongza launched in Canada on August 7, 2012, and reached the one million download mark after 70 days.<ref>Dobby, Christine (August 23, 2012).  [http://business.financialpost.com/2012/08/23/songza-startup-singing-a-canadian-tune/ "Songza startup singing a Canadian tune"].  ''[[Financial Post]]''. August 23, 2012.</ref><ref>Crook, Jordan (October 18, 2012). [http://techcrunch.com/2012/10/18/songzas-canada-launch-nabs-1-million-new-users-in-70-days/ "Songza's Canada Launch Nabs 1 Million New Users in 70 Days"]. [[TechCrunch]].</ref>\u000a\u000aStarting October 2013, Songza began inserting pop-up audio/video ads when initiating a playlist so it is no longer "audio-ad free". Songza reported having 5.5 million regular users at the end of 2013.<ref>{{Cite news|url = http://www.nytimes.com/2014/07/02/business/media/google-buys-songza-a-playlist-app-for-any-occasion.html|title = Google in Deal for Songza, a Music Playlist Service|last = Sisario|first = Ben|date = July 1, 2014|work = New York Times|accessdate = }}</ref>\u000a\u000aSongza was acquired by Google on July 1, 2014.<ref>{{cite web | url=http://techcrunch.com/2014/07/01/google-buys-songza/ | title=Google Buys Songza | publisher= [[TechCrunch]] | accessdate= July 1, 2014}}</ref> No terms were disclosed but speculation put the price at somewhere between $15 million and $39 million. Both companies issued statements saying they were "thrilled" to be doing the deal.<ref name="GoogleSongza">{{cite news|title=Google acquires music app start-up Songza|url=http://www.businesssun.com/index.php/sid/223470673/scat/3a8a80d6f705f8cc/ht/Google-acquires-music-app-start-up-Songza|accessdate= July 3, 2014|publisher=''Business Sun''}}</ref> In October 2014, following the acquisition, the [[Google Play Music|Google Play Music All Access]] service was updated to include functionality adapted from Songza's Concierge system.<ref name=verge-songzagpm>{{cite web|title=Google brings Songza's best feature to Play Music|url=http://www.theverge.com/2014/10/21/7027707/google-brings-best-songza-feature-to-play-music|website=The Verge|accessdate=21 October 2014}}</ref>\u000a\u000a==Similar organizations==\u000a{{div col|colwidth=30em}}\u000a* [[8tracks]]\u000a* [[AccuRadio]]\u000a* [[Deezer]]\u000a* [[Digitally Imported]]\u000a* [[FIT Radio]]\u000a* [[Google Play Music]]\u000a* [[Grooveshark]]\u000a* [[Guvera]]\u000a* [[iHeartRadio]]\u000a* [[Live365]]\u000a* [[MOG (online music)|MOG]]\u000a* [[Musicovery]]\u000a* [[Pandora Radio]]\u000a* [[Rara.com]]\u000a* [[Rdio]]\u000a* [[Rhapsody (online music service)|Rhapsody]]\u000a* [[Slacker Radio]]\u000a* [[Spotify]]\u000a* [[Soundtracker (music streaming)]]\u000a* [[WiMP]]\u000a* [[Xbox Music]]\u000a{{div col end}}\u000a\u000a\u000a{{portal|Companies|Music}}\u000a\u000a==References==\u000a{{reflist|30em}}\u000a==External links==\u000a*{{official website|songza.com}}\u000a\u000a{{Digital distribution platforms}}\u000a{{Google Inc.}}\u000a\u000a[[Category:American companies established in 2007]]\u000a[[Category:Community websites]]\u000a[[Category:Companies based in Queens, New York]]\u000a[[Category:Domain-specific search engines]]\u000a[[Category:Free music]]\u000a[[Category:Google acquisitions]]\u000a[[Category:Internet advertising]]\u000a[[Category:Internet companies of the United States]]\u000a[[Category:Internet properties established in 2007]]\u000a[[Category:Internet radio in the United States]]\u000a[[Category:Long Island City]]\u000a[[Category:Media companies based in New York City]]\u000a[[Category:Music companies of the United States]]\u000a[[Category:Music search engines]]\u000a[[Category:Recommender systems]]\u000a[[Category:Technology companies established in 2007]]
p204
asI119
(lp205
VPersonalization
p206
aV{{cleanup-reorganize|date=June 2008}}\u000a\u000a'''Personalization''', also known as '''customization''', involves using technology to accommodate the differences between individuals.\u000a\u000a==Web pages==\u000a{{see also|Web pages|Adaptive hypermedia}}  \u000a[[Web page]]s are personalized based on the characteristics (interests, social category, context, ...) of an individual. Personalization implies that the changes are based on implicit data, such as items purchased or pages viewed. The term ''customization'' is used instead when the site only uses explicit data such as ratings or preferences. \u000a\u000aOn an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role.  The term '''customization''' in this context refers to the ability of users to modify the page layout or specify what content should be displayed.\u000a\u000aThere are three categories of personalization:\u000a# Profile / Group based\u000a# Behaviour based (also known as Wisdom of the Crowds)\u000a# Collaboration based\u000a\u000a\u000a\u000aThere are three broad methods of personalization:\u000a# Implicit\u000a# Explicit\u000a# Hybrid\u000a\u000aWith implicit personalization the personalization is performed by the web page (or information system) based on the different categories mentioned above. It can also be learned from interactions with the user directly.<ref>{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}</ref> With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system.\u000aHybrid personalization combines the above two approaches to leverage the ''best of both worlds''.\u000a\u000aMany companies offer services for web recommendation and email recommendation that are based on personalization or anonymously collected user behaviors.<ref name=behaviors>[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html?mod=googlenews_wsj ''Wall Street Journal'', \u201cOn the Web's Cutting Edge, Anonymity in Name Only\u201d], August 4, 2010</ref>  \u000a\u000aWeb personalization is closely linked to the notion of '''[[Adaptive hypermedia]]''' (AH). The main difference is that the former would usually work on what is considered an Open Corpus Hypermedia, whilst the latter would traditionally work on Closed Corpus Hypermedia. However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.\u000a\u000aPersonalization is also being considered for use in less overtly commercial applications to improve the user experience online.<ref>[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March \u2013 3 April 2004. Archives & Museum Informatics, pages 63\u201378, 2004.</ref> [[Remote control]] manufacturer [[Ruwido]] developed an [[interactive]] [[IPTV]] platform in 2010 called Voco Media, which controls [[digital media]] in the [[living room]] using web personalization. It uses personalization as a tool that supports modern forms of [[TV]] usage, by allowing users to create different profiles for each family member, personalized menu structures and [[fingerprint recognition]].<ref>[http://www.digitaltveurope.net/news_articles/mar_10/23_mar_10/ruwido_wins_virgin_media_contract,_announces_new_voco_apps Ruwido Wins Virgin Media Contract, Announces New Voco App]{{dead link|date=January 2013}}</ref>\u000a\u000aInternet activist [[Eli Pariser]] has documented that search engines like Google and Yahoo News give different results to different people (even when logged out).  He also points out social media site Facebook changes user's friend feeds based on what it thinks they want to see.  Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.\u000a\u000a==Digital media==\u000aAnother aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards.<ref>{{cite news| url=http://www.guardian.co.uk/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}</ref> Ordnance Survey Open Data This data is structured to allow it to be inter-connected and re-used by third parties.<ref>{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}</ref>\u000a\u000aData available from a user\u2019s personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].\u000a\u000aCurrent [[open data]] standards on the Web include:\u000a# [[Attention Profiling Mark-up Language]] (APML)\u000a# [[DataPortability]]\u000a# [[OpenID]]\u000a# [[OpenSocial]]\u000a\u000a== Mobile phones ==\u000a\u000aOver time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.<ref>May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.</ref>\u000a\u000a==Print media==\u000a{{main|Mail merge}}\u000a\u000aIn print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients\u2019 information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient\u2019s demographics or interests using fields within the database, such as "first name", "last name", "company", etc. \u000a\u000aThe term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing| variable data printing (VDP)]]. This allows for full image and text variability within a printed book.\u000aWith the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.\u000a\u000a== Promotional merchandise ==\u000aPromotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children\u2019s storybooks \u2014 wherein the child becomes the [[protagonist]], with the name and image of the child personalized \u2014 are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained. In addition, with the advent of [[3D printing]], personalised apparel and accessories, such as jewellery made by [[StyleRocks]], is also increasing in popularity.<ref>{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015|quote=StyleRocks founder and CEO, Pascale Helyar-Moray, said the site offers women\u2019s and men\u2019s rings, necklaces, bracelets, earrings and cufflinks. Working alongside an Australian jewellery wholesaler, Helyar-Moray said customers have access to a variety of different styles and designs in an attempt to widen the site\u2019s ability to personalise pieces.}}</ref><ref>{{cite web|url=http://www.ragtrader.com.au/news/style-first|title=Style first|date=15 August 2014|publisher=Ragtrader|language=|accessdate=6 January 2015|quote=Online retailer StyleRocks is about to introduce an Australian first for the jewellery sector. The customisable fine jewellery retailer has introduced 3D printing in conjunction with the launch of a new website.}}</ref>\u000a\u000a== Mass personalization ==\u000a\u000a{{tone|section|date=January 2011}}\u000a\u000aMass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.<ref>{{cite web|url=http://www.answers.com/personalization&r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}</ref> From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers\u2019 individual specific needs with manufacturers\u2019 customization capabilities. <ref>	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167</ref> The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.<ref>Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.</ref> Clothing industry has also adopted the mass customization paradigm and some footwear retailers are producing mass customized shoes.<ref>{{cite web|url=http://www.botisto.com/how.php?language=EN |title=Botisto |publisher=Botisto |date= |accessdate=2013-01-16}}</ref><ref>[http://www.promoline1.com/Custom-T-Shirts-s/1814.htm Clothing ]</ref> The gaming market is seeing personalization in the new custom controller industry. A new, and notable, company called "Experience Custom" gives customers the opportunity to order personalized gaming controllers.<ref>{{cite web|url=http://www.experiencecustom.com/|title=Custom Controllers |publisher=ExperienceCustom.com |date= |accessdate=2014-11-20}}</ref> \u000a\u000aA website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.<ref>{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}</ref>\u000a\u000a[[Behavioral targeting]] represents a concept that is similar to mass personalization.\u000a\u000a== Predictive personalization ==\u000a\u000aPredictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.<ref>{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}</ref>  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.\u000a\u000a==See also==\u000a* [[Adaptation (computer science)]]\u000a* [[Mass customization]]\u000a* [[Adaptive hypermedia]]\u000a* [[Behavioral targeting]]\u000a* [[Bespoke]]\u000a* [[Collaborative filtering]]\u000a* [[Configurator]]\u000a* [[Personalized learning]]\u000a* [[Preorder economy]]\u000a* [[Real-time marketing]]\u000a* [[Recommendation system]]\u000a* [[User modeling]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==External links==\u000a* [http://www.iimcp.org International Institute on Mass Customization & Personalization which organizes MCP, a biannual conference on customization and personalization]\u000a* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''\u000a\u000a[[Category:Human\u2013computer interaction]]\u000a[[Category:World Wide Web]]\u000a[[Category:User interface techniques]]\u000a[[Category:Usability|Personas]]\u000a[[Category:Types of marketing]]\u000a[[Category:Information retrieval]]
p207
asI120
(lp208
VDragomir R. Radev
p209
aV'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]].  \u000aHe is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.\u000a\u000aRadev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006\u2013present) and associate editor of [http://www.jair.org JAIR].\u000a\u000a== Awards ==\u000aAs [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].\u000a\u000a== IOL==\u000aRadev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].\u000a\u000a== Books ==\u000a* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>\u000a* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']\u000a\u000a== Selected Papers ==\u000a* SIGIR 1995 Generating summaries of multiple news articles\u000a* ANLP 1997 Building a generation knowledge source using internet-accessible newswire\u000a* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources\u000a* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities\u000a* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation\u000a* CIKM 2001 Mining the web for answers to natural language questions\u000a* AAAI 2002 Towards CST-enhanced summarization\u000a* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project\u000a* Information Processing and Management 2004 Centroid-based summarization of multiple documents\u000a* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization\u000a* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web\u000a* Communications of the ACM 2005 NewsInEssence: summarizing online news topics\u000a* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing\u000a* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network\u000a* IEEE Intelligent Systems 2008 natural language processing and the web\u000a* NAACL 2009 Generating surveys of scientific paradigms\u000a* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways\u000a* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts\u000a* KDD 2010 Divrank: the interplay of prestige and diversity in information networks\u000a* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs\u000a* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language\u000a* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology\u000a\u000a==External links==\u000a* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]\u000a* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]\u000a\u000a== References ==\u000a{{reflist}}\u000a<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\u000a*\u000a*\u000a*\u000a*\u000a\u000a{{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. -->\u000a| NAME              = Radev, Dragomir R.\u000a| ALTERNATIVE NAMES =\u000a| SHORT DESCRIPTION = American computer scientist\u000a| DATE OF BIRTH     =\u000a| PLACE OF BIRTH    =\u000a| DATE OF DEATH     =\u000a| PLACE OF DEATH    =\u000a}}\u000a\u000a{{DEFAULTSORT:Radev, Dragomir R.}}\u000a[[Category:Year of birth missing (living people)]]\u000a[[Category:Living people]]\u000a\u000a[[Category:Columbia University alumni]]\u000a[[Category:American computer scientists]]\u000a[[Category:University of Michigan faculty]]\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p210
asI121
(lp211
VNess Computing
p212
aV{{Notability|Companies|date=July 2011}}\u000a\u000a'''Ness Computing''' is a personal search company. It was acquired by OpenTable in 2014 and is being shut down in April.<ref>{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}</ref> \u000a\u000aIt was founded in October 2009 by Corey Reese,<ref>http://www.linkedin.com/in/coreyreese</ref> Paul Twohey,<ref>http://www.linkedin.com/in/twohey</ref> Nikhil Raghavan,<ref>http://www.linkedin.com/in/nikhilraghavan</ref> and Steven Schlansker.<ref>http://www.linkedin.com/in/stevenschlansker</ref> The company is headquartered in Los Altos, California.\u000a\u000aThe company, whose mission is to make search personal, is sometimes referred to as the "Palantir for fun". It aims to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. \u000a\u000aNess' mission is to make search personal. The company refers to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that uses [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that serves up results based on these signals. \u000a\u000aThe free Ness Dining App (for iPhone) has been referred to as the [[Netflix]] <ref>http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php</ref> or [[Pandora]] <ref>http://gigaom.com/2011/08/25/ness-restaurant-app/</ref> for restaurants. Based on a user's ratings and preferences, the service will deliver recommendations for a particular time, location, price range, and cuisine preference. Users may view the menu for a place via SinglePlatform,<ref>http://www.singleplatform.com/</ref> browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]]. The app is free and available in the [[App Store (iOS)]].\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies based in California]]
p213
asI91
(lp214
VFuzzy retrieval
p215
aV'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.\u000a\u000a==Mixed Min and Max model (MMM)==\u000a\u000aIn fuzzy-set theory, an element has a varying degree of membership, say ''d<sub>A</sub>'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).<br />\u000aIn MMM<ref>{{citation | last=Fox | first=E. A. | coauthors=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}</ref> each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:<br/>\u000a:<math>d_{A\u005ccap B}= min(d_A, d_B)</math>\u000a:<math>d_{A\u005ccup B}= max(d_A,d_B)</math>\u000a\u000aAccording to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d<sub>A</sub>, d<sub>B</sub>)'' and the similarity of the document to the ''and'' query to be ''min(d<sub>A</sub>, d<sub>B</sub>)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.\u000a\u000aGiven a document ''D'' with index-term weights ''d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>'' for terms ''A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>'', and the queries:\u000a\u000a''Q<sub>or</sub> = (A<sub>1</sub> or A<sub>2</sub> or ... or A<sub>n</sub>)''<br />\u000a''Q<sub>and</sub> = (A<sub>1</sub> and A<sub>2</sub> and ... and A<sub>n</sub>)''\u000a\u000athe query-document similarity in the MMM model is computed as follows:\u000a\u000a''SlM(Q<sub>or</sub>, D) = C<sub>or1</sub> * max(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>or2</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>)''<br />\u000a''SlM(Q<sub>and</sub>, D) = C<sub>and1</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>and2</sub> * max(d<sub>A1</sub>, d<sub>A2</sub> ..., d<sub>An</sub>)''\u000a\u000awhere ''C<sub>or1</sub>, C<sub>or2</sub>'' are "softness" coefficients for the ''or'' operator, and ''C<sub>and1</sub>, C<sub>and2</sub>'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C<sub>or1</sub> > C<sub>or2</sub> and C<sub>and1</sub> > C<sub>and2</sub>''. For simplicity it is generally assumed that ''C<sub>or1</sub> = 1 - C<sub>or2</sub>'' and ''C<sub>and1</sub> = 1 - C<sub>and2</sub>''.\u000a\u000aLee and Fox<ref name="leefox">{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> experiments indicate that the best performance usually occurs with ''C<sub>and1</sub>'' in the range [0.5, 0.8] and with ''C<sub>or1</sub>'' > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].\u000a\u000a==Paice model==\u000a\u000aThe Paice model<ref>{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}</ref> is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:\u000a\u000a:<math>S(D,Q) = \u005csum_{i=1}^n\u005cfrac{r^{i-1}*w_{di}}{\u005csum_{j=1}^n r^{j-1}}</math>\u000a\u000awhere ''r'' is a constant coefficient and ''w<sub>di</sub>'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.\u000a\u000aThe experiments of Lee and Fox<ref name="leefox"/> have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.\u000a\u000a==Improvements over the Standard Boolean model==\u000aLee and Fox<ref name="leefox"/> compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:\u000a{| class="wikitable"\u000a|-\u000a!\u000a! CISI\u000a! CACM\u000a! INSPEC\u000a|-\u000a! MMM\u000a| 68%\u000a| 109%\u000a| 195%\u000a|-\u000a! Paice\u000a| 77%\u000a| 104%\u000a| 206%\u000a|}\u000a\u000aThese are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.\u000a\u000a==Recent work==\u000a\u000aRecently '''Kang ''et al.'''.<ref>{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last=Kang | first=Bo-Yeong | coauthors=Dae-Won Kim, Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}</ref> have devised a fuzzy retrieval system indexed by concept identification.\u000a\u000aIf we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.<br />\u000aThey report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.\u000a\u000aZadrozny<ref>{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first=S\u0142awomir | last=Zadrozny | coauthors=Nowacka, Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}</ref> revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:\u000a* assuming linguistic terms as importance weights of keywords also in documents\u000a* taking into account the uncertainty concerning the representation of documents and queries\u000a* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh\u2019s fuzzy logic (calculus of linguistic statements)\u000a* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries\u000a\u000aThe proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.\u000a\u000a==See also==\u000a*[[Information retrieval]]\u000a\u000a==Further reading==\u000a* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Fuzzy Retrieval}}\u000a[[Category:Information retrieval]]
p216
asI124
(lp217
VAutomatic Content Extraction
p218
aV{{Multiple issues|\u000a{{citation style|date=December 2011}}\u000a{{technical|date=October 2012}}\u000a{{abbreviations|date=October 2012}}\u000a}}\u000a'''Automatic Content Extraction (ACE)''' is a program for developing advanced [[Information extraction]] [[technologies]]. Given a text in [[natural language]], the ACE challenge is to detect:\u000a# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.\u000a# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.\u000a# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.\u000a\u000aThis program began with a [[pilot study]] in 1999.\u000a\u000aWhile the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of ASR and OCR output from such transducers is.\u000a\u000aThe program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.\u000a\u000aThe effort involves:\u000a* defining the research tasks in detail,\u000a* collecting and annotating data needed for training, development, and evaluation,\u000a* supporting the research with evaluation tools and [[research workshop]]s.\u000a\u000aIn general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called \u201cnamed entity\u201d task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an\u000aanswer. In a real sense, the task is to detect things that \u201caren\u2019t there\u201d.\u000a\u000aThe ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.\u000a\u000a==References==\u000a* [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. The automatic content extraction (ACE) program\u2013tasks, data, and evaluation. 2004]\u000a\u000a==External links==\u000a* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.\u000a* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)\u000a* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)\u000a\u000a[[Category:Information retrieval]]
p219
as.None

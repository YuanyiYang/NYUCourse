(dp0
I2
(dp1
S'docBody'
p2
V'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\u000a\u000aDocument retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.\u000a\u000a==Description==\u000aDocument retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.\u000a\u000aA document retrieval system has two main tasks:\u000a# Find relevant documents to user queries\u000a# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].\u000a\u000aInternet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.\u000a\u000a==Variations==\u000aThere are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.\u000a\u000a===Form based===\u000aForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.\u000a\u000a===Content based===\u000aThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.\u000a\u000aA ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.\u000a\u000a==Example: PubMed==\u000aThe [[PubMed]]<ref>{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319\u201323 |year=2001 |pmid=11825203 |pmc=2243528 }}\u000a</ref> form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>\u000a\u000a== See also ==\u000a\u000a* [[Compound term processing]]\u000a* [[Document classification]]\u000a* [[Enterprise search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Latent semantic indexing]]\u000a* [[Search engine]]\u000a\u000a== References ==\u000a\u000a<references/>\u000a\u000a==Further reading==\u000a* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267\u2013288|doi=10.1145/2275.357411}}\u000a* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453\u2013490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}\u000a* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613\u2013633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}\u000a\u000a== External links ==\u000a* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Electronic documents]]\u000a[[Category:Substring indices]]\u000a\u000a[[zh:\u6587\u672c\u4fe1\u606f\u68c0\u7d22]]
p3
sS'docID'
p4
S'2'
p5
sS'title'
p6
VDocument retrieval
p7
ssI132
(dp8
g2
V{{multiple issues|\u000a{{Refimprove|date=May 2014}}\u000a{{Tone|article|date=January 2013}}\u000a}}\u000aA search engine is a type of computer software used to search data in the form of text or a database for specified information.<ref>{{cite web|title=Define search engine|url=http://www.webopedia.com/TERM/S/search_engine.html|accessdate=1 June 2014}}</ref>\u000a\u000aSearch engines normally consist of spiders (also known as bots) which roam the web searching for links and keywords. They send collected data back to the indexing software which categorizes and adds the links to databases with their related keywords. When you specify a search term the engine does not scan the whole web but extracts related links from the database.\u000a\u000a==History of Search Technology==\u000a\u000a{{Empty section|date=July 2014}}\u000a\u000a== The Memex ==\u000a\u000aThe concept of hypertext and a memory extension originates from an article that was published in [[The Atlantic Monthly]] in July 1945 written by [[Vannevar Bush]], titled [[As We May Think]].  Within this article Vannevar urged scientists to work together to help build a body of knowledge for all mankind. He then proposed the idea of a virtually limitless, fast, reliable, extensible, associative memory storage and retrieval system. He named this device a [[memex]].<ref>{{cite journal|last1=Yeo|first1=Richard|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|page=21|doi=10.1017/S0269889706001128}}</ref>\u000a\u000aBush regarded the notion of \u201cassociative indexing\u201d as his key conceptual contri- bution. As he explained, this was \u201ca provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex. The process of tying two items together is the important thing.\u201d This \u201clinking\u201d (as we now say) constituted a \u201ctrail\u201d of documents that could be named, coded, and found again. Moreover, after the original two items were coupled, \u201cnumerous items\u201d could be \u201cjoined together to form a trail\u201d; they could be \u201creviewed in turn, rapidly or slowly, by deflecting a lever like that used for turning the pages of a book. It is exactly as though the physical items had been gathered together from widely separated sources and bound together to form a new book\u201d<ref>{{cite journal|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|pages=21\u201347|doi=10.1017/S0269889706001128|accessdate=1 June 2014|postscript=The example Bush gives is a quest to find information on the relative merits of the Turkish short bow and the English long bow in the crusades}}</ref>\u000a\u000aAll of the documents used in the memex would be in the form of microfilm copy acquired as such or, in the case of personal records, transformed to microfilm by the machine itself. Memex would also employ new retrieval techniques based on a new kind of associative indexing the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another to create personal "trails" through linked documents. The new procedures, that Bush anticipated facilitating information storage and retrieval would lead to the development of wholly new forms of encyclopedia.\u000a\u000aThe most important mechanism, conceived by Bush and considered as closed to the modern hypertext systems is the associative trail. It would be a way to create a new linear sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and side trails.\u000aThe essential feature of the memex [is] the process of tying two items together\u2026 When the user is building a trail, he names it in his code book, and taps it out on his keyboard. Before him are the two items to be joined, projected onto adjacent viewing positions. At the bottom of each there are a number of blank code spaces, and a pointer is set to indicate one of these on each item. The user taps a single key, and the items are permanently joined\u2026 Thereafter, at any time, when one of these items is in view, the other can be instantly recalled merely by tapping a button below the corresponding code space.\u000a\u000aIn the article of Bush is not described any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set. Instead, when the user made an entry, such as a new or annotated manuscript, or image, he was expected to index and describe it in his personal code book. Later on, by consulting his code book, the user could retrace annotated and generated entries.\u000a\u000aIn 1965 Bush took part in the project INTREX of MIT, for developing technology for mechanization the processing of information for library use. In his 1967 essay titled "Memex Revisited", he pointed out that the development of the digital computer, the transistor, the video, and other similar devices had heightened the feasibility of such mechanization, but costs would delay its achievements. He was right again.\u000a\u000aTed Nelson, who later did pioneering work with first practical hypertext system and coined the term "hypertext" in the 1960s, credited Bush as his main influence.<ref>{{cite web|title=The MEMEX of Vannevar Bush|url=http://history-computer.com/Internet/Dreamers/Bush.html}}</ref>\u000a\u000a== SMART ==\u000a\u000aGerard Salton, who died on August 28 of 1995, was the father of modern search technology. His teams at Harvard and Cornell developed the SMART informational retrieval system. Salton\u2019s Magic Automatic Retriever of Text included important concepts like the vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), term discrimination values, and relevancy feedback mechanisms.\u000a\u000aHe authored a 56 page book called A Theory of Indexing which explained many of his tests upon which search is still largely based.\u000a\u000a== String Search Engines ==\u000a\u000aIn 1987 an article was published detailing the development of a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-\u03bcm n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a content addressable memory (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.<ref>{{cite journal|last=Yamada|first=H.|author2=Hirata, M. |author3=Nagai, H. |author4= Takahashi, K. |title=A high-speed string-search engine|journal=IEEE Journal of Solid-State Circuits|date=Oct 1987|volume=22|issue=5|pages=829\u2013834|doi=10.1109/JSSC.1987.1052819|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1052819&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1052819|accessdate=30 May 2014|publisher=IEEE}}</ref>\u000a\u000a<!-- Potential source for article expansion:  http://ieeexplore.ieee.org/search/searchresult.jsp?queryText%3Dsearch-engine&sortType=asc_p_Publication_Year&pageNumber=1&resultAction=SORT -->\u000a\u000a== Web Search Engines ==\u000a\u000a=== Archie ===\u000a\u000aThe first web search engines was Archie, created in 1990<ref name="intelligent-technologies">{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=87|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program "archives," but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. For more information on where Archie is today, see:\u000ahttp://www.bunyip.com/products/archie/\u000a\u000aThe primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\u000a\u000aInitially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, "anonymous" FTP sites became repositories for files, allowing all users to post and retrieve them.\u000a\u000aEven with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\u000a\u000aArchie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.<ref name="wileyhistory">{{cite web|title=A History of Search Engines|url=http://www.wiley.com/legacy/compbooks/sonnenreich/history.html|publisher=Wiley|accessdate=1 June 2014}}</ref>\u000a\u000a=== Veronica ===\u000a\u000aIn 1993, the University of Nevada System Computing Services group developed Veronica.<ref name="intelligent-technologies"/> It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.<ref name="wileyhistory"/>\u000a\u000a=== The Lone Wanderer ===\u000a\u000aThe World Wide Web Wanderer, developed by Matthew Gray in 1993<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=86|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\u000a\u000aMatthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\u000a\u000aIn response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\u000a\u000aALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot doesn't run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they don't submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.<ref name="wileyhistory"/>\u000a\u000a=== Excite ===\u000a\u000aExcite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\u000aTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.<ref name="wileyhistory"/>\u000a\u000aExcite was the first serious commercial search engine which launched in 1995.<ref>{{cite web|title=The Major Search Engines|url=http://www.pccua.edu/kholland/major_search_engines.htm|accessdate=1 June 2014|date=21 January 2014}}</ref> It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\u000a\u000a=== Yahoo! ===\u000a\u000aIn April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\u000a\u000aAs the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\u000a\u000aThe Wanderer captured only URLs, which made it difficult to find things that weren\u2019t explicitly described by their URL. Because URLs are rather cryptic to begin with, this didn\u2019t help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\u000a\u000a=== Lycos ===\u000a\u000aAt Carnegie Mellon University during the July of 1994, Michael Mauldin, on leave from CMU,developed the Lycos search engine.\u000a\u000a== Types of Web Search Engines ==\u000a\u000aSearch engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=85|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref>\u000a\u000a# Finding and selecting full or partial content based on the keywords provided.\u000a# Maintaining index of the content and referencing to the location they find\u000a# Allowing users to look for words or combinations of words found in that index.\u000a\u000aThe process begins when a user enters a query statement into the system through the interface provided.\u000a\u000a{| class="wikitable"\u000a|-\u000a! Type\u000a! Example\u000a! Description\u000a|-\u000a| Conventional\u000a| librarycatalog\u000a| Search by keyword, title, author, etc.\u000a|-\u000a| Text-based\u000a| Lexis-Nexis,Google,Yahoo!\u000a| Search by keywords. Limited search using queries in natural language.\u000a|-\u000a| Multimedia\u000a| QBIC, WebSeek, SaFe\u000a| Search by visual appearance (shapes, colors,..)\u000a|-\u000a| Q/A\u000a| [[Stack Exchange]], NSIR\u000a| Search in (restricted) natural language\u000a|-\u000a| Clustering Systems\u000a| Vivisimo, Clusty\u000a|\u000a|-\u000a| Research Systems\u000a| Lemur, Nutch\u000a|\u000a|}\u000a\u000aThere are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\u000a\u000aCrawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\u000a\u000aHuman-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\u000a\u000aIn both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created \u2014you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index hasn't been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\u000a\u000aSo why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\u000a\u000aOne of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\u000a\u000aAnother common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered "important" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\u000a\u000aModern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. [[Google]]), database or structured data search engines (e.g. [[Dieselpoint]]), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and [[Yahoo!]], utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\u000a\u000a==Search engine categories==\u000a\u000a===Web search engines===\u000aSearch engines that are expressly designed for searching web pages, documents, and images were developed to facilitate searching through a large, nebulous blob of unstructured resources. They are engineered to follow a multi-stage process: crawling the infinite stockpile of pages and documents to skim the figurative foam from their contents, indexing the foam/buzzwords in a sort of semi-structured form (database or something), and at last, resolving user entries/queries to return mostly relevant results and links to those skimmed documents or pages from the inventory.\u000a\u000a====Crawl====\u000aIn the case of a wholly textual search, the first step in classifying web pages is to find an \u2018index item\u2019 that might relate expressly to the \u2018search term.\u2019 In the past, search engines began with a small list of URLs as a so-called seed list, fetched the content, and parsed the links on those pages for relevant information, which subsequently provided new links. The process was highly cyclical and continued until enough pages were found for the searcher\u2019s use.\u000aThese days, a continuous crawl method is employed as opposed to an incidental discovery based on a seed list. The crawl method is an extension of aforementioned discovery method. Except there is no seed list, because the system never stops worming.\u000a\u000aMost search engines use sophisticated scheduling algorithms to \u201cdecide\u201d when to revisit a particular page, to appeal to its relevance. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of chance, popularity, and overall quality of site. The speed of the web server running the page as well as resource constraints like amount of hardware or bandwidth also figure in.\u000a\u000a====Link map====\u000aThe pages that are discovered by web crawls are often distributed and fed into another computer that creates a veritable map of resources uncovered. The bunchy clustermass looks a little like a graph, on which the different pages are represented as small nodes that are connected by  links between the pages. \u000aThe excess of data is stored in multiple data structures that permit quick access to said data by certain algorithms that compute the popularity score of pages on the web based on how many links point to a certain web page, which is how people can access any number of resources concerned with diagnosing psychosis. Another example would be the accessibility/rank of web pages containing information on Mohamed Morsi versus the very best attractions to visit in Cairo after simply entering \u2018Egypt\u2019 as a search term. One such algorithm, [[PageRank]], proposed by Google founders Larry Page and Sergey Brin, is well known and has attracted a lot of attention because it highlights repeat mundanity of web searches courtesy of students that don\u2019t know how to properly research subjects on Google.\u000aThe idea of doing link analysis to compute a popularity rank is older than PageRank. Other variants of the same idea are currently in use \u2013 grade schoolers do the same sort of computations in picking kickball teams. But in all seriousness, these ideas can be categorized into three main categories: rank of individual pages and nature of web site content. Search engines often differentiate between internal links and external links, because web masters and mistresses are not strangers to shameless self-promotion. Link map data structures typically store the anchor text embedded in the links as well, because anchor text can often provide a \u201cvery good quality\u201d summary of a web page\u2019s content.\u000a\u000a===Database Search Engines===\u000aSearching for text-based content in databases presents a few special challenges from which a number of specialized search engines flourish. Databases can be slow when solving complex queries (with multiple logical or string matching arguments). Databases allow pseudo-logical queries which full-text searches do not use. There is no crawling necessary for a database since the data is already structured. However, it is often necessary to index the data in a more economized form to allow a more expeditious search.\u000a\u000a===Mixed Search Engines===\u000aSometimes, data searched contains both database content and web pages or documents. Search engine technology has developed to respond to both sets of requirements. Most mixed search engines are large Web search engines, like Google. They search both through structured and unstructured data sources. Take for example, the word \u2018ball.\u2019 In its simplest terms, it returns more than 40 variations on Wikipedia alone. Did you mean a ball, as in the social gathering/dance? A soccer ball? The ball of the foot? Pages and documents are crawled and indexed in a separate index. Databases are indexed also from various sources. Search results are then generated for users by querying these multiple indices in parallel and compounding the results according to \u201crules.\u201d\u000a\u000a<!-- \u000aWorking on article, loosely pasting in snippets of information to use in improving \u000aarticle later, leaving all this in comments while I work on it\u000a\u000aLOTS OF WORK TO DO\u000a\u000aPotential sections to research into..\u000a\u000a== Models of Information Retrieval ==\u000a=== Boolean Model ===\u000a=== Vector Model ===\u000a\u000a== Document Preprocessing ==\u000a# Tokenization ===\u000a# Stemming ===\u000a# The Porter Algorithm\u000a# Storing, indexing, and searching text\u000a#Inverted indexes\u000a\u000a== Word Distributions ==\u000aThe Zipf distribution\u000aThe Benford distribution\u000aHeap's law. TF*IDF. Vector space similarity and ranking.\u000a\u000a== Retrieval evaluation ==\u000a Precision and Recall. F-measure. Reference collections. The TREC conferences.\u000a\u000a== Automated indexing/labeling ==\u000a. Compression and coding. Optimal codes.\u000a\u000a== String matching ==\u000a. Approximate matching.\u000a\u000a== Query expansion ==. Relevance feedback.\u000a\u000a== Text classification ==\u000a. Naive Bayes. Feature selection. Decision trees.\u000a\u000aLinear classifiers. k-nearest neighbors. Perceptron. Kernel methods. Maximum-margin classifiers. Support vector machines. Semi-supervised learning.\u000aLexical semantics and Wordnet.\u000aLatent semantic indexing. Singular value decomposition. Vector space clustering. k-means clustering. EM clustering.\u000aRandom graph models. Properties of random graphs: clustering coefficient, betweenness, diameter, giant connected component, degree distribution.\u000aSocial network analysis. Small worlds and scale-free networks. Power law distributions. Centrality.\u000aGraph-based methods. Harmonic functions. Random walks. PageRank. Hubs and authorities. Bipartite graphs. HITS. Models of the Web.\u000a\u000aCrawling the web. Webometrics. Measuring the size of the web. The Bow-tie-method.\u000aHypertext retrieval. Web-based IR. Document closures. Focused crawling.\u000aQuestion answering\u000aBurstiness. Self-triggerability\u000aInformation extraction\u000aAdversarial IR. Human behavior on the web. Text summarization\u000a\u000a== Search Engine Parts ==\u000a\u000aThere are three main parts to every search engine: Spider, Index, and Web Interface.\u000a\u000a=== Spider === \u000a   \u000aA spider crawls the web. It follows links and scans web pages. All search engines have periods of deep crawl and quick crawl. During a deep crawl, the spider follows all links it can find and scans web pages in their entirety. During a quick crawl, the spider does not follow all links and may not scan pages in their entirety.\u000a\u000aThe job of the spider is to discover new pages and to collect copies of those pages, which are then analyzed in the index.\u000a\u000a==== Crawl Rate ====\u000a\u000aPages that are considered important get crawled frequently. The crawl rate depends directly on link popularity and domain authority.\u000a\u000aIf many links point to a website, it may be an important site, so it makes sense to crawl it more often than a site with fewer links. This is also a money-saving issue. If search engines were to crawl all sites at an equal rate, it would take more time overall and cost more as a result.\u000a\u000a=== Index ===\u000a\u000aThe index is the place where search engines keep basic copies of web pages and sort search results. When you a do a search, search engines do not search the web; they show results from their index. The number of pages in the index does not represent the entire web, but the number of pages that the spider has discovered, scanned and saved.\u000a\u000aThe index is the place where search engineers apply algorithms, and it is the place where rankings are partially determined. Search engineers may choose to apply an algorithm to the entire index, or only to a portion of it.\u000a\u000a==== Datacenters and Different Indexes ====\u000a\u000aSearch engines have multiple datacenters around the world. When you enter a search term, your query is directed to the closest datacenter.\u000a\u000aDifferent datacenters may have slightly different indexes, especially during an update. As a result, search results may differ depending on your location.\u000a\u000a== History ==\u000a\u000a=== Meta Tags ===\u000a\u000aMeta tags were designed to help search engines sort web pages. Pages included keywords in meta tags telling search engines about the contents of each page. For a short time meta tags worked and helped search engines serve relevant results, but over time marketers learned they could easily rank by stuffing those tags with keywords.\u000a\u000aAs a result, search engine optimization in those days became about cramming "loans, loans, loans, loans, loans" into the meta tag. Search engines got spammed beyond being of any use, and many faced an exodus of users as a result.\u000a\u000aYahoo started as web directory in 1994 and outsourced their search until 2004. Google launched in 1996 and did not have a successful business model until 2001. Microsoft did not come on the search engine scene until 2003.\u000aor more information on search engine history, you may want to investigate Search Engine History, a site entirely devoted to this topic. It also touches on the history of search engine optimization. Additionally, Web Master World has an excellent thread that covers the history of SEO.\u000a\u000aWeb Interface\u000a\u000aWhen you search using a web interface (like Google.com), in many cases results are already presorted to a certain extent. The degree to which results are presorted depends on the complexity of the algorithm. If the time to apply an algorithm to the index is considerable, then that algorithm is applied in advance. On the other hand, some algorithms are applied at the time when the search query is requested.\u000a\u000aSearch queries go through analysis to determine the possible intent behind the query. Google is currently leading in this area.\u000a\u000aStop Words\u000a\u000a"Stop words" are words that are frequently used in the English language. Those words include a, the, all, also, but, down, full, much etc. They are words that are used by everyone regardless of the topic. Generally, search engines ignore "stop" words and will usually correct your search to exclude them. For example, when you search for "cat and dog" search engines will exclude "and" and only search for "cat" "dog."\u000a\u000aGoogle does use stop words to an extent.\u000a\u000aKeyword Density\u000a\u000aKeyword density is a measure of how often a word appears on the page in relation to other words. It is an over-hyped measurement that doesn\u2019t help in search rankings. Search engines use far more than keyword density for on-page analysis. Their technology includes the location of terms on the page, word proximity and natural language processing.\u000a\u000aGoogle has purchased Applied Semantics for its AdSense Network, but may also be using this technology for on-page analysis. Additionally, please keep in mind that one of Google\u2019s current projects involves scanning thousands of books, from which it may learn more about natural language patterns.\u000a\u000aLocation of Terms on The Page\u000a\u000aBy analyzing how terms are located in relation to each other on the page, search engines can determine partial relevancy of the page. The closer terms are to each other, the more relevant a page is.\u000a\u000aIn many cases, keywords appear separately from each other throughout the page. This is considered normal in most cases, but be sure to include a term together at least once in the title, heading or paragraph.\u000a\u000aLink Analysis\u000a\u000aLink analysis is at the core of all search engine relevancy. Apart from Page Rank and general link popularity, Google looks at: link anchor text, the page from which the link comes, age of the link, location of the link, title of the page from which the link comes, authority of the linking page and more.\u000a\u000aLinks are the biggest quality indicators that search engines have at the moment. Before search engines existed, and before the web was commercialized it was much harder to find information. All you had to rely on was links. There were few if any spammers, and people who found interesting sites shared those sites with others by placing a link. Also, the first web pages and servers were universities and colleges; this is why Google is biased toward .edu domains \u2013 they were the first on the scene, and usually contain quality content and resources.\u000a\u000aAs the web became commercial and Google\u2019s Page Rank well known, links became a form of advertising, where a link could be bought or artificially made by spammers. This is the reason for Google\u2019s bias toward older links and links from trusted domains.\u000a\u000aYahoo put less weight on link analysis than Google, while Ask.com is more about "authoritative hubs." Ask.com generally has a harder time ranking documents unless there\u2019s a community around a topic.\u000a\u000aSize and Length of the Page\u000a\u000aThere\u2019s no "best" page copy length for ranking on search results. Search engines have specifically addressed this issue, and both long content and short content have equal chances to rank.\u000a\u000aBehavioral Feedback\u000a\u000aAll major search engines such as Google, Yahoo, Live and Ask collect user feedback about web pages. They look at search queries, prior search queries, time interval between those queries and semantic relationships in order to learn more about intent. They also track click through rates for different listings. If, for example, users click on a listing and then go back right away, search engines may remove that listing and artificially lower its position for one or more keywords.\u000a\u000aThis brings up the fact that user experience is becoming an important part of SEO. As search engines collect more data, they are constantly learning to interpret it. As they get better at it, retaining users on your pages for a certain time period (maybe a benchmark for an industry) may become an important factor in the SEO game.\u000a\u000aBehavior feedback is currently used in personalized search.\u000a<ref>{{cite web|url=http://www.seochat.com/c/a/search-engine-news/the-history-of-search-and-search-technology/|accessdate=1 June 2014}}</ref>\u000a-->\u000a\u000a==See also==\u000a*[[Database search engine]]\u000a*[[Enterprise search]]\u000a*[[Search engine]]\u000a*[[Disambiguation]]\u000a*[[Search engine indexing]]\u000a*[[Web crawler]]\u000a*[[Structured Search]]\u000a\u000a==External links==\u000a* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\u000a* [http://www.urbandictionary.com/define.php?term=Searchency Searchency]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Search Engine Technology}}\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p9
sg4
S'132'
p10
sg6
VSearch engine technology
p11
ssI7
(dp12
g2
VThe '''European Conference on Information Retrieval''' (ECIR) is the main \u000aEuropean research conference for the presentation of new results in the field of [[information retrieval]] (IR).\u000aIt is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).\u000a      \u000aThe event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was \u000aheld in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has\u000aalternated between the United Kingdom and continental Europe. To mark the metamorphosis\u000afrom a small informal colloquium to a major event in the IR research calendar, the \u000aBCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,\u000aECIR has continued to grow and has become the major European forum for the discussion\u000aof research in the field of Information Retrieval. \u000a\u000aSome of the topics dealt with include:\u000a* IR models, techniques, and algorithms\u000a* IR applications\u000a* IR system architectures\u000a* Test and evaluation methods for IR\u000a* [[Natural Language Processing]] for IR\u000a* Distributed IR\u000a* Multimedia and cross-media IR\u000a\u000a==Time and Location==\u000a\u000aTraditionally, the ECIR is held in Spring, near the Easter weekend. Previous locations include\u000athe following:\u000a\u000a* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]\u000a* [[Moscow, Russia]], 2013 [http://ecir2013.org/]\u000a* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]\u000a* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]\u000a* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]\u000a* [[Toulouse]], 2009 [http://ecir09.irit.fr/]\u000a* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]\u000a* [[Rome]], 2007 [http://ecir2007.fub.it/]\u000a* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]\u000a* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]\u000a* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]\u000a* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]\u000a* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*\u000a* [[Darmstadt]], 2001* (organized by GMD)\u000a* [[Cambridge]], 2000* (organized by Microsoft Research)\u000a* [[Glasgow]], 1999*\u000a* [[Grenoble]], 1998*\u000a* [[Aberdeen, Scotland|Aberdeen]], 1997*\u000a* [[Manchester]], 1996*\u000a* [[Crewe]], 1995* (organized by Manchester Metropolitan University)\u000a* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)\u000a* [[Glasgow]], 1993* (organized by Strathclyde University)\u000a* [[Lancaster, Lancashire|Lancaster]], 1992*\u000a* [[Lancaster, Lancashire|Lancaster]], 1991*\u000a* [[Huddersfield]], 1990*\u000a* [[Huddersfield]], 1989*\u000a* [[Huddersfield]], 1988*\u000a* [[Glasgow]], 1987*\u000a* [[Glasgow]], 1986*\u000a* [[Bradford]], 1985*\u000a* [[Bradford]], 1984*\u000a* [[Sheffield]], 1983*\u000a* [[Sheffield]], 1982*\u000a* [[Birmingham]], 1981*\u000a* [[Leeds]], 1980*\u000a* [[Leeds]], 1979*\u000a\u000a<br /> *as the Annual Colloquium on Information Retrieval Research\u000a\u000aFuture locations include:\u000a* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]\u000a\u000a==External links==\u000a* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computer science conferences]]
p13
sg4
S'7'
p14
sg6
VEuropean Conference on Information Retrieval
p15
ssI137
(dp16
g2
V{{Infobox company\u000a| name = PolySpot\u000a| logo = [[File:PolySpot-Logo.jpg|300px]]\u000a| type = [[Privately held company|Private]]\u000a| foundation = [[Paris]] (2001)\u000a| location = [[Paris]], [[London]]\u000a| key_people = Guy Mounier, CEO\u000a| industry = [[Information technology]] <br/> [[Unified Information Access]] <br/> [[Search Engine]]\u000a| products = PolySpot Infowarehouse<br/>PolySpot Information At Work<br/>PolySpot Enterprise Search\u000a| slogan = Open Search Solutions\u000a| homepage = [http://www.polyspot.com/en/ www.polyspot.com]\u000a}}\u000a'''PolySpot''' is a subsidiary of CustomerMatrix, an [[enterprise search]] [[ISV|software company]].\u000a\u000aCreated in 2001, PolySpot has its headquarters in Paris, France. It also has offices in the United Kingdom.\u000a\u000aIn 2011, PolySpot raised EUR 2.5m from [[Newfund]].<ref>{{cite web|url=http://finance.yahoo.com/news/PolySpot-Raises-2-Million-prnews-4194799492.html| title=Yahoo News: PolySpot Raises 2 Million}}</ref> and True Global Ventures.\u000a\u000aIn 2013, CustomerMatrix (US) acquired PolySpot. In December 2013, PolySpot reduced its shareholders equity from 507,209 EUR to 206,120 EUR.\u000a\u000a== Functionalities ==\u000a\u000aPolySpot's infrastructure provide a unified information access to all data, through which users can instantly interact with all available information resources, both inside and outside the company, and regardless of whether or how the data are structured.\u000a\u000aPolySpot's indexing capabilities are based on the [[Apache Software Foundation|Apache]] [[Lucene]] [[Solr]] Java-based open-source projects.\u000a\u000a==References==\u000a{{Portal|Software}}\u000a{{Reflist|colwidth=30em}}\u000a\u000a==External links==\u000a*[http://www.polyspot.com/en/ Website]\u000a*[http://www.customermatrix.com/ CustomerMatrix Website]\u000a*[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=38687243 Company profile in BusinessWeek]\u000a*[http://www.arnoldit.com/search-wizards-speak/polyspot-2.html PolySpot in ArnoldIT]\u000a*[http://arnoldit.com/wordpress/2010/01/13/polyspot-lands-crdit-agricole-sa/ PolySpot lands Credit Agricole in ArnoldIT]\u000a\u000a{{DEFAULTSORT:Polyspot}}\u000a[[Category:Searching]]\u000a[[Category:Search engine software|*Enterprise search vendors]]\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p17
sg4
S'137'
p18
sg6
VPolySpot
p19
ssI12
(dp20
g2
V{{multiple issues|\u000a{{notability|Web|date=February 2012}}\u000a{{refimprove|date=June 2009}}\u000a{{primary sources|date=February 2012}}\u000a{{one source|date=February 2012}}\u000a{{no footnotes|date=February 2012}}\u000a}}\u000a\u000a'''BASE''' ('''Bielefeld Academic Search Engine''') is a multi-disciplinary [[search engine]] to scholarly internet resources, created by [[Bielefeld University]] Library in [[Bielefeld]], [[Germany]]. It is based on search technology provided by [[Fast Search & Transfer]] (FAST), a [[Norway|Norwegian]] company. It [[Web harvesting|harvests]] OAI metadata from scientific [[Digital repository|digital repositories]] that implement the [[Open Archives Initiative Protocol for Metadata Harvesting]] (OAI-PMH), and are [[Index (search engine)|indexed]] using FAST's software. In addition to OAI [[metadata]], the library indexes selected web sites and local data collections, all of which can be searched via a single search interface.\u000a\u000aIt allows those who use the search engine to search metadata, when available, as well as conducting [[full text search]]es. It contrasts with commercial search engines in multiple ways, including in the types and kinds of resources it searches and the information it offers about the results it finds. Where available, [[Bibliographic database|bibliographic data]] is provided, and the results may be sorted by multiple fields, such as by author or year of publication.\u000a\u000a== See also ==\u000a* [[List of academic databases and search engines]]\u000a\u000a==External links==\u000a* [http://www.base-search.net/ BASE search]\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Open access (publishing)]]\u000a[[Category:Bibliographic databases]]\u000a\u000a\u000a{{software-stub}}
p21
sg4
S'12'
p22
sg6
VBASE (search engine)
p23
ssI142
(dp24
g2
V{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}\u000a\u000aIn the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>\u000a\u000aA thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a \u201ccontrolled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.\u201d\u000a\u000aA thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.\u000a\u000a== History ==\u000aWherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a \u201cthesaurus\u201d (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging & Classification Quarterly'', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.</ref>\u000a\u000aThe first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.\u000aHundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:\u000a(a)	Extension from monolingual to multilingual capability; and \u000a(b)	Addition of a conceptually organized display to the basic alphabetical presentation.\u000a\u000aHere we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:\u000a\u000a* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)\u000a* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)\u000a* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)\u000a* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)\u000a* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985\u000a* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)\u000a* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.\u000a\u000aThe most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>''[http://www.niso.org/schemas/iso25964/ ISO 25964 \u2013 the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.</ref>\u000a\u000a== Purpose ==\u000aIn information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.\u000a\u000aThe [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN\u2019s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.\u000a\u000a== Structure ==\u000aInformation retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, \u201ccitrus fruits\u201d might be linked to the broader concept of \u201cfruits\u201d, and the narrower ones of \u201coranges\u201d, \u201clemons\u201d, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as \u201cmad cow disease\u201d, \u201cbovine spongiform encephalopathy\u201d, \u201cBSE\u201d, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.\u000a\u000a== See also ==\u000a* [[Controlled vocabulary]]\u000a* [[ISO 25964]]\u000a* [[Thesaurus]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] \u000a* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Thesauri]]
p25
sg4
S'142'
p26
sg6
VThesaurus (information retrieval)
p27
ssI17
(dp28
g2
V[[Category:Documents]]\u000a[[Category:Digital media]]\u000a[[Category:Information retrieval]]\u000a[[Category:Electronic publishing]]
p29
sg4
S'17'
p30
sg6
VCategory:Electronic documents
p31
ssI147
(dp32
g2
VA '''[[concept]] search''' (or conceptual search) is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query.\u000a\u000a__TOC__\u000a\u000a==Why Concept Search?==\u000aConcept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.\u000a\u000aPolysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>\u000a\u000aIn addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.\u000a\u000aA concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.\u000a\u000a==Approaches to Concept Search==\u000aIn general, information retrieval research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>\u000a\u000aEfforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:\u000a\u000a* Auxiliary structures\u000a* Local [[co-occurrence]] statistics\u000a* Transform techniques (particularly [[matrix decomposition]]s)\u000a\u000a===Auxiliary Structures===\u000aA variety of techniques based on Artificial Intelligence (AI) and [[Natural language processing|Natural Language Processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as [[WordNet]], can be efficiently implemented by reusing retrieval models and data structures of classical [[Information Retrieval]].<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.\u000a\u000aHandcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>\u000a\u000a===Local Co-occurrence Statistics===\u000aInformation retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example,  5 sentences or  50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.\u000a\u000aThis approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only  of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>\u000a\u000a===Transform Techniques===\u000aSome of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>\u000a\u000a* [[Independent component analysis]]\u000a* Semi-discrete decomposition\u000a* [[Non-negative matrix factorization]]\u000a* [[Singular value decomposition]]\u000a\u000aMatrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>\u000a\u000aIndependent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>\u000a\u000aSingular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[Latent semantic indexing|Latent Semantic Indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>\u000a\u000a==Uses of Concept Search==\u000a* '''[[eDiscovery]]''' - Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.\u000aDisability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>\u000a\u000a* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' - Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>\u000a\u000a* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' - Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.\u000a\u000a* '''Multimedia and Publishing''' - Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>\u000a\u000a* '''Digital Libraries and Archives''' - Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.\u000a\u000a* '''Genomic Information Retrieval (GIR)''' - Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.\u000a\u000a* '''Human Resources Staffing and Recruiting''' - Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.\u000a\u000a==Effective Concept Searching==\u000aThe effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:\u000a\u000a* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.\u000a\u000a* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''\u000a\u000a* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.\u000a\u000aAs with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Sprck Jones|Sprck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42\u201349</ref>\u000a\u000a==Relevance Feedback==\u000a[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schtze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.\u000a\u000aIn general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.\u000a\u000aRelevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>\u000a\u000a[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>\u000a\u000a==Guidelines for Evaluating a Concept Search Engine==\u000a# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.\u000a# Result items should be sorted and ranked by relevance.\u000a# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.\u000a# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.\u000a# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.\u000a# Combined queries using concepts, keywords, and metadata should be allowed.\u000a# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.\u000a# Query-ready indexes should be created relatively quickly.\u000a# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.\u000a# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.\u000a\u000a==Search Engine Conferences and Forums==\u000aFormalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>\u000a\u000aIn 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.\u000a\u000aPrecision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>\u000a\u000aAlthough the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven\u2019t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>\u000a\u000a==See also==\u000a* [[approximate string matching]]\u000a* [[Compound term processing]]\u000a* [[Concept mining]]\u000a* [[Computational linguistics]]\u000a* [[Information extraction]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent semantic analysis]]\u000a* [[Semantic network]]\u000a* [[Semantic search]]\u000a* [[Semantic Web]]\u000a* [[Statistical semantics]]\u000a* [[Text mining]]\u000a* [[Word Sense Disambiguation]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]\u000a* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]\u000a* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]\u000a* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]\u000a\u000a[[Category:Information retrieval]]
p33
sg4
S'147'
p34
sg6
VConcept search
p35
ssI22
(dp36
g2
V{{unreferenced|date=January 2012}}\u000a'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.\u000a\u000aThe results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.\u000a\u000aAudio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. \u000a\u000aMusical audio mining (also known as [[Music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.\u000a\u000a==See also==\u000a* [[Speech Analytics]]\u000a\u000a\u000a[[Category:Speech recognition]]\u000a[[Category:Information retrieval]]\u000a[[Category:Computational linguistics]]
p37
sg4
S'22'
p38
sg6
VAudio mining
p39
ssI152
(dp40
g2
V{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}\u000a\u000a{{refimprove|date=July 2014}}\u000a'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, '''domain redirection''' or '''domain forwarding''' is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org].\u000aURL redirection can be used for [[URL shortening]], to prevent [[link rot|broken links]] when web pages are moved, to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]], to guide navigation into and out of a website, for privacy protection, and for less innocuous purposes such as [[phishing]] attacks.\u000a\u000a== Purposes ==\u000aThere are several reasons to use URL redirection :\u000a\u000a=== Similar domain names ===\u000aA user might mis-type a URL\u2014for example, "example.com" and "exmaple.com". Organizations often register these "mis-spelled" domains and re-direct them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.\u000a\u000a=== Moving pages to a new domain ===\u000aWeb pages may be redirected to a new domain for three reasons:\u000a* a site might desire, or need, to change its domain name;\u000a* an author might move his or her individual pages to a new domain;\u000a* two web sites might merge.\u000a\u000aWith URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers.\u000a\u000aThe same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.\u000a\u000a=== Logging outgoing links ===\u000aThe access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link.\u000a\u000aThis information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>\u000a{{cite journal\u000a  | title = Google revives redirect snoopery\u000a  | journal = blog.anta.net\u000a  | date = 2009-01-29\u000a  | url = http://blog.anta.net/2009/01/29/509/\u000a  | issn = 1797-1993\u000a  | archiveurl=http://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/\u000a  | archivedate=2011-08-17\u000a}}</ref>\u000a\u000aThe same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.\u000a\u000a=== Short aliases for long URLs ===\u000a{{Main|URL shortening}}\u000a\u000aWeb applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.\u000a\u000a=== Meaningful, persistent aliases for long or changing URLs ===\u000a{{See also|Permalink|PURL|Link rot}}\u000a\u000aSometimes the URL of a page changes even though the content stays the same. Therefore URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.\u000a\u000a=== Post/Redirect/Get ===\u000a{{Main|Post/Redirect/Get}}\u000a\u000aPost/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).\u000a\u000a=== Manipulating search engines ===\u000aRedirect techniques are used to fool search engines.  For example, one page could show popular search terms to search engines but redirect the visitors to a different target page.  There are also cases where redirects have been used to "steal" the page rank of one popular page and use it for a different page, They will also redirect using searches with search engines as searches, usually involving the 302 [[List of HTTP status codes|HTTP status code]] of "moved temporarily."<ref>{{cite web|url=http://www.pandia.com/sw-2004/40-hijack.html |title=Google's serious hijack problem \u2013 Spammers hijack web site listings in Google |date=September 13, 2004 |publisher=Pandia.com |archiveurl=http://web.archive.org/web/20130605153457/http://www.pandia.com/sw-2004/40-hijack.html |archivedate=2013-06-05}}</ref><ref>[http://www.loriswebs.com/hijacking_web_pages.html "Stop Scrapers From Hijacking your Web Pages"]. Lori's Web Design.com. Retrieved 2013-12-18.</ref>\u000a\u000aSearch engine providers have noticed the problem and are working on appropriate actions.{{Citation needed|date=August 2009}}\u000a\u000aAs a result, today, such manipulations usually result in less rather than more site exposure.\u000a\u000a=== Manipulating visitors ===\u000aURL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.{{Citation needed|date=January 2010}} Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.\u000a\u000a=== Removing <code>referer</code> information ===\u000aWhen a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.\u000a\u000a== Techniques ==\u000aSeveral different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.\u000a\u000a=== Manual redirect ===\u000aThe simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:\u000a\u000a<source lang="html4strict">\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis method is often used as a fall-back&nbsp;\u2014 if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.\u000a\u000a=== HTTP status codes 3xx ===\u000aIn the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page.  The different codes describe the reason for the redirect, which allows for the correct subsequent action (such as changing links in the case of code 301, a permanent change of address).\u000a\u000aHTTP/1.1 defines [http://tools.ietf.org/html/rfc7231#section-6.4 several status codes] for redirection:\u000a* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)\u000a* [[HTTP 301|301 moved permanently]]\u000a* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)\u000a* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)\u000a* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)\u000a\u000aAll of these status codes require that the URL of the redirect target be given in the Location: header of the HTTP response.  The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.\u000a\u000a(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).\u000a\u000aAn [[HTTP]] response with the 301 "moved permanently" redirect looks like this:\u000a\u000a<source lang="html4strict">\u000aHTTP/1.1 301 Moved Permanently\u000aLocation: http://www.example.org/\u000aContent-Type: text/html\u000aContent-Length: 174\u000a\u000a<html>\u000a<head>\u000a<title>Moved</title>\u000a</head>\u000a<body>\u000a<h1>Moved</h1>\u000a<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000a==== Using server-side scripting for redirection ====\u000aWeb authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:\u000a\u000a<source lang="php">\u000aheader('HTTP/1.1 301 Moved Permanently');\u000aheader('Location: http://www.example.com/');\u000aexit();\u000a</source>\u000a\u000a(More headers may be required to prevent caching<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=http://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref>).\u000a\u000aThe programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code>\u000a\u000aHTTP/1.1 [http://tools.ietf.org/html/rfc7231#section-7.1.2 allows for] either a relative URI reference or an absolute URI reference. If the URI reference is relative the client computes the required absolute URI reference according to [http://tools.ietf.org/html/rfc3986#section-5 the rules defined in RFC 3986].\u000a\u000a==== Apache mod_rewrite ====\u000aThe [[Apache HTTP Server]]'s [http://httpd.apache.org/docs/current/mod/mod_alias.html mod_alias] extension can be used to redirect certain requests.  Typical configuration directives look like:\u000a\u000a<source lang="apache">\u000aRedirect permanent /oldpage.html http://www.example.com/newpage.html\u000aRedirect 301 /oldpage.html http://www.example.com/newpage.html\u000a</source>\u000a</blockquote>\u000a\u000aFor more flexible URL rewriting and redirection, Apache [http://httpd.apache.org/docs/current/mod/mod_rewrite.html mod_rewrite] can be used.  E.g. to redirect a requests to a canonical domain name:\u000a<source lang="apache">\u000aRewriteEngine on\u000aRewriteCond %{HTTP_HOST} ^([^.:]+\u005c.)*oldsite\u005c.example\u005c.com\u005c.?(:[0-9]*)?$ [NC]\u000aRewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]\u000a</source>\u000a\u000aSuch configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>.htaccess</code> file.\u000a\u000a==== nginx rewrite ====\u000a[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su \u2014 Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a\u000aFor example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a<source lang="pcre">\u000a	location /DragonFly {\u000a		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;\u000a	}\u000a	location /d {\u000a		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";\u000a		set	$ds	"&section=";\u000a		rewrite	^/./([^/]+)\u005c.([1-9])$		$db$1$ds$2	redirect;\u000a	}\u000a</source>\u000a\u000a=== Refresh Meta tag and HTTP refresh header ===\u000a[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.  See\u000a* [http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]\u000a* [http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]\u000a\u000aA timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.</ref>\u000a\u000aThis is an example of a simple HTML document that uses this technique:\u000a<source lang="html4strict">\u000a<html>\u000a<head>\u000a<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />\u000a</head>\u000a<body>\u000a<p>Please follow <a href="http://www.example.com/">this link</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000aThis technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.\u000a\u000aThe same effect can be achieved with an HTTP <code>refresh</code> header:\u000a<source lang="html4strict">\u000aHTTP/1.1 200 ok\u000aRefresh: 0; url=http://www.example.com/\u000aContent-type: text/html\u000aContent-length: 78\u000a\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis response is easier to generate by CGI programs because one does not need to change the default status code.\u000a\u000aHere is a simple CGI program that effects this redirect:\u000a<source lang="perl">\u000a#!/usr/bin/perl\u000aprint "Refresh: 0; url=http://www.example.com/\u005cr\u005cn";\u000aprint "Content-type: text/html\u005cr\u005cn";\u000aprint "\u005cr\u005cn";\u000aprint "Please follow <a href=\u005c"http://www.example.com/\u005c">this link</a>!"\u000a</source>\u000a\u000aNote: Usually, the HTTP server adds the status line and the Content-length header automatically.\u000a\u000aThe [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].\u000a\u000a=== JavaScript redirects ===\u000a[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location='http://www.example.com/'\u000a</syntaxhighlight>\u000aNormally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://online-marketing-technologies.com/tools/javascript-redirection-generator.html|title=Advanced JavaScript Redirections|publisher=Online Marketing Technologies}}</ref>\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location.replace('http://www.example.com/')\u000a</syntaxhighlight>\u000aHowever, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.\u000a\u000a=== Frame redirects ===\u000aA slightly different effect can be achieved by creating a single HTML [[Iframe|frame]] that contains the target page:\u000a<source lang="html4strict">\u000a<frameset rows="100%">\u000a  <frame src="http://www.example.com/">\u000a  <noframes>\u000a    <body>Please follow <a href="http://www.example.com/">link</a>.</body>\u000a  </noframes>\u000a</frameset>\u000a</source>\u000a\u000aOne main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar.\u000a\u000aThis ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>\u000a\u000aThe same effect can be done with an inline frame:\u000a<source lang="html4strict">\u000a<iframe height="100%" width="100%" src="http://www.example.com/">\u000aPlease follow <a href="http://www.example.com/">link</a>.\u000a</iframe>\u000a</source>\u000a\u000a=== Redirect chains ===\u000aOne redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (note the domain name) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] and then to the correct URL: http://en.wikipedia.org/wiki/URL_redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''rewriting'' the URL as much as possible on the server before returning it to the browser as a redirect.\u000a\u000a=== Redirect loops ===\u000aSometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.\u000a\u000a[http://tools.ietf.org/html/rfc7231#section-6.4 HTTP/1.1] states:\u000a<blockquote>\u000aA client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).\u000a\u000aNote: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.\u000a</blockquote>\u000aNote that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...\u000a\u000a== Services ==\u000aThere exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.\u000a\u000a=== URL redirection services ===\u000aA '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]].\u000a\u000aHyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.\u000a\u000aRecently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs.\u000a\u000aA major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.\u000a\u000a==== History ====\u000aThe first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined.\u000a\u000aWith the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.\u000a\u000a=== Referrer masking ===\u000aRedirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.)\u000a\u000aThis type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .\u000a\u000aHere is a simplistic example of such a service, written in [[PHP]].\u000a<source lang="php">\u000a<?php\u000a$url = htmlspecialchars($_GET['url']);\u000aheader( 'Refresh: 0; url=http://'.$url );\u000a?>\u000a<!-- Fallback using meta refresh. -->\u000a<html>\u000a <head>\u000a  <title>Redirecting...</title>\u000a  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">\u000a </head>\u000a <body>\u000a Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.\u000a </body>\u000a</html>\u000a</source>\u000a\u000aThe above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.\u000a\u000a==Security Issues==\u000aURL redirection can be abused by attackers for [[Phishing]] attacks, such as [[Open Redirect]] and [[Covert Redirect]].\u000a\u000a"An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a"Covert Redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It is disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a== See also ==\u000a* [[Link rot]]\u000a* [[Canonical meta tag]]\u000a* [[Domain masking]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]\u000a* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)\u000a* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification\u000a* [http://www.dancatts.com/articles/htaccess-301-redirects-for-moved-pages.php 301 Redirects for moved pages using .htaccess]\u000a* [http://911-need-code-help.blogspot.com/2011/03/redirecting-visitors-to-preferred.html Redirecting your visitors to your preferred domain] using 301 permanent redirects&nbsp;\u2014 rationale and mod_rewrite/PHP/ASP.NET implementations\u000a\u000a{{Spamming}}\u000a\u000a{{Use dmy dates|date=November 2010}}\u000a\u000a{{DEFAULTSORT:Url Redirection}}\u000a[[Category:Uniform resource locator]]\u000a[[Category:Black hat search engine optimization]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p41
sg4
S'152'
p42
sg6
VURL redirection
p43
ssI27
(dp44
g2
V{{disputed|date=March 2014}}\u000a{{inline|date=March 2014}}\u000a'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.\u000a\u000a==Overview==\u000aDocument clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.\u000a\u000aThe application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared offline applications.\u000a\u000aIn general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment \u2013 each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft \u2013 a document\u2019s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.\u000a\u000aOther algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.\u000a\u000aGiven a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.\u000a\u000a==Clustering in search engines==\u000aA [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by Enterprise Search engines such as [[Northern Light Group|Northern Light]] and [[Vivisimo]], consumer search engines such as [http://www.polymeta.com/ PolyMeta] and [http://www.helioid.com Helioid], or open source software such as [[Carrot2]].\u000a\u000aExamples:\u000a\u000a* Clustering divides the results of a search for "cell" into groups like "biology," "battery," and "prison."\u000a\u000a* [http://FirstGov.gov FirstGov.gov], the official Web portal for the U.S. government, uses document clustering to automatically organize its search results into categories.  For example, if a user submits \u201cimmigration\u201d, next to their list of results they will see categories for \u201cImmigration Reform\u201d, \u201cCitizenship and Immigration Services\u201d, \u201cEmployment\u201d, \u201cDepartment of Homeland Security\u201d, and more.\u000a\u000a== References ==\u000a{{reflist}}\u000aPublications:\u000a* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]\u000a* Claudio Carpineto, Stanislaw Osi\u0144ski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys (CSUR), Volume 41, Issue 3 (July 2009), Article No. 17, ISSN:0360-0300 \u000a* http://semanticsearchart.com/researchBest.html - comparison of several popular clustering algorithms, data and software to reproduce the result.\u000a* Tanmay Basu, C.A. Murthy , CUES: A New Hierarchical Approach for Document Clustering, 2013 [http://jprr.org  JPRR]\u000a\u000a==See Also==\u000a*[[Cluster Analysis]]\u000a*[[Fuzzy clustering]]\u000a[[Category:Information retrieval]]
p45
sg4
S'27'
p46
sg6
VDocument clustering
p47
ssI157
(dp48
g2
V'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>Susan E. Feldman. ''The Answer Machine'', Synthesis Lectures on Information Concepts, Retrieval, and Services. [http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023 http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023]</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>Pitokow, James; Hinrich Schtze; Todd Cass; Rob Cooley; Don Turnbull; Andy Edmonds; Eytan Adar; Thomas Breuel (2002). "Personalized search". [http://www.cond.org/p50-pitkow.pdf http://www.cond.org/p50-pitkow.pdf] Communications of the ACM (CACM) 45 (9): 50\u201355.</ref>\u000a\u000a== Basic Contextual Search ==\u000aThe basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add \u201cmenu\u201d to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.\u000a\u000a== Explicitly Supplied Context ==\u000aCertain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example a user looking for research papers can specify documents with \u201creferences\u201d or \u201cabstracts\u201d to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]</ref>\u000a\u000aExplicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>\u000a\u000a== Automatically Inferred Context ==\u000aThere are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.\u000a\u000aMajor search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>Eric J Glover, Steve Lawrence, Michael D. Gordon, William P. Birmingham, C. Lee Giles. ''Web Search - Your Way'', NEC Research Institution [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7499&rep=rep1&type=pdf]</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.\u000a\u000a=== Contextual Mobile Search ===\u000aThe drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref> Mobile start up company [http://everything.Me Everything.Me] is one company that is moving towards turning the smartphone into an all-in-one device the provides relevant information for specific users. Everything.Me pushes app updates and suggestion to a user's home-screen based on user movement, location, current time, past search queries, and entertainment preferences.<ref>[http://socialtimes.com/mobile-contextual-search-future_b149394 http://socialtimes.com/mobile-contextual-search-future_b149394], Contextual Search through Mobile and Everything.Me</ref> For example when a user opens their phone in the morning Everything.Me will present users with apps relevant to how that users interacts with their phone in the morning\u2014presenting weather apps, bus apps, and news apps.<ref>[http://everything.me/about/ Everything.Me]</ref> Later when that user goes to work Everything.Me will update the work related applications to be prioritized over other apps. Everything.Me anticipates a user's needs based on their current actions and past interactions on the web. This process of automatically obtaining context from mobile phones can help to increase the precision of user queries. For instance if a user searches for a place to eat while at work Everything.Me will take work into context and return restaurants that would be more appropriate for a lunch break at the office.<ref>[http://everything.me/ http://everything.me/], Video Information</ref>\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Contextual Searching}}\u000a[[Category:Internet search engines]]\u000a[[Category:Semantic Web]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p49
sg4
S'157'
p50
sg6
VContextual searching
p51
ssI32
(dp52
g2
V{{multiple issues|\u000a{{Orphan|date=February 2009}}\u000a{{Unreferenced|date=March 2008}}\u000a}}\u000a\u000a== IR Evaluation ==\u000aThe evaluation of information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].\u000a\u000a*'''Precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:\u000a\u000a:<math> \u005cmbox{precision}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{retrieved documents}\u005c}|} </math>\u000a\u000a*'''Recall''' is the fraction of the documents relevant to the query that are successfully retrieved:\u000a\u000a:<math> \u005cmbox{recall}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{relevant documents}\u005c}|} </math>\u000a\u000a*'''F-measure''' is the harmonic mean of precision and recall:\u000a\u000a:<math> \u005cmbox{F-measure}= 2 * \u005cfrac{\u005c{\u005cmbox{precision}\u005c}*\u005c{\u005cmbox{recall}\u005c}}{\u005c{\u005cmbox{precision}\u005c}+\u005c{\u005cmbox{recall}\u005c}} </math>\u000a\u000aFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.\u000a\u000aF-measure tends to be a better single metric when compared to precision and recall because both of them give different information that can complement each other when combined. If one of them excels more than the other, this metric will reflect it.\u000a\u000aVirtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.\u000a\u000a==See also==\u000a* [[Information retrieval]]\u000a* [[Precision and recall]]\u000a* [[Web search engine]]\u000a\u000a==Further reading==\u000a* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.\u000a*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p53
sg4
S'32'
p54
sg6
VIR evaluation
p55
ssI162
(dp56
g2
V'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].\u000a\u000a==Definitions==\u000a\u000aDocuments and queries are represented as vectors.\u000a\u000a:<math>d_j = ( w_{1,j} ,w_{2,j} , \u005cdotsc ,w_{t,j} )</math>\u000a:<math>q = ( w_{1,q} ,w_{2,q} , \u005cdotsc ,w_{n,q} )</math>\u000a\u000aEach [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).\u000a\u000aThe definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).\u000a\u000aVector operations can be used to compare documents with queries.\u000a\u000a==Applications==\u000a\u000a[[Image:vector space model.jpg|right|250px]]\u000a\u000a[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.\u000a\u000aIn practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:\u000a\u000a:<math>\u000a\u005ccos{\u005ctheta} = \u005cfrac{\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|}\u000a</math>\u000a\u000aWhere <math>\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}</math> is the intersection (i.e. the [[dot product]]) of the document (d<sub>2</sub> in the figure to the right) and the query (q in the figure) vectors, <math>\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c|</math> is the norm of vector d<sub>2</sub>, and <math>\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c|</math> is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:\u000a\u000a:<math>\u000a\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c| = \u005csqrt{\u005csum_{i=1}^n q_i^2}\u000a</math>\u000a\u000aAs all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.\u000a\u000a==Example: tf-idf weights==\u000a\u000aIn the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang <ref>[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975</ref> the term-specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is <math>\u005cmathbf{v}_d = [w_{1,d}, w_{2,d}, \u005cldots, w_{N,d}]^T</math>, where\u000a\u000a:<math>\u000aw_{t,d} = \u005cmathrm{tf}_{t,d} \u005ccdot \u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}\u000a</math>\u000a\u000aand\u000a* <math>\u005cmathrm{tf}_{t,d}</math> is term frequency of term ''t'' in document ''d'' (a local parameter)\u000a* <math>\u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}</math> is inverse document frequency (a global parameter). <math>|D|</math> is the total number of documents in the document set; <math>|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|</math> is the number of documents containing the term ''t''.\u000a\u000aUsing the cosine the similarity between document ''d<sub>j</sub>'' and query ''q'' can be calculated as:\u000a\u000a:<math>\u005cmathrm{sim}(d_j,q) = \u005cfrac{\u005cmathbf{d_j} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_j} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|} = \u005cfrac{\u005csum _{i=1}^N w_{i,j}w_{i,q}}{\u005csqrt{\u005csum _{i=1}^N w_{i,j}^2}\u005csqrt{\u005csum _{i=1}^N w_{i,q}^2}}</math>\u000a\u000a==Advantages==\u000a\u000aThe vector space model has the following advantages over the [[Standard Boolean model]]:\u000a\u000a#Simple model based on linear algebra\u000a#Term weights not binary\u000a#Allows computing a continuous degree of similarity between queries and documents\u000a#Allows ranking documents according to their possible relevance\u000a#Allows partial matching\u000a\u000a==Limitations==\u000a\u000aThe vector space model has the following limitations:\u000a\u000a#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])\u000a#Search keywords must precisely match document terms; word [[substring]]s might result in a "[[false positive]] match"\u000a#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "[[false negative]] match".\u000a#The order in which the terms appear in the document is lost in the vector space representation.\u000a#Theoretically assumes terms are statistically independent. \u000a#Weighting is intuitive but not very formal. \u000a\u000aMany of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].\u000a\u000a==Models based on and extending the vector space model==\u000a\u000aModels based on and extending the vector space model include:\u000a* [[Generalized vector space model]]\u000a* [[Latent semantic analysis]]\u000a* [[Term Discrimination]]\u000a* [[Rocchio Classification]]\u000a* [[random_indexing|Random Indexing]]\u000a\u000a==Software that implements the vector space model==\u000a\u000aThe following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.\u000a\u000a===Free open source software===\u000a\u000a* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.\u000a* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.\u000a* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tf\u2013idf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].\u000a* [[Weka (machine learning)|Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.\u000a* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.\u000a* [http://senseclusters.sourceforge.net SenseClusters], an open source package, written in Perl, that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.\u000a* [https://github.com/fozziethebeat/S-Space/wiki S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].\u000a* [http://www.cs.uni.edu/~okane/source/ISR/ Vector Space Model Software Workbench] Collection of 50 source code programs for education.\u000a\u000a==Further reading==\u000a\u000a* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613\u2013620. ''(Article in which a vector space model was presented)''\u000a* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''\u000a* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]\u000a* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the "k-Nearest Neighbor" search]\u000a\u000a==See also==\u000a*[[Bag-of-words model]]\u000a*[[Nearest neighbor search]]\u000a*[[Compound term processing]]\u000a*[[Inverted index]]\u000a*[[w-shingling]]\u000a*[[Eigenvalues and eigenvectors]]\u000a*[[Conceptual Spaces]].\u000a\u000a==References==\u000a<references/>\u000a\u000a[[Category:Vector space model|*]]
p57
sg4
S'162'
p58
sg6
VVector space model
p59
ssI37
(dp60
g2
V{{Infobox company |\u000a  name   = Concept Searching Limited |\u000a  logo = [[Image:conceptSearching.jpg]] |\u000a  company_slogan = "Retrieval Just Got Smarter" |\u000a  type   =  [[Privately held company|Private]] |\u000a  foundation     = 2002|\u000a  location       = [[UK]], [[USA]] |\u000a  area_served    = Global |\u000a  industry       = [[Information retrieval]] |\u000a  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |\u000a  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]\u000a}}\u000a\u000a'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].\u000a\u000a==History==\u000aConcept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>\u000a\u000aCompound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>\u000a\u000aThe company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>\u000a\u000aConcept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>\u000a\u000a== Awards ==\u000a* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-Companies-That-Matter-in-Knowledge-Management-94933.aspx |title=KMWorld Magazine}}</ref>\u000a* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2014-98792.aspx |title=Trend-Setting Products}}</ref>\u000a\u000a==See also==\u000a* [[Compound term processing]]\u000a* [[Enterprise search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Concept Search]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.conceptsearching.com/ Company Website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Privately held companies of the United Kingdom]]
p61
sg4
S'37'
p62
sg6
VConcept Searching Limited
p63
ssI167
(dp64
g2
V{{Commons category|Search algorithms}}\u000a{{Cat main|Search algorithms}}\u000a\u000a[[Category:Algorithms]]\u000a[[Category:Searching]]
p65
sg4
S'167'
p66
sg6
VCategory:Search algorithms
p67
ssI42
(dp68
g2
V{{Other uses|Glimpse (disambiguation)}}\u000a{{Infobox software\u000a| name = Glimpse\u000a| logo = \u000a| screenshot =\u000a| caption =\u000a| developer = [[Internet WorkShop]]\u000a| status = \u000a| latest release version = 4.18.6 (source) / 4.18.5 (binary) \u000a| latest release date = {{release date|2012|06|09}}\u000a| operating system = [[Cross-platform]]\u000a| programming language = [[C (programming language)|C]]\u000a| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]\u000a| license = \u000a| website = {{URL|http://webglimpse.net/}}\u000a}}\u000a'''GLIMPSE''' is a text indexing and [[text retrieval|retrieval]] [[software]] program originally developed at the [[University of Arizona]] by [[Udi Manber]], [[Sun Wu]], and [[Burra Gopal]].  It was released under the ISC [[open source]] license in September 2014.\u000a\u000aGLIMPSE stands for GLobal IMPlicit SEarch. While many text indexing schemes create quite large indexes (usually around 50% of the size of the original text), a GLIMPSE-created index is only 2-4% of the size of the original text.\u000a\u000aGLIMPSE uses and takes a great deal of inspiration from [[Agrep]], which was also developed at the University of Arizona, but GLIMPSE uses a high level index whereas Agrep parses all the text each time.\u000a\u000aThe basic algorithm is similar to other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After one or more of these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a single workstation. In addition, the smaller index can be created more quickly than a full index.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://webglimpse.net/ Glimpse and WebGlimpse home page]\u000a*[http://webglimpse.net/pubs/glimpse.pdf Original Glimpse paper] (PDF)\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Free search engine software]]\u000a[[Category:Search engine software]]
p69
sg4
S'42'
p70
sg6
VGLIMPSE
p71
ssI172
(dp72
g2
V{{refimprove|date=June 2012}}\u000a\u000a'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]], [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designer of the vocabulary, in contrast to natural language vocabularies, where there is no restriction on the vocabulary.\u000a\u000a== In library and information science ==\u000a\u000aIn [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.{{ref|warner}}{{ref|fast}} Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.\u000a\u000aFor example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms -- subject headings in this case -- have to be chosen to handle choices between variant spellings of the same concept (American versus British), choice among scientific and popular terms (Cockroaches versus ''Periplaneta americana''), and choices between synonyms (automobile versus cars), among other difficult issues.\u000a\u000aChoices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).\u000a\u000aControlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term "pool" has to be qualified to refer to either swimming pool, or the game pool to ensure that each authorized term or heading refers to only one concept.\u000a\u000aThere are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.\u000a\u000aHistorically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.\u000a\u000aFor example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "Broader term" and "Narrow term".\u000a\u000aThe [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.\u000a\u000aChoosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.\u000a\u000aControlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].\u000a\u000a== Indexing languages ==\u000a\u000aThere are three main types of indexing languages.\u000a\u000a* Controlled indexing language - Only approved terms can be used by the indexer to describe the document\u000a* [[Natural language]] indexing language - Any term from the document in question can be used to describe the document.\u000a* Free indexing language  - Any term (not only from the document) can be used to describe the document.\u000a\u000aWhen indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each documen\u000a\u000aIn recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.\u000a\u000aControlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word ''football'' for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|Association football]], which also happens to be called ''[[soccer]]'' in several countries. The [[English language]] [[football (word)|word football]] is also applied to [[Rugby football]] ([[Rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.\u000a\u000aCompared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).\u000a\u000aIn some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.\u000a\u000aHowever, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.\u000a\u000aThis is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.\u000a\u000aAnother possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.\u000a\u000aOn the other hand free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.\u000a\u000aControlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.\u000a\u000aThe use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.\u000a\u000aNumerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.\u000a\u000a==Applications==\u000aControlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.\u000a\u000aIn large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.\u000a\u000aWeb searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].\u000a\u000aIt is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.{{ref|doctorow}} To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.{{ref|pilgrim}}\u000a\u000a==See also==\u000a*[[Controlled natural language]]\u000a*[[IMS VDEX|IMS Vocabulary Definition Exchange]]\u000a*[[Nomenclature]]\u000a*[[Ontology (computer science)]]\u000a*[[Terminology]]\u000a*[[Thesaurus]]\u000a*[[Universal Data Element Framework]]\u000a*[[Vocabulary-based transformation]]\u000a\u000a==References==\u000a#{{note|warner}} Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].\u000a#{{note|fast}} Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]\u000a#{{note|doctorow}} Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].\u000a#{{note|pilgrim}} Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].\u000a#[http://www.imresources.fit.qut.edu.au/vocab/ Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes.\u000a#[http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.\u000a\u000a==External links==\u000a* [http://www.controlledvocabulary.com/ controlledvocabulary.com] \u2014 explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.\u000a* [http://www.photo-keywords.com/ photo-keywords.com/] \u2014 useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.\u000a* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]\u000a\u000a{{Lexicography}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Library cataloging and classification]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Technical communication]]\u000a[[Category:Semantic Web]]\u000a[[Category:Ontology (information science)]]\u000a[[Category:Controlled vocabularies]]\u000a[[Category:Library science]]\u000a[[Category:Information science]]
p73
sg4
S'172'
p74
sg6
VControlled vocabulary
p75
ssI47
(dp76
g2
VThe '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. \u000a</ref> \u000a\u000aThey represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).\u000a\u000a==See also==\u000a*[[ASLIB]]\u000a*[[Information history]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Experiments]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{database-stub}}
p77
sg4
S'47'
p78
sg6
VCranfield Experiments
p79
ssI177
(dp80
g2
V{{Cat main|Data search engine}}\u000a\u000a[[Category:Metadata]]\u000a[[Category:XML]]\u000a[[Category:Database management systems]]\u000a[[Category:Searching]]
p81
sg4
S'177'
p82
sg6
VCategory:Data search engines
p83
ssI52
(dp84
g2
V{{Recommender systems}}\u000a'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].\u000a\u000a== Models of collaboration ==\u000a\u000aCollaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization\u000a<ref name=Golo2007>{{citation\u000a | title = Collaborative Exploratory Search\u000a | year = 2007\u000a | author = Golovchinsky Gene, Pickens Jeremy\u000a | journal = Proceedings of HCIR 2007 workshop\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf\u000a}}</ref> and depth of mediation \u000a,<ref name=Pickens2008>{{citation\u000a | title = Collaborative Exploratory Search\u000a | year = 2008\u000a | author = Pickens Jeremy, Golovchinsky Gene, Shah Chirag, Qvarfordt Pernilla, Back Maribeth\u000a | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval\u000a | pages = 315\u2013322\u000a | volume = \u000a | issue = \u000a | doi = 10.1145/1390334.1390389\u000a | isbn = \u000a 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389\u000a| chapter = Algorithmic mediation for collaborative exploratory search\u000a }}</ref> task vs. trait,<ref name=Morris2008>{{citation\u000a | contribution = Understanding Groups\u2019 Properties as a Means of Improving Collaborative Search Systems\u000a | year = 2008\u000a | author = Morris Meredith, Teevan Jaime\u000a | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf\u000a}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation\u000a | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval\u000a | year = 2008\u000a | author = Foley Colum\u000a | booktitle = PhD Thesis, Dublin City University\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf\u000a}}</ref>\u000a\u000a=== Explicit vs. implicit collaboration ===\u000a\u000aImplicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation\u000a | title = Collaborative Web Search\u000a | year = 2003\u000a | author = Barry Smyth, Evelyn Balfe, Peter Briggs, Maurice Coyle, Jill Freyne\u000a | journal = IJCAI\u000a | pages = 1417\u20131419\u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation\u000a | title = Community search assistant\u000a | year = 2001\u000a | author = Natalie S. Glance\u000a | journal = Workshop on AI for Web Search AAAI'02\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation\u000a | title = Discovering the Scope of Privacy Needs in Collaborative Search\u000a | year = 2008\u000a | author = Thorben Burghardt, Erik Buchmann, Klemens Bhm\u000a | journal = Web Intelligence (WI)\u000a | pages = \u000a 910| volume = \u000a | issue = \u000a | doi = 10.1109/WIIAT.2008.165\u000a | isbn = \u000a 978-0-7695-3496-1}}</ref> and the works of Longo et al.\u000a<ref name=Longo2009a>{{citation\u000a | title = Toward Social Search - From Explicit to Implicit Collaboration\u000a               to Predict Users' Interests\u000a | year = 2009\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = WEBIST 2009 - Proceedings of the Fifth International Conference\u000a               on Web Information Systems and Technologies, Lisbon, Portugal,\u000a               March 23\u201326, 2009\u000a | pages = 693\u2013696\u000a | volume = 1\u000a | issue = \u000a | doi = \u000a | isbn = 978-989-8111-81-4\u000a | url = \u000a}}</ref> \u000a<ref name=Longo2010>{{citation\u000a | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time\u000a | year = 2010\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = Transaction Computational Collective Intelligence II\u000a | pages = 46\u201369\u000a | volume = 2\u000a | issue = \u000a | doi = 10.1007/978-3-642-17155-0_3\u000a | isbn = \u000a 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/\u000a| series = Lecture Notes in Computer Science\u000a }}</ref> \u000a<ref name=Longo2009b>{{citation\u000a | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search\u000a | year = 2009\u000a | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\u000a | journal = Computational Collective Intelligence. Semantic Web, Social\u000a               Networks and Multiagent Systems, First International Conference,\u000a               ICCCI 2009, Wroclaw, Poland, October 5\u20137, 2009. Proceedings\u000a | pages = 63\u201374\u000a | volume = 1\u000a | issue = \u000a | doi = \u000a | isbn = 978-3-642-04440-3\u000a | url = http://dl.acm.org/citation.cfm?id=1692026\u000a}}</ref> \u000aall represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.\u000a\u000aExplicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation\u000a | title = SearchTogether: An Interface for Collaborative Web Search\u000a | year = 2007\u000a | author = Meredith Ringel Morris, Eric Horvitz\u000a | journal = UIST\u000a| url = http://portal.acm.org/citation.cfm?id=1294211.1294215\u000a}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation\u000a | title = The Role of Communication in Collaborative Information Searching\u000a | year = 2008\u000a | author = Madhu C. Reddy, Bernhard J. Jansen, Rashmi Krishnappa\u000a | journal = ASTIS\u000a}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.\u000a\u000aHowever, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis, Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88\u201398| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.\u000a\u000a=== Community of practice  ===\u000a\u000aRecent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation\u000a | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries\u000a | year = 2002\u000a | author = Rohini U, Vamshi Ambati\u000a | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>\u000a\u000aCollaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation\u000a | title = Social Aspects of a Collaborative, Community-Based Search Network\u000a | editor4-first = Eelco\u000a | editor3-first = Pearl\u000a | editor2-first = Judy\u000a | editor1-first = Wolfgang\u000a | year = 2008\u000a | editor1-last = Nejdl\u000a | author = Maurice Coyle and Barry Smyth\u000a | journal = Adaptive Hypermedia and Adaptive Web-Based Systems\u000a | pages =  103\u2013112  \u000a | volume = 5149/2008\u000a | issue = \u000a \u000a | series = Volume| doi = 10.1007/978-3-540-70987-9\u000a | isbn = 978-3-540-70984-8\u000a | url = http://portal.acm.org/citation.cfm?id=1485050\u000a | editor2-last = Kay\u000a | editor4-last = Herder\u000a | editor3-last = Pu}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project Jumper 2.0.<ref name=Jumper2010>{{citation\u000a | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features\u000a | year = 2010\u000a | author = Jumper Networks Inc\u000a | journal = Press release\u000a | pages = \u000a | volume =\u000a | issue = \u000a | doi =\u000a | isbn =\u000a | url = http://www.trilexnet.com/labs/jumper}}</ref>\u000a\u000a=== Depth of mediation ===\u000a\u000aThis refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.\u000a\u000a=== Task vs. trait ===\u000a\u000aThis model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />\u000a\u000a== Privacy-aware collaborative search engines ==\u000a\u000aSearch terms and links clicked that are shared among users reveal their interests, habits, social\u000arelations and intentions.<ref name=EUArticle29>{{citation\u000a | title = Article 29 EU Data Protection Working Party\u000a | year = 2008\u000a | author = Data Protection Working Party\u000a | journal = EU\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. \u000a<ref name="Morris2007"/><ref name=Smith2005>{{citation\u000a | title = A Live-User Evaluation of Collaborative Web Search\u000a | year = 2005\u000a | author = Barry Smyth, Evelyn Balfe, Oisin Boydell, Keith Bradley, Peter Briggs, Maurice Coyle, Jill Freyne\u000a | journal = IJCAI\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref>\u000a<ref name=Smith2006>{{citation\u000a | title = Anonymous personalization in collaborative web search\u000a | year = 2005\u000a | author = Smyth,, Barry and Balfe,, Evelyn\u000a | journal = Inf. Retr.\u000a | pages = 165\u2013190\u000a | volume = 9\u000a | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = \u000a | url = \u000a}}</ref>\u000a<ref name=Jung2004>{{citation\u000a | title = Applying Collaborative Filtering for Efficient Document Search\u000a | year = 2004\u000a | author = Seikyung Jung, Juntae Kim, Herlocker, J.L.\u000a | journal = Inf. Retr.\u000a | pages = 640\u2013643\u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = \u000a}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.\u000a\u000aAs CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation\u000a | title = Collaborative Search And User Privacy: How Can They Be Reconciled?\u000a | year = 2008\u000a | author = Thorben Burghardt, Erik Buchmann, Klemens Bhm, Chris Clifton\u000a | journal = CollaborateCom\u000a | pages = \u000a | volume = \u000a | issue = \u000a | doi = \u000a | isbn = \u000a | url = http://dbis.ipd.uni-karlsruhe.de/1184.php\u000a}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.\u000a\u000a== References ==\u000a{{reflist|2}}\u000a{{Internet search}}\u000a\u000a[[Category:Information retrieval]]
p85
sg4
S'52'
p86
sg6
VCollaborative search engine
p87
ssI182
(dp88
g2
V{{multiple issues|\u000a{{Cleanup|date=October 2010}}\u000a{{technical|date=October 2014}}\u000a}}\u000a\u000a[[File:Puggle-search.png|thumb|Puggle Desktop Search]]\u000a[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]\u000a'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.\u000a\u000aOne of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.\u000a\u000aA variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.\u000a\u000aDesktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users needs to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] \u2014 the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.\u000a\u000aCompanies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index \u2014 but not preview \u2014 items they should not even know exist.{{Citation needed|date = November 2009}}\u000a\u000aHistorically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.\u000a\u000a== Technologies ==\u000aMost desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products, such as Lookeen,<ref>{{cite web|title=Real-Time Indexing and Lookeen 8|url=http://www.lookeen.net/2884/News/real-time-ndexing-and-lookeen-8/|publisher=Lookeen|accessdate=26 October 2014}}</ref> have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.\u000a\u000aDesktop search tools typically collect three types of information about files:\u000a* file and folder names\u000a* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]\u000a* file content (for supported types of documents only)\u000a\u000aTo search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.\u000a\u000aLong-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>[http://www.niallkennedy.com/blog/archives/2006/10/video-search.html "The current state of video search", by Niall Kennedy]</ref><ref>[http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html "The current state of audio search", by Niall Kennedy]</ref>\u000a\u000aThe sector attracted considerable attention from the struggle between Microsoft and Google.<ref>[http://news.bbc.co.uk/1/hi/technology/3952285.stm "Search wars hit desktop computers". (Oct 2004) BBC News]</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>[http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/ "Microsoft agrees to change Vista Desktop Search Tool" (Jun 2007)]</ref>\u000a\u000aAs of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more. <ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>  \u000a\u000aX1 makes one of the leading desktop search products on the market. X1 Search 8 is a software alternative to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more. <ref>[http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/ "X1 rises again with Desktop Search 8, Virtual Edition" (May 2013)]</ref>   \u000a\u000a==Platforms & their histories==\u000aThere are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.\u000a\u000a'''Windows'''\u000a\u000aToday's Windows Search replaced WDS (Windows Desktop Search). WDS, in turn, replaced Indexing Service. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.\u000a\u000aIn 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>http://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.\u000a\u000aWith the release of Windows Vista came Windows Search 3.1. Unlike it's predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced. Cutting back indexing times immensely. This brings us to the Windows Search 4.0 which is currently running on all PCs with Windows 7 and up.\u000a\u000a'''Mac OS'''\u000a\u000aMac OS was the first to fully implement Desktop Search, it allowed users to fully search all documents with in their Macintosh computer. This means file format types, meta-data on those file formats and the content within the files. Released in 1994 two years before Windows Search was released, AppleSearch already had content searching. The biggest issue that AppleSearch had large resource requirements "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>http://infomotions.com/musings/tricks/manuscript/1600-0001.html</ref> A Macintosh computer that had these specs cost around $1400 in today's dollars that's around $2050.<ref>http://stats.areppim.com/calc/calc_usdlrxdeflator.php</ref> On top of that, the software it self cost around $1400 for a single licenses.\u000a\u000aIn 1997, Sherlock was released alongside Mac OS 8.5. Sherlock, named after the famous fictional detective Sherlock Holmes, was integrated into Mac OS's file browser: Finder. Sherlock extended the desktop search to the world wide web. Allowing users to now search locally and externally. Adding the web to Sherlock was relatively easy as the plugins only needed to be written in a plain text file. Sherlock was included in every single Mac OS 8, 9 and 10 until 10.5.\u000a\u000aSpotlight was released in 2005, on Mac OSX 10.4, is a Selection-based search which means the user invokes a query using only the mouse. It allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage. Spotlight also uses a built-in Oxford American Dictionary and calculator to offer quick access to definitions and small calculations.<ref>http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html</ref> While Spotlight had a initially long start-up time (for first time set up). The entire hard disk was indexed, and as files are added to the hard disk, the index is constantly being updated in the background. This is done using minimal CPU & RAM resources, making searching relatively easy and quick.\u000a\u000a'''Linux'''\u000a\u000aFor Linux, we will primarily cover the Ubuntu distribution as it was and currently is still the most popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using Tracker<ref>http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/</ref> desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX based systems. Tracker, was released in late 2007 was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting, and meta-data matching, but support for searching through emails and messages (instant messages) was added. Years later, in 2014 Recoll<ref>http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING</ref> was added to Linux distributions, it works with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. The wonderful thing about Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example you could search for just file types or by content.<ref>http://archive09.linux.com/feature/114283</ref>\u000a\u000a==See also==\u000a*[[List of search engines#Desktop search engines|List of desktop search engines]]\u000a\u000a== References ==\u000a<!--* [http://ims.dei.unipd.it/members/agosti/teaching/2006-07/ir/ Maristella Agosti's website]-->\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a* ''[http://www.slate.com/id/2111643/ Keeper Finders]'', by Paul Boutin, ''[[Slate (magazine)|Slate]]'', December 31, 2004 &mdash; A comparison of Google, Ask Jeeves, HotBot, MSN and Copernic desktop search tools.\u000a* [http://www.goebelgroup.com/desktopmatrix.htm GoebelGroup.com's desktop search tools comparison chart] - Date of last update: 15 January 2007.\u000a* [http://labnol.blogspot.com/2004/10/detailed-comparison-of-desktop-search.html A detailed comparison of desktop search tools] - dated 2004.\u000a* [http://www.wikinfo.org/index.php/Comparison_of_desktop_search_software Comparison of desktop search software] - Date of last update: March 2008\u000a* [http://tbox.codeplex.com/ TBox] - DevTool, with ability to do fast search by text files\u000a\u000a{{Navigationbox Desktopsearch}}\u000a\u000a{{DEFAULTSORT:Desktop Search}}\u000a[[Category:Desktop search engines| ]]\u000a[[Category:Searching]]
p89
sg4
S'182'
p90
sg6
VDesktop search
p91
ssI57
(dp92
g2
V{{Multiple issues|\u000a{{unreferenced|date=May 2009}}\u000a{{expert-subject|date=May 2009}}\u000a{{orphan|date=February 2011}}\u000a}}\u000a\u000a{{Infobox software\u000a|name                       = Clairlib\u000a|logo                       = [[Image:Clair logo.jpg]]\u000a|screenshot                 = \u000a|caption                    = \u000a|collapsible                = \u000a|author                     = \u000a|developer                  = CLAIR [[University of Michigan]]\u000a|released                   = \u000a|latest release version     = 1.0.8\u000a|latest release date        = {{release date and age|2009|08|1}}\u000a|latest preview version     = \u000a|latest preview date        = \u000a|frequently updated         = yes\u000a|programming language       = [[Perl]]\u000a|operating system           = \u000a|platform                   = Cross-platform\u000a|size                       = \u000a|language                   = Perl\u000a|status                     = Active\u000a|genre                      = [[Natural Language Processing]], [[Network theory|Network Analysis]], [[Information Retrieval]]\u000a|license                    = [[GNU General Public License]], [[Artistic License]]\u000a|website                    = [http://www.clairlib.org/ www.clairlib.org]\u000a}}\u000a'''Clairlib''' is a suite of open-source [[Perl]] modules developed and maintained by the Computational Linguistics And Information Retrieval (CLAIR) group at the [[University of Michigan]]. Clairlib is intended to simplify a number of generic tasks in [[natural language processing]] (NLP), [[information retrieval]] (IR), and network analysis (NA). The latest version of clairlib is 1.06 which was released on March 2009 and includes about 130 modules implementing a wide range of functionalities.\u000a\u000a==Functionality==\u000a\u000aClairlib is distributed in two forms: Clairlib-core, which has essential functionality and minimal dependence on external software, and Clairlib-ext, which has extended functionality that may be of interest to a smaller audience. Much can be done using Clairlib on its own. Some of the things that Clairlib can do are: Tokenization, Summarization, Document Clustering, Document Indexing, Web Graph Analysis, Network Generation,  [[Power law distribution]] Analysis, [[Network theory|Network Analysis]], [[Random walk]]s on graphs, [[Tf-idf]], [[Perceptron]] learning  and classification, and [[Compound term processing|Phrase Based Retrieval]] and [[Fuzzy logic|Fuzzy OR Queries]].\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a*[http://www.clairlib.org Homepage]\u000a*[http://tangra.si.umich.edu/clair/ Computational Linguistics And Information Retrieval (CLAIR) group]\u000a\u000a[[Category:Free computer libraries]]\u000a[[Category:Perl modules]]\u000a[[Category:University of Michigan]]\u000a[[Category:Information retrieval]]
p93
sg4
S'57'
p94
sg6
VClairlib
p95
ssI187
(dp96
g2
V{{no footnotes|date=December 2014}}\u000a'''Web indexing''' (or '''Internet indexing''') refers to various methods for indexing the contents of a [[website]] or of the [[Internet]] as a whole. Individual websites or [[intranet]]s may use a [[back-of-the-book index]], while [[search engines]] usually use keywords and [[Metadata (computing)|metadata]] to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of [[periodical]]s that have articles online, web indexing is also becoming important for periodical websites.\u000a\u000aBack-of-the-book-style web indexes may be called "web site A-Z indexes". The implication with "A-Z" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a [[Taxonomy (general)|taxonomy]]) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.\u000a\u000aMetadata web indexing involves assigning keywords or phrases to web pages or web sites within a [[metadata tag]] (or "meta-tag") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by [[search engine indexing]].\u000a\u000a==See also==\u000a* [[Information architecture]]\u000a* [[Search engine indexing]]\u000a* [[Search engine optimization]]\u000a* [[Site map]]\u000a* [[Web navigation]]\u000a* [[Web search engine]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a<!--========================({{No More Links}})============================\u000a    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |\u000a    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |\u000a    |                                                                     |\u000a    |           Excessive or inappropriate links WILL BE DELETED.         |\u000a    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\u000a    |                                                                     |\u000a    | If there are already plentiful links, please propose additions or   |\u000a    | replacements on this article's discussion page, or submit your link |\u000a    | to the relevant category at the Open Directory Project (dmoz.org)   |\u000a    | and link back to that category using the {{dmoz}} template.         |\u000a    =======================({{No More Links}})=============================-->\u000a*TheAlphaWeb [http://www.eprodoffice.com/shhh/abcdefghijklmnopqrstuvwxyz.htm ''An example of an Internet A-Z'']\u000a*Glenda Browne and Jonathan Jermey, [http://www.webindexing.biz/ ''Website indexing: enhancing access to information within websites, 2nd Edition''], ISBN 1-875145-56-7\u000a*James Lamb, [http://www.jalamb.com/publications.html ''Website Indexes: visitors to content in two clicks, or website indexing with XRefHT32 freeware''], ISBN 978-1-4116-7937-5\u000a*[http://www.infotoday.com/books/books/BeyondBookIndex.shtml ''Beyond Book Indexing: How to Get Started in Web Indexing, Embedded Indexing, and Other Computer-Based Media''], edited by Marilyn Rowland and Diane Brenner, American Society of Indexers, Info Today, Inc, NJ, 2000, ISBN 1-57387-081-1\u000a* {{Cite web\u000a  |url=http://www.boxesandarrows.com/view/improving_usability_with_a_website_index\u000a  |title=Improving Usability with a Website Index\u000a  |archiveurl=http://www.webcitation.org/5vJwZDVkj\u000a  |archivedate=2010-12-28\u000a  |accessdate=2010-12-28\u000a  |first=Fred\u000a  |last=Leise\u000a  |date=2002-07-15\u000a}}\u000a* [http://www.web-indexing.org/article-brown.htm Why Create an Index?]\u000a* [http://www.theeasybee.com/directory/web-content-extraction Open Social Web 3.0 Directory] \u2013 Compare and review web indexing programs\u000a{{Internet search}}\u000a[[Category:Searching]]\u000a[[Category:Indexing]]\u000a\u000a\u000a{{internet-stub}}
p97
sg4
S'187'
p98
sg6
VWeb indexing
p99
ssI62
(dp100
g2
V{{for|the adware|Isearch (malware)}}\u000a\u000a'''Isearch''' is [[open-source software|open-source]] [[text retrieval]] software first developed in 1994 by Nassib Nassar as part of the Isite [[Z39.50]] information framework. The project started at the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) of the North Carolina supercomputing center MCNC and funded by the [[National Science Foundation]] to follow in the track of [[Wide Area Information Server|WAIS]] and develop prototype systems for distributed information networks encompassing Internet applications, library catalogs and other information resources.\u000a\u000aThe main features of Isearch include full text and field searching, relevance ranking, Boolean queries, and support for many document types such as HTML, mail folders, list digests, MEDLINE, BibTeX, SGML/XML, FGDC Metadata, NASA DIF, ANZLIC metadata, ISO 19115 metadata and many other resource types and document formats.\u000a\u000aIt was the first search engine to be designed from the ground up to support [[SGML]] and ISO [[Z39.50]] search and retrieval. It included many innovations including the "document type" model\u2014which is simply a (object oriented) method of associating each document with a class of functions providing a standard interface for accessing the document. It was one of the first engines (if not the first) to ever support XML.\u000a\u000aThe Isearch search/indexing text algorithms were based on [[Gaston Gonnet]]'s seminal work into PAT arrays and trees for text retrieval--- ideas that were developed for the New Oxford English Dictionary Project at the Univ. of Waterloo, and provided the seeds for [[Tim Bray]]'s PAT SGML engine that formed the basis of [[Open Text]]. One of the limiting factors, however, of the  Isearch design was that it was not well suited to handle the extremely large data sets that became popular in the mid to late 1990s. In many cases Isearch was adapted or modified to use different algorithms but usually retained the document type model and the architectural relationship with Isite.\u000a\u000aIsearch was widely adopted and used in hundreds of public search sites, including  many high profile projects such as the [http://patft1.uspto.gov/ U.S. Patent and Trademark Office (USPTO) patent search],[http://clearinghouse3.fgdc.gov/  the Federal Geographic Data Clearinghouse (FGDC)], the NASA Global Change Master Directory, the NASA EOS Guide System, the NASA Catalog Interoperability Project, the Astronomical pre-print service based at the Space Telescope Science Institute, The PCT Electronic Gazette at the World Intellectual Property Organization (WIPO), Linsearch (a search engine for Open Source Software designed by Miles Efron), the SAGE Project of the Special Collections Department at Emory University, Eco Companion Australasia (an environmental geospatial resources catalog), Australian National Genomic Information Service (ANGIS), the [[Open Directory Project]] and numerous governmental portals in the context of the Government Information Locator Service (GILS) [[United States Government Printing Office|GPO]] mandate (ended in 2005?).\u000a\u000aFrom 1994 to 1998 most of the development was centered around the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) in North Carolina (Engine core) and BSn in Germany (Doctypes). By 1998 much of the open-source Isearch core developers re-focused development into several spin-offs. In 1998 it became part of the Advanced Search Facility reference software platform funded by the U.S. Department of Commerce.\u000a\u000aA/WWW Enterprises now maintains the open source version for public usage, supported by paying government clients, such as the U.S. Patent and Trademark Office, NASA, and the FGDC who have provided support to enhance the functionality and reliability of the software. The software suite is considered a reference implementation of catalog service software.\u000a\u000aAs of 2010, the open source version of Isearch is still used on 250+ nodes of FGDC, and by ANZLIC in Australia and selected Geospatial OneStop contributors to facilitate harvesting by GOS, including NOAA, Census Bureau and the Tenn. Field Office of the US Fish and Wildlife Service, among others.\u000a\u000a==References==\u000a*[http://www.springerlink.com/content/g5e2wfd0lekygvut/ Application of Metadata Concepts to Discovery of Internet Resources]\u000a*[http://www.springerlink.com/content/b5chmkgx8akg4m2h/ An Operational Metadata Framework for Searching, Indexing, and Retrieving Distributed Geographic Information Services on the Internet]\u000a* The UNIX Web Server Book, Second Edition, by R. Douglas Matthews et al. (Ventana Press, 1997).\u000a* [http://www.webtechniques.com/archives/1997/05/nassar/  "Searching With Isearch". May 1997, Web Techniques]\u000a* [http://www.itl.nist.gov/fipspubs/fip192.htm FIPS-192: APPLICATION PROFILE FOR THE GOVERNMENT INFORMATION LOCATOR SERVICE (GILS)]\u000a* [http://www.uneca.org/awich/AWICH%20Workshop/YaoundeWorkshop/Clearinghouse%20Yaounde.pdf Clearinghouse and Metadata Concepts, Danel Behanu, U.N. Economic Commission for Africa,  2004]\u000a* [http://web.archive.org/web/19991006225226/http://www.whitehouse.gov/OMB/memoranda/m9805.html M-98-05 Guidance on the Government Information Locator Service] published by the [[Office of Management and Budget|OMB]]\u000a* [http://www.hpcwire.com/archives/3149.html 01/1995 Press Release: Patent Office Launch Internet AIDS Patent Library]\u000a\u000a==External links==\u000a*[http://www.fgdc.gov/dataandservices/isite U.S. Federal Geographic Data Committee Isite]\u000a*[http://isite.awcubed.com/ Isite/Isearch2 Documentation Site]\u000a*[ftp://ftp.awcubed.com/pub/Software Current Isearch download site]\u000a*[http://www.etymon.com/tr.html Etymon: Isearch]\u000a*[http://www.ibu.de/node/52 BSn/NONMONOTONIC Lab: IB Search Engine], embeddable search engine. A commercial spin-off from the Isearch project.\u000a\u000a===Comparisons===\u000a* [http://www.ukoln.ac.uk/metadata/roads/product-comparison/  Product Comparison: Information Gateway Software]\u000a* [http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf  A Comparison of Open Source Search Engines, Christian Middleton, Ricardo Baeza-Yates]\u000a* [http://www.infomotions.com/musings/opensource-indexers/ Comparing Open Source Indexers]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Free search engine software]]
p101
sg4
S'62'
p102
sg6
VIsearch
p103
ssI192
(dp104
g2
V'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.\u000a\u000a==See also==\u000a* [[Search/Retrieve Web Service]]\u000a\u000a==External links==\u000a* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Search Retrieve via URL}}\u000a[[Category:Data search engines]]\u000a[[Category:Searching]]\u000a[[Category:Uniform resource locator]]\u000a\u000a{{web-stub}}
p105
sg4
S'192'
p106
sg6
VSearch/Retrieve via URL
p107
ssI67
(dp108
g2
V{{Multiple issues|\u000a{{refimprove|date=August 2012}}\u000a{{cleanup|date=September 2009}}\u000a}}\u000a\u000aIn [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).\u000a\u000aIn a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>\u000a\u000a==Indexing==\u000aWhen dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "serial scanning." This is what some tools, such as [[grep]], do when searching.\u000a\u000aHowever, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{Dead link |date=October 2012}}</ref>\u000a\u000aThe indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive."\u000a\u000a==The precision vs. recall tradeoff==\u000a[[Image:Full-text-search-results.png|150px|thumb|right|This diagram represents a low-precision, low-recall search as described in the text.]]\u000aRecall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned divided by all relevant results. Precision is the number of relevant results returned divided by the total number of results returned.\u000a\u000aThe diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only one relevant result of three possible relevant results was returned, so the recall is a very low ratio of 1/3 or 33%. The precision for the example is a very low 1/4 or 25%, since only one of the four results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>\u000a\u000aDue to the ambiguities of [[natural language]], full text search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D.L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}</ref>\u000a\u000a{{See also|Precision and recall}}\u000a\u000a==False-positive problem==\u000a\u000aFree text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).\u000a\u000aClustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "football", clustering can be used to categorize the document/data universe into "American football", "corporate football", etc. Depending on the occurrences of words relevant to the categories, search terms a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain.{{clarify|date=January 2012}}\u000a\u000a==Performance improvements==\u000a\u000aThe deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.\u000a\u000a===Improved querying tools===\u000a\u000a*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.\u000a* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."\u000a* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, "encyclopedia" AND "online" NOT "Encarta") can dramatically increase the precision of a free text search. The AND operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The NOT operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the OR operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, "encyclopedia" AND "online" OR "Internet" NOT "Encarta". This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>\u000a* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as "Wikipedia, the free encyclopedia."\u000a* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.\u000a* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.\u000a* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for "Wikipedia" WITHIN2 "free" would retrieve only those documents in which the words "Wikipedia" and "free" occur within two words of each other.\u000a* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.\u000a* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)\u000a* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example using the asterisk in a search query "s*n" will find "sin", "son", "sun", etc. in a text.\u000a\u000a===Improved search algorithms===\u000aThe [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.\u000a\u000a==Software==\u000a\u000aThe following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.\u000a\u000a=== Free and open source software ===\u000a<!--\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a* [[Apache Solr]]\u000a* [[BaseX]]\u000a* [[DataparkSearch]]\u000a* [[ElasticSearch]]\u000a* [[Ht-//Dig|ht://Dig]]\u000a* [[KinoSearch]]\u000a* [[Lemur Project|Lemur/Indri]]\u000a* [[Lucene]]\u000a* [[mnoGoSearch]]\u000a* [[Searchdaimon]]\u000a* [[Sphinx (search engine)|Sphinx]]\u000a* [[Swish-e]]\u000a* [[Xapian]]\u000a\u000a=== Proprietary software ===\u000a<!--\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a* [[Attivio]]\u000a* [[Autonomy Corporation]]\u000a* [[Bar Ilan Responsa Project]]\u000a* [[Brainware]]\u000a* [[BRS/Search]] \u000a* [[Clusterpoint|Clusterpoint Server]]\u000a* [[Concept Searching Limited]]\u000a* [[Dieselpoint]]\u000a* [[dtSearch]]\u000a* [[Endeca]]\u000a* [[Exalead]]\u000a* [[Fast Search & Transfer]]\u000a* [[Inktomi (company)|Inktomi]]\u000a* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)\u000a* [[Lookeen]]\u000a* [[Lucid Imagination]]\u000a* [[MarkLogic]]\u000a* [[Swiftype]]\u000a* [[Thunderstone Software LLC.]]\u000a* [[Vivsimo]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a==See also==\u000a*[[Pattern matching]] and [[string matching]]\u000a*[[Compound term processing]]\u000a*[[Controlled vocabulary]]\u000a*[[Enterprise search]]\u000a*[[Information Extraction]]\u000a*[[Information retrieval]]\u000a*[[Faceted search]]\u000a*[[Full text database]]\u000a*[[List of enterprise search vendors]]\u000a*[[Search engine]]\u000a*[[WebCrawler]], first FTS engine\u000a*[[Search engine indexing]] - how search engines generate indices to support full text searching\u000a*[[SQL Server Full Text Search|SQL Server Full Text Search (implementation of)]]\u000a\u000a{{DEFAULTSORT:Full Text Search}}\u000a[[Category:Searching]]\u000a[[Category:Text editor features]]\u000a[[Category:Information retrieval]]
p109
sg4
S'67'
p110
sg6
VFull text search
p111
ssI197
(dp112
g2
V{{Orphan|date=February 2009}}\u000aThe '''Daffodil''' system is a virtual [[digital library]] system for strategic support of users during the information search process. It implements mainly high-level search functions, so-called stratagems, which provide functionality beyond today's digital libraries.  The Daffodil system was developed as a research project starting as a collaboration between the University of Dortmund (Germany) and the IZ Bonn (Germany), funded by the [[Deutsche Forschungsgemeinschaft]] (DFG) (2000\u20132004). \u000a\u000aCurrently the Daffodil framework is extended to become an experimental evaluation platform for digital library evaluation at the [[University of Duisburg-Essen]].\u000a\u000a== External links ==\u000a* [http://www.dlib.org/dlib/june04/kriewel/06kriewel.html A description of functions and services]\u000a* [http://www.is.informatik.uni-duisburg.de/projects/daffodil/index.html Project description]\u000a\u000a[[Category:Library science]]\u000a[[Category:Searching]]\u000a\u000a\u000a{{Compu-library-stub}}
p113
sg4
S'197'
p114
sg6
VDaffodil (software)
p115
ssI72
(dp116
g2
V{{more footnotes|date=August 2014}}\u000a{{one source|date=August 2014}}\u000aA '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}\u000a\u000aThe most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].\u000a\u000a==How search engines work==\u000aSearch engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].\u000a\u000a[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]\u000a\u000aThe list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive_or|XOR]]) in a probabilistic context.\u000a\u000aTo provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[serp|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.\u000a\u000aOther types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.\u000a\u000a==See also==\u000a{{Portal|Computer Science}}\u000a{{div col|colwidth=30em}}\u000a*[[Automatic summarization]]\u000a*[[Bibliographic database]]\u000a*[[Desktop search]]\u000a*[[Emanuel Goldberg]] (inventor of early search engine)\u000a*[[Enterprise search]]\u000a*[[Federated search]]\u000a*[[Full text search]]\u000a*[[Human search engine]]\u000a*[[Image search]]\u000a*[[Index (search engine)]]\u000a*[[Inverted index]]\u000a*[[List of search engines]]\u000a*[[List of enterprise search vendors]]\u000a*[[Medical literature retrieval]]\u000a*[[Metasearch engine]]\u000a*[[Search engine optimization]]\u000a*[[Search suggest drop-down list]]\u000a*[[Selection-based search]]\u000a*[[Semantic search]]\u000a* [[Solver (computer science)]]\u000a*[[Spamdexing]]\u000a*[[SQL]]\u000a*[[Text mining]]\u000a*[[Vertical search]]\u000a*[[Video search engine]]\u000a*[[Web search engine]]\u000a{{div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Search Engine (Computing)}}\u000a[[Category:Information retrieval]]\u000a[[Category:Data search engines| Search engine]]
p117
sg4
S'72'
p118
sg6
VSearch engine (computing)
p119
ssI202
(dp120
g2
VThe '''Tversky index''', named after [[Amos Tversky]],<ref>{{cite journal |last=Tversky |first=Amos |title=Features of Similarity |journal=Psychological Reviews |volume=84 |number=4 |year=1977 |pages=327\u2013352 |url=http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf}}</ref> is an asymmetric [[similarity measure]] on [[set theory|sets]] that compares a variant to a prototype. The Tversky index can be seen as a generalization of [[Dice's coefficient]] and [[Tanimoto coefficient]].\u000a\u000aFor sets ''X'' and ''Y'' the Tversky index is a number between 0 and 1 given by \u000a\u000a<math>S(X, Y) = \u005cfrac{| X \u005ccap Y |}{| X \u005ccap Y | + \u005calpha | X - Y | + \u005cbeta | Y - X |} </math>,\u000a\u000aHere, <math>X - Y</math> denotes the  [[Complement (set theory)| relative complement ]] of Y in X.\u000a\u000aFurther, <math>\u005calpha, \u005cbeta \u005cge 0 </math> are parameters of the Tversky index.  Setting <math>\u005calpha = \u005cbeta = 1 </math> produces the Tanimoto coefficient; setting <math>\u005calpha = \u005cbeta = 0.5 </math> produces Dice's coefficient. \u000a\u000aIf we consider ''X'' to be the prototype and ''Y'' to be the variant, then <math>\u005calpha</math> corresponds to the weight of the prototype and <math>\u005cbeta</math> corresponds to the weight of the variant. Tversky measures with <math>\u005calpha + \u005cbeta = 1</math> are of special interest.<ref>http://www.daylight.com/dayhtml/doc/theory/theory.finger.html</ref>\u000a\u000aBecause of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using '''max''' and '''min''' functions <ref>Jimenez, S., Becerra, C., Gelbukh, A. [http://aclweb.org/anthology/S/S13/S13-1028.pdf SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity]. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, p.194-201, June 7\u20138, 2013, Atlanta, Georgia, USA.</ref>\u000a.\u000a\u000a<math>S(X,Y)=\u005cfrac{| X \u005ccap Y |}{| X \u005ccap Y |+\u005cbeta\u005cleft(\u005calpha a+(1-\u005calpha)b\u005cright)}</math>,\u000a\u000a<math>a=\u005cmin\u005cleft(|X-Y|,|Y-X|\u005cright) </math>,\u000a\u000a<math>b=\u005cmax\u005cleft(|X-Y|,|Y-X|\u005cright) </math>,\u000a\u000aThis formulation also re-arranges parameters <math>\u005calpha </math> and <math>\u005cbeta </math>. Thus, <math> \u005calpha </math> controls the balance between <math> |X - Y| </math> and <math> |Y - X| </math> in the denominator. Similarly, <math>\u005cbeta</math> controls the effect of the symmetric difference <math> |X\u005c,\u005ctriangle\u005c,Y\u005c,| </math> versus <math> | X \u005ccap Y | </math> in the denominator.\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a[[Category:Index numbers]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p121
sg4
S'202'
p122
sg6
VTversky index
p123
ssI77
(dp124
g2
V'''The Open Local Search Engine from Taganode''' is a [[search engine]] specifically targeting [[mobile phone]]s. It is based on local [[search algorithm]]s to find new places of interest within a specified distance.\u000a\u000aThe Taganode search engine offers an Open Developer [[Application programming interface|API]] that any one can use freely when writing new applications for [[iPhone]]s, [[Android (operating system)|Android phones]] and other platforms.\u000a\u000aThe search engine is optimized for mobile phones by low [[Bandwidth (computing)|bandwidth]] usage and only makes the simplest service calls to try to be compatible with as many mobile phones as possible. The Taganode search service is at this moment present in [[London]], [[Rome]], [[Venice]], [[Amsterdam]], [[Berlin]], [[Sweden]] and in [[Denmark]].\u000a\u000a== References ==\u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/164362/taganode-gratis-reseguide-finns-nu-aven-i-venedig/|title=Taganode \u2013 Gratis reseguide finns nu ven i Venedig |date=2009-10-12|work=Webfinanser|language=Swedish|accessdate=18 December 2009}}\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/159761/en-ny-och-innovativ-soktjanst-for-resenarer-i-europa/|title=En ny och innovativ sktjnst fr resenrer i Europa|date=September 19, 2009|work=Webfinanser |language=Swedish|accessdate=18 December 2009}}\u000a\u000a== External links ==\u000a* [http://www.taganode.com Official site]\u000a* [http://www.mynewsdesk.com/se/view/pressrelease/taganode-free-guide-now-available-in-rome-325701/  Taganode Service in Venice] (press release)\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Mobile phones]]
p125
sg4
S'77'
p126
sg6
VTaganode Local Search Engine
p127
ssI207
(dp128
g2
V{{No footnotes|date=July 2010}}\u000aIn [[computer science]], the '''string-to-string correction problem''' refers to the minimum number of edit operations necessary to change one [[String (computer science)|string]] into another. A single edit operation may be changing a single [[Character (computing)|symbol]] of the string into another, deleting, or inserting a symbol. The length of the edit sequence provides a measure of the [[Hamming distance|distance]] between the two strings.\u000a\u000aSeveral [[algorithm]]s exist to provide an efficient way to determine string distance and specify the minimum number of transformation operations required. Such algorithms are particularly useful for [[Delta encoding|delta]] creation operations where something is stored as a set of differences relative to a base version. This allows several versions of a single object to be stored much more efficiently than storing them separately. This holds true even for single versions of several objects if they do not differ greatly, or anything in between. \u000aNotably, such difference algorithms are used in [[molecular biology]] to provide some measure of kinship between different kinds of organisms based on the similarities of their [[macromolecule]]s (such as [[protein]]s or [[DNA]]).\u000a\u000a== See also ==\u000a* [[Delta encoding]]\u000a* [[Levenshtein distance]]\u000a* [[Edit distance]]\u000a\u000a== References ==\u000a<div class="references-small">\u000a*{{cite journal |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168\u2013173 |doi= 10.1145/321796.321811}}\u000a*{{cite journal |first=Walter F. |last=Tichy |title=The string-to-string correction problem with block moves |journal=ACM Transactions on Computer Systems |volume=2 |issue=4 |year=1984 |pages=309\u2013321 |doi= 10.1145/357401.357404}}\u000a</div>\u000a\u000a[[Category:Problems on strings]]\u000a[[Category:String similarity measures]]
p129
sg4
S'207'
p130
sg6
VString-to-string correction problem
p131
ssI82
(dp132
g2
V'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481\u2013485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.\u000a\u000a==Definitions==\u000aRijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:\u000a\u000a<math>P(d \u005cto q)</math>\u000a\u000aA user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.\u000aIn many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \u005cto q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\u000aSince ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \u005cto q</math> is uncertain. This will affect the plausibility of a given query.\u000a\u000aBy doing this it accomplishes two things:\u000a* Separate the processes of revising probabilities from the logic\u000a* Separate the treatment of relevance from the treatment of requests\u000a\u000a[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.\u000a\u000aUncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.\u000a\u000a==Example==\u000aIf we have a query of the form:\u000a\u000a<math>q = A \u005cwedge B \u005cwedge C</math>\u000a\u000awhere A, B and C are query assertions, then for a document D we want the probability:\u000a\u000a<math>P (D \u005cto (A \u005cwedge B \u005cwedge C))</math>\u000a\u000aIf we transform this into the [[conditional probability]] <math>P ((A \u005cwedge B \u005cwedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.\u000a\u000a==Further work==\u000aCroft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author=W. B. Croft | coauthors=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.\u000a\u000a[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.\u000a\u000a[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.\u000a\u000a== See also ==\u000a* [[Fuzzy logic]]\u000a* [[Probabilistic logic]]\u000a* [[Plausible reasoning]]\u000a* [[Imprecise probability]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]\u000a[[Category:Inference]]
p133
sg4
S'82'
p134
sg6
VUncertain inference
p135
ssI212
(dp136
g2
VIn [[computer science]], the '''Wagner\u2013Fischer algorithm''' is a [[dynamic programming]] algorithm that computes the [[edit distance]] between two strings of characters.\u000a\u000a==History==\u000aThe Wagner\u2013Fischer algorithm has a history of [[multiple invention]]. Navarro lists the following inventors of it, with date of publication, and acknowledges that the list is incomplete:<ref name="navarro"/>{{rp|43}}\u000a* Vintsyuk, 1968\u000a* [[Needleman\u2013Wunsch algorithm|Needleman and Wunsch]], 1970\u000a* Sankoff, 1972\u000a* Sellers, 1974\u000a* Wagner and Fischer, 1974\u000a* Lowrance and Wagner, 1975\u000a\u000a==Calculating distance==\u000aThe Wagner\u2013Fischer algorithm computes edit distance based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the edit distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix by [[flood fill]]ing the matrix, and thus find the distance between the two full strings as the last value computed.\u000a\u000aA straightforward implementation, as [[pseudocode]] for a function ''EditDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them, looks as follows. Note that the inputs strings are one-indexed, while the matrix ''d'' is zero-indexed, and <code>[i..k]</code> is a closed range.\u000a\u000a  '''int''' EditDistance('''char''' s[1..m], '''char''' t[1..n])\u000a    ''// For all i and j, d[i,j] will hold the Levenshtein distance between''\u000a    ''// the first i characters of s and the first j characters of t.''\u000a    ''// Note that d has (m+1)  x(n+1) values.\u000a    '''let''' d be a 2-d array of '''int''' with dimensions [0..m, 0..n]\u000a   \u000a    '''for''' i '''in''' [0..m]\u000a      d[i, 0] \u2190 i ''// the distance of any first string to an empty second string''\u000a    '''for''' j '''in''' [0..n]\u000a      d[0, j] \u2190 j ''// the distance of any second string to an empty first string''\u000a   \u000a    '''for''' j '''in''' [1..n]\u000a      '''for''' i '''in''' [1..m]\u000a        '''if''' s[i] = t[j] '''then'''  <!-- not: s[i-1] = t[j-1] -->\u000a          d[i, j] \u2190 d[i-1, j-1]       ''// no operation required''\u000a        '''else'''\u000a          d[i, j] \u2190 minimum of\u000a                     (\u000a                       d[i-1, j] + 1,  ''// a deletion''\u000a                       d[i, j-1] + 1,  ''// an insertion''\u000a                       d[i-1, j-1] + 1 ''// a substitution''\u000a                     )\u000a   \u000a    '''return''' d[m,n]\u000a\u000aTwo examples of the resulting matrix (hovering over an underlined number reveals the operation performed to get that number):\u000a<center>\u000a{|\u000a|\u000a{|class="wikitable"\u000a|-\u000a| \u000a| \u000a!k \u000a!i \u000a!t \u000a!t \u000a!e \u000a!n\u000a|-\u000a| ||0 ||1 ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!s\u000a|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!i\u000a|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5\u000a|-\u000a!t\u000a|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4\u000a|-\u000a!t\u000a|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 \u000a|-\u000a!i\u000a|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3\u000a|-\u000a!n\u000a|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}\u000a|-\u000a!g\u000a|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}\u000a|}\u000a|\u000a{|class="wikitable"\u000a|\u000a|\u000a!S\u000a!a\u000a!t\u000a!u\u000a!r\u000a!d\u000a!a\u000a!y\u000a|-\u000a| \u000a|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\u000a|-\u000a!S\u000a|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7\u000a|-\u000a!u\u000a|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6\u000a|-\u000a!n\u000a|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6\u000a|-\u000a!d\u000a|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5 \u000a|-\u000a!a\u000a|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4\u000a|-\u000a!y\u000a|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}\u000a|}\u000a|}\u000a</center>\u000a\u000aThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.\u000a\u000a===Proof of correctness===\u000aAs mentioned earlier, the [[invariant (mathematics)|invariant]] is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. This invariant holds since:\u000a* It is initially true on row and column 0 because <code>s[1..i]</code> can be transformed into the empty string <code>t[1..0]</code> by simply dropping all <code>i</code> characters. Similarly, we can transform <code>s[1..0]</code> to <code>t[1..j]</code> by simply adding all <code>j</code> characters.\u000a* If <code>s[i] = t[j]</code>, and we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code> and just leave the last character alone, giving <code>k</code> operations.\u000a* Otherwise, the distance is the minimum of the three possible ways to do the transformation:\u000a** If we can transform <code>s[1..i]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can simply add <code>t[j]</code> afterwards to get <code>t[1..j]</code> in <code>k+1</code> operations (insertion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j]</code> in <code>k</code> operations, then we can remove <code>s[i]</code> and then do the same transformation, for a total of <code>k+1</code> operations (deletion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code>, and exchange the original <code>s[i]</code> for <code>t[j]</code> afterwards, for a total of <code>k+1</code> operations (substitution).\u000a* The operations required to transform <code>s[1..n]</code> into <code>t[1..m]</code> is of course the number required to transform all of <code>s</code> into all of <code>t</code>, and so <code>d[n,m]</code> holds our result.\u000a\u000aThis proof fails to validate that the number placed in <code>d[i,j]</code> is in fact minimal; this is more difficult to show, and involves an [[Reductio ad absurdum|argument by contradiction]] in which we assume <code>d[i,j]</code> is smaller than the minimum of the three, and use this to show one of the three is not minimal.\u000a\u000a===Possible improvements===\u000aPossible improvements to this algorithm include:\u000a* We can adapt the algorithm to use less space, [[Big O notation|''O'']](''m'') instead of ''O''(''mn''), since it only requires that the previous row and current row be stored at any one time.\u000a* We can store the number of insertions, deletions, and substitutions separately, or even the positions at which they occur, which is always <code>j</code>.\u000a* We can normalize the distance to the interval <code>[0,1]</code>.\u000a* If we are only interested in the distance if it is smaller than a threshold ''k'', then it suffices to compute a diagonal stripe of width ''2k+1'' in the matrix. In this way, the algorithm can be run in [[Big O notation|''O'']](''kl'') time, where ''l'' is the length of the shortest string.<ref>{{cite book |author=Gusfield, Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=0-521-58519-8 }}</ref>\u000a* We can give different penalty costs to insertion, deletion and substitution. We can also give penalty costs that depend on which characters are inserted, deleted or substituted.\u000a* This algorithm [[parallel computing|parallelizes]] poorly, due to a large number of [[data dependency|data dependencies]]. However, all the <code>cost</code> values can be computed in parallel, and the algorithm can be adapted to perform the <code>minimum</code> function in phases to eliminate dependencies.\u000a* By examining diagonals instead of rows, and by using [[lazy evaluation]], we can find the Levenshtein distance in ''O''(''m'' (1 + ''d'')) time (where ''d'' is the Levenshtein distance), which is much faster than the regular dynamic programming algorithm if the distance is small.<ref>{{cite journal |author=Allison L |title=Lazy Dynamic-Programming can be Eager |journal=Inf. Proc. Letters |volume=43 |issue=4 |pages=207\u201312 |date=September 1992 |url=http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html |doi=10.1016/0020-0190(92)90202-7}}</ref>\u000a\u000a==Seller's variant for string search==\u000aBy initializing the first row of the matrix with zeros, we obtain a variant of the Wagner\u2013Fischer algorithm that can be used for [[fuzzy string searching|fuzzy string search]] of a string in a text.<ref name="navarro">{{cite doi|10.1145/375360.375365}}</ref> This modification gives the end-position of matching substrings of the text. To determine the start-position of the matching substrings, the number of insertions and deletions can be stored separately and used to compute the start-position from the end-position.<ref>Bruno Woltzenlogel Paleo. [http://www.logic.at/people/bruno/Papers/2007-GATE-ESSLLI.pdf An approximate gazetteer for GATE based on levenshtein distance]. Student Section of the European Summer School in Logic, Language and Information ([[European Summer School in Logic, Language and Information|ESSLLI]]), 2007.</ref>\u000a\u000aThe resulting algorithm is by no means efficient, but was at the time of its publication (1980) one of the first algorithms that performed approximate search.<ref name="navarro"/>\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a{{DEFAULTSORT:Wagner-Fischer algorithm}}\u000a[[Category:Algorithms on strings]]\u000a[[Category:String similarity measures]]
p137
sg4
S'212'
p138
sg6
VWagner\u2013Fischer algorithm
p139
ssI87
(dp140
g2
V{{Use dmy dates|date=February 2013}}\u000aThe '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, Web Search Engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|last=Bader Aljaber, Nicola Stokes, James Bailey and Jian Pei|url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]\u000a\u000a==Overview==\u000aAnchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:\u000a\u000a:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>\u000a\u000aThe anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.\u000a\u000a==Common misunderstanding of the concept==\u000a\u000aThis proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds [[significant]] [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}\u000a\u000a==Search engine algorithms==\u000aAnchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=\u000aHow the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>\u000a\u000a[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web\u000a|last=Fox\u000a|first=Vanessa\u000a|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html\u000a|title=Get a more complete picture about how other sites link to you\u000a|date=15 March 2007\u000a|publisher=Official Google Webmaster Central Blog\u000a|accessdate=2007-03-27\u000a| archiveurl= http://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\u000aIn the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.<ref>{{cite web\u000a|last=Cutts\u000a|first=Matt\u000a|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html\u000a|title=A quick word about Googlebombs\u000a|date=25 January 2007\u000a|publisher=Official Google Webmaster Central Blog\u000a|accessdate=2007-03-27\u000a| archiveurl= http://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\u000a\u000aIn April 2012, Google announced in its March "Penguin" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}</ref><ref>{{cite web|first=Simon|last=Dalley|accessdate=2012-04-04|date=4 April 2012|url=http://www.growtraffic.co.uk/google-changes-the-way-it-handles-anchor-text|title=Google Changes The way It Handles Anchor Text|publisher=Grow Traffic}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.\u000a.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=\u000aGoogle Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>\u000a\u000a==Anchor Text Terminology==\u000aThere are different classifications of anchor text that are used within the search engine optimization community such as the following:\u000a\u000a'''Exact Match:''' whenever an anchor is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.\u000a\u000a'''Branded:''' whenever a brand is used as the anchor. "[[Wikipedia]]" is a branded anchor text.\u000a\u000a'''Naked Link:''' whenever a URL is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.\u000a\u000a'''Generic:''' whenever a generic word or phrase is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.\u000a\u000a'''Images:''' whenever an image is linked, Google will use the "ALT" tag as the anchor text\u000a.<ref>{{cite web\u000a|last=Gotch\u000a|first=Nathan\u000a|url=http://www.gotchseo.com/anchor-text/\u000a|title=The Epic Guide to Anchor Text\u000a|date=26 October 2014}}</ref>\u000a\u000a==References==\u000a\u000a{{reflist|colwidth=30em}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Internet terminology]]\u000a[[Category:Search engine optimization]]\u000a[[Category:Hypertext]]
p141
sg4
S'87'
p142
sg6
VAnchor text
p143
ssI217
(dp144
g2
V{{Cat main|Citator}}\u000a\u000a[[Category:Citation indices]]\u000a[[Category:Legal research]]\u000a[[Category:Legal citation]]
p145
sg4
S'217'
p146
sg6
VCategory:Legal citators
p147
ssI92
(dp148
g2
V{{For|the Canadian magazine|Exclaim!}}\u000aThe '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' is an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006. It is currently in a beta stage of development, with some support for more than a dozen languages. The lead developers are Justin Nuger and Jesse Saba Kirchner.\u000a\u000aEarly work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>\u000a{{cite web\u000a|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web\u000a|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf\u000a|format=PDF|publisher=ACM-SIGIR 1999\u000a|accessdate=2006-12-02\u000a}}\u000a</ref>\u000a\u000aEXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).\u000a\u000aOne of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.\u000a\u000a==Current status==\u000a\u000aEXCLAIM is in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, is available for the following twenty-three languages:\u000a\u000a{| class="wikitable"\u000a| [[Albanian language|Albanian]]\u000a|-\u000a| [[Amharic]]\u000a|-\u000a| [[Bengali language|Bengali]]\u000a|-\u000a| [[Gothic language|Gothic]]\u000a|-\u000a| [[Greek language|Greek]]\u000a|-\u000a| [[Icelandic language|Icelandic]]\u000a|-\u000a| [[Indonesian language|Indonesian]]\u000a|-\u000a| [[Irish language|Irish]]\u000a|-\u000a| [[Javanese language|Javanese]]\u000a|-\u000a| [[Latvian language|Latvian]]\u000a|-\u000a| [[Malagasy language|Malagasy]]\u000a|-\u000a| [[Mandarin Chinese]]\u000a|-\u000a| [[Nahuatl]]\u000a|-\u000a| [[Navajo language|Navajo]]\u000a|-\u000a| [[Quechua languages|Quechua]]\u000a|-\u000a| [[Sardinian language|Sardinian]]\u000a|-\u000a| [[Swahili language|Swahili]]\u000a|-\u000a| [[Tagalog language|Tagalog]]\u000a|-\u000a| [[Standard Tibetan|Tibetan]]\u000a|-\u000a| [[Turkish language|Turkish]]\u000a|-\u000a| [[Welsh language|Welsh]]\u000a|-\u000a| [[Wolof language|Wolof]]\u000a|-\u000a| [[Yiddish]]\u000a|}\u000a\u000aSupport using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:\u000a\u000a{| class="wikitable"\u000a|-\u000a| [[Dutch language|Dutch]]\u000a|-\u000a| [[Spanish language|Spanish]]\u000a|}\u000a\u000aSignificant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.\u000a\u000aFuture versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.\u000a\u000aThe EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.\u000a\u000a==Further applications==\u000a\u000aEXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web\u000a|title=A crosslinguistic readability framework\u000a|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf\u000a|format=PDF|publisher=ACL-IJNLP 2009\u000a|accessdate=2009-09-04\u000a}}\u000a</ref>\u000a\u000a==Notes and references==\u000a\u000a{{reflist}}\u000a\u000a==External links==\u000a*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website]\u000a*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]\u000a*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]\u000a*[http://ju-st.in/ Justin Nuger's professional webpage]\u000a*[http://people.ucsc.edu/~kirchner/ Jesse Saba Kirchner's professional webpage]\u000a\u000a{{DEFAULTSORT:Exclaim}}\u000a[[Category:Information retrieval]]
p149
sg4
S'92'
p150
sg6
VEXCLAIM
p151
ssI222
(dp152
g2
V{{Third-party|date=February 2013}}\u000a'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>\u000a\u000a''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>\u000a\u000a==Editions==\u000aCoverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>\u000a*Materials Science, Ceramics\u000a*Materials Science, Characterization & Testing\u000a*Materials Science, Biomaterials\u000a*Materials Science, Coatings & Films\u000a*Materials Science, Composites\u000a*Materials Science, Paper & Wood\u000a*Materials Science, Multidisciplinary\u000a*Materials Science, Textiles\u000a\u000a==See also==\u000a* [[Science Citation Index]]\u000a* [[Academic publishing]]\u000a* [[List of academic databases and search engines]]\u000a* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956\u000a* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975\u000a* [[Impact factor]]\u000a* [[VINITI Database RAS]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a{{Thomson Reuters}}\u000a\u000a[[Category:Thomson family]]\u000a[[Category:Bibliographic databases]]\u000a[[Category:Online databases]]\u000a[[Category:Citation indices]]\u000a\u000a\u000a{{science-journal-stub}}
p153
sg4
S'222'
p154
sg6
VMaterials Science Citation Index
p155
ssI97
(dp156
g2
V{{Refimprove|date=June 2009}}\u000a\u000aA '''search [[Suggestion|suggest]] [[drop-down list]]''' is a [[Query language|query]] feature used in [[computing]]. A quick system to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed. Before the query has been typed, a drop-down list with the suggested complete search queries, is given as options to select and access. The suggested queries then enable the searcher to complete the required search quickly.\u000a\u000aIt is a form of [[Autocomplete|autocompletion]] while typing into a query [[text box]], before a detailed search result is entered. Lists can be based on popular searches or other options. The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from data or a [[database]], with search suggested drop-down lists being a common industry standard for an instant search.\u000a\u000aSearch suggested lists are used by [[internet browsers]], [[website]]s and [[search engine]]s, local [[operating system]]s and [[database]]s.\u000a\u000a[[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].\u000a\u000a==See also ==\u000a*[[Autocomplete]]\u000a*[[Search engine (computing)]]\u000a*[[Search box]]\u000a*[[Search algorithm]]\u000a*[[Censorship by Google#Search suggestions]]\u000a\u000a\u000a{{DEFAULTSORT:Search Suggest Drop-Down List}}\u000a[[Category:Data search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Search algorithms]]
p157
sg4
S'97'
p158
sg6
VSearch suggest drop-down list
p159
ssI227
(dp160
g2
V{{primary sources|date=March 2012}}\u000a'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.\u000a\u000a== Purpose ==\u000aFrom 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.\u000a\u000a== Functionality ==\u000aIn Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: \u000a* Scientific capacity and effectiveness of research, and\u000a* Publication activity\u000athrough the following indicators:\u000a* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,\u000a* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and\u000a* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.\u000a\u000a== See also ==\u000a*[[List of academic databases and search engines]]\u000a*[[Science Citation Index]]\u000a*[[Scopus]]\u000a\u000a==External links==\u000a* [http://elibrary.ru/ Scientific Electronic Library]\u000a\u000a\u000a[[Category:Citation indices]]
p161
sg4
S'227'
p162
sg6
VRussian Science Citation Index
p163
ssI102
(dp164
g2
V'''Cosine similarity''' is a measure of similarity between two vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0 is 1, and it is less than 1 for any other angle. It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a Cosine similarity of 1, two vectors at 90 have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\u000a\u000aNote that these bounds apply for any number of dimensions, and Cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[Information Retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35\u201343.</ref>\u000a\u000aThe technique is also used to measure cohesion within [[cluster (computing)|cluster]]s in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>\u000a\u000a''Cosine distance'' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the triangle inequality property and it violates the coincidence axiom; to repair the triangle inequality property whilst maintaining the same ordering, it is necessary to convert to Angular distance (see below.)\u000a\u000aOne of the reasons for the popularity of Cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.\u000a\u000a==Definition==\u000a\u000aThe cosine of two vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:\u000a\u000a:<math>\u005cmathbf{a}\u005ccdot\u005cmathbf{b}\u000a=\u005cleft\u005c|\u005cmathbf{a}\u005cright\u005c|\u005cleft\u005c|\u005cmathbf{b}\u005cright\u005c|\u005ccos\u005ctheta</math>\u000a\u000aGiven two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(\u03b8)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vectors|magnitude]] as\u000a\u000a:<math>\u005ctext{similarity} = \u005ccos(\u005ctheta) = {A \u005ccdot B \u005cover \u005c|A\u005c| \u005c|B\u005c|} = \u005cfrac{ \u005csum\u005climits_{i=1}^{n}{A_i \u005ctimes B_i} }{ \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(A_i)^2}} \u005ctimes \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(B_i)^2}} }</math>\u000a\u000aThe resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.\u000a\u000aFor text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.\u000a\u000aIn the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90.\u000a\u000aIf the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \u005cbar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson_product-moment_correlation_coefficient#For_a_sample|Pearson Correlation Coefficient]].\u000a\u000a=== Angular similarity ===\u000a\u000aThe term "cosine similarity" has also been used on occasion to express a different coefficient, although the most common use is as defined above. Using the same calculation of similarity, the normalised angle between the vectors can be used as a bounded similarity function within [0,1], calculated from the above definition of similarity by:\u000a:<math>1 - \u005cfrac{ \u005ccos^{-1}( \u005ctext{similarity} )}{ \u005cpi} </math>\u000ain a domain where vector coefficients may be positive or negative, or\u000a:<math>1 - \u005cfrac{ 2 \u005ccdot \u005ccos^{-1}( \u005ctext{similarity} ) }{ \u005cpi }</math>\u000ain a domain where the vector coefficients are always positive.  \u000a\u000aAlthough the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.\u000a\u000a=== Confusion with "Tanimoto" coefficient ===\u000a\u000aThe cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:\u000a\u000a:<math>T(A,B) = {A \u005ccdot B \u005cover \u005c|A\u005c|^2 +\u005c|B\u005c|^2 - A \u005ccdot B}</math>\u000a\u000aIn fact, this algebraic form [[Jaccard index#Tanimoto_Similarity_and_Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.\u000a\u000a=== Ochiai coefficient ===\u000aThis coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. \u2116 9. P. 526-530.</ref><ref>''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. \u2013 Assen. Van Gorcum. 1958. 628 p.</ref>\u000a:<math>K =\u005cfrac{n(A \u005ccap B)}{\u005csqrt{n(A) \u005ctimes n(B)}}</math>\u000aHere, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.\u000a\u000a== Properties ==\u000aCosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\u005c|A - B\u005c|</math>, and observe that\u000a\u000a:<math>\u005c|A - B\u005c|^2 = (A - B)^\u005ctop (A - B) = \u005c|A\u005c|^2 + \u005c|B\u005c|^2 - 2 A^\u005ctop B</math>\u000a\u000aby [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\u005c|A\u005c|^2 = \u005c|B\u005c|^2 = 1</math> so the previous is equal to\u000a\u000a:<math>2 (1 - \u005ccos(A, B))</math>\u000a\u000a'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal\u000a | author = Spruill, Marcus C\u000a | year = 2007\u000a | title = Asymptotic distribution of coordinates on high dimensional spheres\u000a | journal = Electronic communications in probability\u000a | volume = 12 | pages = 234-247\u000a | doi = 10.1214/ECP.v12-1294\u000a}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>\u000aFor other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal\u000a | author = Graham L. Giller \u000a | year = 2012\u000a | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity\u000a | journal = Giller Investments Research Notes\u000a | number = 20121024/1\u000a | doi = 10.2139/ssrn.2167044\u000a}}</ref>\u000a\u000a== Soft Cosine Measure ==\u000a'''Soft cosine measure''' <ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gmez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computacin y Sistemas|volume=18|issue=3|pages=491\u2013504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref>\u000ais a measure of \u201csoft\u201d similarity between two vectors, i.e., the measure that considers similarity of pairs of features. The traditional '''cosine similarity''' considers the [[vector space model]] (VSM) features as independent or completely different, while the '''soft cosine measure''' proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).\u000a\u000aFor example, in the field of [[natural language processing]] (NLP) the similarity between features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernndez|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1\u201311|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words \u201cplay\u201d and \u201cgame\u201d are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).\u000a\u000aFor calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.  \u000a\u000aGiven two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:\u000a\u000a:<math>\u005cbegin{align}\u000a    \u005coperatorname{soft\u005c_cosine}_1(a,b)=\u000a    \u005cfrac{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ib_j}{\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ia_j}\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}b_ib_j}},\u000a\u005cend{align}\u000a</math>\u000a\u000awhere {{math|''s<sub>ij</sub>'' {{=}} similarity(feature<sub>''i''</sub>, feature<sub>''j''</sub>)}}.\u000a\u000aIf there is no similarity between features ({{math|''s<sub>ii</sub>'' {{=}} 1}}, {{math|''s<sub>ij</sub>'' {{=}} 0}} for {{math|''i'' \u2260 ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.\u000a\u000aThe complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be even transformed to linear.\u000a\u000a== See also ==\u000a* [[Srensen similarity index|Srensen's quotient of similarity]]\u000a* [[Hamming distance]]\u000a* [[Correlation]]\u000a* [[Dice's coefficient]]\u000a* [[Jaccard index]]\u000a* [[SimRank]]\u000a* [[Information retrieval]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://www.appliedsoftwaredesign.com/archives/cosine-similarity-calculator/ Online Cosine Similarity Calculator]\u000a* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]\u000a* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]\u000a\u000a{{DEFAULTSORT:Cosine Similarity}}\u000a[[Category:Information retrieval]]
p165
sg4
S'102'
p166
sg6
VCosine similarity
p167
ssI232
(dp168
g2
V{{inline|date=August 2012}}\u000a'''Query by humming''' ('''QbH''') is a music retrieval system that branches off the original classification systems of title, artist, composer, and genre. It normally applies to songs or other music with a distinct single theme or melody. The system involves taking a user-hummed [[melody]] (input [[Information retrieval|query]]) and comparing it to an existing [[database]].  The system then returns a ranked list of music closest to the input query. \u000a\u000aOne example of this would be a system involving a [[portable media player]] with a built-in [[microphone]] that allows for faster [[Search engine technology|searching]] through [[Digital media|media]] files.\u000a\u000aThe [[MPEG-7]] standard includes provisions for QbH music searches.\u000a\u000a== Examples of QbH systems ==\u000aSoundHound and Midomi are the only commercially available query by humming services available online at Midomi.com or on the mobile app called SoundHound. \u000aBoth are powered by the same backend and are capable of recognizing humming and singing as well as recorded tracks. \u000aFor the singing and humming search, the searchable database is based on Midomi.com's user contributions. Midomi has collected about one million tracks based on user contributions in multiple languages, making it the largest database of its kind by a large margin. The top four languages are: English, Japanese, Chinese and Spanish. \u000a\u000a"Musipedia" is an example of a QbH system that uses a variety of input methods such as humming, tapping the keyboard, keyboard search (a virtual piano keyboard), draw notes, and a contour search, using [[Parsons_code|Parsons Code]] to encode the music pieces.\u000a\u000a[[Tunebot]] is a music search engine that uses queries from humming, lyrics, and melody. People can contribute to the database and expand the variety of searchable songs. Tunebot also serves as the back-end for a game called [[Karaoke Callout]], in which players' performances are compared by the engine with songs in the database.\u000a\u000a== External links ==\u000a===Online demos===\u000a* [http://www.midomi.com/ Midomi]\u000a* [http://www.soundhound.com/ SoundHound (mobile app)] \u000a* [http://www.musipedia.org/query_by_humming.0.html QbH system] from Musipedia\u000a* [http://querybyhum.cs.nyu.edu/ QbH research project at NYU]\u000a* [http://www.sloud.com/technology/query_by_humming/ Query by Humming at Sloud Inc], [http://www.sloud.com/ QbH applet (Active X)] \u000a* [http://www.musicline.de/de/melodiesuche/input Musicline QbH based on technology from Fraunhofer Institut] {{de icon}}\u000a* [http://maart.sourceforge.net/ MaART at Sourceforge]\u000a* [http://tunebot.cs.northwestern.edu/ Tunebot at Northwestern University]\u000a\u000a===General info and articles===\u000a* {{Wayback|url=http://mirsystems.info/index.php?id=mirsystems|title=Comprehensive list of Music Information Retrieval systems (apparently last updated ca 2003)|date=20081221191111}}\u000a* [http://www.cs.cornell.edu/zeno/papers/humming/humming.html Query By Humming \u2013 Musical Information Retrieval in an Audio Database], paper by Asif Ghias, Jonathan Logan, David Chamberlin, Brian C. Smith; [[ACM Multimedia]] 1995\u000a* [http://cs.nyu.edu/~eugenew/publications/humming-summary.pdf A survey presentation of QBH by Eugene Weinstein, 2006]\u000a* [http://www.dlib.org/dlib/may97/meldex/05witten.html The New Zealand Digital Library MELody inDEX], article by Rodger J. McNab, Lloyd A. Smith, David Bainbridge and Ian H. Witten; [[D-Lib Magazine]] 1997\u000a* [http://deepblue.lib.umich.edu/bitstream/handle/2027.42/35292/10373_ftp.pdf?sequence=1 Name that Tune: A Pilot Study in Finding a Melody from a Sung Query], article by Bryan Pardo, Jonah Shifrin, and William Birmingham, Journal of the American Society for Information Science and Technology, vol. 55 (4), pp. 283-300, 2004\u000a\u000a[[Category:Music search engines]]\u000a[[Category:Acoustic fingerprinting]]
p169
sg4
S'232'
p170
sg6
VQuery by humming
p171
ssI107
(dp172
g2
V#REDIRECT [[Search engine optimization]]\u000a\u000a{{DEFAULTSORT:Keyword Optimization}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet marketing]]
p173
sg4
S'107'
p174
sg6
VKeyword optimization
p175
ssI237
(dp176
g2
V{{distinguish|SoundCloud}}\u000a{{Infobox software\u000a| name                   = Soundcloud\u000a| title                  = \u000a| logo                   = [[File:SoundHound Mobile Icon.png|250px]]\u000a| logo caption           = SoundHound Mobile Icon\u000a| screenshot             = <!-- [[File: ]] -->\u000a| caption                = \u000a| collapsible            = \u000a| author                 = \u000a| developer              = SoundHound, Inc\u000a| released               = {{Start date|2009|01|29|df=yes}}\u000a| discontinued           = \u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| programming language   = \u000a| operating system       = \u000a| platform               = \u000a| size                   = \u000a| language               = \u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| status                 = \u000a| genre                  = \u000a| license                = \u000a| alexa                  = \u000a| website                = {{URL|//www.soundhound.com/}}\u000a}}\u000a'''SoundHound''' (known as '''Midomi''' until December 2009) is a [[mobile device]] service that allows users to identify music by [[Query by humming|humming]], singing or playing a recorded track. The service was launched by Melodis Corporation (now SoundHound Inc), under [[Chief Executive]] Keyvan Mohajer in 2007 and has received funding from Global Catalyst Partners, TransLink Capital and Walden Venture Capital.\u000a\u000a==Features==\u000aSoundHound is a music search engine available on the [[Apple App Store]],<ref name="Apple App Store" /> [[Google Play]],<ref name="Google Play" /> [[Windows Phone Store]], and on June 5, 2013, was available on the BlackBerry 10 platform.<ref name="Windows" /> It enables users to identify music recorded through their device's microphone.<ref name=CNETv3 /> It is also possible to speak or type the name of the artist, composer, song and piece.<ref name=CNETv3 /> Unlike competitor [[Shazam (service)|Shazam]], SoundHound can recognise tracks from singing, humming, speaking, or typing, as well as from a recording.<ref>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=2 October 2012|newspaper=CNET|date=16 July 2008}}</ref> Sound matching is achieved through the company's 'Sound2Sound' technology, which  can match even poorly-hummed performances to professional recordings.<ref name="CNETWVC" />\u000a\u000aThe app then returns the lyrics (if any), links to videos on YouTube, links to iTunes, ringtones, the ability to launch [[Pandora Radio]],<ref name="TNWHound">{{cite news | url=http://thenextweb.com/apps/2011/05/26/soundhounds-new-voice-app-hound-wants-to-change-the-way-we-search/ | title=SoundHound\u2019s new voice app "Hound" wants to change the way we search | date=26 May 2011 | accessdate=2 October 2012 | last=Boyd Myers | first=Courtney | newspaper=The Next Web}}</ref> as well as recommendations for other music.<ref>{{cite news|last=Dolcourt|first=Jessica|title=SoundHound for iPhone channels iTunes, recommends beats|url=http://reviews.cnet.com/8301-19512_7-20037798-233.html|accessdate=2 October 2012|newspaper=CNET|date=2 March 2011}}</ref> A feature called LiveLyrics displays a song's lyrics in time with the music, if they are available. Double-tapping on those lyrics moves the music to that point in the song.<ref>{{cite news|last=Cabebe|first=Jaymar|title=SoundHound adds LiveLyrics|url=http://download.cnet.com/8301-2007_4-20081243-12/soundhound-adds-livelyrics/|accessdate=3 October 2012|newspaper=CNET|date=20 July 2011}}</ref> It is also possible for users to play music from their iPhone's iPod library through the app. If lyrics are available for a song, it will show them as it plays.<ref name="CNETv3" />\u000a\u000aThere are three versions of the app: SoundHound, SoundHound Infinity and Hound. SoundHound is free but has banner ads, while SoundHound Infinity (styled SoundHound \u221e), priced at 4.99 in the UK or $6.99 in the US, is the premium offering and has the same functionality but without banner ads.<ref name="CNETFree" /> Hound only allows users to search for artists or songs by speaking into it. Similar to the SoundHound app, Hound then returns a song preview, lyrics, album art and videos as well as artist bios and tour dates.<ref name="TNWHound" />\u000a\u000a==History==\u000aMidomi, renamed SoundHound in December 2009 with the launch of version 3.0 of the mobile app,<ref name=CNETv3>{{cite news|last=Dolcourt|first=Jessica|title=Midomi 3.0 seeks song lyrics, knows what's hot|url=http://reviews.cnet.com/8301-19512_7-10408563-233.html|accessdate=2 October 2012|newspaper=CNET|date=3 December 2009}}</ref> was launched in [[beta]] in January 2007, as a [http://www.midomi.com website], with 2 million licensed tracks.<ref name="CNET1" /> The technology, dubbed Multimodal Adaptive Recognition System (MARS), considers pitch, tempo variation, speech content and pauses in order to recognise samples.<ref name=CNET1>{{cite news|last=Mills|first=Elinor|title=This Web site can name that tune|url=http://news.cnet.com/This-Web-site-can-name-that-tune/2100-1027_3-6153657.html|accessdate=2 October 2012|newspaper=CNET|date=26 January 2007}}</ref> The company behind the site, Melodis Corporation, was started in 2004 by [[Chief Executive]] Keyvan Mohajer, a [[PhD]] in sound-recognition from [[Stanford]].<ref name=CNET1 /> Melodis changed its name to SoundHound Inc in May 2010.<ref>{{cite press release|title=SoundHound Inc. Announces Name Change from Melodis Corporation|publisher=SoundHound Inc|date=20 May 2010|url=//www.soundhound.com/index.php?action=s.press_release&pr=15|accessdate=2 October 2012}}</ref>\u000a\u000aThe first version of the app was released on the [[Apple App Store]] in July 2008.<ref name="Apple App Store">{{cite news | url=http://news.cnet.com/8301-17938_105-9987892-1.html | title=Sing for search results with iPhone app | date=10 July 2008 | accessdate=2 October 2012 | last=Jackson | first=Holly | newspaper=CNET}}</ref> At the launch of [[Windows Marketplace for Mobile]] in October 2009, Midomi was one of the apps included in the store<ref name="Windows">{{cite news | url=http://reviews.cnet.com/8301-12261_7-10368174-10356022.html | title=Windows mobile app store, My Phone service officially opening | date=6 October 2009 | accessdate=2 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> and could be purchased for $4.99.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=2 October 2012|newspaper=CNET|date=7 October 2009}}</ref> It joined the [[Android OS|Android]] app store in June 2010.<ref name="Google Play">{{cite news | url=http://www.cnet.com/8301-19736_1-20007745-251.html | title=New SoundHound names that tune--for free (Android) | date=15 June 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> On January 2013, the [[BlackBerry]] version of the app was then available in [[BlackBerry World]] following the announcement and launch of [[BlackBerry 10]].<ref name="BlackBerry">{{cite web\u000a  |title=BlackBerry shows off some of its 70,000 new third-party apps, including Skype, Rdio, Kindle, and Whatsapp\u000a  |publisher=[[The Verge]]\u000a  |url=http://www.theverge.com/2013/1/30/3932042/blackberry-10-apps-announcement\u000a  |accessdate=2013-01-30}}</ref>\u000a\u000aA free version of the app was released in April 2010, with all the functionality of the premium version, while limiting the number of searches to five per month, and adding banners ads.<ref name="CNETFree">{{cite news | url=http://reviews.cnet.com/8301-19512_7-20003228-233.html | title=Sonic freebie: New, free SoundHound music-ID app for iPhone, iPad | date=27 April 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> The premium version was now renamed SoundHound Infinity.<ref name="CNETFree" /> A stripped-down version, Hound, was released in May 2011.<ref name="TNWHound" />\u000a\u000aIn January 2011, Apple revealed that SoundHound was the top paid iPad app  on its [[Apple App Store|App Store]] was SoundHound, while rival Shazam was fourth in the top ten list of free iPhone apps.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=2 October 2012|newspaper=CNET|date=19 January 2011}}</ref>\u000a\u000aIn June 2012, the firm announced that it had 80 million users while version 5.0 was released, with a new design and features that include an in-built player and integration with LiveLyrics.<ref name="TNW80m">{{cite news | url=http://thenextweb.com/apps/2012/06/07/shazam-competitor-soundhound-passes-80m-users-and-rolls-out-updated-mobile-apps/ | title=Shazam competitor SoundHound passes 80m users and rolls out updated mobile apps | work=The Next Web | date=7 June 2012 | accessdate=2 October 2012 | author=Sawers, Paul}}</ref>\u000a\u000aIn December 2013, the app passed 185 million users.<ref>{{cite news|title=SoundHound Reveals Its Top Songs of 2013|url=http://www.heraldonline.com/2013/12/16/5509030/soundhound-reveals-its-top-songs.html|accessdate=16 December 2013|newspaper=The Next Web|date=16 December 2013}}</ref>\u000a\u000aIn December 2013, the app launches iTunes Radio integration.<ref>{{cite news|title=SoundHound App Update Adds iTunes Radio Integration for iPad and iPhone Users. |url=http://www.padgadget.com/2013/12/20/soundhound-app-update-adds-itunes-radio-integration-for-ipad-and-iphone-users/|accessdate=11 February 2014|newspaper=PadGadget|date=20 December 2013}}</ref>\u000a\u000aIn September 2013, the app enables 170 million global users to sync, save, and transfer music search & discovery history across multiple devices.<ref>{{cite news|title=SoundHound adds cloud history sync on iOS and Android apps|url=http://www.intomobile.com/2013/09/25/soundhound-adds-cloud-history-sync-ios-and-android-apps/|accessdate=11 February 2014|newspaper=INTOMOBILE|date=20 September 2013}}</ref>\u000a\u000aIn January 2014, SoundHound and Hyundai Motor Group partnered to embed music search and discovery into select 2014 Hyundai & Kia models.<ref>{{cite news|title=Hyundai and Kia tap SoundHound to help you identify music in your car|url=http://www.engadget.com/2014/01/14/hyundai-kia-soundhound-music-tagging/|accessdate=11 February 2014|newspaper=Engadget|date= January 14, 2014}}</ref>\u000a\u000aIn January 2014, the app launched an "immersive second screen GRAMMYs experience".<ref>{{cite news|title=SoundHound's music search app turns its focus to the Grammys with real-time updates and more|url=http://www.engadget.com/2014/01/25/soundhound-grammys-2014/|accessdate=11 February 2014|newspaper=Engadget|date= January 24, 2014}}</ref>\u000a\u000aIn April 2014, the app passed 200 million users.<ref>//www.soundhound.com/index.php?action=s.press_release&pr=67</ref>\u000a\u000a===Funding===\u000aMelodis secured $7 million in a Series B funding round in October 2008, bringing total funds raised to $12 million. The round was led by TransLink Capital with the participation of JAIC America and [[Series A round|Series A]] investor Global Catalyst Partners.<ref>{{cite press release|title=Search and Sound Recognition Innovator MELODIS and Creator of Midomi Raises $7 Million in Series B Funding|publisher=Melodis Corporation|date=7 October 2008|url=//www.soundhound.com/index.php?action=s.press_release&pr=5|accessdate=2 October 2012}}</ref>\u000a\u000aIn 2009, Melodis attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups [[Pandora Radio|Pandora]] and [[SNOCAP|Snocap]].<ref name=CNETWVC>{{cite news|last=Needleman|first=Rafe|title=Midomi music search gets funding and opportunities|url=http://news.cnet.com/8301-19882_3-10298068-250.html|accessdate=2 October 2012|newspaper=CNET|date=28 July 2009}}</ref> The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.<ref>{{cite press release|title=Melodis, Sound Search Technology and Applications Innovator, Raises $4M Led by Walden Venture Capital and a Strategic Investor|publisher=Melodis Corporation|date=4 August 2009|url=//www.soundhound.com/index.php?action=s.press_release&pr=12|accessdate=2 October 2012}}</ref>\u000a\u000a==See also==\u000a*[[Query by humming]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==External links==\u000a*[http://midomi.com midomi.com]\u000a*[//www.soundhound.com/ SoundHound website]\u000a\u000a[[Category:Android (operating system) software]]\u000a[[Category:IOS software]]\u000a[[Category:Symbian software]]\u000a[[Category:Music search engines]]\u000a[[Category:Companies established in 2005]]\u000a[[Category:Companies based in California]]\u000a[[Category:BlackBerry software]]
p177
sg4
S'237'
p178
sg6
VSoundHound
p179
ssI112
(dp180
g2
V'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. \u000a\u000aLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526\u2013557 }}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].\u000a\u000a==Swanson linking==\u000a[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]\u000a''Swanson linking'' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111\u2013135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.\u000a\u000a==See also==\u000a*[[Arrowsmith System]]\u000a*[[Implicature]]\u000a*[[Latent semantic indexing]]\u000a*[[Metaphor]]\u000a\u000a==References==\u000a* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&nbsp;9958\u20139964.\u000a*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]\u000a\u000a; Further readings\u000a* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.\u000a\u000a; Footnotes\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Medical research]]\u000a\u000a\u000a{{science-stub}}
p181
sg4
S'112'
p182
sg6
VLiterature-based discovery
p183
ssI117
(dp184
g2
V{{Infobox company |\u000a name   = Pikimal |\u000a logo   = [[File:Pikimal logo.jpg|200px]] |\u000a type   = [[Limited liability company|LLC]]|\u000a company_slogan = |\u000a|founder = Eric Silver|\u000a foundation     = 2010|\u000a location       = [[Pittsburgh, Pennsylvania]], [[United States]]|\u000a key_people     = Eric Silver, Chief Executive Officer|\u000a industry       = [[Search engine technology|Search]] |\u000a<!--Please fill in:-->\u000a revenue        = |\u000a operating_income = |\u000a net_income     = | \u000a num_employees  = 13<ref name="businesstimes">{{cite news|url=http://www.bizjournals.com/pittsburgh/print-edition/2011/03/25/pikimal-comparison-shopping-stand-out.html | title=Pikimal, a comparison shopping website, hopes to stand out from crowd | work= Pittsburgh Business Times | first=Malia | last=Spencer | date=25 March 2011}}</ref> |\u000a homepage       = [http://pikimal.com/ www.pikimal.com]|\u000a}}\u000a\u000a'''Pikimal''' (pronounced as pick-em-all)<ref>{{cite web|url=http://www.popcitymedia.com/innovationnews/pikimal033011.aspx | title=Shop Smarter With Pikimal \u2013 POP City Media }}</ref> is a website, designed as a [[decision engine]] that uses consumer input to provide specialized search results for products and categories.\u000a\u000aUnlike typical [[Web search engine|search engines]], Pikimal mines data to provide users with only the facts pertaining to their search, as a hopeful solution to [[SEO]] and marketing biased search results.\u000a\u000aAs of April 2011, Pikimal had 13 full-time employees in Pittsburgh, PA, interns, and various contractors around the world.<ref name="businesstimes" />\u000a\u000a== History ==\u000a\u000aPikimal was founded in January 2010 by [[Eric Silver]], previously the chief marketing officer at [[Modcloth]]. The Pikimal site was launched in [[public beta]] form in October 2010.<ref name="businesstimes" />\u000a\u000a== Functionality ==\u000a\u000aPikimal allows users to adjust sliders to express what facts of a product are particularly important to them. These percentages are combined with an algorithm to provide users with product recommendations that are rooted directly in facts, but only the facts they find most relevant.<ref>{{cite web|url=http://www.youtube.com/watch?v=imOUklpphcM&feature=player_embedded | title=What is Pikimal? Video}}</ref>\u000a\u000a== Pivot and Shutdown ==\u000a\u000aIn 2012 Pikimal changed it name to [[Webkite]] and pivoted to provide faceted search solutions to other companies. As of September 2014 pikimal.com and all associated sites has been shutdown.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* {{official website|http://pikimal.com}}\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet properties established in 2010]]\u000a[[Category:Knowledge markets]]\u000a[[Category:Companies based in Pittsburgh, Pennsylvania\u200e]]
p185
sg4
S'117'
p186
sg6
VPikimal
p187
ssI122
(dp188
g2
V{{Too many see alsos|date=December 2012}}\u000a'''Search engine indexing''' collects, parses, and stores [[data (computing)|data]] to facilitate fast and accurate [[information retrieval]]. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, [[Information technology|informatics]], and computer science.  An alternate name for the process in the context of [[search engine]]s designed to find web pages on the Internet is ''[[web indexing]]''.\u000a\u000aPopular engines focus on the full-text indexing of online, natural language documents.<ref>Clarke, C., Cormack, G.: Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. TechRep MT-95-01, University of Waterloo, February 1995.</ref> [[Multimedia|Media types]] such as video and audio<ref>http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</ref> and graphics<ref>Charles E. Jacobs, Adam Finkelstein, David H. Salesin. [http://grail.cs.washington.edu/projects/query/mrquery.pdf Fast Multiresolution Image Querying]. Department of Computer Science and Engineering, University of Washington. 1995. Verified Dec 2006</ref> are also searchable.\u000a\u000a[[Metasearch engine|Meta search engines]] reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the  [[text corpus|corpus]]. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while [[Intelligent agent|agent]]-based search engines index in [[Real time business intelligence|real time]].\u000a\u000a==Indexing==\u000aThe purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would [[Lexical analysis|scan]] every document in the corpus, which would require considerable time and computing power.  For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval.\u000a\u000a===Index design factors===\u000aMajor factors in designing a search engine's architecture include:\u000a\u000a; Merge factors : How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the [[Web crawling|data collection]] policy. Search engine index merging is similar in concept to the [[Merge (SQL)|SQL Merge]] command and other merge algorithms.<ref>Brown, E.W.: Execution Performance Issues in Full-Text Information Retrieval. Computer Science Department, University of Massachusetts Amherst, Technical Report 95-81, October 1995.</ref>\u000a; Storage techniques : How to store the index [[data]], that is, whether information should be data compressed or filtered.\u000a; Index size : How much computer storage is required to support the index.\u000a; Lookup speed : How quickly a word can be found in the inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science.\u000a; Maintenance : How the index is maintained over time.<ref>Cutting, D., Pedersen, J.: Optimizations for dynamic inverted index maintenance. Proceedings of SIGIR, 405-411, 1990.</ref>\u000a;Fault tolerance : How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, [[partition (database)|partitioning]], and schemes such as [[hash function|hash-based]] or composite partitioning,<ref>[http://dev.mysql.com/doc/refman/5.1/en/partitioning-linear-hash.html Linear Hash Partitioning]. MySQL 5.1 Reference Manual. Verified Dec 2006</ref> as well as [[Replication (computer science)|replication]].\u000a\u000a===Index data structures===\u000aSearch engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.\u000a\u000a;[[Suffix tree]] : Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [[trie]]. Tries support extendable hashing, which is important for search engine indexing.<ref>[http://www.nist.gov/dads/HTML/trie.html trie], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology].</ref> Used for searching for patterns in [[DNA]] sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.<ref name="Gus97">{{cite book\u000a | last = Gusfield\u000a | first = Dan\u000a | origyear = 1997\u000a | year = 1999\u000a | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology\u000a | publisher = Cambridge University Press\u000a | location = USA\u000a | isbn = 0-521-58519-8}}.\u000a</ref> An alternate representation is a [[suffix array]], which is considered to require less virtual memory and supports data compression such as the [[Burrows-Wheeler transform|BWT]] algorithm.\u000a\u000a;[[Inverted index]] : Stores a list of occurrences of each atomic search criterion,<ref>Black, Paul E., [http://www.nist.gov/dads/HTML/invertedIndex.html inverted index], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology] Oct 2006. Verified Dec 2006.</ref> typically in the form of a [[hash table]] or [[binary tree]].<ref>C. C. Foster, Information retrieval: information storage and retrieval using AVL trees, Proceedings of the 1965 20th national conference, p.192-205, August 24\u201326, 1965, Cleveland, Ohio, United States</ref><ref>Landauer, W. I.: The balanced tree and its utilization in information retrieval. IEEE Trans. on Electronic Computers, Vol. EC-12, No. 6, December 1963.</ref>\u000a\u000a;[[Citation index]] : Stores citations or hyperlinks between documents to support citation analysis, a subject of [[Bibliometrics]].\u000a;[[N-gram|Ngram index]] : Stores sequences of length of data to support other types of retrieval or text mining.<ref>[http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 Google Ngram Datasets] for sale at [http://www.ldc.upenn.edu/ LDC] Catalog</ref>\u000a;[[Document-term matrix]] : Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [[sparse matrix]].\u000a\u000a===Challenges in parallelism===\u000aA major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for [[race conditions]] and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or [[Text corpus|corpus]]). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a '''producer-consumer model'''. The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve [[distributed computing]], where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture.<ref>Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Google, Inc. OSDI. 2004.</ref>\u000a\u000a===Inverted indices===\u000aMany search engines incorporate an [[inverted index]] when evaluating a [[search query]] to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct [[random access|access]] to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Inverted Index\u000a|-\u000a! Word !! Documents\u000a|-\u000a| the || Document 1, Document 3, Document 4, Document 5, Document 7\u000a|-\u000a| cow || Document 2, Document 3, Document 4\u000a|-\u000a| says || Document 5\u000a|-\u000a| moo || Document 7\u000a|}\u000a\u000aThis index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a [[boolean datatype|boolean]] index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document.<ref>Grossman, Frieder, Goharian. [http://www.cs.clemson.edu/~juan/CPSC862/Concept-50/IR-Basics-of-Inverted-Index.pdf IR Basics of Inverted Index]. 2002. Verified Aug 2011.</ref> Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of [[information retrieval]].\u000a\u000aThe inverted index is a [[sparse matrix]], since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional [[Array data structure|array]]. The index is similar to the [[document-term matrix|term document matrices]] employed by [[latent semantic analysis]]. The inverted index can be considered a form of a hash table. In some cases the index is a form of a [[binary tree]], which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a [[distributed hash table]].<ref>Tang, Hunqiang. [[Sandhya Dwarkadas|Dwarkadas, Sandhya]]. "Hybrid Global Local Indexing for Efficient\u000aPeer to Peer Information Retrieval". University of Rochester. Pg 1. http://www.cs.rochester.edu/u/sandhya/papers/nsdi04.ps</ref>\u000a\u000a===Index merging===\u000aThe inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing,<ref>Tomasic, A., et al.: Incremental Updates of Inverted Lists for Text Document Retrieval. Short Version of Stanford University Computer Science Technical Note STAN-CS-TN-93-1, December, 1993.</ref> where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives.\u000a\u000aAfter parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is  so named because it is an inversion of the forward index.\u000a\u000a===The forward index===\u000aThe forward index stores a list of words for each document. The following is a simplified form of the forward index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Forward Index\u000a|-\u000a! Document !! Words\u000a|-\u000a| Document 1 || the,cow,says,moo\u000a|-\u000a| Document 2 || the,cat,and,the,hat\u000a|-\u000a| Document 3 || the,dish,ran,away,with,the,spoon\u000a|}\u000a\u000aThe rationale behind developing a forward index is that as documents are parsing, it is better to immediately store the words per document.  The delineation enables Asynchronous system processing, which partially circumvents the inverted index update [[wikt:bottleneck|bottleneck]].<ref>Sergey Brin and Lawrence Page. [http://infolab.stanford.edu/~backrub/google.html The Anatomy of a Large-Scale Hypertextual Web Search Engine]. [[Stanford University]]. 1998. Verified Dec 2006.</ref> The forward index is [[Sorting algorithm|sorted]] to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index.\u000a\u000a===Compression===\u000aGenerating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on [[computer storage|disk]].<ref>H.S. Heaps. Storage analysis of a compression coding for a document database. 1NFOR, I0(i):47-61, February 1972.</ref> Consider the following scenario for a full text, Internet search engine.\u000a\u000a* It takes 8 bits (or 1 [[byte]]) to store a single character. Some [[character encoding|encodings]] use 2 bytes per character<ref>[http://www.unicode.org/faq/basic_q.html#15 The Unicode Standard - Frequently Asked Questions]. Verified Dec 2006.</ref><ref>[http://www.uplink.freeuk.com/data.html Storage estimates]. Verified Dec 2006.</ref>\u000a* The average number of characters in any given word on a page may be estimated at 5 ([[Wikipedia:Size comparisons]])\u000a\u000aGiven this scenario, an uncompressed index (assuming a non-[[conflation|conflated]], simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression.\u000a\u000aNotably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost.\u000a\u000a==Document parsing==\u000aDocument parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called ''tokens'', and so, in the context of search engine indexing and [[natural language processing]], parsing is more commonly referred to as [[Tokenization (lexical analysis)|tokenization]]. It is also sometimes called [[word boundary disambiguation]], [[Part-of-speech tagging|tagging]], [[text segmentation]], [[content analysis]], text analysis, [[text mining]], [[Agreement (linguistics)|concordance]] generation, [[speech segmentation]], [[Lexical analysis|lexing]], or [[lexical analysis]]. The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang.\u000a\u000aNatural language processing, as of 2006, is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.\u000a\u000a=== Challenges in natural language processing ===\u000a; Word Boundary Ambiguity : Native [[English language|English]] speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a [[multilingual]] indexer.  In digital form, the texts of other languages such as [[Chinese language|Chinese]], [[Japanese language|Japanese]] or [[Arabic language|Arabic]] represent a greater challenge, as words are not clearly delineated by [[Whitespace (computer science)|whitespace]]. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax).\u000a\u000a; Language Ambiguity : To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its [[language]] or [[lexical category]] ([[part of speech]]). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document.\u000a\u000a; Diverse File Formats : In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document.\u000a\u000a; Faulty Storage : The quality of the natural language data may not always be perfect.  An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol.  [[Binary data|Binary]] characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade.\u000a\u000a=== Tokenization ===\u000aUnlike [[literacy|literate]] humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word, referred to as a token. Such a program is commonly called a [[tokenizer]] or [[parser]] or [[Lexical analysis|lexer]]. Many search engines, as well as other natural language processing software, incorporate [[Comparison of parser generators|specialized programs]] for parsing, such as [[YACC]] or [[Lex programming tool|Lex]].\u000a\u000aDuring tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify [[Entity extraction|entities]] such as [[email]] addresses, phone numbers, and [[Uniform Resource Locator|URL]]s. When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number.\u000a\u000a=== Language recognition ===\u000aIf the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as [[stemming]] and [[part of speech]] tagging). [[Language identification|Language recognition]] is the process by which a computer program attempts to automatically identify, or categorize, the [[language]] of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in [[natural language processing]]. Finding which language the words belongs to may involve the use of a [[language recognition chart]].\u000a\u000a=== Format analysis ===\u000aIf the search engine supports multiple [[File format|document formats]], documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example, [[HTML]] documents contain HTML tags, which specify formatting information such as new line starts, '''bold''' emphasis, and [[font]] size or [[Font family|style]].  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning, and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include:\u000a\u000a* [[HTML]]\u000a* [[ASCII]] text files (a text document without specific computer readable formatting)\u000a* [[Adobe Systems|Adobe]]'s Portable Document Format ([[PDF]])\u000a* [[PostScript]] (PS)\u000a* [[LaTeX]]\u000a* [[UseNet]] netnews server formats\u000a* [[XML]] and derivatives like [[RSS]]\u000a* [[SGML]]\u000a* [[Multimedia]] [[meta data]] formats like [[ID3]]\u000a* [[Microsoft Word]]\u000a* [[Microsoft Excel]]\u000a* [[Microsoft PowerPoint]]\u000a* IBM [[Lotus Notes]]\u000aOptions for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom [[parser]].\u000a\u000aSome search engines support inspection of files that are stored in a [[Compressor (software)|compressed]] or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported [[list of archive formats|compressed file format]]s include:\u000a\u000a* [[ZIP (file format)|ZIP]] - Zip archive file\u000a* [[RAR]] - Roshal ARchive file\u000a* [[Cabinet (file format)|CAB]] - [[Microsoft Windows]] Cabinet File\u000a* [[Gzip]] - File compressed with gzip\u000a* [[Bzip2|BZIP]] - File compressed using bzip2\u000a* [[Tar (file format)|Tape ARchive (TAR)]], [[Unix]] archive file, not (itself) compressed\u000a* TAR.Z, TAR.GZ or TAR.BZ2 - [[Unix]] archive files compressed with Compress, GZIP or BZIP2\u000a\u000aFormat analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for [[spamdexing]]:\u000a\u000a* Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden [[Span and div|"div" tag]] in [[HTML]], which may incorporate the use of [[CSS]] or [[JavaScript]] to do so).\u000a* Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer.\u000a\u000a=== Section recognition ===\u000aSome search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the [[Internet|web]], such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted:\u000a\u000a* Content in different sections is treated as related in the index, when in reality it is not\u000a* Organizational 'side bar' content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents.\u000a\u000aSection analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the [[Noscript tag]] to ensure that the web page is indexed properly.  At the same time, this fact can also be [[spamdexing|exploited]] to cause the search engine indexer to 'see' different content than the viewer.\u000a\u000a=== HTML Priority System ===\u000a{{Section OR|date=November 2013}}\u000aIndexing often has to recognize the [[HTML]] tags to organize priority. Indexing low priority to high margin to labels like ''strong'' and ''link'' to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like [[Google]] and [[Bing]] ensure that the [[search engine]] does not take the large texts as relevant source due to[[ strong type system]] compatibility.<ref>Google Webmaster Tools, "Hypertext Markup Language 5", Conference for SEO January 2012.</ref>\u000a\u000a=== Meta tag indexing ===\u000aSpecific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the [[meta tag]] contains keywords which are also included in the index. Earlier Internet [[search engine technology]] would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was [[computer hardware]] able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization.<ref>Berners-Lee, T., "Hypertext Markup Language - 2.0", RFC 1866, Network Working Group, November 1995.</ref>\u000a\u000aAs the Internet grew through the 1990s, many [[brick and mortar business|brick-and-mortar corporations]] went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to [[spamdexing]], which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the [[customer lifetime value]] equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies.\u000a\u000aIn [[Desktop search]], many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index.\u000a\u000a== See also ==\u000a{{div col|colwidth=25em}}\u000a* [[Compound term processing]]\u000a* [[Concordance (publishing)|Concordance]]\u000a* [[Content analysis]]\u000a* [[Controlled vocabulary]]\u000a* [[Desktop search]]\u000a* [[Documentation]]\u000a* [[Document retrieval|Document Retrieval]]\u000a* [[Full text search]]\u000a* [[Index (database)]]\u000a* [[Information extraction]]\u000a* [[Information retrieval]]\u000a* [[Key Word in Context|Keyword In Context Indexing]]\u000a* [[Latent semantic indexing]]\u000a* [[List of search engines]]\u000a* [[Natural language processing]]\u000a* [[Search engine]]\u000a* [[Selection-based search]]\u000a* [[Semantic Web]]\u000a* [[Site map]]\u000a* [[Text mining]]\u000a* [[Text retrieval|Text Retrieval]]\u000a* [[Vertical search]]\u000a* [[Web crawler]]\u000a* [[Web indexing]]\u000a* [[Website Parse Template]]\u000a*[[Windows indexing service]]<ref>Krishna Nareddy. [http://msdn2.microsoft.com/en-us/library/ms951558.aspx Indexing with Microsoft Index Server]. MSDN Library. Microsoft Corporation. January 30, 1998. Verified Dec 2006. Note that this is a commercial, external link.</ref>\u000a{{div col end}}\u000a\u000a== References ==\u000a<references/>\u000a\u000a==Further reading==\u000a*R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998.\u000a*[[Gerald Salton]]. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988.\u000a*[[Gerard Salton]]. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986.\u000a*[[Gerard Salton]]. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968.\u000a*[[Gerard Salton]]. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971.\u000a*[[Gerard Salton]]. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989.\u000a*Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999.\u000a*G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949.\u000a*Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962).\u000a*[[Edward H. Sussenguth Jr.]], Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p.&nbsp;272-279, May 1963\u000a*Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 28\u201343, 1992.\u000a*Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133\u2013146, 2001.\u000a*Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003.\u000a*Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349\u2013379, October 1996, Volume 14, Number 4.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]], [[Mark Overmars|Overmars, M.H.]]: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 93\u201398, 1981.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 1\u201316, 1981.\u000a*Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Int'l World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp.&nbsp;175\u2013182)\u000a*[[Serge Abiteboul]] and [[Victor Vianu]]. [http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1996-20&format=text&compression=&name=1996-20.text Queries and Computation on the Web]. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997.\u000a*Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994.\u000a*A. Emtage and P. Deutsch, "Archie--An Electronic Directory Service for the Internet." Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp.&nbsp;93\u2013110.\u000a*M. Gray, [http://www.mit.edu/people/mkgray/net/ World Wide Web Wanderer].\u000a*D. Cutting and J. Pedersen. "Optimizations for Dynamic Inverted Index Maintenance." Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp.&nbsp;405\u2013411, September 1990.\u000a*Stefan Bttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Index (Search Engine)}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a[[Category:Indexing]]\u000a[[Category:Internet search algorithms]]
p189
sg4
S'122'
p190
sg6
VSearch engine indexing
p191
ssI127
(dp192
g2
V{{Infobox Website\u000a| name           = Macroglossa\u000a| logo           = [[File:Macroglossa Visual Search Engine Logo, 2012.gif]]\u000a| screenshot     = \u000a| caption        = Macroglossa logo\u000a| url            = [http://www.macroglossa.com macroglossa.com]\u000a| type           = [[Visual search engine|Visual]] [[Search engine|Search Engine]]\u000a| language       = English\u000a| registration   = optional\u000a| author         = MVE\u000a| launch date    = 2010\u000a| current status = beta 0.1\u000a| slogan         = search is visual\u000a| alexa          = {{IncreaseNegative}} 2,828,096 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/macroglossa.com |title= Macroglossa.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->\u000a}}\u000a'''Macroglossa''' is a [[visual search engine]] based on the comparison of images,<ref>Nicola Mattina. "[http://blog.wired.it/startupcloud/2010/12/29/macroglossa-usare-le-immagini-per-effettuare-ricerche-sul-web.html Macroglossa: usare le immagini per effettuare ricerche sul web]", Wired.it, Retrieved December 29, 2010.</ref><ref>GreatStartups.com . "[http://greatstartups.com/2010/10/13/macroglossa-com-whats-in-the-picture/ Macroglossa.com-What\u2019s In The Picture ]", greatstartups.com, Retrieved October 13, 2010.</ref> coming from an Italian Group. The development of the project began in 2009. In April 2010 is released the first public [[Alpha stage#Alpha|alpha]].<ref>Liva Judic. "[http://searchenginewatch.com/article/2050950/Macroglossas-Visual-Search-Engine-fails-to-meet-basic-expectations Macroglossa's Visual Search Engine fails to meet basic expectations ]", SEW - searchenginewatch, Retrieved April 26, 2010.</ref>\u000aUsers can upload photos or images that they aren't sure what they are to determine what the images contain. Macroglossa compares images to return search results based on specific search categories. \u000aThe engine does not use technologies and solutions such as [[Optical character recognition|OCR]], [[Tag (metadata)|tags]], vocabulary trees. The comparison is directly based on the contents of the image which the user wants to know more.\u000a\u000aInteresting features are the categorization of the elements, the ability to search specific portions of the image or start a search from a video file,<ref>Mve. "[http://www.macroglossa.com/press_macrog_eng_a2dot0.pdf - Macroglossa PR]",  - Retrieved 2011.</ref> but the main function is to simulate a digital eye on trying to find similarities of an unknown subject. This feature makes the engine unique.\u000a\u000aThis technology has several advantages. First, it allows users to pull results from collections of visual content<ref>Make Use OF . "[http://www.makeuseof.com/dir/macroglossa-identify-objects-in-image/ - MacroGlossa: Find Similar Images & Identify Objects In Image ]",  - Makeuseof.com. 2010.</ref> without using tags for search. Second, the visuals can be [[Crowdsourcing|crowd sourced]]. In fact by being a search engine, rather than simply a tool, Macroglossa should be able to crowdsourced and scale its recognition vocabulary faster than anyone else and a technology like this would increase the cognitive and spatial skills in [[humanoid]] robotics.<ref>J. Sturm, A. Visser. "[http://cvpr.in.tum.de/old/pub/pub/sturm09ras.pdf An appearance-based visual compass for mobile robots ]", Appearance-based, mobile robot localization, active vision, machine learning. 2000.</ref> In addition Macroglosssa can also be used as a Reverse Image Search to find [[orphan works]] and possible violations of copyright of images.\u000a\u000aMacroglossa supports all popular image extensions such [[Jpg|jpeg]], [[Portable Network Graphics|png]], [[BMP file format|bmp]], [[gif]] and video formats such [[Audio Video Interleave|avi]], [[.mov|mov]], [[mp4]], [[m4v]], [[3gp]], [[wmv]], [[mpeg]].\u000a\u000aMacroglossa enters [[Beta stage#Beta beta|beta]] stage in September 2011<ref>Mve. "[http://www.macroglossa.com/disclaimer.html - macroglossa.com]",  - Releases and Features. 2011.</ref> and at the same time open to the public the opportunity to use the developed [[Interface (object-oriented programming)|interfaces]] ( Api for web and mobile applications ) in order to expand the use of the engine in the [[Business-to-business|B2B]] and [[Business-to-consumer|B2C]] fields. Macroglossa becomes a [[Software as a service|SaaS]].\u000a\u000a[[Api|API]] are distributed on three levels : free, basic, and premium. The free API has limited use, but basic and premium do not. The premium API also offers custom services allowing customers to extend and mold the features offered by computer vision.<ref>J. R. Martnez-de Dios, C. Serna y A. Ollero. "[http://grvc.us.es/publica/revistas/documentos/FishFarms.pdf Computer vision and robotics techniques in fish farms ]", Robotica. Vo. 21. No. 3. Editor Cambridge University Press. June 2003.</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==Notes==\u000a* ''Wired.it, Retrieved December 29, 2010 :'' Macroglossa is an Italian project born from a passion for research and innovation by the MVE group of independent developers. The startup has developed a visual search engine based on the comparison of the subjects in the images. The owners of the project define it as "a sort of digital eye can capture, compare and draw conclusions." The purpose of this service is to provide a new type of research within the network. The search engine allows you to upload a picture on the platform and look for similar images on the web. The engine is not based on text tags and does not use OCR to extract strings from images to locate the target. Everything focuses on the key points of the image uploaded by the user. The aim is to give as much information as possible on the results obtained. Each image has a direct result of the source.\u000a\u000a==External links==\u000a* [http://www.macroglossa.com Macroglossa] home page\u000a* Macroglossa [http://www.macroglossa.com/api.html Api program]\u000a* Macroglossa on [http://www.killerstartups.com/Search/macroglossa-com-carry-out-visual-searches Killer Startups]\u000a* [http://yourstory.in/2011/07/macroglossa-reaches-alpha-version-4-0-a-picture-search-engine/ Yourstory.in] talks about Macroglossa\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Data search engines]]\u000a[[Category:Multimedia]]\u000a[[Category:Image search]]
p193
sg4
S'127'
p194
sg6
VMacroglossa Visual Search
p195
ss.
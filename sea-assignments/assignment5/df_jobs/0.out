(dp0
I0
(lp1
VCategory:Searching
p2
aV{{Commons category|Searching}}\u000a[[Category:Information retrieval]]\u000a<!-- used to be on cat search engines that was merged into this, but didn't think it really applied... [[Category:Information technology]] -->
p3
asI3
(lp4
VSubsetting
p5
aVIn research communities (for example, [[earth science]]s), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client\u2014server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.\u000a\u000aSubsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=http://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>\u000a* restrict the time range\u000a* select [[Cross-sectional data|cross section]]s of data\u000a* select particular kinds of [[time series]]\u000a* exclude particular obersvations\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a==External links==\u000a*[http://www.subset.org/index.jsp Subset.org]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Statistics-stub}}
p6
asI6
(lp7
VIFACnet
p8
aV'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.\u000a\u000aThe following 31 organizations participate in IFACnet:\u000a\u000a*[[American Institute of Certified Public Accountants]] (AICPA)\u000a*[[Association of Chartered Certified Accountants]] (ACCA)\u000a*[[Canadian Institute of Chartered Accountants]]\u000a*[[Certified General Accountants Association of Canada]]\u000a*[[Chartered Institute of Management Accountants]] (CIMA)\u000a*[[Chartered Institute of Public Finance and Accountancy]]\u000a*[[CMA Canada]]\u000a*[[Compagnie Nationale des Commissaires aux Comptes]]\u000a*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]\u000a*[[Consiglio Nazionale Dottori Commercialisti]]\u000a*[[CPA Australia]]\u000a*[[Délégation Internationale Pour l'Audit et la Comptabilité]]\u000a*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)\u000a*[[International Federation of Accountants]]  (IFAC)\u000a*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)\u000a*[[Institute of Certified Public Accountants in Ireland]]\u000a*[[Institute of Certified Public Accountants of Singapore]]\u000a*[[Institute of Chartered Accountants of Australia]]\u000a*[[Institute of Chartered Accountants in England & Wales]] (ICAEW)\u000a*[[Institute of Chartered Accountants in Ireland]]\u000a*[[Institute of Chartered Accountants of India]]\u000a*[[Institute of Chartered Accountants of Pakistan]]\u000a*[[Institute of Chartered Accountants of Scotland]] (ICAS)\u000a*[[Institute of Management Accountants]]\u000a*[[Japanese Institute of Certified Public Accountants]] (JICPA)\u000a*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)\u000a*[[Malaysian Institute of Accountants]]\u000a*[[Malta Institute of Accountants]]\u000a*[[National Association of State Boards of Accountancy]] (NASBA)\u000a*[[South African Institute of Chartered Accountants]] (SAICA)\u000a*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)\u000a\u000a==External links==\u000a*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]\u000a*[http://www.ifac.org/ International Federation of Accountants Homepage]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Accounting organizations]]
p9
asI7
(lp10
VEuropean Conference on Information Retrieval
p11
aVThe '''European Conference on Information Retrieval''' (ECIR) is the main \u000aEuropean research conference for the presentation of new results in the field of [[information retrieval]] (IR).\u000aIt is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).\u000a      \u000aThe event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was \u000aheld in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has\u000aalternated between the United Kingdom and continental Europe. To mark the metamorphosis\u000afrom a small informal colloquium to a major event in the IR research calendar, the \u000aBCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,\u000aECIR has continued to grow and has become the major European forum for the discussion\u000aof research in the field of Information Retrieval. \u000a\u000aSome of the topics dealt with include:\u000a* IR models, techniques, and algorithms\u000a* IR applications\u000a* IR system architectures\u000a* Test and evaluation methods for IR\u000a* [[Natural Language Processing]] for IR\u000a* Distributed IR\u000a* Multimedia and cross-media IR\u000a\u000a==Time and Location==\u000a\u000aTraditionally, the ECIR is held in Spring, near the Easter weekend. Previous locations include\u000athe following:\u000a\u000a* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]\u000a* [[Moscow, Russia]], 2013 [http://ecir2013.org/]\u000a* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]\u000a* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]\u000a* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]\u000a* [[Toulouse]], 2009 [http://ecir09.irit.fr/]\u000a* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]\u000a* [[Rome]], 2007 [http://ecir2007.fub.it/]\u000a* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]\u000a* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]\u000a* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]\u000a* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]\u000a* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*\u000a* [[Darmstadt]], 2001* (organized by GMD)\u000a* [[Cambridge]], 2000* (organized by Microsoft Research)\u000a* [[Glasgow]], 1999*\u000a* [[Grenoble]], 1998*\u000a* [[Aberdeen, Scotland|Aberdeen]], 1997*\u000a* [[Manchester]], 1996*\u000a* [[Crewe]], 1995* (organized by Manchester Metropolitan University)\u000a* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)\u000a* [[Glasgow]], 1993* (organized by Strathclyde University)\u000a* [[Lancaster, Lancashire|Lancaster]], 1992*\u000a* [[Lancaster, Lancashire|Lancaster]], 1991*\u000a* [[Huddersfield]], 1990*\u000a* [[Huddersfield]], 1989*\u000a* [[Huddersfield]], 1988*\u000a* [[Glasgow]], 1987*\u000a* [[Glasgow]], 1986*\u000a* [[Bradford]], 1985*\u000a* [[Bradford]], 1984*\u000a* [[Sheffield]], 1983*\u000a* [[Sheffield]], 1982*\u000a* [[Birmingham]], 1981*\u000a* [[Leeds]], 1980*\u000a* [[Leeds]], 1979*\u000a\u000a<br /> *as the Annual Colloquium on Information Retrieval Research\u000a\u000aFuture locations include:\u000a* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]\u000a\u000a==External links==\u000a* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computer science conferences]]
p12
asI10
(lp13
VHarshness
p14
aV{{Original research|date=September 2007}}\u000a{{Other uses|Harsh (disambiguation)}}\u000a'''Harshness''' (also called '''raucousness'''), in [[music information retrieval]], is a Non-Contextual Low-Level Audio Descriptors (NLDs) that represents one dimension of the multi-dimensional [[psychoacoustic]] feature called as musical [[timbre]].\u000a\u000aClassical timbre\u2019 NLDs are [[surface roughness|roughness]], [[spectral centroid]], and [[spectral flux]]. While harmonicity and inharmonicity can also be considered NLDs, harshness differs from them, as well as from roughness, once it reckons for a distinguished perceptual audio feature expressed by the summary spectral periodicity. This feature is especially clear in single-[[Pitch (music)|pitch]], single-[[note]], musical audio, where the timbre of two different musical instruments can greatly differ in levels of harshness (e.g., the difference in harshness between a flute and a saxophone is evident). As it is supposed to be, harshness is independent of all others NLDs.\u000a\u000a[[Category:Musicology]]\u000a[[Category:Music technology]]\u000a[[Category:Information retrieval]]
p15
asI11
(lp16
VChemRefer
p17
aV{{Orphan|date=February 2009}}\u000a{{Infobox website\u000a| name = Chemrefer\u000a| logo = [[Image:Chemrefer.png]]\u000a| screenshot = \u000a| caption = \u000a| url = http://www.chemrefer.com\u000a| commercial = Yes\u000a| type = [[Search engine]]\u000a| language = English\u000a| registration = Not Applicable\u000a| owner = ChemRefer Limited\u000a| author = William James Griffiths\u000a| launch date = 2006\u000a| current status = Offline\u000a| revenue = \u000a}}\u000a'''ChemRefer''' is a service that allows searching of freely-available and full-text chemical and pharmaceutical literature that is published by authoritative sources.<ref>{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}</ref>\u000a\u000aFeatures include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.\u000a\u000aChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.\u000a\u000a==See also==\u000a*[[Google Scholar]]\u000a*[[Windows Live Academic]]\u000a*[[BASE (search engine)|BASE]]\u000a*[[PubMed]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a===Recommendations & reviews===\u000a*[http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]\u000a*[http://infoweb.nrl.navy.mil/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]\u000a*[http://www.mta.ca/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]\u000a*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine\u000a*[http://recherche-technologie.wallonie.be/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium\u000a*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki\u000a\u000a===Background===\u000a*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine\u000a*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College\u000a\u000a[[Category:Scholarly search services]]\u000a[[Category:Chemistry literature]]\u000a[[Category:Information retrieval]]\u000a[[Category:Open access projects]]\u000a\u000a{{searchengine-website-stub}}
p18
asI16
(lp19
VCategory:Data management
p20
aV{{catdiffuse}}\u000a\u000a'''[[Data management]]''' comprises all the disciplines related to managing data as a valuable resource.\u000a\u000a\u000a{{Commons cat|Data management}}\u000a\u000a[[Category:Computer data|Management]]\u000a[[Category:Data|Management]]\u000a[[Category:Project management]]\u000a[[Category:Information retrieval]]\u000a[[Category:Information technology management]]
p21
asI18
(lp22
VPoliqarp
p23
aV'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].\u000a\u000a==Features==\u000a\u000a* Custom [[query language]].\u000a* Two-level [[regular expressions]]:\u000a** operating at the level of characters in words\u000a** operating at the level of words in statements/paragraphs\u000a* Good performance\u000a* Compact corpus representation (compared to similar projects)\u000a* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]\u000a* Lack of portability across [[endianness]] (current release works only on little endian devices)\u000a\u000a==External links==\u000a\u000a* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]\u000a* [http://poliqarp.sourceforge.net/ Project website on SourceForge]\u000a* [http://poliqarp.suxx.pl/ Search plugin for Firefox]\u000a<br />\u000a[[Category:Information retrieval]]
p24
asI19
(lp25
VInformation retrieval applications
p26
aVAreas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):\u000a\u000a==General applications of information retrieval==\u000a* [[Digital libraries]]\u000a*  [[Information filtering]]\u000a** [[Recommender systems]]\u000a*  Media search\u000a** Blog search\u000a** [[Image retrieval]]\u000a** [[Music information retrieval|Music retrieval]]\u000a** News search\u000a** Speech retrieval\u000a** Video retrieval\u000a* [[Search engines]]\u000a** [[Desktop search]]\u000a** [[Enterprise search]]\u000a** [[Federated search]]\u000a** [[Mobile search]]\u000a** [[Social search]]\u000a** [[Web search engine|Web search]]\u000a\u000a==Domain specific applications of information retrieval==\u000a* Expert search finding\u000a* Genomic information retrieval\u000a* [[Geographic information retrieval]]\u000a*  Information retrieval for chemical structures\u000a* Information retrieval in [[software engineering]]\u000a* [[Legal information retrieval]]\u000a* [[Vertical search]]\u000a\u000a==Other retrieval methods==\u000aMethods/Techniques in which [[information retrieval]] techniques are employed include:\u000a* [[Adversarial information retrieval]]\u000a* [[Automatic summarization]]\u000a**[[Multi-document summarization]]\u000a* [[Compound term processing]]\u000a* [[Cross-language information retrieval|Cross-lingual retrieval]]\u000a* [[Document classification]]\u000a* [[Spam filtering]]\u000a* [[Question answering]]\u000a\u000a== See also ==\u000a* [[Information retrieval]]\u000a\u000a{{DEFAULTSORT:Information Retrieval Applications}}\u000a[[Category:Information retrieval|*]]
p27
asI20
(lp28
VSemantic technology
p29
aV{{no footnotes|date=March 2013}}\u000a[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]\u000aIn [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. \u000a\u000aThis enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.\u000a\u000aWith traditional [[information technology]], on the other hand, meanings and relationships must be predefined and \u201chard wired\u201d into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.\u000a\u000aOff-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.\u000a\u000aSemantic technologies are \u201cmeaning-centered.\u201d They include tools for:\u000a\u000a* autorecognition of topics and concepts, \u000a* information and meaning extraction, and\u000a* categorization. \u000a\u000aGiven a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.\u000a\u000aSemantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.\u000a\u000a== See also ==\u000a* [[Business Intelligence 2.0]] (BI 2.0)\u000a* [[Metadata]]\u000a* [[Ontology (computer science)]]\u000a* [[Semantic targeting]]\u000a* [[Semantic web]]\u000a\u000a==References==\u000a\u000a* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004\u000a* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 \u2014 Proc. of the 12th international conference on World Wide Web'', pp 700\u2013709. [[ACM Press]], 2003.\u000a* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.\u000a* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.\u000a* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.\u000a* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, \u000a* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September\u000a2004.\u000a* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5\u000a\u000a== External links ==\u000a* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]\u000a* [http://www.semanticarts.com Semantic Technology and the Enterprise]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Semantics]]
p30
asI26
(lp31
VZyLAB Technologies
p32
aV{{advert|date=April 2012}}\u000a{{Infobox company |\u000a  name   = ZyLAB |\u000a  logo   = <!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --> |\u000a  slogan = "eDiscovery & Information Risk Management" |\u000a  type   = Private |\u000a  foundation     = 1983 |\u000a  location       = [[McLean, Virginia]]<br>[[Amsterdam]] |\u000a  key_people     = [[Pieter Varkevisser]], president & CEO<br>[[Dr. Johannes C. Scholtes]], chairman & chief strategy officer | Mary Mack, Enterprise Technology Counsel\u000a  num_employees  = 140 |\u000a  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |\u000a  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email & SharePoint archiving, text-analytics, visualization, contract management, and workflow. |\u000a\u000a  homepage       = [http://www.zylab.com/ www.zylab.com]\u000a}}\u000a\u000a'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB\u2019s most important products are ZyLAB eDiscovery & Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.\u000a\u000a== History ==\u000aIn 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.\u000a\u000aIn 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]\u2019s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.\u000a\u000aIn 1998, the company developed support to full-text search email, including attachments.\u000a\u000aIn 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.\u000a\u000aIn 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].\u000a\u000a2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.\u000a\u000aPlatforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.\u000a\u000a2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.\u000a\u000a==Customers==\u000aInitial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).\u000a\u000aOther well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milo\u0161evi\u0107]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB\u2019s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].\u000a\u000aPublic websites also use the ZyLAB Webserver.\u000a\u000a[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.\u000a\u000aZyLAB\u2019s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.\u000a\u000a==System overview and compatibility==\u000aAccording to the company\u2019s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:\u000a\u000aSystems:\u000a*ZyLAB eDiscovery and Production\u000a*ZyLAB Compliance and Litigation readiness\u000a*ZyLAB Law Enforcement and Investigations\u000a*ZyLAB Communications Intelligence\u000a*ZyLAB Digital Print and Media Archiving\u000a*ZyLAB Enterprise Information Management\u000a\u000aBundles:\u000a*E-Mail Archiving Bundle\u000a*Microsoft SharePoint Bundle\u000a*Analytics Bundle\u000a*eDiscovery EDRM Processing bundle\u000a*DoD and Sox Compliant RMA Bundle\u000a*TIFF Archiving and Production Bundle\u000a*WebPublishing Bundle\u000a*Commercial Publishing Bundle\u000a*Business Process Automation Bundle\u000a*Development and Integrators Bundle\u000a*Scanning Bundle\u000a*Digital Copier Bundle\u000a*Professional Text Mining\u000a*Machine translation\u000a\u000a===Supported configurations===\u000a*'''Server OS''': Windows 2003, Windows 2008\u000a*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL\u000a*'''Web Servers''': IIS\u000a*'''Client OS''': Windows XP, Windows Vista, Windows 7\u000a*'''Clustering''': Support for Active/Passive Failover.\u000a*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.\u000a*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.\u000a\u000a===Languages supported===\u000a*'''Unicode'''. Support for documents in all languages.\u000a*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.\u000a\u000a==Zy-IMAGE-nation Annual Conference==\u000aThe annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.\u000a\u000a==See also==\u000a* [[Electronic discovery|e-Discovery]]\u000a* [[Optical character recognition|Optical Character Recognition (OCR)]]\u000a* [[Document Imaging]]\u000a* [[E-mail archiving|E-mail Archiving]]\u000a* [[Knowledge Management]]\u000a* [[Document management system|Document Management (System)]]\u000a* [[Enterprise content management|Enterprise Content Management]]\u000a* [[Records management|Records Management]]\u000a* [[Contract management|Contract Management]]\u000a* [[Workflow]]\u000a* [[Text mining|Text Mining]]\u000a* [[Text analytics|Text Analytics]]\u000a* [[Machine translation|Automatic Machine Translation]]\u000a* [[Data visualization|Data Visualization]]\u000a\u000a==References==\u000a{{Reflist}}\u000a*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia\u000a*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia\u000a*[http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''\u000a*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]\u000a*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''\u000a*[http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)\u000a*[http://www.computerwoche.de/index.cfm?pid=2123&pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)\u000a*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&s_site=miami&p_multi=MH&p_theme=realcities&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EB367D56736E685&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''\u000a*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]\u000a*[http://www.fcw.com/print/6_31/news/70014-1.html Review] of ZyIMAGE on ''Federal Computer Week (FCW.com)''\u000a*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.\u000a*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&nbsp;20.\u000a*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&nbsp;22.\u000a*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&nbsp;88\u201389.\u000a*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&nbsp;73, 76, and 77.\u000a*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&nbsp;100.\u000a*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&nbsp;127, 129, 133 and 137.\u000a*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&nbsp;56.\u000a*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&nbsp;46.\u000a*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&nbsp;22.\u000a*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&nbsp;62.\u000a\u000a===Gartner reports===\u000a*Introduction to Investigative Case Management Products (18 April 2007)\u000a*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)\u000a*MarketScope for Contract Management, 2007 (16 July 2007)\u000a*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)\u000a*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)\u000a*Magic Quadrant for Information Access Technology, 2008\u000a*Magic Quadrant for Information Access Technology, 2009\u000a*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)\u000a*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)\u000a*MarketScope for E-Discovery Product Vendors, 2008\u000a*MarketScope for E-Discovery Product Vendors, 2009\u000a*MarketScope for Records Management (20 May 2008)\u000a*Hype Cycle for Content Management, 2008 (8 July 2008)\u000a*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)\u000a*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)\u000a*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)\u000a\u000a==External links==\u000a*[http://www.zylab.com/ ZyLAB official website]\u000a*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]\u000a*[http://www.aiim.org/ AIIM]\u000a\u000a[[Category:Companies established in 1983]]\u000a[[Category:Software companies of the United States]]\u000a[[Category:Information retrieval]]
p33
asI29
(lp34
VInformation needs
p35
aV'''Information need''' is an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The \u2018information\u2019 and \u2018need\u2019 in \u2018information need\u2019 are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:\u000a# The explanation of observed phenomena of information use or expressed need;\u000a# The prediction of instances of information uses;\u000a# The control and thereby improvement of the utilization of information manipulation of essentials conditions.\u000a\u000aInformation needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.\u000a\u000a== Background ==\u000a\u000aThe concept of information needs was coined by an American information gernalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (Now is Journal of the American Society of Information Science and Technology).\u000a\u000aIn this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.\u000a\u000aAccording to Taylor, information need has four levels:\u000a# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the \u201cideal question\u201d \u2014 the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information\u000a# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.\u000a# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer\u2019s doubts.\u000a# The question as presented to the information system.\u000a\u000aThere are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).\u000a\u000aHerbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:\u000a# When approached from the point of view of the scientist or technologists, these are studies of scientists\u2019 communication behaviour;\u000a# When approached from the point of view of any communication medium, they are use studies;\u000a# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.\u000a\u000aWilliam J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:\u000a# The scientist within his culture.\u000a# The scientist within a political system.\u000a# The scientist within a membership group.\u000a# The scientist within a reference group.\u000a# The scientist within an invisible college.\u000a# The scientist within a formal organization.\u000a# The scientist within a work team.\u000a# The scientist within his own head.\u000a# The scientist within a legal/economical system.\u000a# The scientist within a formal.\u000a\u000a==See also==\u000a* [[Information retrieval]]\u000a\u000a==References==\u000a* Menzel, Herbert. \u201cInformation Needs and Uses in Science and Technology.\u201d Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.\u000a* Paisley, William J. \u201cInformation Needs and Uses.\u201d Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.\u000a* Taylor, Robert S. \u201cThe Process of Asking Questions\u201d American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.\u000a* Wilson, T.D. \u201cOn User Studies and Information Needs.\u201d Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p36
asI31
(lp37
VTerm Discrimination
p38
aV'''Term Discrimination''' is a way to rank keywords in how useful they are for [[Information Retrieval]].\u000a\u000a== Overview ==\u000a\u000aThis is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.\u000a\u000aThis method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.\u000a\u000aAn optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.  \u000a\u000aThe discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.\u000a\u000a Let:\u000a <math>A</math> be the occurrence matrix\u000a <math>A_k</math> be the occurrence matrix without the index term <math>k</math>\u000a and <math>Q(A)</math> be density of <math>A</math>.\u000a Then:\u000a The discrimination value of the index term <math>k</math> is: \u000a <math>DV_k = Q(A) - Q(A_k)</math>\u000a\u000a== How to compute ==\u000a\u000aGiven an [[occurrency matrix]]: <math>A</math> and one keyword: <math>k</math>\u000a* Find the global document [[centroid]]: <math>C</math> (this is just the average document vector)\u000a* Find the average [[euclidean distance]] from every document vector, <math>D_i</math> to <math>C</math>\u000a* Find the average euclidean distance from every document vector, <math>D_i</math> to <math>C</math> ''IGNORING'' <math>k</math>\u000a* The difference between the two values in the above step is the ''discrimination value'' for keyword <math>K</math>\u000a\u000aA higher value is better because including the keyword will result in better information retrieval.\u000a\u000a== Qualitative Observations ==\u000aKeywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''\u000awhereas\u000akeywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''\u000a\u000a== References ==\u000a* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613\u2013620. ''(The article in which the vector space model was first presented)''\u000a\u000a* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.\u000a\u000a[[Category:Information retrieval]]
p39
asI39
(lp40
VFigaro Systems
p41
aV{{Infobox company\u000a|name = Figaro Systems, Inc.\u000a|logo = [[Image:Figaro-logo.png|Figaro logo]]\u000a|type = [[Privately held company|Private]]\u000a|foundation = 1993\u000a|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]\u000a|location_country =[[United States]]\u000a|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]\u000a|homepage = [http://www.figarosystems.com figarosystems.com]\u000a}}\u000a\u000a'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 <ref>Andrew Webb, \u201cOpera Subtitle Firm Eyes New Game,\u201d ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]</ref>\u000aby Patrick Markle, [[Geoff Webb]], and Ron Erkman  <ref name="figaro-systems.com"/> and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.\u000a<ref>[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, \u201cNothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,\u201d] ''Albuquerque Journal'', June 4, 2006</ref>\u000a\u000a==History==\u000aFigaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.<ref>[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005</ref>\u000a\u000aThe [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.<ref name="figaro-systems.com">[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]</ref> Markle, Webb, and Erkman were further reinforced by their understanding of technology\u2019s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.<ref>[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf \u201c[[User-friendly]] art: In-seat text displays that subtitle and translate\u201d, ''Auditoria'', May 2007]</ref> Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.\u000a<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=11&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]</ref><ref>[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]</ref>\u000a\u000aPhilanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.<ref>[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]</ref><ref>[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']</ref>  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.\u000a\u000aIn 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." <ref>[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com</ref>\u000a\u000a==Products and technology==\u000aThe company\u2019s products are known variously as seat back titles, [[surtitles]],\u000a<ref>[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/</ref> [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.\u000a\u000aOpera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]\u000a<ref>[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov</ref> although the software enables the reading of the libretto in any [[written language]].\u000a<ref name="entertanmentengineering.com">[http://www.entertanmentengineering.com/v4.issue04/page.06.html  \u201cGiving the Opera a New Voice,\u201d] ''Entertainment Engineering," Volume 4, Issue 2, p. 6</ref> Translation is provided by one screen and delivery system per person.<ref>[http://www.figarosystems.com  Figaro Systems Official Website]</ref>\u000a\u000aTypically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues\u2019 existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.<ref name="entertanmentengineering.com"/>\u000a\u000aThe company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens \u000a<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database</ref> for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.\u000a\u000a==Venues==\u000aIn the US, the company\u2019s systems are in use in the [[Ellie Caulkins Opera House]] \u000a<ref>[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,</ref> in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,<ref>[http://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],</ref> the [[Brooklyn Academy of Music]]<ref>[http://www.appliancemagazine.com/editorial.php?article=1768&zone=210&first=1  \u201cAn Operatic Performance,\u201d ''Appliance Magazine'', June 2007],</ref> the [[Metropolitan Opera]], New York, where it is called "MetTitles"),<ref>[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],</ref> the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.<ref name="figaro-systems.com"/>\u000a\u000aIn the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].\u000a<ref>[http://www.entertainmentengineering.com/v4.issue04/page.06.html \u201cGiving the Opera a New Voice,\u201d ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com</ref>\u000a\u000a==Awards==\u000aIn 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories\u2019 Technology Commercialization Award for its Simultext system.<ref>[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.</ref>\u000aIn 2008, the company\u2019s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies based in New Mexico]]\u000a[[Category:Assistive technology]]\u000a[[Category:Educational technology]]\u000a[[Category:Companies based in New Mexico]]\u000a[[Category:Privately held companies based in New Mexico]]\u000a[[Category:Companies established in 1993]]
p42
asI41
(lp43
VScientific data archiving
p44
aV'''Scientific data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of [[scientific data]] and methods. The various scientific journals have differing policies regarding how much of their data and methods scientists are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.\u000a\u000a[[Data archiving]] is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.\u000a\u000aThe requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].<ref>\u201dPolicy on Referencing Data in and Archiving Data for AGU Publications\u201d [http://www.agu.org/pubs/authors/policies/data_policy.shtml]</ref> This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.\u000a\u000aPrior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The science community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.\u000a\u000aThe need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.<ref>"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]</ref><ref>[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]</ref>\u000a\u000a==Selected policies by journals==\u000a\u000a===The American Naturalist===\u000a{{quote|[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR<ref>[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]</ref>}}\u000a\u000a===Journal of Heredity===\u000a{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.\u000a\u000aThe American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org<ref>[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]</ref>}}\u000a\u000a===Molecular Ecology===\u000a{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley<ref>[http://www.wiley.com/bw/submit.asp?ref=0962-1083&site=1 Policy on data archiving]</ref>}}\u000a\u000a===Nature===\u000a{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.<ref>[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]</ref>\u000a\u000a''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]<ref>{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}</ref>}}\u000a\u000a===''Science''===\u000a{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.<ref>[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]</ref>\u000a\u000a"Materials and methods" \u2013 ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]<ref>[http://www.sciencemag.org/about/authors/prep/prep_online.dtl \u201dPreparing Your Supporting Online Material\u201d]</ref>}}\u000a\u000a== Royal Society Publishing==\u000a{{quote|As a condition of acceptance authors agree to honour any reasonable request by other researchers for materials, methods, or data necessary to verify the conclusion of the article. Supplementary data up to 10Mb is placed on the Society's website free of charge and is publicly accessible. Large datasets must be deposited in a recognised public domain database by the author prior to submission. The accession number should be provided for inclusion in the published article.|{{citation needed |date=September 2013}}}}\u000a\u000a==Policies by funding agencies==\u000aIn the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.<ref>[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html \u201dNSF to Ask Every Grant Applicant for Data Management Plan\u201d]</ref>\u000a\u000aThe NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.\u000a\u000a==Data archives==\u000aThe following list refers to scientific data archives. See [[Data archive]] for social science archives. \u000a* [[CISL Research Data Archive]]\u000a* [[Dryad (repository)|Dryad]]\u000a* [[ESO/ST-ECF Science Archive Facility]]\u000a* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]\u000a* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]\u000a* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]\u000a* [[National Archive of Computerized Data on Aging]]\u000a* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]\u000a* [[National Climatic Data Center]]\u000a* [[National Geophysical Data Center]]\u000a* [[National Snow and Ice Data Center]]\u000a* [[National Oceanographic Data Center]]\u000a* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]\u000a* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth & Environmental Science]]\u000a* [[World Data Center]]\u000a* [[DataONE]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]\u000a* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]\u000a* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]\u000a* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]\u000a* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]\u000a* Data sharing and replication \u2013 Gary King website [http://gking.harvard.edu/replication.shtml]\u000a* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]\u000a* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]\u000a* \u201cHow to encourage the right behaviour\u201d An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]\u000a* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]\u000a* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]\u000a* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]
p45
asI45
(lp46
VExpertise finding
p47
aV{{cleanup|date=November 2010}}{{External links|date=January 2012}}\u000a\u000a'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.\u000a\u000a== Importance of expertise ==\u000a\u000aIt can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and \u201clicensing\u201d expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.\u000a\u000aUntil very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one\u2019s judgment about those individuals is justified and that their answers are thoughtful.\u000a\u000aIn the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed \u201cexpertise locating systems\u201d.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|\u201crecommender systems\u201d]].\u000a\u000aAt the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.\u000a\u000aStill other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed \u201cgated objects\u201d, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.\u000a\u000aExamples of the systems outlined above are listed in Table 1.\u000a\u000a'''Table 1: A classification of expertise location systems'''\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Type\u000a! Application domain\u000a! Data source\u000a! Examples\u000a|-\u000a| Social networking\u000a| Professional networking\u000a| User-generated\u000a|\u000a* [[LinkedIn]]\u000a|-\u000a| [[Scientific literature]]\u000a| Identifying publications with strongest research impact\u000a| Third-party generated\u000a|\u000a* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]\u000a|-\u000a| [[Scientific literature]]\u000a| Expertise search\u000a| Software\u000a|\u000a* [[Arnetminer]][http://arnetminer.org]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| User-Generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)\u000a* [[Tacit Software]] (Oracle Corporation)\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| User-generated\u000a|\u000a* [[Community of Science]] Expertise [http://expertise.cos.com]\u000a* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| Third party-generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* MindServer Expertise ([[Recommind]], Inc.)\u000a* Tacit Software\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| Third party-generated\u000a|\u000a* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)\u000a* [http://authoratory.com/ authoratory.com]\u000a* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)\u000a* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)\u000a* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)\u000a* [http://researchcrossroads.org ResearchCrossroads.org] (Innolyst, Inc.)\u000a|-\u000a| Blog [[search engine]]s\u000a|\u000a| Third party-generated\u000a|\u000a* [[Technorati]] [http://technorati.com/]\u000a|}\u000a\u000a== Technical problems ==\u000aA number of interesting problems follow from the use of expertise finding systems:\u000a\u000a* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.\u000a* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).\u000a* Finding ways to avoid \u201cgaming\u201d of the system to reap unjustified expertise [[credibility]].\u000a\u000a== Expertise ranking ==\u000a\u000aMeans of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:\u000a\u000a* How can expertise be assessed objectively? Is that even possible?\u000a* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?\u000a* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?\u000a* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?\u000a\u000a== Sources of data for assessing expertise ==\u000aMany types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.\u000a\u000aUnfiltered data sources that have been used to assess expertise, in no particular ranking order:\u000a\u000a* user recommendations\u000a* help desk tickets: what the problem was and who fixed it\u000a* e-mail traffic between users\u000a* documents, whether private or on the web, particularly publications\u000a* user-maintained web pages\u000a* reports (technical, marketing, etc.)\u000a\u000aFiltered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:\u000a\u000a* [[patent]]s, particularly if issued\u000a* scientific publications\u000a* issued grants (failed grant proposals are rarely know beyond the authors)\u000a* [[clinical trial]]s\u000a* product launches\u000a* pharmaceutical drugs\u000a\u000a== Approaches for creating expertise content ==\u000a* Manual, either by experts themselves (e.g., LinkedIn) or by a curator\u000a* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard])\u000a\u000a== Interesting expertise systems over the years ==\u000aIn no particular order...\u000a\u000a* Autonomy's IDOL\u000a* AskMe\u000a* Tacit Knowledge Systems' ActiveNet\u000a* Triviumsoft's SEE-K\u000a* MIT\u2019s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)\u000a* MITRE\u2019s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]\u000a* MITRE\u2019s XpertNet\u000a* Arnetminer (ref 2)\u000a* Dataware II Knowledge Directory\u000a* Thomson\u2019s tool\u000a* Hewlett-Packard\u2019s CONNEX\u000a* Microsoft\u2019s SPUD project\u000a* [http://www.xperscore.com Xperscore]\u000a* [http://intunex.fi/skillhive/ Skillhive]\u000a\u000a== Conferences ==\u000a# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]\u000a\u000a== References ==\u000a\u000a# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.\u000a# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.\u000a# Maybury, M., D\u2019Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.\u000a# Maybury, M., D\u2019Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.\u000a# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.\u000a# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.\u000a# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.\u000a\u000a[[Category:Evaluation methods]]\u000a[[Category:Metrics]]\u000a[[Category:Analysis]]\u000a[[Category:Impact assessment]]\u000a[[Category:Intellectual works]]\u000a[[Category:Knowledge sharing]]\u000a[[Category:Library science]]\u000a[[Category:Information retrieval]]\u000a[[Category:Science studies]]
p48
asI46
(lp49
VCategory:Sound production technology
p50
aV{{Commons category|Sound production technology}}\u000a[[Category:Audio electronics|Production]]\u000a[[Category:Information retrieval]]\u000a[[Category:Sound production|Technology]]
p51
asI47
(lp52
VCranfield Experiments
p53
aVThe '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. \u000a</ref> \u000a\u000aThey represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).\u000a\u000a==See also==\u000a*[[ASLIB]]\u000a*[[Information history]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Experiments]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{database-stub}}
p54
asI48
(lp55
VCommunication engine
p56
aV{{Orphan|date=February 2009}}\u000aA '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computing terminology]]\u000a\u000a\u000a{{Web-stub}}
p57
asI49
(lp58
VSubject indexing
p59
aV'''Subject indexing''' is the act of describing or [[Document classification|classifying]] a [[document]] by [[keyword (search)|index terms]] or other symbols in order to indicate what the document is '''[[Aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]].  In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents.  Indexes are constructed, separately, on three distinct levels:  terms in a document such as a book;  objects in a collection such as a library;  and documents (such as books and articles) within a field of knowledge.\u000a\u000aSubject indexing is used in [[information retrieval]] especially to create [[bibliographic database]]s to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.\u000a\u000aThe process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].<ref name="Lancaster2003a">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6</ref> The terms in the index are then presented in a systematic order.\u000a\u000aIndexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.\u000a\u000a== Subject analysis ==\u000aThe first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".<ref name="Chowdhury2004">G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71</ref> As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyse the content differently and so come up with different index terms. This will impact on the success of retrieval.\u000a\u000a=== Automatic vs. manual subject analysis ===\u000aAutomatic indexing follows set processes of analysing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analysing the full text in depth is costly and time consuming <ref name="Lancaster2003b">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 24</ref> An automated system takes away the time limit and allows the entire document to be analysed, but also has the option to be directed to particular parts of the document.\u000a\u000a== Term selection ==\u000aThe second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular.  Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval.  These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]].  The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials.  With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.<ref name="Voss2007">\u000a{{cite conference\u000a  | first= Jakob | last = Voss\u000a  | title = Tagging, Folksonomy & Co - Renaissance of Manual Indexing?\u000a  | booktitle = Proceedings of the International Symposium of Information Science\u000a  | pages = 234\u2013254\u000a  | year = 2007\u000a  | arxiv = cs/0701072\u000a}}</ref>\u000a\u000aOne application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.\u000a\u000a=== Extraction/Derived indexing ===\u000aExtraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words such as the, and would be referred to and such [[stop words]] would be excluded as index terms. Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases.\u000aAutomated extraction indexing also has the problem that even with use of a stop-list to remove common words such as \u201cthe,\u201d some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques  would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded. Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.<ref name="Lamb2008">J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.</ref>\u000a\u000a=== Assignment indexing ===\u000aAn alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.<ref name="Tenopir">C. Tenopir (1999): "Human or automated, indexing is important". ''Library Journal'' '''124'''(18) pages 34-38.</ref> It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.<ref name="Chowdhury2004" />\u000a\u000a== Index presentation ==\u000aThe final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination <ref name="Bodoff1998">D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.</ref>\u000a\u000a== Depth of Indexing ==\u000aIndexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity <ref name="Cleveland2001">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105</ref>\u000a\u000a=== Exhaustivity ===\u000aAn exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.<ref name="Weinberg1999">B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". ''Key Words'', '''7'''(5), pages 1+.</ref> Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.\u000a\u000a=== Specificity ===\u000aThe specificity describes how closely the index terms match the topics they represent <ref name="Anderson1997">J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.</ref> An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.<ref name="Cleveland2001b">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106</ref> Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.\u000a\u000a==Indexing theory==\u000a[[Birger Hjørland|Hjørland]] (2011)<ref>Hjørland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.</ref> found that theories of indexing is at the deepest level connected to different theories of knowledge:\u000a\u000a'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques.  '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of \u201crelevant\u201d documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hjørland, 1997)<ref>Hjørland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.</ref> is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by Ørom (2003)<ref>Ørom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.</ref> and in music by Abrahamsen (2003).<ref>Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169. \u000a</ref>\u000a\u000aThe core of indexing is, as stated by Rowley & Farrow<ref name=rowley2000>Rowley, J. E. & Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company</ref> to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjørland (1992,<ref>Hjørland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF</ref> 1997) to index its informative potentials.\u000a\u000a"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject  and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley & Farrow, 2000,<ref name=rowley2000/> p.&nbsp;99).\u000a\u000a== See also ==\u000a* [[Indexing and abstracting service]]\u000a* [[Document classification]]\u000a* [[Metadata]]\u000a* [[Overcategorization]]\u000a* [[Thomas of Ireland]], a medieval pioneer in subject indexing\u000a\u000a== References ==\u000a<references/>\u000a*{{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}\u000a*{{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81\u2013101|doi=10.1108/eb026855}}\u000a\u000a[[Category:Library science]]\u000a[[Category:Information science]]\u000a[[Category:Information retrieval]]
p60
asI50
(lp61
VRelevance feedback
p62
aV'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.\u000a\u000a== Explicit feedback ==\u000a\u000aExplicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.\u000a\u000aUsers may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.\u000a\u000aThe relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000aA performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].\u000a\u000a== Implicit feedback ==\u000a\u000aImplicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf].\u000a\u000aThe key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:\u000a\u000a# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and\u000a# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback\u000a\u000aAn example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.\u000a\u000a== Blind feedback ==\u000a\u000aPseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:\u000a\u000a# Take the results returned by initial query as relevant results (only top k with k being between 10 to 50 in most experiments).\u000a# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.\u000a# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.\u000a\u000aSome experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.\u000a\u000aThis automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> \u000aSpecifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.\u000a\u000aBlind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.\u000a\u000a== Using relevance information ==\u000a\u000aRelevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000a==Further reading==\u000a*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's\u000a*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''\u000a*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a[[Category:Information retrieval]]\u000a\u000a[[zh:\u76f8\u5173\u53cd\u9988]]
p63
asI51
(lp64
VIndex term
p65
aVAn '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts also. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.\u000a\u000aKeywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it is inefficient to do so. Almost every English-language site on the Internet has the article "''the''", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.\u000a\u000aThe term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[thesaurus]]. \u000a\u000aThe [[Simple Knowledge Organisation System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].\u000a\u000a==Author keywords==\u000aMany journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.\u000a\u000a==Examples==\u000a*[[Canadian Subject Headings]] (CSH)\u000a*[[Library of Congress Subject Headings]] (LCSH)\u000a*[[Medical Subject Headings]] (MeSH)\u000a*[[Polythematic Structured Subject Heading System]] (PSH)\u000a\u000a==See also==\u000a*[[Dynamic keyword insertion]]\u000a<!-- *Key-objects -->\u000a*[[Keyword cloud]]\u000a*[[Keyword density]]\u000a*[[Keyword optimization]]\u000a*[[knowledge tags|Keyword tagging]]\u000a*[[Subject (documents)]]\u000a\u000a== References ==\u000a{{commonscat|Information retrieval}}\u000a*{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Library-stub}}
p66
asI53
(lp67
VNegative search
p68
aV{{Multiple issues|\u000a{{unreferenced|date=March 2009}}\u000a{{orphan|date=February 2009}}\u000a{{confusing|date=March 2009}}\u000a}}\u000a\u000a'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.\u000a\u000aNegative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.\u000a\u000aNegative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.\u000a\u000aNegative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.\u000a\u000aExamples of Negative Intent are:\u000a\u000a- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.\u000a\u000a- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.\u000a\u000a- An investigator is looking for a car but has no other information on that car on which to base a search.\u000a\u000a==Negative Search Classifiers==\u000a\u000aIf there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.\u000a\u000a[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.\u000a\u000a==Irrelevancy as a Desirable Construct==\u000a\u000aPositive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.\u000a\u000aIt follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.\u000a\u000a==Degrees of Passivity==\u000a\u000aPositive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\u005c's Eve|New Years Eve]]."\u000a\u000aDiscovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"\u000a\u000aNegative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."\u000a\u000aSearchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]
p69
asI56
(lp70
VEnterprise search
p71
aV'''Definition:''' '''Enterprise search''' is the organized retrieval of '''structured''' and '''unstructured''' data within an organization. \u000a\u000a\u000a'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.\u000a\u000a==Enterprise search summary==\u000a"Enterprise Search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).<ref>[http://www.aiim.org/What-is-Enterprise-Search What is Enterprise Search?]</ref> Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.\u000a\u000aEnterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.<ref>[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]</ref> Enterprise search systems also use access controls to enforce a security policy on their users.<ref>[http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search - Part 1: Defining Specific Security Requirements]</ref>\u000a\u000aEnterprise search can be seen as a type of [[vertical search]] of an enterprise.\u000a\u000a==Components of an enterprise search system==\u000aIn an enterprise search system, content goes through various phases from source repository to search results:\u000a\u000a=== Content awareness ===\u000aContent awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.<ref>[http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html Understanding Content Collection and Indexing]</ref>\u000a\u000a=== Content processing and analysis ===\u000aContent from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.\u000a\u000aAs part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.<ref>[http://packages.python.org/Whoosh/stemming.html Stemming, Variations, and Accent Folding]</ref>\u000a\u000a=== Indexing ===\u000aThe resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].\u000a\u000a=== Query Processing ===\u000aUsing a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.\u000a\u000a=== Matching ===\u000aThe processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.\u000a\u000a==Differences from web search==\u000aBeyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:\u000a*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].\u000a*[[Federated search]], which consists of\u000a# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,\u000a# merging the results collected from the databases,\u000a# presenting them in a succinct and unified format with minimal duplication, and\u000a# providing a means, performed either automatically or by the portal user, to sort the merged result set.\u000a*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.\u000a*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\u000a*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.\u000a*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).\u000a*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.\u000a*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.\u000a\u000a==Relevance factors for enterprise search==\u000aThe factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.\u000a\u000a==Access Control - early binding vs late binding==\u000aSecurity and restricted access to documents is an important matter in Enteprise Search. There are two main approaches to apply restricted access: early binding vs late binding.<ref>[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]</ref>\u000a\u000a===Late binding===\u000aPermissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).\u000a\u000a===Early binding===\u000aPermissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).\u000a\u000a==Search Relevance Testing options==\u000aSearch application relevance can be determined by following relevance testing options like<ref>[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]</ref>\u000a*Focus groups\u000a*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)\u000a*Empirical testing\u000a*[[A/B testing]]\u000a*Log analysis on a Beta production site\u000a*Online ratings\u000a\u000a==See also==\u000a*[[Comparison of enterprise search software]]\u000a*[[List of enterprise search vendors]]\u000a*[[List of Search Engines]]\u000a*[[Collaborative search engine]]\u000a*[[Data Defined Storage]] \u000a*[[Enterprise bookmarking]]\u000a*[[Enterprise information access]]\u000a*[[Knowledge management]]\u000a*[[Text mining]]\u000a*[[Faceted search]]\u000a*[[Information Extraction]]\u000a*[[Vertical search|Vertical Search]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a{{DEFAULTSORT:Enterprise Search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p72
asI57
(lp73
VClairlib
p74
aV{{Multiple issues|\u000a{{unreferenced|date=May 2009}}\u000a{{expert-subject|date=May 2009}}\u000a{{orphan|date=February 2011}}\u000a}}\u000a\u000a{{Infobox software\u000a|name                       = Clairlib\u000a|logo                       = [[Image:Clair logo.jpg]]\u000a|screenshot                 = \u000a|caption                    = \u000a|collapsible                = \u000a|author                     = \u000a|developer                  = CLAIR [[University of Michigan]]\u000a|released                   = \u000a|latest release version     = 1.0.8\u000a|latest release date        = {{release date and age|2009|08|1}}\u000a|latest preview version     = \u000a|latest preview date        = \u000a|frequently updated         = yes\u000a|programming language       = [[Perl]]\u000a|operating system           = \u000a|platform                   = Cross-platform\u000a|size                       = \u000a|language                   = Perl\u000a|status                     = Active\u000a|genre                      = [[Natural Language Processing]], [[Network theory|Network Analysis]], [[Information Retrieval]]\u000a|license                    = [[GNU General Public License]], [[Artistic License]]\u000a|website                    = [http://www.clairlib.org/ www.clairlib.org]\u000a}}\u000a'''Clairlib''' is a suite of open-source [[Perl]] modules developed and maintained by the Computational Linguistics And Information Retrieval (CLAIR) group at the [[University of Michigan]]. Clairlib is intended to simplify a number of generic tasks in [[natural language processing]] (NLP), [[information retrieval]] (IR), and network analysis (NA). The latest version of clairlib is 1.06 which was released on March 2009 and includes about 130 modules implementing a wide range of functionalities.\u000a\u000a==Functionality==\u000a\u000aClairlib is distributed in two forms: Clairlib-core, which has essential functionality and minimal dependence on external software, and Clairlib-ext, which has extended functionality that may be of interest to a smaller audience. Much can be done using Clairlib on its own. Some of the things that Clairlib can do are: Tokenization, Summarization, Document Clustering, Document Indexing, Web Graph Analysis, Network Generation,  [[Power law distribution]] Analysis, [[Network theory|Network Analysis]], [[Random walk]]s on graphs, [[Tf-idf]], [[Perceptron]] learning  and classification, and [[Compound term processing|Phrase Based Retrieval]] and [[Fuzzy logic|Fuzzy OR Queries]].\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a*[http://www.clairlib.org Homepage]\u000a*[http://tangra.si.umich.edu/clair/ Computational Linguistics And Information Retrieval (CLAIR) group]\u000a\u000a[[Category:Free computer libraries]]\u000a[[Category:Perl modules]]\u000a[[Category:University of Michigan]]\u000a[[Category:Information retrieval]]
p75
asI64
(lp76
VGain (information retrieval)
p77
aV{{other uses2|Gain}}\u000a{{Unreferenced|date=August 2009}}\u000aThe '''gain''', also called '''improvement over random''' {{cn|date=March 2013}} can be specified for a [[classifier (mathematics)|classifier]] and is an important measure {{dubious|date=March 2013}} to describe the performance of it.\u000a\u000a== Definition ==\u000aIn the following a random classifier is defined such that it randomly predicts the same amount of either class.\u000a\u000aThe gain is defined as described in the following:\u000a\u000a=== Gain in Precision ===\u000a\u000aThe random [[positive predictive value|precision]] of a classifier is defined as\u000a\u000a<math>\u000ar = \u005cfrac{TP+FN}{TP+TN+FP+FN} = \u005cfrac{\u005ctextit{Positives}}{N}\u000a</math>\u000a\u000awhere TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives respectively, positives is the number of positive instances in the target dataset and N is the size of the dataset.\u000a\u000aThe random precision defines the lowest baseline of a classifier.\u000a\u000aAnd '''Gain''' is defined as \u000a\u000a<math>\u000aG = \u005cfrac{\u005ctextit{precision}}{r}\u000a</math>\u000a\u000awhich gives a factor by which a classifier is better when compared to its random counterpart. A Gain of 1 would indicate a classifier that is not better than random. The larger the gain, the better.\u000a\u000a=== Gain in Overall Accuracy ===\u000a\u000aThe [[accuracy]] of a classifier in general is defined as\u000a\u000a<math>\u000aAcc = \u005cfrac{TP+TN}{TP+TN+FP+FN} = \u005cfrac{\u005ctextit{Corrects}}{N}\u000a</math>\u000a\u000aHere, the random accuracy of a classifier can be defined as\u000a\u000a<math>\u000ar = \u005cleft ( \u005cfrac{\u005ctextit{Positives}}{N} \u005cright ) ^2+ \u005cleft ( \u005cfrac{\u005ctextit{Negatives}}{N} \u005cright ) ^2=f(\u005ctextit{Positives})^2 + f(\u005ctextit{Negatives})^2\u000a</math>\u000a\u000af(Positives) and f(Negatives) is the fraction of positive and negative classes in the dataset.\u000a\u000aAnd again '''gain''' is\u000a\u000a<math>\u000aG = \u005cfrac{\u005ctextit{Acc}}{r}\u000a</math>\u000a\u000aThis time the gain is measured not only with respect to the prediction of a so-called positive class, but with respect to the overall classifier ability to distinguish the two equally important classes.\u000a\u000a== Application ==\u000aIn [[Bioinformatics]] as an example, the gain is measured for methods that predict residue contacts in proteins.\u000a\u000a== See also ==\u000a* [[Accuracy and precision]]\u000a* [[Binary classification]]\u000a* [[Brier score]]\u000a* [[Confusion matrix]]\u000a* [[Detection theory]]\u000a* [[F-score]]\u000a* [[Information retrieval]]\u000a* [[Matthews correlation coefficient]]\u000a* [[Receiver operating characteristic]] or ROC curve\u000a* [[Selectivity (electronic)|Selectivity]]\u000a* [[Sensitivity and specificity]]\u000a* [[Sensitivity index]]\u000a* [[Statistical significance]]\u000a* [[Youden's J statistic]]\u000a\u000a{{DEFAULTSORT:Gain (Information Retrieval)}}\u000a[[Category:Logic]]\u000a[[Category:Information retrieval]]
p78
asI65
(lp79
VTeLQAS
p80
aV'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>\u000a\u000a==Architecture==\u000aTeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.\u000a\u000a==References==\u000a<references/>\u000a\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing software]]
p81
asI66
(lp82
VInstant indexing
p83
aV{{multiple issues|\u000a{{Orphan|date=February 2009}}\u000a{{Refimprove|article|date=November 2006}}\u000a}}\u000a\u000a'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].\u000a\u000a==Delayed inclusion==\u000aCertain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]]{{Citation needed|date=February 2007}}.\u000a\u000aDelayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index{{Citation needed|date=February 2007}}.\u000a\u000a==Criticisms==\u000aA criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}\u000a\u000aInstant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.\u000a\u000aSelect search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.\u000a\u000a==External links==\u000a* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html <!-- Bot retrieved archive --> |archivedate = 2006-04-27}}\u000a* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories \u2014 A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}\u000a\u000a== See also ==\u000a* [[Search engine]]\u000a* [[Search engine indexing]]\u000a* [[Web crawling]]\u000a\u000a[[Category:Internet terminology]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{website-stub}}
p84
asI69
(lp85
VCategory:Citation indices
p86
aV{{Cat main|Citation index}}\u000a{{cat see also|Bibliographic databases|Bibliographic indexes}}\u000a\u000a[[Category:Bibliometrics]]\u000a[[Category:Reference works]]\u000a[[Category:Indexes]]\u000a[[Category:Information retrieval]]\u000a[[Category:Bibliographic databases| ]]
p87
asI70
(lp88
VDtSearch
p89
aV{{Lowercase}}\u000a\u000a{{Infobox company |\u000a  name   = dtSearch Corp. |\u000a  slogan = "The Smart Choice for Text Retrieval since 1991" |\u000a  type   =  Private company |\u000a  foundation     = 1991 |\u000a  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]] |\u000a  key_people     = David Thede, President |\u000a  industry       = [[Software]] |\u000a\u000a  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]\u000a}}\u000a\u000a'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.\u000a\u000a==History==\u000adtSearch Corp was founded by David Thede<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm; Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[AskSAM]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.\u000a\u000aIn the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft\u2019s initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]</ref>\u000a\u000aIn 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>\u000a\u000a==Products==\u000aThe current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.\u000a\u000a*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)\u000a*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)\u000a*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)\u000a*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)\u000a*dtSearch Engine for Linux - SDK with C++ and Java APIs\u000a*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\u005cDVD publishing (32 and 64 bit indexers)\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[List of enterprise search vendors]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.dtsearch.com/ Company Website]\u000a*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]\u000a*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]\u000a*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]\u000a*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan\u2013Feb]\u000a*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233\u20131246] \u000a*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]\u000a\u000a{{DEFAULTSORT:Dtsearch Corp.}}\u000a[[Category:Desktop search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies based in Maryland]]
p90
asI77
(lp91
VTaganode Local Search Engine
p92
aV'''The Open Local Search Engine from Taganode''' is a [[search engine]] specifically targeting [[mobile phone]]s. It is based on local [[search algorithm]]s to find new places of interest within a specified distance.\u000a\u000aThe Taganode search engine offers an Open Developer [[Application programming interface|API]] that any one can use freely when writing new applications for [[iPhone]]s, [[Android (operating system)|Android phones]] and other platforms.\u000a\u000aThe search engine is optimized for mobile phones by low [[Bandwidth (computing)|bandwidth]] usage and only makes the simplest service calls to try to be compatible with as many mobile phones as possible. The Taganode search service is at this moment present in [[London]], [[Rome]], [[Venice]], [[Amsterdam]], [[Berlin]], [[Sweden]] and in [[Denmark]].\u000a\u000a== References ==\u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/164362/taganode-gratis-reseguide-finns-nu-aven-i-venedig/|title=Taganode \u2013 Gratis reseguide finns nu även i Venedig |date=2009-10-12|work=Webfinanser|language=Swedish|accessdate=18 December 2009}}\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/159761/en-ny-och-innovativ-soktjanst-for-resenarer-i-europa/|title=En ny och innovativ söktjänst för resenärer i Europa|date=September 19, 2009|work=Webfinanser |language=Swedish|accessdate=18 December 2009}}\u000a\u000a== External links ==\u000a* [http://www.taganode.com Official site]\u000a* [http://www.mynewsdesk.com/se/view/pressrelease/taganode-free-guide-now-available-in-rome-325701/  Taganode Service in Venice] (press release)\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Mobile phones]]
p93
asI83
(lp94
VChampion list
p95
aV{{orphan|date=January 2011}}\u000a\u000aA '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].\u000a\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{computing-stub}}
p96
asI84
(lp97
VQueries per second
p98
aV{{refimprove|date=February 2010}}\u000a'''Queries Per Second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.<ref>[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]</ref><ref>[http://www.answers.com/topic/qps QPS definition at answers.com]</ref>\u000a\u000aHigh-traffic systems must watch their QPS in order to know when to scale the system to handle more load.\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Units of measurement]]\u000a[[Category:Information retrieval]]\u000a\u000a{{computer-stub}}
p99
asI85
(lp100
VDatabase search engine
p101
aVThere are several categories of search engine software:  Web search or full-text search (example: [[Lucene]]), database or structured data search (example: [[Dieselpoint]]), and mixed or [[enterprise search]] (example: [[Google Search Appliance]]).  The largest web search engines such as [[Google]] and [[Yahoo!]] utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:\u000a\u000aSearching for text-based content in [[databases]] or other [[structured data]] formats ([[XML]], [[Comma-separated values|CSV]], etc.) presents some special challenges and opportunities which a number of specialized search engines resolve.  Databases are slow when solving complex queries (with multiple logical or [[string matching]] arguments.  Databases allow logical queries which full-text search doesn't (use of multi-field boolean logic for instance).  There is no crawling necessary for a database since the data is already structured but it is often necessary to index the data in a more compact form designed to allow for faster search.\u000a\u000aDatabase search engines were initially (and still usually are) included with major database software products.  As such, they are usually called indexing engines.  However, these indexing engines are relatively limited in their ability to customize indexing formats (compounding, normalization, transformation, [[transliteration]], etc.)   Usually they do not provide sophisticated data matching technology ([[string matching]], [[boolean logic]], algorithmic methods, search scripting, etc.).\u000a\u000aIn more advanced Database search systems relational databases are indexed by compounding multiple tables into a single table containing only the fields that need to be queried (or displayed in search results).  The actual data matching engines can include any functions from basic string matching, normalization, transformation,  Database search technology is heavily used by government database services, e-commerce companies, web advertising platforms, telecommunications service providers, etc.\u000a\u000a==See also==\u000a\u000a*[[Search engine]]\u000a*[[Web crawler]]\u000a*[[Search engine indexing]]\u000a*[[Enterprise search]]\u000a\u000a==External links==\u000a* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\u000a\u000a{{DEFAULTSORT:Search Engine Technology}}\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p102
asI90
(lp103
VFederated search
p104
aV{{Citations missing|date=June 2008}}\u000a\u000a'''Federated search''' is an [[information retrieval]] technology that allows the simultaneous search of multiple searchable resources.  A user makes a single query request which is distributed to the [[search engine]]s participating in the federation.  The federated search then aggregates the results that are received from the [[search engine]]s for presentation to the user.\u000a\u000a==Purpose==\u000aFederated search came about to meet the need of searching multiple  disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user.\u000a\u000a==Process==\u000aAs described by Peter Jacso (2004<ref>Thoughts About Federated Searching.  Jacsó, Péter, Information Today,  Oct 2004, Vol. 21, Issue 9</ref>), federated searching consists of (1) transforming a [[Web search query|query]] and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set.\u000a\u000aFederated search portals, either commercial or open access, generally search public access [[bibliographic databases]], public access Web-based library catalogues ([[OPAC]]s), Web-based search engines like [[Google]] and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely [[screen scrape]] the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources.\u000a\u000aThis process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time.\u000a\u000a==Implementation==\u000a[[File:Fed_search.png|thumb|alt=federated search engine|Federating across three search engines]]\u000a\u000aOne application of federated searching is the [[metasearch engine]]; however, this is not a complete solution as many documents are not currently indexed. These documents are on what is known as the [[deep Web]], or invisible Web. Many more information sources are not yet stored in electronic form. [[Google Scholar]] is one example of many projects trying to address this.\u000a\u000aWhen the search vocabulary or [[data model]] of the search system is different from the data model of one or more of the foreign target systems the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require [[semantic translation]].\u000a\u000aA challenge faced in the implementation of federated search engines is scalability, in other words, the performance of the site as the number of information sources comprising the federated search engine increase. One federated search engine that has begun to address this issue is [[WorldWideScience]], hosted by the [[U.S. Department of Energy]]'s [[Office of Scientific and Technical Information]].  WorldWideScience <ref>[http://www.worldwidescience.org WorldWideScience]</ref> is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov <ref name="Science.gov">[http://www.science.gov Science.gov]</ref> which itself federates more than 30 information sources representing most of the R&D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience.<ref name="Science.gov"/> This approach of cascaded federated search enables large number of information sources to be searched via a single query.\u000a\u000aAnother application [[Sesam]] running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat,<ref>[http://sesat.no Sesat]</ref> an acronym for [[Sesam Search Application Toolkit]], is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning.\u000a\u000a==Challenges==\u000a\u000aWhen federated search is performed against secure data sources, the users' credentials must be passed on\u000ato each underlying search engine, so that appropriate security is maintained.  If the user has different\u000alogin credentials for different systems, there must be a means to map their login ID to each search\u000aengine's security domain.<ref>[http://www.ideaeng.com/tabId/98/itemId/124/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search]</ref>\u000a\u000aAnother challenge is mapping results list navigators into a common form.  Suppose 3 real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges.<ref>[http://www.ideaeng.com/tabId/98/itemId/154/20-Differences-Between-Internet-vs-Enterprise-Se.aspx#fed_facets 20+ Differences Between Internet vs. Enterprise Search - part 1]</ref>  The system also needs to understand "next page" links if it's going to allow the user to page through the combined results.\u000a\u000a==Further reading==\u000a*[http://www.libraryjournal.com/article/CA6571320.html Federated Search 101. Linoski, Alexis, Walczyk, Tine, Library Journal, Summer 2008 Net Connect, Vol. 133]{{Dead link|date=November 2010}} Note: this content has been moved [http://www.accessmylibrary.com/article-1G1-182034526/federated-search-101-alexis.html here], but you will need a remote access account through your local library to get the whole article.\u000a\u000a*Cox, Christopher N. Federated Search: Solution or Setback for Online Library Services. Binghamton, NY: Haworth Information Press, 2007.[http://lccn.loc.gov/2006101753 Table of Contents]\u000a*[http://www.altsearchengines.com/2009/01/11/federated-search-finds-content-that-google-cant-reach-part-i-of-iii/ Federated Search Primer. Lederman, S., AltSearchEngines, January 2009] {{Dead link|date=July 2010}} Note: This material has been reposted [http://deepwebtechblog.com/federated-search-finds-content-that-google-can%E2%80%99t-reach-part-i-of-iii/ here], on the blog of a commercial search engine company.\u000a\u000a* Milad Shokouhi and Luo Si, Federated Search, Foundations and Trends® in Information Retrieval: Vol. 5: No 1, pp 1-102., [http://dx.doi.org/10.1561/1500000010 http://dx.doi.org/10.1561/1500000010]\u000a\u000a==See also==\u000a* [[Search aggregator]]\u000a* [[Deep Web]]\u000a\u000a==References==\u000a{{Reflist}}\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Federated Search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]\u000a[[Category:Searching]]\u000a[[Category:Internet search algorithms]]\u000a[[Category:Applications of distributed computing]]
p105
asI92
(lp106
VEXCLAIM
p107
aV{{For|the Canadian magazine|Exclaim!}}\u000aThe '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' is an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006. It is currently in a beta stage of development, with some support for more than a dozen languages. The lead developers are Justin Nuger and Jesse Saba Kirchner.\u000a\u000aEarly work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>\u000a{{cite web\u000a|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web\u000a|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf\u000a|format=PDF|publisher=ACM-SIGIR 1999\u000a|accessdate=2006-12-02\u000a}}\u000a</ref>\u000a\u000aEXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).\u000a\u000aOne of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.\u000a\u000a==Current status==\u000a\u000aEXCLAIM is in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, is available for the following twenty-three languages:\u000a\u000a{| class="wikitable"\u000a| [[Albanian language|Albanian]]\u000a|-\u000a| [[Amharic]]\u000a|-\u000a| [[Bengali language|Bengali]]\u000a|-\u000a| [[Gothic language|Gothic]]\u000a|-\u000a| [[Greek language|Greek]]\u000a|-\u000a| [[Icelandic language|Icelandic]]\u000a|-\u000a| [[Indonesian language|Indonesian]]\u000a|-\u000a| [[Irish language|Irish]]\u000a|-\u000a| [[Javanese language|Javanese]]\u000a|-\u000a| [[Latvian language|Latvian]]\u000a|-\u000a| [[Malagasy language|Malagasy]]\u000a|-\u000a| [[Mandarin Chinese]]\u000a|-\u000a| [[Nahuatl]]\u000a|-\u000a| [[Navajo language|Navajo]]\u000a|-\u000a| [[Quechua languages|Quechua]]\u000a|-\u000a| [[Sardinian language|Sardinian]]\u000a|-\u000a| [[Swahili language|Swahili]]\u000a|-\u000a| [[Tagalog language|Tagalog]]\u000a|-\u000a| [[Standard Tibetan|Tibetan]]\u000a|-\u000a| [[Turkish language|Turkish]]\u000a|-\u000a| [[Welsh language|Welsh]]\u000a|-\u000a| [[Wolof language|Wolof]]\u000a|-\u000a| [[Yiddish]]\u000a|}\u000a\u000aSupport using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:\u000a\u000a{| class="wikitable"\u000a|-\u000a| [[Dutch language|Dutch]]\u000a|-\u000a| [[Spanish language|Spanish]]\u000a|}\u000a\u000aSignificant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.\u000a\u000aFuture versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.\u000a\u000aThe EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.\u000a\u000a==Further applications==\u000a\u000aEXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web\u000a|title=A crosslinguistic readability framework\u000a|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf\u000a|format=PDF|publisher=ACL-IJNLP 2009\u000a|accessdate=2009-09-04\u000a}}\u000a</ref>\u000a\u000a==Notes and references==\u000a\u000a{{reflist}}\u000a\u000a==External links==\u000a*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website]\u000a*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]\u000a*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]\u000a*[http://ju-st.in/ Justin Nuger's professional webpage]\u000a*[http://people.ucsc.edu/~kirchner/ Jesse Saba Kirchner's professional webpage]\u000a\u000a{{DEFAULTSORT:Exclaim}}\u000a[[Category:Information retrieval]]
p108
asI98
(lp109
VCluster labeling
p110
aVIn [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find labeling that summarize the topic of each cluster and distinguish the clusters from each other.\u000a\u000a==Differential cluster labeling==\u000aDifferential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>\u000a\u000a===Pointwise mutual information===\u000a\u000a{{Main|Pointwise mutual information}}\u000a\u000aIn the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:\u000a\u000a<math>I(X, Y) = \u005csum_{x\u005cin X}{ \u005csum_{y\u005cin Y} {p(x, y)log_2\u005cleft(\u005cfrac{p(x, y)}{p_1(x)p_2(y)}\u005cright)}}</math>\u000a\u000awhere ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p<sub>1</sub>(x)'' is the probability distribution of X, and ''p<sub>2</sub>(y)'' is the probability distribution of Y.\u000a\u000aIn the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>I(C, T) = \u005csum_{c\u005cin {0, 1}}{ \u005csum_{t\u005cin {0, 1}} {p(C = c, T = t)log_2\u005cleft(\u005cfrac{p(C = c, T = t)}{p(C = c)p(T = t)}\u005cright)}}</math>\u000a\u000aIn this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.\u000a\u000a===Chi-Squared Selection===\u000a{{Main|Pearson's chi-squared test}}\u000aThe Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin A}{\u005csum_{b \u005cin B}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000awhere ''O<sub>a,b</sub>'' is the ''observed'' frequency of a and b co-occurring, and ''E<sub>a,b</sub>'' is the ''expected'' frequency of co-occurrence.\u000a\u000aIn the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin {0,1}}{\u005csum_{b \u005cin {0,1}}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000aFor example, ''O<sub>1,0</sub>'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E<sub>1,0</sub>'' is the expected number of documents that are in a particular cluster but don't contain a certain term.\u000aOur initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>\u000a\u000a''E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)''\u000a\u000awhere N is the total number of documents in the collection.\u000a\u000a==Cluster-Internal Labeling==\u000aCluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.\u000aCluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.\u000a\u000a===Centroid Labels===\u000a{{Main|Vector space model}}\u000aA frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.\u000aOne downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.\u000a\u000a===Contextualized centroid labels===\u000aA simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection. <ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters\u2019 contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>\u000aIn this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)\u000aIn a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\u005ctilde{t}_{i}</math> and <math>\u005ctilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.\u000a\u000a===Title labels===\u000aAn alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.\u000a\u000a===External knowledge labels===\u000aCluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.\u000a\u000a==External links==\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]\u000a* [http://erulemaking.ucsur.pitt.edu/doc/papers/dgo06-labeling.pdf Automatically Labeling Hierarchical Clusters]\u000a\u000a==References==\u000a<references/>\u000a\u000a{{DEFAULTSORT:Cluster Labeling}}\u000a[[Category:Information retrieval]]
p111
asI102
(lp112
VCosine similarity
p113
aV'''Cosine similarity''' is a measure of similarity between two vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a Cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\u000a\u000aNote that these bounds apply for any number of dimensions, and Cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[Information Retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35\u201343.</ref>\u000a\u000aThe technique is also used to measure cohesion within [[cluster (computing)|cluster]]s in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>\u000a\u000a''Cosine distance'' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the triangle inequality property and it violates the coincidence axiom; to repair the triangle inequality property whilst maintaining the same ordering, it is necessary to convert to Angular distance (see below.)\u000a\u000aOne of the reasons for the popularity of Cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.\u000a\u000a==Definition==\u000a\u000aThe cosine of two vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:\u000a\u000a:<math>\u005cmathbf{a}\u005ccdot\u005cmathbf{b}\u000a=\u005cleft\u005c|\u005cmathbf{a}\u005cright\u005c|\u005cleft\u005c|\u005cmathbf{b}\u005cright\u005c|\u005ccos\u005ctheta</math>\u000a\u000aGiven two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(\u03b8)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vectors|magnitude]] as\u000a\u000a:<math>\u005ctext{similarity} = \u005ccos(\u005ctheta) = {A \u005ccdot B \u005cover \u005c|A\u005c| \u005c|B\u005c|} = \u005cfrac{ \u005csum\u005climits_{i=1}^{n}{A_i \u005ctimes B_i} }{ \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(A_i)^2}} \u005ctimes \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(B_i)^2}} }</math>\u000a\u000aThe resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.\u000a\u000aFor text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.\u000a\u000aIn the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90°.\u000a\u000aIf the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \u005cbar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson_product-moment_correlation_coefficient#For_a_sample|Pearson Correlation Coefficient]].\u000a\u000a=== Angular similarity ===\u000a\u000aThe term "cosine similarity" has also been used on occasion to express a different coefficient, although the most common use is as defined above. Using the same calculation of similarity, the normalised angle between the vectors can be used as a bounded similarity function within [0,1], calculated from the above definition of similarity by:\u000a:<math>1 - \u005cfrac{ \u005ccos^{-1}( \u005ctext{similarity} )}{ \u005cpi} </math>\u000ain a domain where vector coefficients may be positive or negative, or\u000a:<math>1 - \u005cfrac{ 2 \u005ccdot \u005ccos^{-1}( \u005ctext{similarity} ) }{ \u005cpi }</math>\u000ain a domain where the vector coefficients are always positive.  \u000a\u000aAlthough the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.\u000a\u000a=== Confusion with "Tanimoto" coefficient ===\u000a\u000aThe cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:\u000a\u000a:<math>T(A,B) = {A \u005ccdot B \u005cover \u005c|A\u005c|^2 +\u005c|B\u005c|^2 - A \u005ccdot B}</math>\u000a\u000aIn fact, this algebraic form [[Jaccard index#Tanimoto_Similarity_and_Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.\u000a\u000a=== Ochiai coefficient ===\u000aThis coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. \u2116 9. P. 526-530.</ref><ref>''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. \u2013 Assen. Van Gorcum. 1958. 628 p.</ref>\u000a:<math>K =\u005cfrac{n(A \u005ccap B)}{\u005csqrt{n(A) \u005ctimes n(B)}}</math>\u000aHere, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.\u000a\u000a== Properties ==\u000aCosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\u005c|A - B\u005c|</math>, and observe that\u000a\u000a:<math>\u005c|A - B\u005c|^2 = (A - B)^\u005ctop (A - B) = \u005c|A\u005c|^2 + \u005c|B\u005c|^2 - 2 A^\u005ctop B</math>\u000a\u000aby [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\u005c|A\u005c|^2 = \u005c|B\u005c|^2 = 1</math> so the previous is equal to\u000a\u000a:<math>2 (1 - \u005ccos(A, B))</math>\u000a\u000a'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal\u000a | author = Spruill, Marcus C\u000a | year = 2007\u000a | title = Asymptotic distribution of coordinates on high dimensional spheres\u000a | journal = Electronic communications in probability\u000a | volume = 12 | pages = 234-247\u000a | doi = 10.1214/ECP.v12-1294\u000a}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>\u000aFor other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal\u000a | author = Graham L. Giller \u000a | year = 2012\u000a | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity\u000a | journal = Giller Investments Research Notes\u000a | number = 20121024/1\u000a | doi = 10.2139/ssrn.2167044\u000a}}</ref>\u000a\u000a== Soft Cosine Measure ==\u000a'''Soft cosine measure''' <ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491\u2013504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref>\u000ais a measure of \u201csoft\u201d similarity between two vectors, i.e., the measure that considers similarity of pairs of features. The traditional '''cosine similarity''' considers the [[vector space model]] (VSM) features as independent or completely different, while the '''soft cosine measure''' proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).\u000a\u000aFor example, in the field of [[natural language processing]] (NLP) the similarity between features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1\u201311|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words \u201cplay\u201d and \u201cgame\u201d are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).\u000a\u000aFor calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.  \u000a\u000aGiven two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:\u000a\u000a:<math>\u005cbegin{align}\u000a    \u005coperatorname{soft\u005c_cosine}_1(a,b)=\u000a    \u005cfrac{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ib_j}{\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ia_j}\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}b_ib_j}},\u000a\u005cend{align}\u000a</math>\u000a\u000awhere {{math|''s<sub>ij</sub>'' {{=}} similarity(feature<sub>''i''</sub>, feature<sub>''j''</sub>)}}.\u000a\u000aIf there is no similarity between features ({{math|''s<sub>ii</sub>'' {{=}} 1}}, {{math|''s<sub>ij</sub>'' {{=}} 0}} for {{math|''i'' \u2260 ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.\u000a\u000aThe complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be even transformed to linear.\u000a\u000a== See also ==\u000a* [[Sørensen similarity index|Sørensen's quotient of similarity]]\u000a* [[Hamming distance]]\u000a* [[Correlation]]\u000a* [[Dice's coefficient]]\u000a* [[Jaccard index]]\u000a* [[SimRank]]\u000a* [[Information retrieval]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://www.appliedsoftwaredesign.com/archives/cosine-similarity-calculator/ Online Cosine Similarity Calculator]\u000a* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]\u000a* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]\u000a\u000a{{DEFAULTSORT:Cosine Similarity}}\u000a[[Category:Information retrieval]]
p114
asI105
(lp115
VQuery likelihood model
p116
aVThe '''query likelihood model''' is a [[language model]] used in [[Information Retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.\u000a\u000a==Calculating the likelihood==\u000aUsing [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:\u000a\u000a:<math>\u000a P(d|q) = \u005cfrac{P(q|d) P(d)}{P(q)}\u000a</math>\u000a\u000aSince the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.\u000a\u000a:<math>\u000a P(d|q) = P(q|d)\u000a</math>\u000a\u000aDocuments are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:\u000a:<math>\u000a P(q|M_d) = K_q \u005cprod_{t \u005cin V} P(t|M_d)^{tf_{t,q}}\u000a</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tM,q}!)</math> for query {{math|q}}.\u000a\u000aIn practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.\u000a\u000a<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>\u000a\u000a==References==\u000a <references/>\u000a\u000a[[Category:Information retrieval]]
p117
asI107
(lp118
VKeyword optimization
p119
aV#REDIRECT [[Search engine optimization]]\u000a\u000a{{DEFAULTSORT:Keyword Optimization}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet marketing]]
p120
asI112
(lp121
VLiterature-based discovery
p122
aV'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. \u000a\u000aLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526\u2013557 }}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].\u000a\u000a==Swanson linking==\u000a[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]\u000a''Swanson linking'' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111\u2013135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.\u000a\u000a==See also==\u000a*[[Arrowsmith System]]\u000a*[[Implicature]]\u000a*[[Latent semantic indexing]]\u000a*[[Metaphor]]\u000a\u000a==References==\u000a* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&nbsp;9958\u20139964.\u000a*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]\u000a\u000a; Further readings\u000a* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.\u000a\u000a; Footnotes\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Medical research]]\u000a\u000a\u000a{{science-stub}}
p123
asI113
(lp124
VPersonalized search
p125
aV{{essay-like|date=January 2015}}\u000a{{original research|date=January 2015}}\u000a'''Personalized search''' refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user\u2019s query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM (CACM)|year=2002|volume=45|issue=9|pages=50\u201355|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>\u000a\u000a==History==\u000a\u000aGoogle introduced Personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search set up for not just those who have a Google account but everyone as well. There is not very much information on how exactly Google personalizes their searches, however, it is believed that they use user language, location, and web history.<ref>http://personalization.ccs.neu.edu/paper.pdf</ref>\u000a\u000aEarly search engines, like [[Yahoo!]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by [[Google]], has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref>{{citation | last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to an from sites; the more links a site has, the higher it is placed on the page.<ref>{{cite AV media|last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results  indicating that those near the top are more relevant to a user's wants than those below.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref>\u000a\u000aWhile many [[search engines]] take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237\u2013251|doi=10.1002/asi.20477}}</ref> But user supplied information can be hard to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287\u2013296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581\u2013590}}</ref><ref>{{cite journal|last=Shen|first=X.|coauthors=Tan, B. and Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824\u2013831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675\u2013684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415\u2013422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>\u000a\u000aThere are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. The more you keep going the same site through a search result from Google, it believes that you like that page. So when you do certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if you're signed out, Google may personalize your results because it keeps a 180 day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>\u000a\u000aIn order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University set out to answer this question. By comparing an aggregate set of searches from logged in users against a control group, the research team found that 11.7% of results show differences due to personalization, however this varies widely by search query and result ranking position.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the IP address of the searching users. It should also be noted that results with high degress of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if you searched for "used car sales", Google may churn out results of local car dealerships in your area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000aWhen measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when you perform a search and follow it with a subsequent search, the results of the second search is influenced by the first search. An interesting point to note is that the top ranked URLs are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization, based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000a==The Filter Bubble==\u000a{{Main|Filter bubble}}\u000a\u000aSeveral concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by biasing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome." As a result people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aThe methods of personalization, and how useful it is to \u201cpromote\u201d certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the Filter Bubble happens. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal Of Pattern Recognition & Artificial Intelligence |year=2007|pages=183\u2013205}}</ref>\u000a\u000aAn area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information. This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref>http://{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|accessdate= 27 April 2014}}</ref>\u000a\u000aMany search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|coauthors=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEE transaction on knowledge and data engineering|year=2010|volume=22|issue=7|pages=969\u2013982|doi=10.1109/tkde.2009.144}}</ref>\u000a\u000aThe feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aSome have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>\u000a\u000a==The Case of Google==\u000a{{Main|Google Personalized Search}}\u000a\u000aThe perfect example of search personalization is [[Google]]. Google is not just a search engine, but a corporation that is entering every facet of our lives. Personalization with Google has gone far beyond just search. There are a host of new applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20\u201331}}</ref> that keeps track of all the information directly under one\u2019s name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the [[New York Times]]. The high level of personalization that was available with Google played a significant part in helping remain the world\u2019s most favorite search engine.\u000a\u000aOne large example of Google\u2019s ability to personalized search is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed as interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information you want. The concern, however, is that the very important information can be held back because it does not match with the criteria that the program sets for the particular user. This can create the \u201c[[filter bubble]]\u201d as described earlier.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aAn interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff, is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.\u000aGoogle\u2019s popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although many people would say that having Google personalize your search results based on what you searched previously would be a good thing, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>\u000aWith the power from this information, Google has chosen to bully its way into other sectors it owned such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.\u000a\u000aUsing Search Personalization, Google has doubled its video market share to about eighty percent. The legal definition of a monopoly is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.\u000a\u000aThe analytical firm Experian Hitwise stated that since two thousand and seven, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from thirty one percent in two thousand and seven to ten percent in two thousand and ten. Even Yahoo Images has gone from twelve percent to seven percent. It becomes very apparent that the decline of these companies has come because of Google\u2019s increase in market share from forty three percent in two thousand and seven to about fifty five percent in two thousand and nine.\u000a\u000aIt might be easy to say that all of this has come from Google being more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google\u2019s bullying because their numbers went from one point three million unique visitors to eleven point nine unique visitors in one month. That kind of growth can only come with the change of a process.\u000a\u000aIn the end, there are two things in common theme with all of these graphs. The first is that Google\u2019s market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around two thousand and seven, which is around the time that Google began to use its \u201cUniversal Search\u201d method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>\u000a\u000a==Benefits==\u000a\u000aOne of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human\u2019s capability of processing information has not expanded much.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show positive correlation between personalized search and the quality of consumers\u2019 decisions.\u000a\u000aThe first study was conducted by Kristin Diehl from University of South Carolina. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that \u2018consumers make worse choices because lower search costs cause them to consider inferior options.\u2019 It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> The study by Gerald Haubl from University of Alberta and Benedict G.C. Dellaert from Maastricht University mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers\u2019 decision quality and reduced the number of products inspected.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref>\u000a\u000a==Models==\u000a\u000aPersonalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>Coyle, M., & Smyth, B. (2007). Information recovery and discovery in collaborative web search. Advances in Information Retrieval (pp. 356\u2013367).</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.\u000a\u000aThe first model available is based on the users\u2019 historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.\u000a\u000aThere is another way to personalize search results. In Bracha Shapira and Boaz Zabar\u2019s \u201cPersonalized Search: Integrating Collaboration and Social Networks\u201d, Shapira and Zabar focused on a model that utilizes a recommendation system.<ref>Shapira, B., & Zabar, B. (2011). Personalized search: Integrating collaboration and social networks. Journal Of The American Society For Information Science & Technology, 62(1), 146-160. doi:10.1002/asi.21446</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.\u000a\u000a==Disadvantages==\u000a\u000aWhile there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users\u2019 search engine results to material that aligns with the users\u2019 interests and history. It limits the users\u2019 ability to become exposed to material that would be relevant to the user\u2019s search query but due to the fact that some of this material differs from the user\u2019s interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. \u201cObjectivity matters little when you know what you are looking for, but its lack is problematic when you do not\u201d.<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426\u2013445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref>  One of the main functions of the internet is the collection and sharing of information. This is the criticism of search personalization. It limits a core function of the web. It helps prevent users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user\u2019s search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue. The user\u2019s search results will reflect that. The user not be displayed both sides of the issue if the user\u2019s interests lean to one side or another. The user may be missing out on information that could be important. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users even though each user entered the same search query. \u201cWhen I further distilled the results, I saw that only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on\u201d.<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|volume=35.6}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.\u000a\u000aAnother disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling your internet interests and histories to other companies. This raises a privacy issue. The issue is if people are content with companies gather and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.\u000a\u000a==Sites that use Personalized Search==\u000a\u000aE. Pariser author of the Filter Bubble explains how there are differences that search personalization has on both Facebook and Google. Facebook implements personalization when it comes to the amount of things we share and also what pages we \u201clike\u201d. It also takes into consideration our social interactions, whose profile we visit the most, who we message or chat with are all indicators that are used when Facebook uses personalization. Rather than what we share being an indicator of what is filtered out, but Google takes into consideration what we \u201cclick\u201d to filter out what comes up in our searches. In addition Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and we share what other people want to see. Even while tagging photographs, Facebook uses personalization and recognition that will automatically assign a name to face for you without you having to tag them. In terms of Google we are provided similar websites and resources based on what we initially click on. This doesn't just affect Google and Facebook. There are even other websites that use the filter tactic to better adhere to user preferences. For example, Netflix also judges from the users search history to suggest movies that they may be interested in for the future. There are cites like Amazon and personal shopping cites also use other peoples history in order to serve their interests better. Twitter also uses personalization by \u201csuggesting\u201d other people to follow. In addition, based on who we \u201cfollow\u201d and who we \u201ctweet\u201d and \u201cretweet\u201d at Twitter filters out to peoples best interest for us.  Mark Zuckerberg, founder of Facebook, believed that we only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn\u2019t true. Although personalized search may seem helpful it is not a very accurate representation of who we are as people. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles in order to make themselves look better. Search personalization is not an ideal representation of any person. There are so many cites used for different purposes and that does not make up one person\u2019s identity at all that, but are in fact false representations of ourselves.<ref>http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf</ref>\u000a\u000a==Personalized Search and Online Shopping==\u000a\u000aSearch engines, such as Google and Yahoo!, utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual\u2019s web clicks, search engines can use personalized search to put forth advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in brick-and-mortar stores. These types of products and services are called long tail items.<ref>Badke, William. \u201cPersonalization and Information Literacy\u201d. Online, 47. Feb. 2012.</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.\u000a\u000aAside from aiding consumers and businesses in finding one-another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref>Inside Google. "Traffic Report: How Google Is Squeezing Out Competitors and Muscling Into New Markets." Consumer Watchdog. http://www.consumerwatchdog.org, 2 June 2010. Web.</ref> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word \u201cshoes\u201d using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer\u2019s queries.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a{{DEFAULTSORT:Personalized search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]\u000a[[Category:Internet terminology]]\u000a[[Category:Personalized search|*]]
p126
asI114
(lp127
VNearest neighbor search
p128
aV'''Nearest neighbor search''' ('''NNS'''), also known as '''proximity search''', '''similarity search''' or '''[[Closest pair of points problem|closest point search]]''',  is an [[optimization problem]] for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&nbsp;\u2208&nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.\u000a\u000aMost commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example are asymmetric [[Bregman divergence]]s, for which the triangle inequality does not hold.<ref name=Cayton2008>{{Cite journal\u000a | last1 = Cayton | first1 = Lawerence\u000a | year = 2008\u000a | title =  Fast nearest neighbor retrieval for bregman divergences.\u000a | journal = Proceedings of the 25th international conference on Machine learning\u000a | pages = 112\u2013119\u000a}}</ref>\u000a\u000a==Applications==\u000a\u000aThe nearest neighbor search problem arises in numerous fields of application, including:\u000a*[[Pattern recognition]] - in particular for [[optical character recognition]]\u000a*[[Statistical classification]]- see [[k-nearest neighbor algorithm]]\u000a*[[Computer vision]]\u000a*[[Computational Geometry]] - see [[Closest pair of points problem]]\u000a*[[Database]]s - e.g. [[content-based image retrieval]]\u000a*[[Coding theory]] - see [[Decoding methods|maximum likelihood decoding]]\u000a*[[Data compression]] - see [[MPEG-2]] standard\u000a*[[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]\u000a*[[Internet marketing]] - see [[contextual advertising]] and [[behavioral targeting]]\u000a*[[DNA sequencing]]\u000a*[[Spell checking]] - suggesting correct spelling\u000a*[[Plagiarism detection]]\u000a*[[Contact searching algorithms in FEA]]\u000a*[[Similarity score]]s for predicting career paths of professional athletes.\u000a*[[Cluster analysis]] - assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]\u000a*[[Chemical similarity]]\u000a*[[Motion planning#Sampling-Based Algorithms|Sampling-Based Motion Planning]]\u000a\u000a==Methods==\u000a\u000aVarious solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.\u000a\u000a===Linear search===\u000aThe simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far".  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN'') where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.<ref>{{cite web|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|author=Weber, Schek, Blott | url=http://www.vldb.org/conf/1998/p194.pdf}}</ref>\u000a\u000a===Space partitioning===\u000aSince the 1970s, [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach is known as [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&nbsp;''N'') <ref>{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&language=en}}</ref> in the case of randomly distributed points, worst case complexity analyses have been performed.<ref name=Lee1977>{{Cite journal\u000a | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee\u000a | last2 = Wong | first2 = C. K.\u000a | year = 1977\u000a | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees\u000a | journal = Acta Informatica\u000a | volume = 9\u000a | issue = 1\u000a | pages = 23\u201329\u000a | doi = 10.1007/BF00263763\u000a | postscript = .\u000a}}</ref>\u000aAlternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].<ref>{{cite doi|10.1145.2F223784.223794}}</ref> R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.\u000a\u000aIn case of general metric space branch and bound approach is known under the name of [[metric trees]]. Particular examples include [[vp-tree]] and [[BK-tree]].\u000a\u000aUsing a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.\u000a\u000a===Locality sensitive hashing===\u000a\u000a[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.<ref>{{cite web|author=A. Rajaraman and J. Ullman| url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}</ref>\u000a\u000a===Nearest neighbor search in spaces with small intrinsic dimension===\u000a\u000aThe [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''<sup>12</sup>&nbsp;log&nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.\u000a\u000a===Vector approximation files===\u000a\u000aIn high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.<ref>{{cite web|title=An Approximation-Based Data Structure for Similarity Search|author=Weber, Blott}}</ref>\u000a\u000a===Compression/clustering based search===\u000aThe VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most "promising" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.<ref>{{cite web|title=Adaptive cluster-distance bounding for similarity search in image databases|author=Ramaswamy, Rose, ICIP 2007}}</ref><ref>{{cite web|title=Adaptive cluster-distance bounding for high-dimensional indexing|author=Ramaswamy, Rose, TKDE 2010}}</ref> Also note the parallels between clustering and LSH.\u000a\u000a===Greedy walks===\u000aOne possible way to solve NNS is to construct a graph <math>G(V,E)</math>, where every point <math>x_i \u005cin S </math> is uniquely associated with vertex <math>v_i \u005cin V </math>. The search of the point in the set ''S'' closest to the query ''q'' takes the form of the search of vertex in the graph <math>G(V,E)</math>.\u000aOne of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex <math>v_i \u005cin V </math>. The algorithm computes a distance value from the query q to each vertex from the neighborhood <math>\u005c{v_j:(v_i,v_j) \u005cin E\u005c}</math> of  the current vertex <math>v_i</math>, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.\u000aThis idea was exploited in VoroNet system <ref name=voroNet>{{Cite journal\u000a | last1 = Olivier | first1 = Beaumont  \u000a | last2 = Kermarrec | first2 = Anne-Marie\u000a | last3 = Marchal | first3 = Loris \u000a | last4 = Rivière | first4 = Etienne   \u000a | year = 2006\u000a | title = VoroNet: A scalable object network based on Voronoi tessellations\u000a | journal = INRIA\u000a | volume = RR-5833\u000a | issue = 1\u000a | pages = 23\u201329\u000a | doi = 10.1007/BF00263763\u000a | postscript = .\u000a}}</ref> for the plane, in RayNet system <ref name=rayNet>{{Cite journal\u000a | last1 = Olivier | first1 = Beaumont  \u000a | last2 = Kermarrec | first2 = Anne-Marie\u000a | last4 = Rivière | first4 = Etienne   \u000a | year = 2007\u000a | title = Peer to Peer Multidimensional Overlays: Approximating Complex Structures\u000a | journal = Principles of Distributed Systems\u000a | volume =  4878\u000a | issue = .\u000a | pages = 315\u2013328\u000a | doi = 10.1007/978-3-540-77096-1_23\u000a | isbn = 978-3-540-77095-4\u000a | postscript = .\u000a}}</ref> for the <math>\u005cmathbb{E}^n</math> and for the general metric space in Metrized Small World algorithm <ref name=msw2014>{{Cite journal\u000a | last1 = Malkov | first1 = Yury  \u000a | last2 = Ponomarenko | first2 = Alexander\u000a | last3 = Krylov | first3 = Vladimir \u000a | last4 = Logvinov | first4 = Andrey   \u000a | year = 2014\u000a | title = Approximate nearest neighbor algorithm based on navigable small world graphs\u000a | journal = Information Systems\u000a | volume = 45\u000a | pages = 61\u201368\u000a | doi = 10.1016/j.is.2013.10.006\u000a | postscript = .\u000a}}</ref>\u000a\u000a==Variants==\u000a\u000aThere are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[&epsilon;-approximate nearest neighbor search]].\u000a\u000a===<span id="K-nearest neighbor"> ''k''-nearest neighbor </span>===\u000a\u000a[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.\u000a\u000a===Approximate nearest neighbor===\u000aIn some applications it may be acceptable to retrieve a "good guess" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.\u000a\u000aAlgorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.<ref>S. Arya, [[David Mount|D. M. Mount]], [[Nathan Netanyahu|N. S. Netanyahu]], R. Silverman and A. Wu, An optimal algorithm for approximate nearest neighbor searching, Journal of the ACM, 45(6):891-923, 1998. [http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf]</ref>\u000a\u000a===Nearest neighbor distance ratio===\u000a\u000a[[Nearest neighbor distance ratio]] do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a "query by example" using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.\u000a\u000a===Fixed-radius near neighbors===\u000a\u000a[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.\u000a\u000a===All nearest neighbors===\u000a\u000aFor some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.\u000a\u000aGiven a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L<sup>p</sup> norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&nbsp;log&nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&nbsp;log&nbsp;''n'') time.<ref>{{citation\u000a | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson\u000a | contribution = Fast algorithms for the all nearest neighbors problem\u000a | doi = 10.1109/SFCS.1983.16\u000a | pages = 226\u2013232\u000a | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)\u000a | year = 1983| isbn = 0-8186-0508-1 }}.</ref><ref name=Vaidya>{{Cite journal\u000a | doi = 10.1007/BF02187718\u000a | last1 = Vaidya | first1 = P. M.\u000a | year = 1989\u000a | title = An ''O''(''n''&nbsp;log&nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem \u000a | journal = [[Discrete and Computational Geometry]]\u000a | volume = 4\u000a | issue = 1\u000a | pages = 101\u2013115\u000a | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&pi=8\u000a | postscript = .\u000a}}</ref>\u000a\u000a==See also==\u000a{{div col|colwidth=20em}}\u000a* [[Range search]]\u000a* [[Set cover problem]]\u000a*[[Statistical distance]]\u000a*[[Closest pair of points problem]]\u000a*[[Ball tree]]\u000a*[[Cluster analysis]]\u000a*[[Neighbor joining]]\u000a*[[Content-based image retrieval]]\u000a*[[Curse of dimensionality]]\u000a*[[Digital signal processing]]\u000a*[[Dimension reduction]]\u000a*[[Fixed-radius near neighbors]]\u000a*[[Fourier analysis]]\u000a*[[Instance-based learning]]\u000a*[[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]\u000a*[[Linear least squares (mathematics)|Linear least squares]]\u000a*[[Locality sensitive hashing]]\u000a*[[Multidimensional analysis]]\u000a*[[Nearest-neighbor interpolation]]\u000a*[[Principal component analysis]]\u000a*[[Singular value decomposition]]\u000a*[[Time series]]\u000a*[[Voronoi diagram]]\u000a*[[Wavelet]]\u000a*[[MinHash]]\u000a{{div col end}}\u000a\u000a==Notes==\u000a<references/>\u000a\u000a==References==\u000a*Andrews, L.. A template for the nearest neighbor problem.  ''C/C++ Users Journal'', vol. 19, no 11 (November 2001), 40 - 49, 2001, ISSN:1075-2838, [http://www.ddj.com/architect/184401449 www.ddj.com/architect/184401449]\u000a*Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu.  An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions.  ''Journal of the ACM'', vol. 45, no. 6, pp.&nbsp;891\u2013923\u000a*Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. 1999. When is nearest neighbor meaningful? In Proceedings of the 7th ICDT, Jerusalem, Israel.\u000a*Chung-Min Chen and Yibei Ling - A Sampling-Based Estimator for Top-k Query. ICDE 2002: 617-627\u000a*Samet, H. 2006. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9\u000a*Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN 0-387-29146-6\u000a\u000a==Further reading==\u000a*{{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 0-387-00857-8 }}\u000a\u000a==External links==\u000a*[http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search] \u2013 a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits\u000a*[http://sswiki.tierra-aoi.net Similarity Search Wiki] \u2013 a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours\u000a*[http://www.kgraph.org KGraph] \u2013 a C++ library for fast approximate nearest neighbor search with user-provided distance metric by Wei Dong.\u000a*[http://www.cs.ubc.ca/research/flann/ FLANN] \u2013 a library for performing fast approximate nearest neighbor searches in high dimensional spaces  by Marius Muja and David G. Low\u000a*[http://sisap.org/?f=library Metric Spaces Library] \u2013 An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar Chávez\u000a*[https://github.com/searchivarius/NonMetricSpaceLib  Non-Metric Space Library] \u2013 An open-source C++ library for exact and approximate searching in non-metric and metric spaces\u000a*[http://www.cs.umd.edu/~mount/ANN/ ANN] \u2013 A library for Approximate Nearest Neighbor searching by David M. Mount and Sunil Arya\u000a*[http://www.irisa.fr/texmex/people/jegou/ann.php Product Quantization] \u2013 Matlab implementation of approximate nearest neighbor search in the compressed domain by Herve Jegou\u000a*[http://lsd.fi.muni.cz/trac/messif MESSIF] \u2013 Metric Similarity Search Implementation Framework by Michal Batko and David Novak\u000a*[http://www.obsearch.net/ OBSearch] \u2013 Similarity Search engine for Java (GPL); implementation by Arnoldo Muller, developed during Google Summer of Code 2007\u000a*[http://mrim.imag.fr/georges.quenot/freesoft/knnlsb/ KNNLSB] \u2013 K Nearest Neighbors Linear Scan Baseline (distributed, LGPL); implementation by Georges Quénot (LIG-CNRS)\u000a*[http://neartree.sourceforge.net/ NearTree] \u2013 An API for finding nearest neighbors among points in spaces of arbitrary dimensions by Lawrence C. Andrews and Herbert J. Bernstein\u000a*[http://nearpy.io/ NearPy] \u2013 Python framework for fast approximated nearest neighbor search by Ole Krause-Sparmann\u000a*[http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]] \u2013 the Computational Geometry Algorithms Library\u000a*[https://github.com/ryanrhymes/panns Panns] \u2013 A Python library for searching approximate nearest neighbors, optimized for large dataset with high dimensional features, developed by Liang Wang\u000a\u000a{{DEFAULTSORT:Nearest Neighbor Search}}\u000a[[Category:Approximation algorithms]]\u000a[[Category:Classification algorithms]]\u000a[[Category:Data mining]]\u000a[[Category:Discrete geometry]]\u000a[[Category:Geometric algorithms]]\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]\u000a[[Category:Numerical analysis]]\u000a[[Category:Mathematical optimization]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]
p129
asI116
(lp130
VOnline search
p131
aV'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].<ref name="whatis?">{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12\u201318|id=Eric:EJ214713| accessdate=2011-04-04}}</ref> Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.<ref name="whatis?"/> In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.<ref name="whatis?"/> Today, searches through [[web search engine]]s constitute the majority of online searches.\u000a\u000aOnline searches often supplement reference transactions.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a\u000a[[Category:Internet terminology]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{web-stub}}
p132
asI117
(lp133
VPikimal
p134
aV{{Infobox company |\u000a name   = Pikimal |\u000a logo   = [[File:Pikimal logo.jpg|200px]] |\u000a type   = [[Limited liability company|LLC]]|\u000a company_slogan = |\u000a|founder = Eric Silver|\u000a foundation     = 2010|\u000a location       = [[Pittsburgh, Pennsylvania]], [[United States]]|\u000a key_people     = Eric Silver, Chief Executive Officer|\u000a industry       = [[Search engine technology|Search]] |\u000a<!--Please fill in:-->\u000a revenue        = |\u000a operating_income = |\u000a net_income     = | \u000a num_employees  = 13<ref name="businesstimes">{{cite news|url=http://www.bizjournals.com/pittsburgh/print-edition/2011/03/25/pikimal-comparison-shopping-stand-out.html | title=Pikimal, a comparison shopping website, hopes to stand out from crowd | work= Pittsburgh Business Times | first=Malia | last=Spencer | date=25 March 2011}}</ref> |\u000a homepage       = [http://pikimal.com/ www.pikimal.com]|\u000a}}\u000a\u000a'''Pikimal''' (pronounced as pick-em-all)<ref>{{cite web|url=http://www.popcitymedia.com/innovationnews/pikimal033011.aspx | title=Shop Smarter With Pikimal \u2013 POP City Media }}</ref> is a website, designed as a [[decision engine]] that uses consumer input to provide specialized search results for products and categories.\u000a\u000aUnlike typical [[Web search engine|search engines]], Pikimal mines data to provide users with only the facts pertaining to their search, as a hopeful solution to [[SEO]] and marketing biased search results.\u000a\u000aAs of April 2011, Pikimal had 13 full-time employees in Pittsburgh, PA, interns, and various contractors around the world.<ref name="businesstimes" />\u000a\u000a== History ==\u000a\u000aPikimal was founded in January 2010 by [[Eric Silver]], previously the chief marketing officer at [[Modcloth]]. The Pikimal site was launched in [[public beta]] form in October 2010.<ref name="businesstimes" />\u000a\u000a== Functionality ==\u000a\u000aPikimal allows users to adjust sliders to express what facts of a product are particularly important to them. These percentages are combined with an algorithm to provide users with product recommendations that are rooted directly in facts, but only the facts they find most relevant.<ref>{{cite web|url=http://www.youtube.com/watch?v=imOUklpphcM&feature=player_embedded | title=What is Pikimal? Video}}</ref>\u000a\u000a== Pivot and Shutdown ==\u000a\u000aIn 2012 Pikimal changed it name to [[Webkite]] and pivoted to provide faceted search solutions to other companies. As of September 2014 pikimal.com and all associated sites has been shutdown.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* {{official website|http://pikimal.com}}\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet properties established in 2010]]\u000a[[Category:Knowledge markets]]\u000a[[Category:Companies based in Pittsburgh, Pennsylvania\u200e]]
p135
asI118
(lp136
VMasterseek
p137
aV{{refimprove|date=July 2014}}\u000a{{Infobox company\u000a| name     = Masterseek\u000a| logo     = [[Image:masterseek logo.png|260px]]\u000a| type     = [[Private company|Private]]\u000a| traded_as        = \u000a| foundation       = [[Denmark]] (1999)\u000a| founder          = [[Rasmus Refer]]\u000a| location_city    = [[New York City]]\u000a| location_country = {{nowrap|United States}}\u000a| area_served      = Worldwide\u000a| key_people       = Rasmus Refer <small>(Co-Founder, [[Chief executive officer|CEO]])</small><br />Jørgen Trygved <small>(Co-Founder)</small><br />Qasim Raza <small>([[Chief technology officer|CTO]])</small><br />Robert Perz <small>(COO, 2005-08)</small>\u000a| industry         = Internet<br>Computer software\u000a| products         = [[Business-to-business|B2B]] [[Search Engine]]\u000a| revenue          = \u000a| operating_income =\u000a| net_income       = \u000a| assets           = \u000a| num_employees    =\u000a| subsid           = [[Accoona]]\u000a| homepage         = {{URL|http://www.masterseek.com/}}\u000a| intl = yes\u000a}}\u000a\u000a'''Masterseek Corp.''' is a [[Business-to-business|B2B]] (Business to Business) [[search engine]] founded in [[Denmark]] in 1999.<ref name="hartzer"/> It currently hosts over 83 million worldwide company profiles from 75 countries,<ref name="usti"/> and business subscribers are given complete control over their corporate profiles.<ref name="hartzer"/> According to the amount of listed profiles, they are the largest B2B search engine worldwide.<ref name="seochat"/>\u000a\u000a==Founding==\u000a<!-- Deleted image removed: [[File: Rasmus refer.png|thumb|150px|left|[[Rasmus Refer]]]] -->\u000aMasterseek was founded in Denmark by [[Rasmus Refer]] in 1999.<ref name="hartzer"/> Their Denmark headquarters is located at Bredgade 29, DK-1260 Kbh. K, and they also have a current headquarters in [[New York City]], at 82 [[Wall Street]].<ref name="csc"/>\u000a\u000aAccording to its executives, Masterseek utilizes a business model based on an annual business subscription fee of USD $149, in return for which subscribers receive full editing control over their corporate profile, content and advertising, and control over widgets and embedded video, among other factors.<ref name="betaversion"/>\u000a\u000a==Finances==\u000aAs of June 2008, accountancy firm Horwart International had approximated the raw market value of the Masterseek company at $150 million.<ref name="investors"/> The company remains privately owned, but also in June 2008, it sold 10% of its authorized stocks to a range of foreign investors.<ref name="investors"/> The company announced on January 31, 2009 that they company was again offering a limited number of shares for sale in order to raise $4\u20136 million in order to gain a listing on the [[Swedish people|Swedish]] marketplace AktieTorget. Founder Refer also announced there were plans for an [[Initial Public Offering|IPO]].<ref name="ipo"/> By October 2009, they had signed with the Swedish-based company Thenberg & Kinde Fondkommission AB for financing.<ref name="seochat"/>\u000a\u000aIn the Danish company register the name Masterseek is coming up in three bankruptcies and one compulsory dissolution.<ref name="businessdk-cheats" />\u000a\u000a==Statistics==\u000aIn June 2008, the company stated it had 50 million company profiles, from over 75 countries, and handled 90,000 B2B searches daily.<ref name="hartzer"/><ref name="ipo"/><ref name="strengthen"/> The company stated they had 82 million profiles on March 21, 2011, with an average of 300,000 new profiles added monthly.<ref name="betaversion"/>\u000a\u000a==Acquiring Accoona==\u000a[[File:accoona logo.png|right|220px]]\u000aOn October 30, 2008, it was announced that Masterseek had acquired the B2B search engine [[Accoona]].<ref name="hartzer"/><ref name="paidcontent"/> \u000a"When [Business Insider] first heard about the money-losing Jersey City-based startup filing for IPO last year, [their] impulse was to run away screaming."<ref name="accoonnaddead"/> The search engine had been fairly successful in the United States and [[China]],<ref name="search"/> where it had an exclusive partnership with ''[[China Daily News]]''.<ref name="accoonnaddead"/> On August 3, 2006, ''[[Time Magazine|TIME]]'' had dubbed Accoona one of its "50 best websites," illustrating how the search engine used [[artificial intelligence]] to "understand" the meaning of keyword queries.<ref name="coolest"/> Accoona had run into difficulties and gone defunct by early October 2008, withdrawing its [[Initial Public Offering|IPO]].<ref name="accoonnaddead"/>\u000a\u000aAfter Masterseek bought the remaining search engine codes, domain name, and assets,<ref name="hartzer"/> Accoona was integrated with Masterseek, and re-launched in the USA and China. It was launched in Europe in January 2009.<ref name="search"/> Accoona information was also integrated into the Masterseek search engine.<ref name="hartzer"/>\u000a\u000a==Technology==\u000aThe Masterseek search engine relies on web crawlers that automatically collect and sort company details from the internet.<ref name="strengthen"/> Searches can look up company profiles, contact information, and descriptions of products and services. Searches can be global, national, regional, or involved local markets. Hits are listed by relevance according to search terms. There are different search options, including a specific product search, company searches, and people searches. Results can be displayed in most languages.<ref name="seochat"/> The search engine also offers MasterRank, a point system for ranking corporate websites.<ref name="Bussinessweek"/>\u000a\u000a===Beta version===\u000aMasterseek released a new [[Beta Version]] of its search engine on March 25, 2011. Before then, it was accessible to business owners, managers, and other professionals, but the Beta Version made searching the site free and open to the public.<ref name="betaversion"/>\u000a\u000a== Warnings against Masterseek Corp. ==\u000aIn December 2009 the Swedish [[Financial Supervisory Authority (Sweden)|Financial Supervisory Authority]] issued a warning against Masterseek Corp. to warn investors <ref name="fise" /><ref name="businessdk" />\u000a\u000aIn February 2010 the  issued a warning against Masterseek and related companies Bark Group and Blogger Wave.<ref name="shareholdersdk" />\u000a\u000a==Sponsorships==\u000aOn July 5, 2007, Masterseek announced they were cosponsors to [[Team CSC]], Denmark's cycling team, beginning with the team's involvement in the [[Tour de France]]. The Masterseek name began to be displayed on the team's apparel that week, with the Tour's start in [[London]].<ref name="csc"/>\u000a\u000a==Management==\u000a*[[Rasmus Refer]] - Founder, Director, President of Technology\u000a*Qasim Raza - Chief Technology Officer<ref name="Bussinessweek"/>\u000a\u000a==See also==\u000a*[[Accoona]]\u000a*[[Business-to-business|B2B]]\u000a*[[Search engines]]\u000a\u000a== References ==\u000a{{reflist|2| refs =\u000a\u000a<ref name="shareholdersdk">{{cite news\u000a|url=http://www.shareholders.dk/art/templates/pressemeddelelse.aspx?articleid=512&zoneid=29\u000a|title=Danish Shareholders Association warns against Bark Group, Blogger Wave and Masterseek\u000a|publisher=[http://shareholders.dk/ Dansk Aktionærforening]\u000a|accessdate=2014-07-27}}</ref>\u000a\u000a<ref name="businessdk">{{cite news\u000a|url=http://www.business.dk/digital/it-firma-snyder-investorer\u000a|title=IT-firma snyder investorer (IT firm cheating investors)\u000a|publisher=[http://business.dk/ Business.dk]\u000a|accessdate=2014-07-03}}</ref>\u000a\u000a<ref name="businessdk-cheats">{{cite news\u000a|url=http://bizzen.blogs.business.dk/2010/02/09/plattenslagere-skamrider-danske-varem%C3%A6rker-som-carlsberg-danisco-og-coop/\u000a|title=Plattenslagere skamrider danske varemærker som Carlsberg, Danisco og Coop (Cheats shame rides Danish brands)\u000a|publisher=[http://business.dk/ Business.dk]\u000a|accessdate=2014-07-25}}</ref>\u000a\u000a<ref name="fise">{{cite news\u000a|url=http://www.fi.se/Folder-EN/Startpage/Register/Investor-alerts/Warning-list/Warning-against-Masterseek-Corp/\u000a|title=Warning against Masterseek Corp.\u000a|publisher=Finansinspektionen\u000a|accessdate=2014-07-03}}</ref>\u000a\u000a<ref name="Bussinessweek">{{cite news\u000a|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=29327873\u000a|title=Masterseek Corp.\u000a|last=\u000a|first=\u000a|date=\u000a|publisher=''[[Businessweek]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="coolest">{{cite news\u000a|url=http://www.time.com/time/business/article/0,8599,1222614,00.html\u000a|title=50 Coolest Websites: 2006\u000a|last=Buechner\u000a|first=Maryanne\u000a|date=August 3, 2006\u000a|publisher=''[[Time Magazine|TIME]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="strengthen">{{cite news\u000a|url=http://www.reuters.com/article/2008/06/26/idUS149456+26-Jun-2008+MW20080626\u000a|title=Global Business Search Engine to Strengthen Its Advertising Network for B2B Search\u000a|last=\u000a|first=\u000a|date=June 26, 2008\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="investors">{{cite news\u000a|url=http://www.reuters.com/article/2008/06/27/idUS85249+27-Jun-2008+MW20080627\u000a|title=Search Engine is Looking for Strategic Investors\u000a|last=\u000a|first=Masterseek\u000a|date=June 27, 2008\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="accoonnaddead">{{cite news\u000a|url=http://www.businessinsider.com/2008/10/dead-search-engine-accoona-officially-dead\u000a|title=Dead Search Engine Accoona Officially Dead\u000a|last=Krangel\u000a|first=Eric\u000a|date=October 3, 2008\u000a|publisher=''[[Business Insider]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="hartzer">{{cite news\u000a|url=https://www.billhartzer.com/pages/b2b-search-engine-accoona-acquired-by-masterseek/\u000a|title=B2B Search Engine Accoona Acquired by Masterseek\u000a|last=Hartzer\u000a|first=Bill\u000a|date=November 5, 2008\u000a|publisher=BillHartzer.com: Search Engine Marketing\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="search">{{cite news\u000a|url=http://blog.searchenginewatch.com/081105-115108\u000a|title=Accoona Acquired by Masterseek\u000a|last=Johnson\u000a|first=Nathania \u000a|date=November 5, 2008\u000a|publisher=''[[Search Engine Watch]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="paidcontent">{{cite news\u000a|url=http://paidcontent.org/article/419-almost-dead-search-engine-accoona-bought-by-denmarks-masterseek/\u000a|title=Almost-Dead Search Engine Accoona Bought by Denmark's Masterseek\u000a|last=\u000a|first=\u000a|date=November 2008\u000a|publisher=''Paid Content''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="ipo">{{cite news\u000a|url=http://www.reuters.com/article/2009/01/31/idUS85625+31-Jan-2009+MW20090131\u000a|title=Masterseek.com Is Planning for an IPO\u000a|last=\u000a|first=Masterseek\u000a|date=January 31, 2009\u000a|publisher=''[[Reuters]]''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="seochat">{{cite news\u000a|url=http://www.seochat.com/c/a/Search-Engine-News/Masterseek-a-Global-Business-Search-Engine/\u000a|title=Masterseek: a Global Business Search Engine\u000a|last=Morgan\u000a|first=KC\u000a|date=October 27, 2009\u000a|publisher=''SEOchat''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="betaversion">{{cite news\u000a|url=http://www.wiredprnews.com/2011/03/21/masterseek-expeced-to-release-beta-version-of-search-engine-continue-dominance-in-business-to-business-search_2011032117895.html\u000a|title=Masterseek Expected to Release Beta Version of Search Engine\u000a|last=\u000a|first=\u000a|date=March 21, 2011\u000a|publisher=WirePRNews\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="usti">{{cite news\u000a|url=http://usbusinesstimes.com/internet/3424-business-search-powerhouse-masterseek-partners-with-cutting-edge-job-database-simply-hired.html\u000a|title=Masterseek.com Partners with Simply Hired\u000a|last=\u000a|first=admin\u000a|date=April 16, 2011\u000a|publisher=''US Business Times''\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a<ref name="csc">{{cite news\u000a|url=http://www.global-business-profiles.com/masterseek-cosponsors-team-csc/\u000a|title=Masterseek Cosponsors Team Csc\u000a|last=\u000a|first=\u000a|date=May 4, 2011\u000a|publisher=Global Business Profiles\u000a|accessdate=2011-05-08}}</ref>\u000a\u000a}}\u000a\u000a==External links==\u000a*{{Official website|http://www.masterseek.com/}}\u000a*[http://twitter.com/#!/masterseek_tw1 Masterseek] on [[Twitter]]\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Web service providers]]\u000a[[Category:Internet properties established in 1999]]\u000a[[Category:Business services companies established in 1999]]\u000a[[Category:Information retrieval]]\u000a[[Category:Online companies]]\u000a[[Category:Software companies established in 1999]]\u000a\u000a[[da:Dansk Aktionærforening|Danish Shareholders Association]]
p138
asI129
(lp139
VPreference learning
p140
aV'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The\u000aMIT Press ISBN 9780262018258.</ref> In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.\u000a\u000aWhile the concept of preference learning has been emerged for some time in many fields such as [[economics]],<ref name="SHOG00" /> it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.<ref name="WEB:WORKSHOP" />\u000a\u000a==Tasks==\u000a\u000aThe main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':<ref name="FURN11" />\u000a\u000a===Label ranking===\u000a\u000aIn label ranking, the model has an instance space <math>X=\u005c{x_i\u005c}\u005c,\u005c!</math> and a finite set of labels <math>Y=\u005c{y_i|i=1,2,\u005ccdots,k\u005c}\u005c,\u005c!</math>. The preference information is given in the form <math>y_i \u005csucc_{x} y_j\u005c,\u005c!</math> indicating instance <math>x\u005c,\u005c!</math> shows preference in <math>y_i\u005c,\u005c!</math> rather than <math>y_j\u005c,\u005c!</math>. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.\u000a\u000aIt was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:<ref name="HARP03" /> if a training instance <math>x\u005c,\u005c!</math> is labeled as class <math>y_i\u005c,\u005c!</math>, it implies that <math>\u005cforall j \u005cneq i, y_i \u005csucc_{x} y_j\u005c,\u005c!</math>. In [[Multi-label classification|multi-label]] situation, <math>x\u005c,\u005c!</math> is associated with a set of labels <math>L \u005csubseteq Y\u005c,\u005c!</math> and thus the model can extract a set of preference information <math>\u005c{y_i \u005csucc_{x} y_j | y_i \u005cin L, y_j \u005cin Y\u005cbackslash L\u005c}\u005c,\u005c!</math>. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.\u000a\u000a===Instance ranking===\u000a\u000aInstance ranking also has the instance space <math>X\u005c,\u005c!</math> and label set <math>Y\u005c,\u005c!</math>. In this task, labels are defined to have a fixed order <math>y_1 \u005csucc y_2 \u005csucc \u005ccdots \u005csucc y_k\u005c,\u005c!</math> and each instance <math>x_l\u005c,\u005c!</math> is associated with a label <math>y_l\u005c,\u005c!</math>. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.\u000a\u000a===Object ranking===\u000a\u000aObject ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form <math>x_i \u005csucc x_j\u005c,\u005c!</math> and the model should find out a ranking order among instances.\u000a\u000a==Techniques==\u000a\u000aThere are two practical representations of the preference information <math>A \u005csucc B\u005c,\u005c!</math>. One is assigning <math>A\u005c,\u005c!</math> and <math>B\u005c,\u005c!</math> with two real numbers <math>a\u005c,\u005c!</math> and <math>b\u005c,\u005c!</math> respectively such that <math>a > b\u005c,\u005c!</math>. Another one is assigning a binary value <math>V(A,B) \u005cin \u005c{0,1\u005c}\u005c,\u005c!</math> for all pairs <math>(A,B)\u005c,\u005c!</math> denoting whether <math>A \u005csucc B\u005c,\u005c!</math> or <math>B \u005csucc A\u005c,\u005c!</math>. Corresponding to these two different representations, there are two different techniques applied to the learning process.\u000a\u000a===Utility function===\u000a\u000aIf we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function <math>f: X \u005ctimes Y \u005crightarrow \u005cmathbb{R}\u005c,\u005c!</math> such that <math>y_i \u005csucc_x y_j \u005cRightarrow f(x,y_i) > f(x,y_j)\u005c,\u005c!</math>. For instance ranking and object ranking, the mapping is a function <math>f: X \u005crightarrow \u005cmathbb{R}\u005c,\u005c!</math>.\u000a\u000aFinding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.\u000a\u000a===Preference relations===\u000a\u000aThe binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.<ref name="FURN03" /> For object ranking, there is an early approach by Cohen et al.<ref name="COHE98" />\u000a\u000aUsing preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.<ref name="FURN03" />\u000a\u000a==Uses==\u000a\u000aPreference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.<ref name="LIU09" />\u000a\u000aAnother application of preference learning is [[recommender systems]].<ref name="GEMM09" /> Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.\u000a\u000a==See also==\u000a*[[Learning to rank]]\u000a\u000a==References==\u000a\u000a{{Reflist|\u000arefs=\u000a\u000a<ref name="SHOG00">{{\u000acite journal\u000a|last       = Shogren\u000a|first      = Jason F.\u000a|coauthors  = List, John A.; Hayes, Dermot J.\u000a|year       = 2000\u000a|title      = Preference Learning in Consecutive Experimental Auctions\u000a|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm\u000a|journal    = American Journal of Agricultural Economics\u000a|volume     = 82\u000a|pages      = 1016\u20131021\u000a|doi=10.1111/0002-9092.00099\u000a}}</ref>\u000a\u000a<ref name="WEB:WORKSHOP">{{\u000acite web\u000a|title      = Preference learning workshops\u000a|url        = http://www.preference-learning.org/#Workshops\u000a}}</ref>\u000a\u000a<ref name="FURN11">{{\u000acite book\u000a|last       = F&uuml;rnkranz\u000a|first      = Johannes\u000a|coauthors  = H&uuml;llermeier, Eyke\u000a|year       = 2011\u000a|title      = Preference Learning\u000a|url        = http://books.google.com/books?id=nc3XcH9XSgYC\u000a|chapter    = Preference Learning: An Introduction\u000a|chapterurl = http://books.google.com/books?id=nc3XcH9XSgYC&pg=PA4\u000a|publisher  = Springer-Verlag New York, Inc.\u000a|pages      = 3\u20138\u000a|isbn       = 978-3-642-14124-9\u000a}}</ref>\u000a\u000a<ref name="HARP03">{{\u000acite journal\u000a|last       = Har-peled\u000a|first      = Sariel\u000a|coauthors  = Roth, Dan; Zimak, Dav\u000a|year       = 2003\u000a|title      = Constraint classification for multiclass classification and ranking\u000a|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02\u000a|pages      = 785\u2013792\u000a}}</ref>\u000a\u000a<ref name="FURN03">{{\u000acite journal\u000a|last       = F&uuml;rnkranz\u000a|first      = Johannes\u000a|coauthors  = H&uuml;llermeier, Eyke\u000a|year       = 2003\u000a|title      = Pairwise Preference Learning and Ranking\u000a|journal    = Proceedings of the 14th European Conference on Machine Learning\u000a|pages      = 145\u2013156\u000a}}</ref>\u000a\u000a<ref name="COHE98">{{\u000acite journal\u000a|last       = Cohen\u000a|first      = William W.\u000a|coauthors  = Schapire, Robert E.; Singer, Yoram\u000a|year       = 1998\u000a|title      = Learning to order things\u000a|url        = http://dl.acm.org/citation.cfm?id=302528.302736\u000a|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems\u000a|pages      = 451\u2013457\u000a}}</ref>\u000a\u000a<ref name="LIU09">{{\u000acite journal\u000a|last       = Liu\u000a|first      = Tie-Yan\u000a|year       = 2009\u000a|title      = Learning to Rank for Information Retrieval\u000a|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304\u000a|journal    = Foundations and Trends in Information Retrieval\u000a|volume     = 3\u000a|issue      = 3\u000a|pages      = 225\u2013331\u000a|doi        = 10.1561/1500000016\u000a}}</ref>\u000a\u000a<ref name="GEMM09">{{\u000acite journal\u000a|last       = Gemmis\u000a|first      = Marco De\u000a|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni \u000a|year       = 2009\u000a|title      = Preference Learning in Recommender Systems\u000a|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45\u000a|journal    = PREFERENCE LEARNING\u000a|volume     = 41\u000a|pages      = 387\u2013407\u000a|doi=10.1007/978-3-642-14125-6_18\u000a}}</ref>\u000a\u000a}}\u000a\u000a==External links==\u000a*[http://www.preference-learning.org/ Preference Learning site]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]
p141
asI130
(lp142
VQuestion answering
p143
aV{{other uses|question|answer}}\u000a{{multiple issues|\u000a{{cleanup|date=January 2012|reason=extensive use of jargon to define jargon, and inconsistent use of bold and italics font styles}}\u000a{{cleanup-rewrite|date=January 2012}}\u000a{{more footnotes|date=February 2014}}\u000a}}\u000a\u000a'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].\u000a\u000aA QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011  \u000a</ref>\u000a\u000aSome examples of natural language document collections used for QA systems include:\u000a* a local collection of reference texts\u000a\u000a* internal organization documents and web pages\u000a* compiled [[newswire]] reports\u000a* a set of [[Wikipedia]] pages\u000a* a subset of [[World Wide Web]] pages\u000a\u000aQA research attempts to deal with a wide range of question types including: fact, list, definition, ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.\u000a\u000a* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer\u2019s Disease. CLEF 2012 Evaluation Labs and Workshop. September 17 2012</ref>\u000a* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\u000a\u000a==History==\u000a\u000aTwo early QA systems were BASEBALL and LUNAR.{{when|date=November 2012}}{{who|date=November 2012}}{{citation needed|date=November 2012}} BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.\u000a\u000a[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\u000a\u000aIn the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these [[expert system]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.\u000a\u000aThe 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\u000a\u000aRecently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.\u000a\u000a==Architecture==\u000aMost modern QA systems use [[natural language]] text documents as their underlying knowledge source.  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted. An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge. However, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates...) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.\u000a\u000aIn an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s. It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system. Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors. An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.\u000a\u000aCurrent QA systems<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> typically include a '''question classifier''' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a '''document retrieval module''' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a '''filter''' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is "Who invented\u000aPenicillin" the filter returns text that contain names of people. Finally, an '''answer extraction''' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.\u000a\u000aA '''multiagent''' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The meta\u2013agent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124\u000a}}</ref>\u000a\u000a==Question answering methods==\u000aQA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:\u000a# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.\u000a# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\u000a\u000aQuestion answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=http://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].\u000a\u000a===Open domain question answering===\u000aIn [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user\u2019s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.\u000a\u000aThe system takes a [[natural language]] question as an input rather than a set of keywords, for example, \u201cWhen is the national day of China?\u201d The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\u000a\u000aKeyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. \u201cWho\u201d, \u201cWhere\u201d or \u201cHow many\u201d, these words tell the system that the answers should be of type \u201cPerson\u201d, \u201cLocation\u201d, \u201cNumber\u201d respectively. In the example above, the word \u201cWhen\u201d indicates that the answer should be of type \u201cDate\u201d. POS tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is \u201cChinese National Day\u201d, the predicate is \u201cis\u201d and the adverbial modifier is \u201cwhen\u201d, therefore the answer type is \u201cDate\u201d. Unfortunately, some interrogative words like \u201cWhich\u201d, \u201cWhat\u201d or \u201cHow\u201d do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.\u000a\u000aOnce the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as \u201cWho\u201d or \u201cWhere\u201d, a Named Entity Recogniser is used to find relevant \u201cPerson\u201d and \u201cLocation\u201d names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\u000a\u000aA [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is \u201c1st Oct.\u201d\u000a\u000a==Issues==\u000aIn 2002 a group of researchers wrote a roadmap of research in question answering.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R. [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues, Tasks and Program Structures to Roadmap Research in Question Answering (QA)].</ref> The following\u000aissues were identified.<!-- much of the text in this section is copied and pasted from the "roadmap" document; somebody may try and simplify the text -->\u000a\u000a;Question classes : Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}}\u000a\u000a;Question processing : The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.\u000a\u000a;Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.)\u000a\u000a;Data sources for QA : Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained.\u000a\u000a;Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}}\u000a\u000a;Answer formulation : The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.\u000a\u000a;Real time question answering : There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.\u000a\u000a;Multilingual (or cross-lingual) question answering : The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].)\u000a\u000a;Interactive QA : It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions.<ref>Perera, R. and Nand, P. 2014. [http://link.springer.com/chapter/10.1007%2F978-3-319-11716-4_11 Interaction History Based Answer Formulation for Question Answering.]</ref> (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.)\u000a\u000a;Advanced reasoning for QA : More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system.\u000a\u000a;Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process.<ref>Perera, R. 2012. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6305919&isnumber=6305918 IPedagogy: Question Answering System Based on Web Information Clustering.]</ref>\u000a\u000a;User profiling for QA : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}}\u000a\u000a==Progress==\u000aQA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:\u000a\u000a* interactivity\u2014clarification of questions or answers\u000a* answer reuse or caching\u000a* knowledge representation and reasoning\u000a* social media analysis with QA systems\u000a* [[sentiment analysis]]<ref>[http://totalgood.com/bitcrawl/ BitCrawl] by Hobson Lane</ref>\u000a* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://www.aclweb.org/anthology/W12-6004 Towards a thematic role based target identification model for question answering.]</ref>\u000a* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference | author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430\u2013437 | publisher=Springer Berlin Heidelberg | title= [http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 The impact of semantic class identification and semantic role labeling on natural language answer extraction]}}</ref>\u000a* utilization of linguistic resources,<ref>{{cite journal |author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma|title=[http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false The impact of frame semantic annotation levels, frame\u2010alignment techniques, and fusion methods on factoid answer processing] | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247\u2013263 |year =2009}}</ref> such as [[WordNet]], [[FrameNet]], and the similar\u000a\u000aIBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.\u000a<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>\u000a\u000a==References==\u000a* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\u000a* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\u000a*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}\u000a* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.\u000a\u000a<references/>\u000a\u000a==External links==\u000a* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]\u000a* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]\u000a* [http://celct.fbk.eu/QA4MRE/ Question Answering Evaluation at CLEF]\u000a\u000a{{Computable knowledge}}\u000a{{Natural Language Processing}}\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p144
asI131
(lp145
VNatural language user interface
p146
aV'''Natural Language User Interfaces''' (LUI or NLUI) are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\u000a\u000aIn [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.<ref>Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.</ref>\u000aNatural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general Natural language interface is one of the active goals of the [[Semantic Web]].\u000a\u000aText interfaces are 'natural' to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a 'shallow' Natural language user interface.\u000a\u000a==Overview==\u000aA natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.\u000a\u000a==History==\u000a\u000aPrototype Nl interfaces had already appeared in the late sixties and early seventies.<ref name="edin">Natural Language Interfaces to Databases \u2013 An Introduction,\u000aI. Androutsopoulos,\u000aG.D. Ritchie,\u000aP. Thanisch,\u000aDepartment of Artificial Intelligence, University of Edinburgh</ref>\u000a\u000a*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"\u000a*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].\u000a*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]</ref>\u000a*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]</ref>\u000a* ''Janus'' is also one of the few systems to support temporal questions.\u000a* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).\u000a* BBN\u2019s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.\u000a* [[IBM]] ''Languageaccess''\u000a* [[Q&A (software)|Q&A]] from [[Symantec]].\u000a* ''Datatalker'' from Natural Language Inc.\u000a* ''Loqui''  from [[Bim]].\u000a* ''English Wizard'' from [[Linguistic Technology Corporation]].\u000a* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001<ref>{{cite book | last = Galitsky\u000a | first = Boris\u000a | title = Natural Language Question Answering: technique of semantic headers\u000a | publisher = Advance Knowledge International\u000a | date = 2003\u000a | location = Adelaide, Australia\u000a | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799\u000a | isbn = 0868039799\u000a  }}</ref>\u000a\u000a==Challenges==\u000aNatural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.\u000a\u000aA [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases \u2013 An Introduction', describes some challenges:<ref name="edin"/>\u000a* ''Modifier attachment''\u000aThe request \u201cList all employees in the company with a driving licence\u201d is ambiguous unless you know companies can't have drivers licences.\u000a\u000a* ''Conjunction and disjunction''\u000a\u201cList all applicants who live in California and Arizona\u201d is ambiguous unless you know that a person can't live in two places at once.\u000a* ''[[Anaphora resolution]]''\u000a- resolve what a user means by 'he', 'she' or 'it', in a self-referential query.\u000a\u000aOther goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market.\u000a\u000aFinally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.\u000a\u000a==Uses and applications==\u000a\u000aThe natural language interface gives rise to technology used for many different applications. \u000a\u000aSome of the main uses are:\u000a\u000a* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.\u000a* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.\u000a* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.\u000a* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.\u000a* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.\u000a* ''Embedded applications'', some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].\u000a\u000aBelow are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.\u000a\u000a===Ubiquity===\u000a{{main|Ubiquity (Firefox)}}\u000aUbiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.\u000a\u000a===Wolfram Alpha===\u000a{{main|Wolfram Alpha}}\u000aWolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.<ref>{{cite news |url=http://www.guardian.co.uk/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}</ref> It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.<ref name="launch date">{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}</ref>\u000a\u000a===Siri===\u000a{{main|Siri (software)}}\u000aSiri is a [[personal assistant]] application for the operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations. The iPhone app is the first public product by its makers, who are focused on [[artificial intelligence]] applications.\u000a\u000aSiri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.<ref>[http://www.apple.com/iphone/features/siri.html Siri webpage]</ref>\u000a\u000a===Others===\u000a* [[Anboto Group]] provides Web customer service and e-commerce technology based on semantics and natural language processing. The main offer of [http://www.anbotogroup.com/en/index.php Anboto Group] are the virtual sales agent and intelligent chat.\u000a* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.\u000a* [[Braina]]<ref>[http://www.brainasoft.com/braina/ Braina]</ref> - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.\u000a* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.\u000a* C-Phrase<ref>[http://code.google.com/p/c-phrase/ C-Phrase]</ref> - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English. C-Phrase is hosted on [[Google Code]] site.\u000a* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.\u000a[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]\u000a* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).<ref>Ubuntu 10.04 Add/Remove Applications description for GNOME Do</ref>\u000a* [[Invention Machine]] Goldfire - powered by a semantic research engine that has the capability to transform unstructured documents from various electronic sources into an index that, when searched, delivers answers to research questions. Goldfire\u2019s Natural Language query interface enables the user to put a question in a free text format, which would be the same format as if the question were given to another person. And, once knowledge has been retrieved, Goldfire presents the results in a way that makes their meaning readily apparent.\u000a* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.\u000a* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.\u000a* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.\u000a* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit'.\u000a* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.\u000a* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.\u000a* [[Powerset (company)|Powerset]] \u2014 On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.<ref>{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}</ref> On July 1, 2008, it was purchased by [[Microsoft]].<ref>{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=http://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}</ref>\u000a* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company\u2019s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011\u000a* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.\u000a* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).\u000a* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.<ref>Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.</ref>\u000a\u000a==See also==\u000a*[[Natural language programming]]\u000a**[[xTalk]], a family of English-like programming languages\u000a*[[Chatterbot]], a computer program that simulates human conversations\u000a*[[Noisy text]]\u000a*[[Question answering]]\u000a*[[Selection-based search]]\u000a*[[Semantic search]]\u000a*[[Semantic Web]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a{{Computable knowledge}}\u000a\u000a{{DEFAULTSORT:Natural language user interface}}\u000a[[Category:User interfaces]]\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p147
asI135
(lp148
VVocabulary mismatch
p149
aV'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.\u000a\u000aFurnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].\u000a\u000aThe vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\u000a\u000a== Techniques that solve mismatch ==\u000aZhao provided a survey of common techniques that can solve mismatch in the dissertation on term mismatch.<ref>Zhao, L., Modeling and Solving Term Mismatch in Full-text Retrieval, PhD Dissertation, Carnegie Mellon University, 2012. [http://www.cs.cmu.edu/~lezhao/thesis/diss-Le.pdf URL] retrieved 9/3/2012.</ref>\u000a\u000a===Stemming===\u000a\u000a===Full-text indexing versus only indexing keywords or abstracts===\u000a\u000a===Usages of inlink anchor text or other social tagging===\u000a\u000a===Query expansion===\u000aA recent study by Zhao and Callan (2012)<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].\u000a\u000aA wiki website called [http://www.wikiquery.org WikiQuery] has been developed by one of the authors of the above study, which helps users create, store and share effective Conjunctive normal form queries.\u000a\u000a===Translation based models===\u000a\u000a== References ==\u000a\u000a{{Reflist}}\u000a\u000a[[language code:Title]]\u000a\u000a[[Category:Linguistic research]]\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]
p150
asI139
(lp151
VCategory:Personalized search
p152
aV{{catmore}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]
p153
asI140
(lp154
VSørensen\u2013Dice coefficient
p155
aVThe '''Sørensen\u2013Dice index''', also known by other names (see Names, below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]<ref>{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1\u201334 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297\u2013302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.\u000a\u000a==Name==\u000aThe index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the \u2013sen ending.\u000a\u000aOther names include:\u000a*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index<ref name ="gallagher"/>\u000a\u000a==Quantitative version==\u000aThe expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:\u000a* Quantitative Sørensen\u2013Dice index<ref name ="gallagher"/>\u000a* Quantitative Sørensen index<ref name ="gallagher"/>\u000a* Quantitative Dice index<ref name ="gallagher"/>\u000a* [[Bray Curtis dissimilarity|Bray-Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')<ref name ="gallagher"/>\u000a* [[Jan Czekanowski|Czekanowski]]'s quantitative index<ref name ="gallagher"/>\u000a* Steinhaus index<ref name ="gallagher"/>\u000a* [[E. C. Pielou|Pielou]]'s percentage similarity<ref name ="gallagher"/>\u000a* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1948 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326\u2013349 |doi=10.2307/1942268 }}</ref>\u000a\u000a==Formula==\u000aSørensen's original formula was intended to be applied to presence/absence data, and is\u000a\u000a:<math> QS = \u005cfrac{2C}{A + B} = \u005cfrac{2 |A \u005ccap B|}{|A| + |B|}</math>\u000a\u000awhere ''A'' and ''B'' are the number of species in samples A and B, respectively, and ''C'' is the number of species shared by the two samples; QS is the quotient of similarity and ranges between 0 and 1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref>\u000a\u000aIt can be viewed as a similarity measure over sets:\u000a\u000a:<math>s = \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000aSimilarly to Jaccard, the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':\u000a\u000a<math>s_v = \u005cfrac{2 | A \u005ccdot B |}{| A |^2 + | B |^2} </math>\u000a\u000awhich gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.\u000a\u000aFor sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979\u000a|title=Information Retrieval\u000a|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>\u000a\u000aWhen taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003\u000a|title=Cognates Can Improve Statistical Translation Models\u000a|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics\u000a|pages=46\u201348 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>\u000a\u000a:<math>s = \u005cfrac{2 n_t}{n_x + n_y}</math>\u000a\u000awhere ''n''<sub>''t''</sub> is the number of character bigrams found in both strings, ''n''<sub>''x''</sub> is the number of bigrams in string ''x'' and ''n''<sub>''y''</sub> is the number of bigrams in string ''y''. For example, to calculate the similarity between:\u000a\u000a:<code>night</code>\u000a:<code>nacht</code>\u000a\u000aWe would find the set of bigrams in each word:\u000a:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}\u000a:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}\u000a\u000aEach set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.\u000a\u000aInserting these numbers into the formula, we calculate, ''s''&nbsp;=&nbsp;(2&nbsp;·&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.\u000a\u000a==Difference from Jaccard ==\u000aThis coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>\u000a\u000aThe function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function\u000a\u000a:<math>d = 1 -  \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000ais not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.\u000a\u000a==Applications==\u000aThe Sørensen\u2013Dice coefficient is mainly useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409\u2013416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123\u2013131.]</ref>). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref>\u000a\u000a==See also==\u000a* [[Correlation]]\u000a* [[Czekanowski similarity index]]\u000a* [[Jaccard index]]\u000a* [[Hamming distance]]\u000a* [[Horn\u2019s index]]\u000a* [[Hurlbert\u2019s index]]\u000a* [[Kulczy\u0144ski similarity index]]\u000a* [[Pianka's index]]\u000a* [[MacArthur and Levin's index]]\u000a* [[Mantel test]]\u000a* [[Morisita's overlap index]]\u000a* [[Most frequent k characters]]\u000a* [[Overlap coefficient]]\u000a* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])\u000a* [[Simplified Morisita\u2019s index]]\u000a* [[Tversky index]]\u000a* [[Universal adaptive strategy theory (UAST)]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/DiceSorensenMetric.scala Dice / Sorensen] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a{{DEFAULTSORT:Sorensen-Dice coefficient}}\u000a[[Category:Information retrieval]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p156
asI143
(lp157
VArtificial Solutions
p158
aV{{Infobox company\u000a|name= Artificial Solutions\u000a|logo=[[Image:Artificial Solutions Logo.png]]\u000a|type=[[Private company]]\u000a|foundation=(2001)\u000a|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström \u000a|location=[[Stockholm]], [[Sweden]]\u000a|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]] and [[Stockholm]] \u000a|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], \u000a|products= Teneo platform\u000a|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]\u000a}}\u000a\u000a'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>\u000a\u000a==History==\u000aArtificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>\u000a\u000aThe company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>\u000a\u000aIn 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>\u000a[[Elbot]], Artificial Solutions\u2019 test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>\u000a\u000aWith a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>\u000aIn 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>\u000aA new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>\u000a\u000aIn February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==External links==\u000a*[http://www.hello-indigo.com Indigo]\u000a*[http://www.elbot.com Elbot]\u000a\u000a[[Category:Natural language processing software]]\u000a[[Category:Intelligent software assistants]]\u000a[[Category:User interfaces]]\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p159
asI144
(lp160
VInformation retrieval
p161
aV{{Information science}}\u000a\u000a'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[metadata]] or on [[Full text search|full-text]] (or other content-based) indexing.\u000a\u000aAutomated information retrieval systems are used to reduce what has been called "[[information overload]]". Many universities and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].\u000a\u000a== Overview ==\u000a\u000aAn information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[relevance|relevancy]].\u000a\u000aAn object is an entity that is represented by information in a [[database]]. User queries are matched against the database information. Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite journal |first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |contribution=Information Retrieval On Mind Maps - What Could It Be Good For? |contribution-url=http://www.sciplore.org/publications_en.php |title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\u000a\u000aMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>\u000a\u000a== History ==\u000a{{Rquote|right|But do you know that, although I have kept the diary [on a phonograph] for months past, it never once struck me how I was going to find any particular part of it in case I wanted to look it up?|[[John Seward|Dr Seward]]| [[Bram Stoker]]'s ''[[Dracula]]'',\u000a 1897}}\u000aThe idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering |volume=24 |issue=4 |pages=35\u201343 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\u000a\u000aIn 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.\u000a\u000a== Model types ==\u000a[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]]\u000aFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\u000a\u000a=== First dimension: mathematical basis ===\u000a* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\u000a** [[Standard Boolean model]]\u000a** [[Extended Boolean model]]\u000a** [[Fuzzy retrieval]]\u000a* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\u000a** [[Vector space model]]\u000a** [[Generalized vector space model]]\u000a** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]\u000a** [[Extended Boolean model]]\u000a** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]\u000a* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.\u000a** [[Binary Independence Model]]\u000a** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function\u000a** [[Uncertain inference]]\u000a** [[Language model]]s\u000a** [[Divergence-from-randomness model]]\u000a** [[Latent Dirichlet allocation]]\u000a* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just a yet another feature.\u000a\u000a=== Second dimension: properties of the model ===\u000a* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.\u000a* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.\u000a* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\u000a\u000a== Performance and correctness measures ==\u000a{{main|Precision and recall}}\u000a\u000aMany different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be [[ill-posed]] and there may be different shades of relevancy.\u000a\u000a=== Precision ===\u000a\u000aPrecision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.\u000a\u000a:<math> \u005cmbox{precision}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{retrieved documents}\u005c}|} </math>\u000a\u000aIn [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.\u000a\u000aNote that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].\u000a\u000a=== Recall ===\u000a\u000aRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\u000a\u000a:<math>\u005cmbox{recall}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{relevant documents}\u005c}|} </math>\u000a\u000aIn binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.\u000a\u000aIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\u000a\u000a=== Fall-out ===\u000aThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\u000a\u000a:<math> \u005cmbox{fall-out}=\u005cfrac{|\u005c{\u005cmbox{non-relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{non-relevant documents}\u005c}|} </math>\u000a\u000aIn binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\u005cmbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.\u000a\u000aIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\u000a\u000a=== F-measure ===\u000a{{main|F-score}}\u000aThe weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:\u000a\u000a:<math>F = \u005cfrac{2 \u005ccdot \u005cmathrm{precision} \u005ccdot \u005cmathrm{recall}}{(\u005cmathrm{precision} + \u005cmathrm{recall})}.\u005c,</math>\u000a\u000aThis is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\u000a\u000aThe general formula for non-negative real <math>\u005cbeta</math> is:\u000a:<math>F_\u005cbeta = \u005cfrac{(1 + \u005cbeta^2) \u005ccdot (\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall})}{(\u005cbeta^2 \u005ccdot \u005cmathrm{precision} + \u005cmathrm{recall})}\u005c,</math>.\u000a\u000aTwo other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.\u000a\u000aThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\u005cbeta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\u005cbeta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \u005cfrac{1}{\u005cfrac{\u005calpha}{P} + \u005cfrac{1-\u005calpha}{R}}</math>.  Their relationship is <math>F_\u005cbeta = 1 - E</math> where <math>\u005calpha=\u005cfrac{1}{1 + \u005cbeta^2}</math>.\u000a\u000a=== Average precision ===\u000a<!-- [[Average precision]] redirects here -->\u000aPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |contribution=Recall, Precision and Average Precision |contribution-url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>\u000a:<math>\u005coperatorname{AveP} = \u005cint_0^1 p(r)dr</math>\u000aThat is the area under the precision-recall curve.\u000aThis integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\u000a:<math>\u005coperatorname{AveP} = \u005csum_{k=1}^n P(k) \u005cDelta r(k)</math>\u000awhere <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\u005cDelta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />\u000a\u000aThis finite sum is equivalent to:\u000a:<math> \u005coperatorname{AveP} = \u005cfrac{\u005csum_{k=1}^n (P(k) \u005ctimes \u005coperatorname{rel}(k))}{\u005cmbox{number of relevant documents}} \u005c!</math>\u000awhere <math>\u005coperatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06\u201311, 2006) |publisher=ACM |location=New York, NY |pages=11\u201318 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\u000a\u000aSome authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303\u2013338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />\u000a:<math>\u005coperatorname{AveP} = \u005cfrac{1}{11} \u005csum_{r \u005cin \u005c{0, 0.1, \u005cldots, 1.0\u005c}} p_{\u005coperatorname{interp}}(r)</math>\u000awhere <math>p_{\u005coperatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:\u000a:<math>p_{\u005coperatorname{interp}}(r) = \u005coperatorname{max}_{\u005ctilde{r}:\u005ctilde{r} \u005cgeq r} p(\u005ctilde{r})</math>.\u000a\u000aAn alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>\u000a\u000a=== R-Precision ===\u000a\u000aPrecision at '''R'''-th position in the ranking of results for a query that has '''R''' relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the '''R'''-th position.\u000a\u000a=== Mean average precision ===\u000a<!-- [[Mean average precision]] redirects here -->\u000aMean average precision for a set of queries is the mean of the average precision scores for each query.\u000a:<math> \u005coperatorname{MAP} = \u005cfrac{\u005csum_{q=1}^Q \u005coperatorname{AveP(q)}}{Q} \u005c!</math>\u000awhere ''Q'' is the number of queries.\u000a\u000a=== Discounted cumulative gain ===\u000a{{main|Discounted cumulative gain}}\u000aDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\u000a\u000aThe DCG accumulated at a particular rank position <math>p</math> is defined as:\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = rel_{1} + \u005csum_{i=2}^{p} \u005cfrac{rel_{i}}{\u005clog_{2}i}. </math>\u000a\u000aSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:\u000a\u000a:<math> \u005cmathrm{nDCG_{p}} = \u005cfrac{DCG_{p}}{IDCG{p}}. </math>\u000a\u000aThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\u000a\u000a=== Other Measures ===\u000a* [[Mean reciprocal rank]]\u000a* [[Spearman's rank correlation coefficient]]\u000a\u000a=== Timeline ===\u000a\u000a* Before the '''1900s'''\u000a*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.\u000a*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\u000a*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.\u000a* '''1920s-1930s'''\u000a*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine\u201d a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\u000a* '''1940s\u20131950s'''\u000a*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\u000a*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.\u000a*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\u000a*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).\u000a*: '''1950''': The term "information retrieval" appears to have been coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; ''Theory Digital Handling Non-numerical Information'' (Zator Technical Bulletin No. 48) 5, cited in "information, n.". OED Online. December 2011. Oxford University Press.</ref>\u000a*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref>\u000a*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.\u000a*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)\u000a*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."\u000a* '''1960s''':\u000a*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.\u000a*: '''1960''': [[Melvin Earl Maron]] and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971\u2013972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216\u2013244, July 1960.\u000a*: '''1962''':\u000a*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\u000a*:* Kent published ''Information Analysis and Retrieval''.\u000a*: '''1963''':\u000a*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].\u000a*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).\u000a*: '''1964''':\u000a*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.\u000a*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.\u000a*:'''mid-1960s''':\u000a*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\u000a*::* Project Intrex at MIT.\u000a*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.\u000a*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.\u000a*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\u000a*:: '''1968''':\u000a*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.\u000a*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.\u000a*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\u000a* '''1970s'''\u000a*: '''early 1970s''':\u000a*::* First online systems\u2014NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\u000a*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.\u000a*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."<ref>{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217\u2013240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}</ref>\u000a*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\u000a*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)\u000a*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)\u000a*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)\u000a*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.\u000a*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.\u000a* '''1980s'''\u000a*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\u000a*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\u000a*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.\u000a*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\u000a*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.\u000a*:: '''1985\u20131993''': Key papers on and experimental systems for visualization interfaces.\u000a*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.\u000a*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].\u000a* '''1990s'''\u000a*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.\u000a*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems.\u000a*: '''late 1990s''': Web [[Web search engine|search engines]] implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\u000a\u000a== Awards in the field ==\u000a\u000a* [[Tony Kent Strix award]]\u000a* [[Gerard Salton Award]]\u000a\u000a==See also==\u000a\u000a{{col-begin}}\u000a{{col-break}}\u000a\u000a* [[Adversarial information retrieval]]\u000a* [[Collaborative information seeking]]\u000a* [[Controlled vocabulary]]\u000a* [[Cross-language information retrieval]]\u000a* [[Data mining]]\u000a* [[European Summer School in Information Retrieval]]\u000a* [[Human\u2013computer information retrieval]]\u000a* [[Information extraction]]\u000a* [[Information Retrieval Facility]]\u000a* [[Knowledge visualization]]\u000a* [[Multimedia Information Retrieval]]\u000a* [[List of information retrieval libraries]]\u000a{{col-break}}\u000a* [[Personal information management]]\u000a* [[Relevance (Information Retrieval)]]\u000a* [[Relevance feedback]]\u000a* [[Rocchio Classification]]\u000a* [[Index (search engine)|Search index]]\u000a* [[Social information seeking]]\u000a* [[Special Interest Group on Information Retrieval]]\u000a* [[Structured Search]]\u000a* [[Subject indexing]]\u000a* [[Temporal information retrieval]]\u000a* [[Tf-idf]]\u000a* [[XML-Retrieval]]\u000a* Key-objects\u000a\u000a{{col-end}}\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{wikiquote}}\u000a* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]\u000a* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]\u000a* [http://trec.nist.gov Text Retrieval Conference (TREC)]\u000a* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]\u000a* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]\u000a* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]\u000a* [http://ir-facility.org/ Information Retrieval Facility]\u000a* [http://www.nonrelevant.net Information Retrieval @ DUTH]\u000a* [http://nlp.stanford.edu/IR-book/ Introduction to Information Retrieval (online book) by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press. 2008.  ]\u000a\u000a{{DEFAULTSORT:Information Retrieval}}\u000a[[Category:Articles with inconsistent citation formats]]\u000a[[Category:Information retrieval| ]]\u000a[[Category:Natural language processing]]
p162
asI148
(lp163
VGerard Salton Award
p164
aVThe '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].\u000a\u000a==Chronological honorees and lectures==\u000a* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."\u000a* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : "A look back and a look forward."\u000a* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."\u000a* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"\u000a* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." \u000a* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''\u000a* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''\u000a* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."\u000a* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."\u000a* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."\u000a\u000a==External links==\u000a* [http://www.acm.org/sigir/ ACM SIGIR homepage]\u000a* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]\u000a\u000a[[Category:Association for Computing Machinery]]\u000a[[Category:Computer science awards]]\u000a[[Category:Information retrieval]]
p165
asI153
(lp166
VSwiftype
p167
aV{{Infobox company\u000a|name             = Swiftype\u000a|logo             =Black_Swiftype_Logo.png \u000a|type             = [[Privately held company|Private]]\u000a|industry         = [[Software]] <br/> [[Information Technology]] <br/> [[Search Engines]]\u000a|area_served      = Worldwide\u000a|location_city    = [[San Francisco, California|San Francisco]], [[California (state)|California]]\u000a|location_country = U.S.\u000a|founders       = {{unbulleted list|Matt Riley, Quin Hoxie}}\u000a|key_people       = {{unbulleted list|Matt Riley (CEO), Quin Hoxie (CTO)}}\u000a|services         = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}\u000a|genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]]\u000a|num_employees    = 25\u000a|foundation       = [[San Francisco, California|San Francisco]], [[California (state)|California]], [[U.S.A]] [[January 2012]]\u000a|homepage         = {{URL|https://www.swiftype.com/|Swiftype.com}}\u000a|intl             = yes\u000a|footnotes             = {{unbulleted list|[http://www.crunchbase.com/organization/swiftype Crunchbase] [http://www.Swiftype.com Official Website]}}\u000a|alt = Black text and red icon edition of the full Swiftype logo|products = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}}}\u000a\u000a'''Swiftype''' is a company that sells [[search engines]] for websites and mobile applications (also known as [[enterprise search]]) and creates a [[PageRank]] specific to individual websites and mobile applications.<ref name="TechCrunch-Ha">{{cite news|last1=Ha|first1=Anthony|title=Y Combinator-Backed Swiftype Builds Site Search That Doesn\u2019t Suck|url=http://techcrunch.com/2012/05/08/swiftype-launch/|accessdate=21 July 2014|publisher=TechCrunch|date=May 8, 2012|ref=TechCrunch-Ha}}</ref><ref name=BetaBeat>{{cite news|last1=Roy|first1=Jessica|title=Can This Y Combinator Startup Solve the Site Search Problem?|url=http://betabeat.com/2012/05/can-this-y-combinator-startup-solve-the-site-search-problem/|accessdate=21 July 2014|publisher=BetaBeat|date=July 21, 2014|ref=BetaBeat}}</ref><ref name="Crunchbase">{{cite web|url=http://www.crunchbase.com/organization/swiftype|website=Crunchbase|accessdate=19 July 2014|title = <nowiki>Swiftype | CrunchBase</nowiki>}}</ref><ref name=VatorNews>{{Cite news|url = http://vator.tv/news/2013-08-15-swiftype-bags-17m-from-big-names-for-better-search|title = Swiftype bags $1.7M from big names for better search|last = Marino|first = Faith|date = August 15, 2013|work = |accessdate = July 21, 2014|ref = VatorNews|publisher = VatorNews}}</ref>  The company is based in [[San Francisco, CA]] and is funded mainly through [[venture capital]].<ref name=Crunchbase />\u000a\u000a==History==\u000aSwiftype was founded in 2012 by former [[Scribd]] engineers Matt Riley and Quin Hoxie.<ref name=Crunchbase /> The two met while working on an internal search tool for [[Scribd]].<ref name="TechCrunch-Ha" /><ref name="BetaBeat" /><ref name=Forbes>{{cite news|last1=Casserly|first1=Meghan|title=Site Search (Should Be) Sexy: How Swiftype Raised $1.7M In Seed Funding From SV Bigwigs|url=http://www.forbes.com/sites/meghancasserly/2013/08/15/site-search-should-be-sexy-how-swiftype-raised-1-7-in-seed-funding-from-sv-bigwigs/|accessdate=19 July 2014|publisher=Forbes|ref = Forbes|date=2013-08-15}}</ref> Swiftype participated in [[Y Combinator (company)|Y Combinator]] in 2012 and received investment from a number of prominent sources.<ref name=VentureBeat>{{Cite news|url = http://venturebeat.com/2013/08/15/yc-startup-swiftype-raises-1-7m-seed-round-from-andreessen-nea-kleiner/|title = YC startup Swiftype raises $1.7M seed round from Andreessen; NEA; Kleiner|last = Grant|first = Rebecca|date = August 15, 2013|work = |accessdate = July 21, 2014|publisher = VentureBeat|ref = VentureBeat}}</ref><ref name=VatorNews /><ref name="TechCrunch-Yang">{{cite news|last1=Yang|first1=Anthony|title=Site Search Engine Creator Swiftype Raises $1.7M From A16Z, Others|url=http://techcrunch.com/2013/08/15/swiftype-1-7m/|accessdate=19 July 2014|publisher=TechCrunch|date=2013-08-15|ref = TechCrunch-Yang}}</ref><ref name=AllThingsD>{{cite news|last1=Gannes|first1=Liz|title=Swiftype Raises $1.7M for Smarter Site Search|url=http://allthingsd.com/20130815/swiftype-raises-1-7m-for-site-search/|accessdate=19 July 2014|publisher=All Things D|ref = AllThingsD|date=2013-08-15}}</ref> In September 2013, the company obtained [[Series A]] funding.<ref name=Crunchbase /><ref name=VatorNews /><ref name=VentureBeat /><ref name=TechCrunch-Yang /><ref name=AllThingsD /><ref name=StartUpBeatBeat>{{Cite news|url = http://startupbeat.com/2013/08/26/swiftype-wants-to-dramatically-improve-search-on-websites-and-mobile-apps-of-all-types-and-sizes-id3402/|title = Swiftype wants to dramatically improve search on websites and mobile apps of all types and sizes|last = Editor|first = |date = August 26, 2013|work = |accessdate = July 21, 2014|publisher = StartUpBeat|ref = StartUpBeat}}</ref>\u000a\u000aAs of August 2013, Swiftype had over 70,000 websites using their search bar, powering over 130 million queries per month.<ref name=Forbes /><ref name=AllThingsD />\u000a\u000a==Features==\u000aSwiftype is available as an [[API]] or [[web crawler]] based engine.<ref name=TechCrunch-Yang />  The company also offers a VIP-approved [[WordPress]] Plugin, a [[Shopify]] App, and a [[Magento]] extension.<ref>{{Cite web|url = https://apps.shopify.com/swiftype|title = Autocomplete & Site Search by Swiftype \u2013 Ecommerce Plugins for Online Stores \u2013 Shopify App Store|date = 2014-09-26|accessdate = 2014-09-26|website = Shopify App Store|publisher = Shopify|last = |first = }}</ref><ref>{{Cite web|url = http://www.magentocommerce.com/magento-connect/modern-site-search-by-swiftype.html|title = Modern Site Search by Swiftype - Magento Connect|date = 2014-09-26|accessdate = 2014-09-26|website = Magento Connect|publisher = Magento|last = |first = }}</ref><ref>{{Cite web|url = https://wordpress.org/plugins/swiftype-search/|title = <nowiki>WordPress | Swiftype Search | WordPress Plugins</nowiki>|date = 2014-09-26|accessdate = 2014-09-26|website = WordPress Plugin Directory|publisher = WordPress|last = |first = }}</ref> Swiftype sells [[eCommerce]] search, [[enterprise search]], [[faceted search]], [[full text search]], [[enterprise search]], [[real-time search]], [[concept search]], and website [[search engines]] for websites and mobile applications.<ref name=Crunchbase /><ref name=VatorNews /> The company's paid plans offer on demand and live recrawls and indexing of websites.<ref name=AllThingsD /> Other features include drag and drop result customization<ref name=Forbes /><ref name=VentureBeat /><ref name=AllThingsD /> and  real-time analytics.<ref name=TechCrunch-Ha /><ref name=Forbes />\u000a<!--Swiftype website lists several additional features that I've been unable to find neutral third party discussion of -->\u000a\u000a==Competitors==\u000a* [[Algolia]]<ref name=Crunchbase />\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[Search engines]]\u000a* [[Faceted search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Concept search]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* {{Official website|swiftype.com}}\u000a\u000a__FORCETOC__\u000a__INDEX__\u000a__NEWSECTIONLINK__\u000a\u000a[[Category:Search engine software]]\u000a[[Category:Companies established in 2012]]\u000a[[Category:Companies based in San Francisco, California]]\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies]]\u000a[[Category:Y Combinator companies]]\u000a[[Category:Semantic Web]]\u000a[[Category:Software startup companies]]\u000a[[Category:Online companies]]
p168
asI157
(lp169
VContextual searching
p170
aV'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>Susan E. Feldman. ''The Answer Machine'', Synthesis Lectures on Information Concepts, Retrieval, and Services. [http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023 http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023]</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>Pitokow, James; Hinrich Schütze; Todd Cass; Rob Cooley; Don Turnbull; Andy Edmonds; Eytan Adar; Thomas Breuel (2002). "Personalized search". [http://www.cond.org/p50-pitkow.pdf http://www.cond.org/p50-pitkow.pdf] Communications of the ACM (CACM) 45 (9): 50\u201355.</ref>\u000a\u000a== Basic Contextual Search ==\u000aThe basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add \u201cmenu\u201d to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.\u000a\u000a== Explicitly Supplied Context ==\u000aCertain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example a user looking for research papers can specify documents with \u201creferences\u201d or \u201cabstracts\u201d to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]</ref>\u000a\u000aExplicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>\u000a\u000a== Automatically Inferred Context ==\u000aThere are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.\u000a\u000aMajor search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>Eric J Glover, Steve Lawrence, Michael D. Gordon, William P. Birmingham, C. Lee Giles. ''Web Search - Your Way'', NEC Research Institution [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7499&rep=rep1&type=pdf]</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.\u000a\u000a=== Contextual Mobile Search ===\u000aThe drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref> Mobile start up company [http://everything.Me Everything.Me] is one company that is moving towards turning the smartphone into an all-in-one device the provides relevant information for specific users. Everything.Me pushes app updates and suggestion to a user's home-screen based on user movement, location, current time, past search queries, and entertainment preferences.<ref>[http://socialtimes.com/mobile-contextual-search-future_b149394 http://socialtimes.com/mobile-contextual-search-future_b149394], Contextual Search through Mobile and Everything.Me</ref> For example when a user opens their phone in the morning Everything.Me will present users with apps relevant to how that users interacts with their phone in the morning\u2014presenting weather apps, bus apps, and news apps.<ref>[http://everything.me/about/ Everything.Me]</ref> Later when that user goes to work Everything.Me will update the work related applications to be prioritized over other apps. Everything.Me anticipates a user's needs based on their current actions and past interactions on the web. This process of automatically obtaining context from mobile phones can help to increase the precision of user queries. For instance if a user searches for a place to eat while at work Everything.Me will take work into context and return restaurants that would be more appropriate for a lunch break at the office.<ref>[http://everything.me/ http://everything.me/], Video Information</ref>\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Contextual Searching}}\u000a[[Category:Internet search engines]]\u000a[[Category:Semantic Web]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p171
asI165
(lp172
VTf\u2013idf
p173
aV{{Lowercase|title=tf\u2013idf}}\u000a{{More footnotes|date=July 2012}}\u000a\u000a'''tf\u2013idf''', short for '''term frequency\u2013inverse document frequency''', is a numerical statistic that is intended to reflect how important a word is to a [[document]] in a collection or [[Text corpus|corpus]].<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref>{{rp|8}} It is often used as a weighting factor in [[information retrieval]] and [[text mining]].\u000aThe tf-idf value increases [[Proportionality (mathematics)|proportionally]] to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\u000a\u000aVariations of the tf\u2013idf weighting scheme are often used by [[search engine]]s as a central tool in scoring and ranking a document's [[Relevance (information retrieval)|relevance]] given a user [[Information retrieval|query]]. tf\u2013idf can be successfully used for [[stop-words]] filtering in various subject fields including [[automatic summarization|text summarization]] and classification.\u000a\u000aOne of the simplest [[ranking function]]s is computed by summing the tf\u2013idf for each query term; many more sophisticated ranking functions are variants of this simple model.\u000a\u000a==Motivation==\u000aSuppose we have a set of English text documents and wish to determine which document is most relevant to the query "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its ''term frequency''.\u000a\u000aHowever, because the term "the" is so common, this will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "brown" and "cow". Hence an ''inverse document frequency'' factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\u000a\u000a==Definition==\u000atf\u2013idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest choice is to use the ''raw frequency'' of a term in a document, i.e. the number of times that term ''t'' occurs in document ''d''. If we denote the raw frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is tf(''t'',''d'') = f(''t'',''d''). Other possibilities include<ref>{{cite doi|10.1017/CBO9780511809071.007}}</ref>{{rp|128}}\u000a\u000a* [[boolean data type|Boolean]] "frequencies": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise;\u000a* [[logarithm]]ically scaled frequency: tf(''t'',''d'') = 1 + log f(''t'',''d''), or zero if f(''t'', ''d'') is zero;\u000a* augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:\u000a:<math>\u005cmathrm{tf}(t,d) = 0.5 + \u005cfrac{0.5 \u005ctimes \u005cmathrm{f}(t, d)}{\u005cmax\u005c{\u005cmathrm{f}(w, d):w \u005cin d\u005c}}</math>\u000a\u000aThe '''inverse document frequency''' is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of [[documents]] by the number of documents containing the term, and then taking the logarithm of that [[quotient]].\u000a\u000a:<math> \u005cmathrm{idf}(t, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000awith\u000a\u000a* <math>N</math>: total number of documents in the corpus\u000a* <math> |\u005c{d \u005cin D: t \u005cin d\u005c}| </math> : number of documents where the term <math> t </math> appears (i.e., <math> \u005cmathrm{tf}(t,d) \u005cneq 0</math>). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <math>1 + |\u005c{d \u005cin D: t \u005cin d\u005c}|</math>.\u000a\u000aMathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.\u000a\u000aThen tf\u2013idf is calculated as\u000a\u000a:<math>\u005cmathrm{tfidf}(t,d,D) = \u005cmathrm{tf}(t,d) \u005ctimes \u005cmathrm{idf}(t, D)</math>\u000a\u000aA high weight in tf\u2013idf is reached by a high term [[frequency (statistics)|frequency]] (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.\u000a\u000a==Justification of idf==\u000aIdf was introduced, as "term specificity", by [[Karen Spärck Jones]] in a 1972 paper. Although it has worked well as a [[heuristic]], its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find [[information theory|information theoretic]] justifications for it.<ref name="understanding">{{cite doi|10.1108/00220410410560582}}</ref>\u000a\u000aSpärck Jones's own explanation didn't propose much theory, aside from a connection to [[Zipf's law]].<ref name="understanding"/> Attempts have been made to put idf on a [[probability theory|probabilistic]] footing,<ref>See also [http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html#p:justificationofidf Probability estimates in practice] in ''Introduction to Information Retrieval''.</ref> by estimating the probability that a given document {{mvar|d}} contains a term {{mvar|t}} as\u000a\u000a<math>\u000aP(t|d) = \u005cfrac{|\u005c{d \u005cin D: t \u005cin d\u005c}|}{N}\u000a</math>\u000a\u000aso that we can define idf as\u000a\u000a<math>\u000a\u005cbegin{align}\u000a\u005cmathrm{idf} & = -\u005clog P(t|d) \u005c\u005c\u000a             & = \u005clog \u005cfrac{1}{P(t|d)} \u005c\u005c\u000a             & = \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}\u000a\u005cend{align}\u000a</math>\u000a\u000aThis probabilistic interpretation in turn takes the same form as that of [[self-information]]. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate [[event space]]s for the required [[probability distribution]]s: not only documents need to be taken into account, but also queries and terms.<ref name="understanding"/>\u000a\u000a==Example of tf\u2013idf==\u000aSuppose we have term frequency tables for a collection consisting of only two documents, as listed on the right, then calculation of tf\u2013idf for the term "this" in document 1 is performed as follows.\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 2\u000a! Term\u000a! | Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| another\u000a| 2\u000a|-\u000a| example\u000a| 3\u000a|}\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 1\u000a! Term\u000a! Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| a\u000a| 2\u000a|-\u000a| sample\u000a| 1\u000a|}\u000a\u000aTf, in its basic form, is just the frequency that we look up in appropriate table. In this case, it's one.\u000a\u000aIdf is a bit more involved:\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000aThe numerator of the fraction is the number of documents, which is two. The number of documents in which "this" appears is also two, giving\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{2}{2} = 0</math>\u000a\u000aSo tf\u2013idf is zero for this term, and with the basic definition this is true of any term that occurs in all documents.\u000a\u000aA slightly more interesting example arises from the word "example", which occurs three times but in only one document. For this document, tf\u2013idf of "example" is:\u000a:<math>\u005cmathrm{tf}(\u005cmathsf{example}, d_2) = 3</math>\u000a:<math>\u005cmathrm{idf}(\u005cmathsf{example}, D) = \u005clog \u005cfrac{2}{1} \u005capprox 0.3010</math>\u000a:<math>\u005cmathrm{tfidf}(\u005cmathsf{example}, d_2) = \u005cmathrm{tf}(\u005cmathsf{example}, d_2) \u005ctimes \u005cmathrm{idf}(\u005cmathsf{example}, D) = 3 \u005ctimes 0.3010 \u005capprox 0.9030</math>\u000a\u000a(using the [[base 10 logarithm]]).\u000a\u000a==See also==\u000a{{Div col||25em}}\u000a* [[Okapi BM25]]\u000a* [[Noun phrase]]\u000a* [[Word count]]\u000a* [[Vector space model]]\u000a* [[PageRank]]\u000a* [[Kullback\u2013Leibler divergence]]\u000a* [[Mutual information]]\u000a* [[Latent semantic analysis]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent Dirichlet allocation]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a* {{Cite doi|10.1108/eb026526}}\u000a* {{Cite book\u000a | last1 = Salton | first1 = G | authorlink1 = Gerard Salton\u000a | last2 = McGill | first2 = M. J.\u000a | year = 1986\u000a | title = Introduction to modern information retrieval\u000a | publisher = [[McGraw-Hill]]\u000a | isbn = 978-0070544840\u000a}}\u000a* {{Cite doi|10.1145/182.358466}}\u000a* {{Cite doi|10.1016/0306-4573(88)90021-0}}\u000a* {{Cite doi|10.1145/1361684.1361686}}\u000a\u000a==External links and suggested reading==\u000a* [[Gensim]] is a Python library for vector space modeling and includes tf\u2013idf weighting.\u000a* [http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&format=html&collection=Wilensky_papers&id=3&show_doc=yes Robust Hyperlinking]: An application of tf\u2013idf for stable document addressability.\u000a* [http://infinova.wordpress.com/2010/01/26/distance-between-documents/ A demo of using tf\u2013idf with PHP and Euclidean distance for Classification]\u000a* [http://www.codeproject.com/KB/IP/AnatomyOfASearchEngine1.aspx Anatomy of a search engine]\u000a* [http://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html tf\u2013idf and related definitions] as used in [[Lucene]]\u000a* [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer TfidfTransformer] in [[scikit-learn]]\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tf\u2013idf.\u000a* [http://blog.christianperone.com/?p=1589 Pyevolve: A tutorial series explaining the tf-idf calculation].\u000a* [http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html TF/IDF with Google n-Grams and POS Tags]\u000a\u000a{{DEFAULTSORT:Tf-Idf}}\u000a[[Category:Statistical natural language processing]]\u000a[[Category:Ranking functions]]\u000a[[Category:Vector space model]]
p174
asI167
(lp175
VCategory:Search algorithms
p176
aV{{Commons category|Search algorithms}}\u000a{{Cat main|Search algorithms}}\u000a\u000a[[Category:Algorithms]]\u000a[[Category:Searching]]
p177
asI168
(lp178
VAgrep
p179
aV{{lowercase|title=agrep}}\u000a{{Infobox software\u000a| name                   = agrep\u000a| logo                   = <!-- Image name is enough -->\u000a| logo caption           = \u000a| logo_size              = \u000a| logo_alt               = \u000a| screenshot             = <!-- Image name is enough -->\u000a| caption                = \u000a| screenshot_size        = \u000a| screenshot_alt         = \u000a| collapsible            = \u000a| developer              = {{Plainlist|\u000a* [[Udi Manber]]\u000a* Sun Wu\u000a}}\u000a| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| discontinued           = \u000a| latest release version = \u000a| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| latest preview version = \u000a| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| status                 = \u000a| programming language   = \u000a| operating system       = {{Plainlist|\u000a* [[Unix-like]]\u000a* [[OS/2]]\u000a* [[DOS]]\u000a* [[Microsoft Windows|Windows]]\u000a}}\u000a| platform               = \u000a| size                   = \u000a| language               = \u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| genre                  = [[Pattern matching]]\u000a| license                = \u000a| website                = <!-- {{URL|example.org}} -->\u000a| standard               = \u000a}}\u000a\u000a'''agrep''' (approximate [[grep]]) is a [[proprietary software|proprietary]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].\u000a\u000aIt selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.\u000a\u000aagrep is also the [[search engine]] in the indexer program [[GLIMPSE]]. agrep is free for private and non-commercial use only, and belongs to the University of Arizona.\u000a\u000a== Alternative implementations ==\u000aA more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].\u000a\u000aFREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* Wu-Manber agrep\u000a**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)\u000a**[http://www.tgries.de/agrep For DOS, Windows and OS/2 home page]\u000a*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]\u000a\u000a*See also\u000a**[http://laurikari.net/tre TRE regexp matching package]\u000a**[http://www.bell-labs.com/project/wwexptools/cgrep/ cgrep a command line approximate string matching tool]\u000a**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool\u000a**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]\u000a\u000a[[Category:Searching]]\u000a[[Category:Unix text processing utilities]]
p180
asI170
(lp181
VReverse DNS lookup
p182
aV'''{{Redirect|Reverse DNS}}\u000a\u000aIn [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' (rDNS) is the determination of a [[domain name]] that is associated with a given  [[IP address]] using the [[Domain Name System]] (DNS) of the [[Internet]].\u000a\u000aComputer networks use the Domain Name System to determine the IP address associated with a domain name. This process is also known as ''forward'' DNS resolution.  ''Reverse'' DNS lookup is the inverse process, the resolution of an IP address to its designated domain name.\u000a\u000aThe reverse DNS database of the Internet is rooted in the ''Address and Routing Parameter Area'' (<tt>[[.arpa|arpa]]</tt>) [[top-level domain]] of the Internet. [[IPv4]] uses the <tt>in-addr.arpa</tt> domain and the <tt>ip6.arpa</tt> domain is delegated for [[IPv6]]. The process of reverse resolving an IP address uses the ''pointer'' DNS record type ([[List of DNS record types#Resource records|PTR record]]).\u000a\u000aInformational RFCs (RFC 1033, RFC 1912 Section 2.1) specify that ''"Every Internet-reachable host should have a name"'' and that such names match with a reverse pointer record, but it is not a requirement of standards governing operation of the DNS itself.\u000a\u000a==IPv4 reverse resolution==\u000aReverse DNS lookups for [[IPv4]] addresses use a ''reverse IN-ADDR entry'' in the special domain <tt>in-addr.arpa</tt>. In this domain, an IPv4 address is represented as a concatenated sequence of ''four decimal numbers'', separated by dots, to which is appended the second level domain suffix <tt>.in-addr.arpa</tt>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four 8-bit portions and converting each 8-bit portion into a decimal number.  These decimal numbers are then concatenated in the order: least significant 8-bit portion first (leftmost), most significant 8-bit portion last (rightmost). It is important to note that ''this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses'' in textual form.\u000aFor example, an address (A) record for <tt>mail.example.com</tt> points to the IP address 192.0.2.5.\u000aIn pointer records of the reverse database, this IP address is stored as the domain name <tt>5.2.0.192.in-addr.arpa</tt> pointing back to its designated host name <tt>mail.example.com</tt>. \u000aThis allows it to pass the [[Forward Confirmed reverse DNS]] process.\u000a\u000a===Classless reverse DNS method===\u000aHistorically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A.  By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using ''canonical name'' ([[CNAME]]) DNS records.\u000a\u000a==IPv6 reverse resolution==\u000aReverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code>. An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.\u000a\u000a==Multiple pointer records==\u000aWhile most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended, unless there is a specific need.  For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically.  Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause responses to be truncated if they exceed the DNS 512 byte UDP message limit.\u000a\u000a==Records other than PTR records==\u000aRecord types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]] (RFC 4025), [[Secure Shell|SSH]] (RFC 4255) and [[Internet Key Exchange|IKE]] (RFC 4322).\u000a[[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] (RFC 6763) uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC\u202f6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref>\u000aLess standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.\u000a\u000a==Uses==\u000aThe most common uses of the reverse DNS include:\u000a* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.\u000a* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, dynamically assigned addresses, or other inexpensive Internet services.  Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com."  Some corporate anti-spam services take the view that the vast majority, but by no means all, of e-mail that originates from these computers is spam with spam filters refusing e-mail with such rDNS names.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL]</ref> However data has shown that just as much if not more spam has originated from unpatched machines within corporate networks that are more likely to use out of date browsers than cheaper services such as DSL networks not to mention the difficulty of blocking spam from major providers like Yahoo and Hotmail. A recent shift has shown that spamming has switched to mainly coming from hosting companies making using rDNS even less useful.<ref>http://www.mailchannels.com/blog/2013/03/worlds-largest-spam-sources-are-all-hosting-companies/</ref> All of this adds to the argument that the few services that choose to block email servers purely on the basis of rDNS are simply discriminating without merit and often miss out more pro-active and useful indiscriminate anti spam measures.<ref>http://ask.slashdot.org/story/11/10/13/1643202/ask-slashdot-is-reverse-dns-a-worthy-standard-for-fighting-spam</ref>\u000a* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, mainly because [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually can't pass verification for it when they use [[zombie computer]]s to forge domains.\u000a* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address\u000a\u000a==See also==\u000a*[[Forward-confirmed reverse DNS]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a*{{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}\u000a* [http://dns.icann.org ICANN DNS Operations]\u000a* RFC 2317 documents a way to do rDNS delegation for [[Classless Inter-Domain Routing|CIDR]] blocks\u000a* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]\u000a* RDNS policies: [http://postmaster.aol.com/Postmaster.Errors.php#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]\u000a\u000a[[Category:Searching]]\u000a[[Category:Domain name system]]\u000a\u000a[[nl:Domain Name System#Omgekeerde lookups]]'''
p183
asI171
(lp184
VStatistically Improbable Phrases
p185
aV'''Statistically Improbable Phrases''', '''Statimprophrases''' or '''SIPs''' constitute a system developed by [[Amazon.com]] to compare all of the books they index in the Search Inside! program and find phrases in each that are the most unlikely to be found in any other book indexed.<ref>{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}</ref> The system is used to find the most nearly unique portions of books for use as a summary or keyword.\u000a\u000a== Example == \u000aThe Statistically Improbable Phrases of Darwin's [[On the Origin of Species]] are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.<ref>[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005</ref>\u000a\u000a==See also==\u000a*[[Googlewhack]] \u2014 a pair of words occurring on a single webpage, as indexed by Google\u000a*[[tf-idf]] \u2014 a statistic used in information retrieval and text mining.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Amazon}}\u000a\u000a[[Category:Amazon.com]]\u000a[[Category:Searching]]\u000a[[Category:Bookselling]]
p186
asI172
(lp187
VControlled vocabulary
p188
aV{{refimprove|date=June 2012}}\u000a\u000a'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]], [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designer of the vocabulary, in contrast to natural language vocabularies, where there is no restriction on the vocabulary.\u000a\u000a== In library and information science ==\u000a\u000aIn [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.{{ref|warner}}{{ref|fast}} Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.\u000a\u000aFor example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms -- subject headings in this case -- have to be chosen to handle choices between variant spellings of the same concept (American versus British), choice among scientific and popular terms (Cockroaches versus ''Periplaneta americana''), and choices between synonyms (automobile versus cars), among other difficult issues.\u000a\u000aChoices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).\u000a\u000aControlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term "pool" has to be qualified to refer to either swimming pool, or the game pool to ensure that each authorized term or heading refers to only one concept.\u000a\u000aThere are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.\u000a\u000aHistorically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.\u000a\u000aFor example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "Broader term" and "Narrow term".\u000a\u000aThe [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.\u000a\u000aChoosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.\u000a\u000aControlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].\u000a\u000a== Indexing languages ==\u000a\u000aThere are three main types of indexing languages.\u000a\u000a* Controlled indexing language - Only approved terms can be used by the indexer to describe the document\u000a* [[Natural language]] indexing language - Any term from the document in question can be used to describe the document.\u000a* Free indexing language  - Any term (not only from the document) can be used to describe the document.\u000a\u000aWhen indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each documen\u000a\u000aIn recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.\u000a\u000aControlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word ''football'' for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|Association football]], which also happens to be called ''[[soccer]]'' in several countries. The [[English language]] [[football (word)|word football]] is also applied to [[Rugby football]] ([[Rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.\u000a\u000aCompared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).\u000a\u000aIn some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.\u000a\u000aHowever, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.\u000a\u000aThis is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.\u000a\u000aAnother possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.\u000a\u000aOn the other hand free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.\u000a\u000aControlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.\u000a\u000aThe use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.\u000a\u000aNumerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.\u000a\u000a==Applications==\u000aControlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.\u000a\u000aIn large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.\u000a\u000aWeb searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].\u000a\u000aIt is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.{{ref|doctorow}} To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.{{ref|pilgrim}}\u000a\u000a==See also==\u000a*[[Controlled natural language]]\u000a*[[IMS VDEX|IMS Vocabulary Definition Exchange]]\u000a*[[Nomenclature]]\u000a*[[Ontology (computer science)]]\u000a*[[Terminology]]\u000a*[[Thesaurus]]\u000a*[[Universal Data Element Framework]]\u000a*[[Vocabulary-based transformation]]\u000a\u000a==References==\u000a#{{note|warner}} Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].\u000a#{{note|fast}} Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]\u000a#{{note|doctorow}} Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].\u000a#{{note|pilgrim}} Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].\u000a#[http://www.imresources.fit.qut.edu.au/vocab/ Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes.\u000a#[http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.\u000a\u000a==External links==\u000a* [http://www.controlledvocabulary.com/ controlledvocabulary.com] \u2014 explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.\u000a* [http://www.photo-keywords.com/ photo-keywords.com/] \u2014 useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.\u000a* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]\u000a\u000a{{Lexicography}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Library cataloging and classification]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Technical communication]]\u000a[[Category:Semantic Web]]\u000a[[Category:Ontology (information science)]]\u000a[[Category:Controlled vocabularies]]\u000a[[Category:Library science]]\u000a[[Category:Information science]]
p189
asI173
(lp190
VMultimedia search
p191
aV'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.\u000aMultimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.\u000aWe can distinguish two methodologies in multimedia search:\u000a*'''Metadata search''': the search is made on the layers of [[metadata]].\u000a* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.\u000a\u000a\u000a==Metadata search==\u000a\u000aSearch is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.\u000a\u000aThere are three processes which should be done in this method:\u000a*'''[[Multimedia Information Retrieval#Feature Extraction Methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.\u000a*'''[[ Multimedia Information Retrieval#Feature Extraction Methods |Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])\u000a*'''[[ Multimedia Information Retrieval#Categorization Methods | Categorization of media descriptions ]]''' into classes.\u000a\u000a==[[Query by Example]]==\u000a\u000aIn [[query by example]] the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often it\u2019s used [[Search engine indexing |audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:\u000a*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].\u000a*Compare descriptors of the query and our database\u2019s media.\u000a*List the media sorted by maximum coincidence.\u000a\u000a==Multimedia search engine==\u000aThere are two big search families, in function of the content:\u000a* [[Visual search engine]]\u000a*[[Audio search engine]]\u000a\u000a===[[Visual search engine]]===\u000aInside this family we can distinguish two topics: [[image search]] and [[video search]]\u000a\u000a*'''[[Image search]]''': Although usually it\u2019s used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example [[QR codes]].\u000a*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.\u000a\u000a===[[Audio search engine]]===\u000aThere are different methods of audio searching:\u000a*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].\u000a*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album\u2026) . There are some programs of [[music recognition]]. for example: [[Shazam (service)|Shazam]] or [[SoundHound]].\u000a\u000a==See also==\u000a*[[Search engine indexing]]\u000a*[[Multimedia]]\u000a*[[Multimedia Information Retrieval]]\u000a*[[Streaming media]]\u000a*[[Journal of Multimedia]]\u000a*[[List of search engines#Multimedia|List of search engines]]\u000a*[[Video search engine]]\u000a\u000a==External links==\u000a\u000a[[Category:Searching]]\u000a[[Category:Multimedia]]
p192
asI176
(lp193
VContextual Query Language
p194
aV'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',<ref>[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress</ref> is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].\u000a\u000a== Examples of query syntax ==\u000a\u000aSimple queries:\u000a\u000a<blockquote><tt>dinosaur<br/>\u000a"complete dinosaur"<br/>\u000atitle = "complete dinosaur"<br/>\u000atitle exact "the complete dinosaur"</tt></blockquote>\u000a\u000aQueries using [[Boolean logic]]:\u000a\u000a<blockquote><tt>dinosaur or bird<br/>\u000aPalomar assignment and "ice age"<br/>\u000adinosaur not reptile<br/>\u000adinosaur and bird or dinobird<br/>\u000a(bird or dinosaur) and (feathers or scales)<br/>\u000a"feathered dinosaur" and (yixian or jehol)</tt></blockquote>\u000a\u000aQueries accessing [[index (publishing)|publication indexes]]:\u000a\u000a<blockquote><tt>publicationYear < 1980<br/>\u000alengthOfFemur > 2.4<br/>\u000abioMass >= 100</tt></blockquote>\u000a\u000aQueries based on the proximity of words to each other in a document:\u000a\u000a<blockquote><tt>ribs prox/distance<=5 chevrons<br/>\u000aribs prox/unit=sentence chevrons<br/>\u000aribs prox/distance>0/unit=paragraph chevrons</tt></blockquote>\u000a\u000aQueries across multiple [[Dimension (data warehouse)|dimensions]]:\u000a\u000a<blockquote><tt>date within "2002 2005"<br/>\u000adateRange encloses 2003</tt></blockquote>\u000a\u000aQueries based on [[Relevance (information retrieval)|relevance]]:\u000a\u000a<blockquote><tt>subject any/relevant "fish frog"<br/>\u000asubject any/rel.lr "fish frog"</tt></blockquote>\u000a\u000aThe latter example specifies using a specific [[algorithm]] for [[logistic regression]].<ref>[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]</ref>\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.loc.gov/standards/sru/cql/ CQL home page]\u000a* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]\u000a* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]\u000a\u000a{{Query languages}}\u000a\u000a{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}\u000a{{LOC-stub}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Library science]]\u000a[[Category:Library of Congress]]\u000a[[Category:Query languages]]\u000a[[Category:Knowledge representation languages]]
p195
asI177
(lp196
VCategory:Data search engines
p197
aV{{Cat main|Data search engine}}\u000a\u000a[[Category:Metadata]]\u000a[[Category:XML]]\u000a[[Category:Database management systems]]\u000a[[Category:Searching]]
p198
asI179
(lp199
VIBM Omnifind
p200
aV'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].\u000aIt did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.<ref>[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]</ref> IBM OmniFind as a standalone product was withdrawn in April 2011<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS911-075 IBM US Announcement Letter]</ref> and is now part of [[IBM Watson Content Analytics with Enterprise Search]].<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&subtype=CA&htmlfid=897/ENUS211-133 IBM US Announcement Letter]</ref>\u000a\u000a'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS910-115 IBM US Announcement Letter]</ref>\u000a\u000a'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.\u000a\u000a== See also ==\u000a* [[Languageware]]\u000a* [[UIMA]]\u000a* [[Comparison of enterprise search software]]\u000a* [[List of enterprise search vendors]]\u000a\u000a==External links==\u000a* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]\u000a* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}\u000a* [http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] {{Dead link|date=January 2012}}\u000a* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a[[Category:IBM software|OmniFind]]\u000a[[Category:Searching]]
p201
asI185
(lp202
VConcordance (publishing)
p203
aV{{sister\u000a|project=wiktionary\u000a|text=See the [[Wiktionary:Wiktionary:Concordances|list of concordances]] in [[Wiktionary]], the free dictionary\u000a}}\u000a\u000aA '''concordance''' is an alphabetical list of the principal words used in a book or body of work, with their immediate [[context (language use)#Verbal context|context]]s.  Because of the time, difficulty, and expense involved in creating a concordance in the pre-[[computer]] era, only works of special importance, such as the [[Vedas]],<ref>{{cite book|first = Maurice | last = Bloomfield| authorlink = Maurice Bloomfield | title = A Vedic Concordance| year  = 1990| publisher = Motilal Banarsidass Publ| isbn = 81-208-0654-9}}</ref> [[Bible]], [[Qur'an]] or the works of [[William Shakespeare|Shakespeare]] or classical Latin and Greek authors,<ref>{{cite journal | first = Roy | last = Wisby | authorlink = Roy Wisby | title = Concordance Making by Electronic Computer: Some Experiences with the Wiener Genesis| journal = The Modern Language Review | publisher = Modern Humanities Research Association | volume = 57 | issue = 2 | pages = 161\u2013172 | date = April 1962 | doi=10.2307/3720960}}</ref> had concordances prepared for them. \u000a[[File:Mordechai nathan hebrew latin concordance.jpg|right|thumb|225px|Mordecai Nathan's Hebrew-Latin Concordance of the Bible]]\u000aA concordance is more than an index; additional material, such as commentary, definitions, and topical cross-indexing make producing them a labor-intensive process, even when assisted by computers.\u000a\u000aAlthough an automatically generated [[subject indexing|index]] lacks the richness of a published concordance, the ability to combine the result of queries concerning multiple terms (such as searching for words near other words) has reduced interest in concordance publishing.  In addition, mathematical techniques such as [[Latent Semantic Indexing]] have been proposed as a means of automatically identifying linguistic information based on word context.\u000a\u000aA '''bilingual concordance''' is a concordance based on [[aligned parallel text]].\u000a\u000aA '''topical concordance''' is a list of subjects that a book (usually The Bible) covers, with the immediate context of the coverage of those subjects. Unlike a traditional concordance, the indexed word does not have to appear in the verse. The most well known topical concordance is [[Nave's Topical Bible]].\u000a\u000aThe first concordance, to the [[Vulgate]] Bible, was compiled by [[Hugh of St Cher]] (d.1262), who employed 500 monks to assist him. In 1448 Rabbi Mordecai Nathan completed a concordance to the Hebrew Bible. It took him ten years. 1599 saw a concordance to the Greek New Testament published by Henry Stephens and the Septuagint was done a couple of years later by Conrad Kircher in 1602. The first concordance to the English bible was published in 1550 by Mr Marbeck. According to Cruden it did not employ the verse numbers devised by Robert Stephens in 1545 but "the pretty large concordance" of Mr Cotton did. Then followed [[Cruden's Concordance]] and [[Strong's Concordance]].\u000a\u000a==Use in linguistics==\u000aConcordances are frequently used in [[linguistics]], when studying a text. For example:    \u000a* comparing different usages of the same word\u000a* analysing keywords\u000a* analysing [[word frequencies]]\u000a* finding and analysing phrases and [[idioms]]\u000a* finding [[translation]]s of subsentential elements, e.g. [[terminology]], in [[Bitext#Bitexts and translation memories|bitexts and translation memories]]\u000a* creating indexes and word lists (also useful for publishing)\u000a\u000aConcordancing techniques are widely used in national corpora such as [[American National Corpus]], [[British National Corpus]], and [[Corpus of Contemporary American English]] available on-line.  Stand-alone applications that employ concordancing techniques are known as concordancers.<ref>[http://www.lexically.net/wordsmith/introduction.htm?gclid=COjFnvGKhakCFVJX4Qod-RqjjQ Introduction to WordSmith]</ref> Some of them have integrated part-of-speech taggers and enable the user to create his/her own pos-annotated corpora to conduct various type of searches adopted in corpus linguistics.<ref>[http://yatsko.zohosites.com/linguistic-toobox-a-concordancer.html Linguistic Toolbox]</ref>\u000a\u000a==Inversion==\u000a\u000aThe reconstruction of the text of some of the [[Dead Sea Scrolls]] involved a concordance.\u000a\u000aAccess to some of the scrolls was governed by a "secrecy rule" that allowed only the original International Team or their designates to view the original materials. After the death of [[Roland de Vaux]] in 1971, his successors repeatedly refused to even allow the publication of photographs to other scholars. This restriction was circumvented by [[Martin Abegg]] in 1991, who used a computer to "invert" a concordance of the missing documents made in the 1950s which had come into the hands of scholars outside of the International Team, to obtain an approximate reconstruction of the original text of 17 of the documents.<ref>{{cite web |last= Hawrysch |first= George |title= Dr. George Hawrysch's speech on concordance book launch |work= The Ukrainian Weekly, No. 31, Vol. LXX |publisher= Ukrainian National Association |date= 2002-08-04 |url= http://www.ukrweekly.com/old/archive/2002/310217.shtml |accessdate= 2008-06-19}}</ref><ref>{{cite web |last= Jillette |first= Penn |title= You May Already be a "Computer Expert" |url= http://pennandteller.com/sincity/penn-n-teller/pcc/deadsea.html |accessdate= 2008-06-14}}</ref> This was soon followed by the release of the original text of the scrolls.\u000a\u000a== See also ==\u000a* [[Back-of-the-book index]]\u000a* [[A Vedic Word Concordance]]\u000a* [[Bible concordance]]\u000a* [[Bitext]]\u000a* [[Concordancer]]\u000a* [[Cross-reference]]\u000a* [[Index (publishing)|Index]]\u000a* [[Key Word in Context|KWIC]]\u000a* [[Text mining]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.opensourceshakespeare.org/concordance/ Shakespeare concordance] - A concordance of Shakespeare's complete works (from Open Source Shakespeare)\u000a* [http://www.arts.ualberta.ca/~ukr/skovoroda/NEW/ Online Concordance to the Complete Works of Hryhorii Skovoroda] - A concordance to Hryhorii Skovoroda's complete works (University of Alberta, Edmonton, Canada)\u000a* [http://infomotions.com/alex/ Alex Catalogue of Electronic Texts] - The Alex Catalogue is a collection of public domain electronic texts from American and English literature as well as Western philosophy. Each of the 14,000 items in the Catalogue are available as full-text but they are also complete with a concordance. Consequently, you are able to count the number of times a particular word is used in a text or list the most common (10, 25, 50, etc.) words.\u000a* [http://victorian.lang.nagoya-u.ac.jp/concordance/ Hyper-Concordance] - The Hyper-Concordance is written in C++, a program that scans and displays lines based on a command entered by the user. The main advantage of the C++ program is that it not only identifies the concordance lines but the words occurring to the left and the right of the word or phrase searched. It also reports the total number of text lines, the total word count and the number of occurrences of the word or phrase searched. The full text of the book is displayed in a box at the bottom of the screen. Each line of the text is numbered, and the line number and the term(s) searched provide a link to the full text.\u000a* [http://cherry.conncoll.edu/cohar/Programs.htm Concord] - Page includes link to Concord, an on-the-fly KWIC concordance generator.  Works with at least some non-Latin scripts (modern Greek, for instance).  Multiple choices for sorting results; multi-platform; Open Source.\u000a* [http://buschmeier.org/bh/study/ccd/ ConcorDance] - A concordance interface to the WorldWideWeb, it uses Google's or Yahoo's search engine to find concordances and can be used directly from the browser.\u000a* [http://ctext.org/tools/concordance Chinese Text Project Concordance Tool] - Concordance lookup and discussion of the continued importance of printed concordances in [[Sinology]] - [[Chinese Text Project]]\u000a* [http://khc.sourceforge.net/en/ KH Coder] - A free software for KWIC concordance and collocation stats generation. Various statistical analysis functions are also available such as co-occurrence network, multidimensional scaling, hierarchical cluster analysis, and correspondence analysis of words.\u000a\u000a{{DEFAULTSORT:Concordance (Publishing)}}\u000a[[Category:Concordances (publishing)| ]]\u000a[[Category:Indexing]]\u000a[[Category:Searching]]\u000a[[Category:Library science]]\u000a[[Category:Information science]]\u000a[[Category:Reference works]]
p204
asI188
(lp205
VSearch-based software engineering
p206
aV{{copy edit|date=October 2013}}\u000a\u000a{{Use dmy dates|date=November 2011}}\u000a\u000a'''Search-based software engineering''' ('''SBSE''') is an approach to apply [[metaheuristic]] search techniques like [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. It is inspired by the observation that many activities in [[software engineering]] can be formulated as [[Optimization (mathematics)|optimization]] problems. Due to the [[computational complexity]] of these problems, exact [[Optimization (mathematics)|optimization]] techniques of [[operations research]] like [[linear programming]] or [[dynamic programming]] are mostly impractical for large scale [[software engineering]] problems. Because of this, researchers and practitioners have used [[metaheuristic]] search techniques to find near optimal or good-enough solutions.\u000a\u000aBroadly speaking SBSE problems can be divided into two types. The first are black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem). \u000aWith this sort of problem domain, the underlying problem could have come from the software industry, but equally it could have originated from any domain where people are assigned tasks. \u000aThe second type are white-box problems where operations on source code need to be considered.<ref>\u000a{{Cite conference\u000a| doi = 10.1109/SCAM.2010.28\u000a| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| pages = 7\u201319\u000a| last = Harman\u000a| first = Mark\u000a| title = Why Source Code Analysis and Manipulation Will Always be Important\u000a| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| year = 2010\u000a}}</ref>\u000a\u000a__TOC__\u000a\u000a==Definition==\u000a\u000aThe basic idea of SBSE is to take a software engineering problem and convert it into a computational search problem which can be tackled with a [[metaheuristic]]. \u000aThis essentially involves a number of stages. Firstly defining a search space (the set of possible solutions to the problem). \u000aThis space is typically too large to be explored exhaustively and therefore a  [[metaheuristic]] is employed to sample this space. \u000aSecondly, a metric <ref>\u000a{{Cite conference\u000a| doi = 10.1109/METRIC.2004.1357891\u000a| conference = 10th International Symposium on Software Metrics, 2004\u000a| pages = 58\u201369\u000a| last = Harman\u000a| first = Mark\u000a|author2=John A. Clark\u000a | title = Metrics are fitness functions too\u000a| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 \u000a| year = 2004\u000a}}</ref> (also called a fitness function, cost function, objective function or quality measure) is used to measure the quality of a potential solution. Many software engineering problems can be reformulated as a computational search problem.<ref>{{Cite journal\u000a| doi = 10.1049/ip-sen:20030559\u000a| issn = 1462-5970\u000a| volume = 150\u000a| issue = 3\u000a| pages = 161\u2013175\u000a| last = Clark\u000a| first = John A.\u000a| coauthors = Dolado, José Javier; Harman, Mark; Hierons, Robert M.; Jones, Bryan F.; Lumkin, M.; Mitchell, Brian S.; Mancoridis, Spiros; Rees, K.; Roper, Marc; Shepperd, Martin J.\u000a| title = Reformulating software engineering as a search problem\u000a| journal = IEE Proceedings - Software \u000a| year = 2003\u000a}}</ref>\u000a\u000aThe term "[[search-based application]]", in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.\u000a\u000a==Brief history==\u000a\u000aOne of the earliest attempts in applying [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of software testing.<ref>\u000a{{Cite journal\u000a| doi = 10.1109/TSE.1976.233818\u000a| issn = 0098-5589\u000a| volume = SE-2\u000a| issue = 3\u000a| pages = 223\u2013226\u000a| last = Miller\u000a| first = Webb\u000a| last2 = Spooner\u000a| first2 = David L. \u000a| title = Automatic Generation of Floating-Point Test Data\u000a| journal = IEEE Transactions on Software Engineering\u000a| year = 1976\u000a}}</ref> \u000aIn 1992, Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.<ref>S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, "Application of genetic algorithms to software testing," in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&nbsp;625\u2013636</ref> \u000aThe term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00189-6\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 833\u2013839\u000a| last = Harman\u000a| first = Mark\u000a| last2 = Jones\u000a| first2 = Bryan F.\u000a| title = Search-based software engineering\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001896\u000a}}</ref> Since then, the research community has grown to include more than 800 authors in 2013, from approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}\u000a\u000a==Application areas==\u000a\u000aSearch-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications of search techniques in [[software engineering]].<ref>\u000a{{Cite journal\u000a| doi = 10.1002/stvr.294\u000a| issn = 1099-1689\u000a| volume = 14\u000a| issue = 2\u000a| pages = 105\u2013156\u000a| last = McMinn\u000a| first = Phil\u000a| title = Search-based software test data generation: a survey\u000a| journal = Software Testing, Verification and Reliability\u000a| accessdate = 2013-10-31\u000a| year = 2004\u000a| url = http://onlinelibrary.wiley.com/doi/10.1002/stvr.294/abstract\u000a}}</ref> Search techniques have also been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.infsof.2003.07.002\u000a| issn = 0950-5849\u000a| volume = 46\u000a| issue = 4\u000a| pages = 243\u2013253\u000a| last = Greer\u000a| first = Des\u000a| last2 = Ruhe\u000a| first2 = Guenther\u000a| title = Software release planning: an evolutionary and iterative approach\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-09-06\u000a| date = 2004-03-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S095058490300140X\u000a}}</ref>\u000a<ref>{{Cite conference\u000a| doi = 10.1109/SBES.2009.23\u000a| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| pages = 207\u2013215\u000a| last = Colares\u000a| first = Felipe\u000a| last2 = Souza\u000a| first2 = Jerffeson\u000a| last3 = Carmo\u000a| first3 = Raphael\u000a| last4 = Pádua\u000a| first4 = Clarindo\u000a| last5 = Mateus\u000a| first5 = Geraldo R.\u000a| title = A New Approach to the Software Release Planning\u000a| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| year = 2009\u000a}}</ref> [[software design]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00195-1\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 891\u2013904\u000a| last = Clark\u000a| first = John A.\u000a| last2 = Jacob\u000a| first2 = Jeremy L. \u000a| title = Protocols are programs too: the meta-heuristic search for security protocols\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001951\u000a}}</ref> [[software development]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.ins.2006.12.020\u000a| issn = 0020-0255\u000a| volume = 177\u000a| issue = 11\u000a| pages = 2380\u20132401\u000a| last = Alba\u000a| first = Enrique\u000a| last2 = Chicano\u000a| first2 = J. Francisco \u000a| title = Software project management with GAs\u000a| journal = Information Sciences\u000a| accessdate = 2013-10-31\u000a| date = 2007-06-01\u000a| url = http://www.sciencedirect.com/science/article/pii/S0020025507000175\u000a}}</ref> and [[software maintenance]].<ref>\u000a{{Cite conference\u000a| doi = 10.1109/ICSM.2005.79\u000a| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| pages = 240\u2013249\u000a| last = Antoniol\u000a| first = Giuliano\u000a| last2 = Di Penta\u000a| first2 = Massimiliano \u000a| last3 = Harman\u000a| first3 = Mark\u000a| title = Search-based techniques applied to optimization of project planning for a massive maintenance project\u000a| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| year = 2005\u000a}}</ref>\u000a\u000a===Requirements engineering===\u000a\u000a[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches users' requests and different constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally speaking, involves presenting the decision maker with a range of good compromises between cost and user satisfaction.<ref>\u000a{{Cite thesis\u000a| type = Ph.D.\u000a| publisher = University of London\u000a| last = Zhang\u000a| first = Yuanyuan\u000a| title = Multi-Objective Search-based Requirements Selection and Optimisation\u000a| location = Strand, London, UK\u000a| date = February 2010\u000a| url = http://eprints.ucl.ac.uk/170695/\u000a}}</ref>\u000a<ref>\u000aY.&nbsp;Zhang and M.&nbsp;Harman and S.&nbsp;L.&nbsp;Lim, "[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management]," Department of Computer Science, University College London, Research Note RN/11/12, 2011.\u000a</ref>\u000a\u000a===Debugging and maintenance===\u000a\u000aIdentifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is supported by a number of tools. One objective of SBSE is to automatically identify bugs (for example via [[mutation testing]]), then automatically fix them.\u000a\u000a[[Genetic programming]], a biologically-inspired technique which involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software was shown to be able to repair 55 out of 105 bugs for approximately $8 each.<ref>{{Cite conference\u000a| doi = 10.1109/ICSE.2012.6227211\u000a| conference = 2012 34th International Conference on Software Engineering (ICSE)\u000a| pages = 3\u201313\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Dewey-Vogt\u000a| first2 = Michael\u000a| last3 = Forrest\u000a| first3 = Stephanie\u000a| last4 = Weimer\u000a| first4 = Westley \u000a| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each\u000a| booktitle = 2012 34th International Conference on Software Engineering (ICSE)\u000a| year = 2012\u000a}}</ref>\u000a\u000a[[Coevolution]] has also been used as an approach. It follows a predator and prey metaphor where a population of programs and a population of [[Unit Testing|unit tests]] evolve together and influence each other.<ref>{{Cite conference\u000a| doi = 10.1109/CEC.2008.4630793\u000a| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| pages = 162\u2013168\u000a| last = Arcuri\u000a| first = Andrea\u000a| last2 = Yao\u000a| first2 = Xin \u000a| title = A novel co-evolutionary approach to automatic software bug fixing\u000a| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| year = 2008\u000a}}</ref>\u000a\u000a===Testing===\u000a\u000aSearch-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.\u000a\u000a===Optimizing software===\u000aThe use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of developing research interest and success. Genetic programming has been used to improve programs. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.<ref>\u000a{{Cite journal\u000a| last = Langdon\u000a| first = William B.\u000a| last2 = Harman\u000a| first2 = Mark \u000a| title = Optimising Existing Software with Genetic Programming\u000a| journal = IEEE Transactions on Evolutionary Computation\u000a| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf\u000a}}</ref>\u000a\u000a===Project management===\u000aA number of decisions which are normally made by a project manager can be done automatically, for example, project scheduling.<ref>\u000a{{Cite conference\u000a| publisher = ACM\u000a| doi = 10.1145/2330163.2330332\u000a| isbn = 978-1-4503-1177-9\u000a| pages = 1221\u20131228\u000a| last = Minku\u000a| first = Leandro L.\u000a| last2 = Sudholt\u000a| first2 = Dirk\u000a| last3 = Yao\u000a| first3 = Xin \u000a| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design\u000a| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference\u000a| location = New York, NY, USA\u000a| series = GECCO '12\u000a| accessdate = 2013-10-31\u000a| year = 2012\u000a| url = http://doi.acm.org/10.1145/2330163.2330332\u000a}}</ref>\u000a\u000a==Tools==\u000a\u000aThere are a number of tools available for SBSE approaches. These include tools like [[OpenPAT]].<ref> \u000a{{cite conference\u000a|ref        = harv\u000a|last       = Mayo\u000a|first      = M.\u000a|coauthors  = Spacey, S.\u000a|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics\u000a|url        = http://rd.springer.com/chapter/10.1007/978-3-642-39742-4_13\u000a|format     = PDF\u000a|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)\u000a|volume     = 8084\u000a|pages      = 158\u2013171\u000a|year       = 2013\u000a}}</ref>\u000aand Evosuite <ref>(http://www.evosuite.org/)</ref>\u000aand a code coverage measurement for Python\u000a<ref>\u000ahttps://pypi.python.org/pypi/coverage\u000a</ref>\u000a\u000a==Methods and techniques==\u000a\u000aThere are a number of methods and techniques available. \u000aA non-exhaustive list of these tools includes:\u000a\u000a\u2022[[profiling (computer programming)|Profiling]]\u000a<ref>http://java-source.net/open-source/profilers</ref> via [[instrumentation]] in order to monitor certain parts of a program as it is executed.\u000a\u000a\u2022Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into the structure of a program.\u000a\u000a\u2022Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]], [[Program analysis (computer science)|program analysis]].\u000a\u000a\u2022[[Code coverage]] allows measuring how much of the code is executed with a given \u000aset of input data.\u000a\u000a\u2022[[Static program analysis]]\u000a\u000a==Industry acceptance==\u000a\u000aAs a relatively new area of research, SBSE does not yet benefit from broad industry acceptance. One issue is that software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are quite different from the ones humans would produce.<ref>\u000a{{cite web\u000a |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/\u000a |title      = Programming using genetic algorithms: isn\u2019t that what humans already do ;-)\u000a |last       = Jones\u000a |first      = Derek\u000a |date       = 18 October 2013\u000a |website    = The Shape of Code\u000a |accessdate = 31 October 2013\u000a}}\u000a</ref>\u000aIn the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to favor program maintainability.<ref>\u000a{{Cite journal\u000a| doi = 10.1007/s11219-013-9208-0\u000a| issn = 1573-1367\u000a| volume = 21\u000a| issue = 3\u000a| pages = 421\u2013443\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Forrest \u000a| first2 = Stephanie \u000a| last3 = Weimer\u000a| first3 = Westley\u000a| title = Current challenges in automatic software repair\u000a| journal = Software Quality Journal\u000a| accessdate = 2013-10-31\u000a| date = 2013-09-01\u000a| url = http://link.springer.com/article/10.1007/s11219-013-9208-0\u000a}}\u000a</ref>\u000a\u000aAnother concern is that SBSE might make the software engineer redundant. Researchers have argued that, on the contrary, the motivation for SBSE is to enhance the relationship between the engineer and the program.<ref>\u000a{{Cite conference\u000a| publisher = IEEE Press\u000a| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering\u000a| pages = 49\u201350\u000a| last = Simons\u000a| first = Christopher L.\u000a| title = Whither (away) software engineers in SBSE?\u000a| location = San Francisco, USA\u000a| accessdate = 2013-10-31\u000a| date = May 2013\u000a| url = http://eprints.uwe.ac.uk/19938/\u000a}}</ref>\u000a\u000a==See also==\u000a{{Portal|Software Testing}}\u000a*[[Program analysis (computer science)]]\u000a*[[Dynamic program analysis]]\u000a\u000a==References==\u000a{{reflist|colwidth=30em}}\u000a\u000a==External links==\u000a*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]\u000a*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]\u000a*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]\u000a*[http://2013.icse-conferences.org/ International Conference on Software Engineering]\u000a*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]\u000a*[http://scholar.google.co.uk/citations?view_op=search_authors&hl=en&mauthors=label:sbse Google Scholar page on Search-based software engineering]\u000a\u000a[[Category:2001 introductions]]\u000a[[Category:Software engineering]]\u000a[[Category:Software testing]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]\u000a[[Category:Optimization algorithms and methods]]\u000a[[Category:Genetic algorithms]]
p207
asI189
(lp208
VFind
p209
aV{{other uses}}\u000a{{unreferenced|date=September 2013}}\u000a{{lowercase|title=find}} \u000aIn [[Unix-like]] and some other [[operating system]]s, <code>'''find'''</code> is a [[command-line utility]] that [[Search engine (computing)|searches]] through one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[file name]] or a time range to match against the modification time or access time of the file. By default, <code>find</code> returns a list of all files below the current [[working directory]].\u000a\u000aThe related <code>[[locate (Unix)|locate]]</code> programs use a database of indexed files obtained through <code>find</code> (updated at regular intervals, typically by <code>[[cron]]</code> job) to provide a faster method of searching the entire filesystem for files by name.\u000a\u000a==History==\u000a<code>find</code> appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project.<ref name="reader">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971\u20131986 |series=CSTR |number=139 |institution=Bell Labs}}</ref>\u000a\u000a== Find syntax ==\u000a{{expand section|date=August 2008}}\u000a\u000a<code>find [-H] [-L] [-P] path... [expression]</code>\u000a\u000aThe three options control how the <code>find</code> command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the <code>find</code> command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of <code>find</code>.\u000a\u000aAt least one path must precede the expression. <code>find</code> is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].\u000a\u000aExpression elements are whitespace-separated and evaluated from left to right.  They can contain logical elements such as AND (&#x2011;and or &#x2011;a) and OR (&#x2011;or &#x2011;o) as well as more complex predicates.\u000a\u000aThe [[GNU Find Utilities|GNU]] <code>find</code> has a large number of additional features not specified by POSIX.\u000a\u000a== POSIX protection from infinite output ==\u000a\u000aReal-world filesystems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]].  The [[POSIX|POSIX standard]] requires that\u000a{{Quotation|\u000aThe <code>find</code> utility shall detect infinite loops; that is, entering a previously visited\u000adirectory that is an ancestor of the last file encountered. When it detects an infinite\u000aloop, <code>find</code> shall write a diagnostic message to standard error and shall either recover\u000aits position in the hierarchy or terminate.\u000a}}\u000a\u000a==Operators ==\u000aOperators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:\u000a\u000a*'''( expr )''' Force precedence. \u000a*'''! expr''' True if expr is false.\u000a*'''expr1 expr2''' And (implied); expr2 is not evaluated if expr1 is false. \u000a*'''expr1 -a expr2''' Same as expr1 expr2.  \u000a*'''expr1 -o expr2''' Or; expr2 is not evaluated if expr1 is true.\u000a\u000a find . -name 'fileA_*' -o -name 'fileB_*'\u000a\u000aThis command searches files whose name has a prefix of "fileA_" or "fileB_" in the current directory.\u000a\u000a find . -name 'foo.cpp' '!' -path '.svn'\u000a\u000aThis command searches for files with the name "foo.cpp" in all subdirectories of the current directory (current directory itself included) other than ".svn".   We quote the ! so that it's not interpreted by the shell as the history substitution character.\u000a\u000a==Type filter explanation==\u000a\u000a'''-type''' ''option used to specify search for only file, link or directory.''\u000aVarious type filters are supported by find. They are activated using the\u000a\u000a find -type c\u000a\u000aconfiguration switch where c may be any of:\u000a* '''b '''[[Device file|block (buffered) special]]\u000a* '''c '''[[Device file|character (unbuffered special)]]\u000a* '''d [[Directory (computing)|directory]]'''\u000a* '''p '''[[Named pipe|named pipe (FIFO)]]\u000a* '''f [[regular file]]'''\u000a* '''l '''[[symbolic link]]; this is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension).\u000a* '''s '''[[Unix domain socket|socket]]\u000a* '''D '''[[Doors (computing)|door (Solaris)]]\u000a\u000aThe configuration switches listed in bold are most commonly used.\u000a\u000a==Examples==\u000a\u000a===From current directory===\u000a find . -name 'my*'\u000a\u000aThis searches in the current directory (represented by the dot character) and below it, for files and directories with names starting with ''my''. The quotes avoid the [[shell (computing)|shell]] expansion \u2014 without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current directory. In newer versions of the program, the directory may be omitted, and it will imply the current directory.\u000a\u000a===Files only===\u000a find . -name 'my*' -type f\u000aThis limits the results of the above search to only regular files, therefore excluding directories, special files, pipes, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of  files in the current directory starting with ''my''......\u000a\u000a===Commands===\u000aThe previous examples created listings of results because, by default, <code>find</code> executes the '-print' action.   (Note that early versions of the <code>find</code> command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)\u000a\u000a find . -name 'my*' -type f -ls\u000aThis prints extended file information.\u000a\u000a===Search all directories===\u000a find / -name myfile -type f -print\u000aThis searches every file on the computer for a file with the name ''myfile'' and prints it to the screen. It is generally not a good idea to look for data files this way.  This can take a considerable amount of time, so it is best to specify the directory more precisely.  Some operating systems may mount dynamic filesystems that are not congenial to <code>find</code>.   More complex filenames including characters special to the shell may need to be enclosed in single quotes.\u000a\u000a===Search all but one directory subtree===\u000a find / -path excluded_path -prune -o -type f -name myfile -print\u000aThis searches every folder on the computer except the subtree ''excluded_path'' (full path including the leading /), for a file with the name ''myfile''.  It will not detect directories, devices, links, doors, or other "special" filetypes.\u000a\u000a===Specify a directory===\u000a find /home/weedly -name 'myfile' -type f -print\u000aThis searches for files named ''myfile'' in the ''/home/weedly'' directory, the home directory for userid ''weedly''.  You should always specify the directory to the deepest level you can remember.  The quotes are optional in this example because "myfile" contains no characters special to the shell.\u000a\u000a===Search several directories===\u000a find local /tmp -name mydir -type d -print\u000aThis searches for directories named ''mydir'' in the ''local'' subdirectory of the current working directory and the ''/tmp'' directory.\u000a\u000a===Ignore errors===\u000aIf you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors.  Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null.  The following example shows how to do this in the bash shell: \u000a find / -name 'myfile' -type f -print 2>/dev/null\u000a\u000aIf you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well.  You can use sh to run the <code>find</code> command to get around this:\u000a sh -c find / -name 'myfile' -type f -print 2>/dev/null\u000a\u000aAn alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.\u000a find . -name 'myfile' |& grep -v 'Permission denied'\u000a\u000a===Find any one of differently named files===\u000a find . \u005c( -name '*jsp' -o -name '*java' \u005c) -type f -ls\u000a\u000aThe <code>-ls</code> option prints extended information, and the example finds any file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. Also note that the operator "or" can be abbreviated as "o". The "and" operator is assumed where no operator is given.  In many shells the parentheses must be escaped with a backslash, "\u005c(" and "\u005c)", to prevent them from being interpreted as special shell characters. The <code>-ls</code> option and the <code>-or</code> operator are not available on all versions of <code>find</code>.\u000a\u000a===Execute an action===\u000a find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \u005c;\u000aThis command changes the [[File system permissions|permissions]] of all files with a name ending in ''.mp3'' in the directory ''/var/ftp/mp3''. The  action is carried out by specifying the option <code>-exec [[chmod]] 644 {} \u005c;</code> in the command. For every file whose name ends in <code>.mp3</code>, the command <code>chmod 644 {}</code> is executed replacing <code>{}</code> with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission <code>644</code>, usually shown as <code>rw-r--r--</code>, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the <code>{}</code> must be quoted.  The trailing ";" is customarily quoted with a leading "\u005c", but could just as effectively be enclosed in single quotes.\u000a\u000aNote that the command itself should *not* be quoted; otherwise you get error messages like\u000a\u000a find: echo "mv ./3bfn rel071204": No such file or directory\u000a\u000awhich means that <code>find</code> is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.\u000a\u000aIf you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.\u000a\u000a find . -exec COMMAND {} +\u000a\u000aThis will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.\u000a\u000a===Delete files and directories===\u000a'''Caveats''': the -delete action is a GNU extension, and using it turns on -depth.   So, if you are testing a find command with -print instead of -delete in order to figure out what will happen before going for it, you need to use -depth -print.\u000a\u000aDelete empty files and directories and print the names (note that -empty is a vendor unique extension from GNU find that may not be available in all find implementations)\u000a find /foo -empty -delete -print\u000a\u000aDelete empty files\u000a find /foo -type f -empty -delete\u000a\u000aDelete empty directories\u000a find /foo -type d -empty -delete\u000a\u000aDelete files and directories (if empty) named <code>bad</code> \u000a find /foo -name bad -empty -delete\u000a\u000a'''Warning''': <code>-delete</code> should be used with other operators such as\u000a<code>-empty</code> or <code>-name</code>.\u000a\u000a find /foo -delete  # this deletes '''all''' in /foo\u000a\u000a===Search for a string===\u000aThis command will search for a string in all files from the /tmp directory and below:\u000a<source lang="bash">\u000a $ find /tmp -type f -exec grep 'search string' '{}' /dev/null \u005c+\u000a</source>\u000aThe <tt>[[/dev/null]]</tt> argument is used to show the name of the file before the text that is found. Without it, only the text found is printed.  An equivalent mechanism is to use the "-H" or "--with-filename" option to grep:\u000a<source lang="bash">\u000a $ find /tmp -type f -exec grep -H 'search string' '{}' '+' \u000a</source>\u000aGNU grep can be used on its own to perform this task:\u000a\u000a $ grep -r 'search string' /tmp\u000a\u000aExample of search for "LOG" in jsmith's home directory\u000a<source lang="bash" highlight="1">\u000a $ find ~jsmith -exec grep LOG '{}' /dev/null \u005c; -print\u000a /home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME\u000a /home/jsmith/scripts/errpt.sh:cat $LOG\u000a /home/jsmith/scripts/title:USER=$LOGNAME\u000a</source>\u000aExample of search for the string "ERROR" in all XML files in the current directory and all sub-directories\u000a<source lang="bash">\u000a\u000a $ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \u005c+ \u000a</source>\u000aThe double quotes (" ") surrounding the search string and single quotes (<nowiki>' '</nowiki>) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string.  Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since '''double quotes do not prevent all special interpretation'''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes.  Example:\u000a<source lang="bash">\u000a\u000a $ find . -name "file-containing-can't" -exec grep "can't" '{}' \u005c; -print\u000a</source>\u000a\u000a===Search for all files owned by a user===\u000a find . -user <userid>\u000a\u000a===Search in case insensitive mode===\u000aNote that -iname is not in the standard and may not be supported by all implementations.\u000a\u000a find . -iname ''''MyFile'''*'\u000a\u000aIf the <code>-iname</code> switch is not supported on your system then workaround techniques may be possible such as:\u000a\u000a find . -name '[m'''M''']['''y'''Y][f'''F''']['''i'''I]['''l'''L]['''e'''E]*'\u000a\u000aThis uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):\u000a\u000a echo "''''MyFile'''*'" |perl -pe 's/([a-zA-Z])/[\u005cL\u005c1\u005cU\u005c1]/g;s/(.*)/find . -name \u005c1/'|sh\u000a\u000a===Search files by size===\u000aExample of searching files with size between 100 kilobytes and 500 kilobytes.\u000a find . -size +100k -a -size -500k\u000aExample of searching empty files.\u000a find . -size 0k\u000aExample of searching non-empty files.\u000a find . ! -size 0k\u000a\u000a===Search files by name and size ===\u000a '''find''' /usr/src {{abbr|!|the negation of the expression that follows}} {{abbr|\u005c(|the start of a complex expression.}} -name '*,v' {{abbr|-o|a logical or of a complex expression. In this case the complex expression is all files like '*,v' or '.*,v'}} -name '.*,v' {{abbr|\u005c)|the end of a complex expression.}} '{}' \u005c; -print\u000a\u000aThis command will search in the /usr/src directory and all sub directories. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.\u000a\u000a<source lang="bash" enclose="div">\u000afor file in `find /opt \u005c( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o\u000a -name 'catalina.out' \u005c) -size +300000k -a -size -5000000k`; do \u000a    cat /dev/null > $file\u000adone\u000a</source>\u000aThe units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.\u000a\u000a==Related utilities==\u000a* <code>[[locate (Unix)|locate]]</code> is a Unix search tool that searches through a prebuilt database of files instead of directory trees of a file system. This is faster than <code>find</code> but less accurate because the database may not be up-to-date.\u000a* <code>[[grep]]</code> is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].\u000a* <code>[[tree (Unix)|tree]]</code> is a command-line utility that recursively lists files found in a directory tree, indenting the file names according to their position in the file hierarchy.\u000a* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools <code>find</code> and [[xargs]].\u000a* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of <code>find</code>.\u000a* <code>[[dir (command)|dir]]</code> has the /s option that recursively searches for files or folders.\u000a\u000a==See also==\u000a*[[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]\u000a*[[List of Unix programs]]\u000a*[[List of DOS commands]]\u000a*[[List of duplicate file finders]]\u000a*[[Filter (higher-order function)]]\u000a*[[find (command)]], a DOS and Windows command that is very different from UNIX <code>find</code>\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*{{man|cu|find|SUS|find files}}\u000a*[http://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]\u000a\u000a{{Unix commands}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Standard Unix programs]]\u000a[[Category:Unix SUS2008 utilities]]
p210
asI192
(lp211
VSearch/Retrieve via URL
p212
aV'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.\u000a\u000a==See also==\u000a* [[Search/Retrieve Web Service]]\u000a\u000a==External links==\u000a* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Search Retrieve via URL}}\u000a[[Category:Data search engines]]\u000a[[Category:Searching]]\u000a[[Category:Uniform resource locator]]\u000a\u000a{{web-stub}}
p213
asI193
(lp214
VCategory:Search engine software
p215
aV[[Category:Searching]]\u000a[[Category:Data search engines]]\u000a[[Category:Utility software by type]]\u000a[[Category:Marketing software]]\u000a[[Category:Web software]]
p216
asI196
(lp217
VCategory:Concordances (publishing)
p218
aV{{Cat main|Concordance (publishing)}}\u000a[[Category:Biblical studies]]\u000a[[Category:Indexing]]\u000a[[Category:Linguistics]]\u000a[[Category:Reference works]]\u000a[[Category:Searching]]\u000a[[Category:Hypertext]]
p219
asI200
(lp220
VInversion (discrete mathematics)
p221
aV{{contradict-other-multiple|Permutation|Lehmer code|Factorial number system|date=March 2013}}\u000a[[File:Inversion set and vector of a permutation.svg|thumb|right|380px|The permutation (4,1,5,2,6,3) has the inversion vector (0,1,0,2,0,3) and the inversion set {(1,2),(1,4),(3,4),(1,6),(3,6),(5,6)}. The inversion vector [[w:Factorial number system|converted]] to decimal is 373.]]\u000a[[File:Inversion set 16; wp(13,11, 7,15).svg|thumb|250px|Inversion set of the permutation<br>(0,15, 14,1, 13,2, 3,12,<br>11,4, 5,10, 6,9, 8,7)<br>showing the pattern of the<br>[[Thue\u2013Morse sequence]]]]\u000aIn [[computer science]] and [[discrete mathematics]], an '''inversion''' is a pair of places of a sequence where the elements on these places are out of their natural [[total order|order]].\u000a\u000a== Definitions ==\u000a\u000aFormally, let <math>(A(1), \u005cldots, A(n))</math> be a sequence of ''n'' distinct numbers.  If <math>i < j</math> and <math>A(i) > A(j)</math>, then the pair <math>(i, j)</math> is called an inversion of <math>A</math>.{{sfn|Cormen|Leiserson|Rivest|Stein|2001|pp=39}}{{sfn|Vitter|Flajolet|1990|pp=459}}\u000a\u000aThe '''inversion number''' of a sequence is one common measure of its sortedness.{{sfn|Barth|Mutzel|2004|pp=183}}{{sfn|Vitter|Flajolet|1990|pp=459}}  Formally, the inversion number is defined to be the number of inversions, that is, \u000a:<math>\u005ctext{inv}(A) = \u005c# \u005c{(A(i),A(j)) \u005cmid i < j \u005ctext{ and } A(i) > A(j)\u005c}</math>.{{sfn|Barth|Mutzel|2004|pp=183}}  \u000aOther measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted "runs" within the sequence, and the smallest number of exchanges needed to sort the sequence.{{sfn|Mahmoud|2000|pp=284}} Standard [[comparison sort]]ing algorithms can be adapted to compute the inversion number in time {{math|O(''n'' log ''n'')}}.\u000a\u000aThe '''inversion vector''' ''V(i)'' of the sequence is defined for ''i'' = 2, ..., ''n'' as <math>V[i] = \u005cleft\u005cvert\u005c{k \u005cmid k < i \u005ctext{ and } A(k) > A(i)\u005c}\u005cright\u005cvert</math>.  In other words each element is the number of elements preceding the element in the original sequence that are greater than it.  Note that the inversion vector of a sequence has one less element than the sequence, because of course the number of preceding elements that are greater than the first is always zero.  Each permutation of a sequence has a unique inversion vector and it is possible to construct any given permutation of a (fully sorted) sequence from that sequence and the permutation's inversion vector.{{sfn|Pemmaraju|Skiena|2003|pp=69}}\u000a\u000a==Weak order of permutations==\u000aThe set of permutations on ''n'' items can be given the structure of a [[partial order]], called the '''weak order of permutations''', which forms a [[lattice (order)|lattice]].\u000a\u000aTo define this order, consider the items being permuted to be the integers from 1 to ''n'', and let Inv(''u'') denote the set of inversions of a permutation ''u'' for the natural ordering on these items. That is, Inv(''u'') is the set of ordered pairs (''i'', ''j'') such that 1 \u2264 ''i'' < ''j'' \u2264 ''n'' and ''u''(''i'') > ''u''(''j''). Then, in the weak order, we define ''u'' \u2264 ''v'' whenever Inv(''u'') \u2286 Inv(''v'').\u000a\u000aThe edges of the [[Hasse diagram]] of the weak order are given by permutations ''u'' and ''v'' such that ''u < v'' and such that ''v'' is obtained from ''u'' by interchanging two consecutive values of ''u''. These edges form a [[Cayley graph]] for the [[symmetric group|group of permutations]] that is isomorphic to the [[skeleton (topology)|skeleton]] of a [[permutohedron]].\u000a\u000aThe identity permutation is the minimum element of the weak order, and the permutation formed by reversing the identity is the maximum element.\u000a\u000a== See also ==\u000a{{wikiversity|Inversion (discrete mathematics)}}\u000a{{commons|Category:Inversion (discrete mathematics)|Inversion (discrete mathematics)}}\u000a* [[Factorial number system]] (a factorial number is a reflected inversion vector)\u000a* [[Permutation group#Transpositions, simple transpositions, inversions and sorting|Transpositions, simple transpositions, inversions and sorting]]\u000a* [[Damerau\u2013Levenshtein distance]]\u000a* [[Parity of a permutation]]\u000a\u000a'''Sequences in the [[On-Line Encyclopedia of Integer Sequences|OEIS]]:'''\u000a* [https://oeis.org/wiki/Index_to_OEIS:_Section_Fa#factorial Index entries for sequences related to factorial numbers]\u000a* Reflected inversion vectors: {{OEIS link|A007623}} and {{OEIS link|A108731}}\u000a* Sum of inversion vectors, cardinality of inversion sets: {{OEIS link|A034968}}\u000a* Inversion sets of finite permutations interpreted as binary numbers: {{OEIS link|A211362}} &nbsp; (related permutation: {{OEIS link|A211363}})\u000a* Finite permutations that have only 0s and 1s in their inversion vectors: {{OEIS link|A059590}} &nbsp; (their inversion sets: {{OEIS link|A211364}})\u000a* Numbers of permutations of n elements with k inversions; Mahonian numbers: {{OEIS link|A008302}} &nbsp; (their row maxima; Kendall-Mann numbers: {{OEIS link|A000140}})\u000a* Number of connected labeled graphs with n edges and n nodes: {{OEIS link|A057500}}\u000a* Arrays of permutations with similar inversion sets and inversion vectors: {{OEIS link|A211365}}, {{OEIS link|A211366}}, {{OEIS link|A211367}}, {{OEIS link|A211368}}, {{OEIS link|A211369}}, {{OEIS link|A100630}}, {{OEIS link|A211370}}, {{OEIS link|A051683}}\u000a\u000a== References ==\u000a{{reflist|4|refs=}}\u000a\u000a=== Source bibliography ===\u000a{{refbegin|1}}\u000a* {{cite journal|ref=harv|first1=Wilhelm|last1=Barth|first2=Petra|last2=Mutzel|author2-link=Petra Mutzel|title=Simple and Efficient Bilayer Cross Counting|journal=[[Journal of Graph Algorithms and Applications]]|volume=8|issue=2|pages=179&ndash;194|year=2004|doi=10.7155/jgaa.00088}}\u000a* {{cite book|ref=harv\u000a | first1=Thomas H.|last1=Cormen|authorlink1=Thomas H. Cormen\u000a | last2=Leiserson|first2=Charles E.|authorlink2=Charles E. Leiserson\u000a | last3=Rivest|first3=Ronald L.|authorlink3=Ron Rivest\u000a | last4=Stein|first4=Clifford|authorlink4=Clifford Stein\u000a | title = [[Introduction to Algorithms]]\u000a | publisher = MIT Press and McGraw-Hill\u000a | year = 2001\u000a | isbn = 0-262-53196-8\u000a | edition = 2nd\u000a }}\u000a* {{cite book|ref=harv|title=Sorting: a distribution theory|chapter=Sorting Nonrandom Data|volume=54|series=Wiley-Interscience series in discrete mathematics and optimization|first=Hosam Mahmoud|last=Mahmoud|publisher=Wiley-IEEE|year=2000|isbn=978-0-471-32710-3}}\u000a* {{cite book|ref=harv|title=Computational discrete mathematics: combinatorics and graph theory with Mathematica|chapter=Permutations and combinations|first1=Sriram V.|last1=Pemmaraju|first2=Steven S.|last2=Skiena|publisher=Cambridge University Press|year=2003|isbn=978-0-521-80686-2}}\u000a* {{cite book|ref=harv|title=Algorithms and Complexity|volume=1|editor1-first=Jan|editor1-last=van Leeuwen|editor1-link=Jan van Leeuwen|edition=2nd|publisher=Elsevier|year=1990|isbn=978-0-444-88071-0|chapter=Average-Case Analysis of Algorithms and Data Structures|first1=J.S.|last1=Vitter|first2=Ph.|last2=Flajolet}}\u000a{{refend}}\u000a\u000a=== Further reading ===\u000a* {{cite journal|ref=harv|journal=Journal of Integer Sequences|volume=4|year=2001|title=Permutations with Inversions|first=Barbara H.|last=Margolius}}\u000a\u000a=== Presortedness measures ===\u000a* {{cite journal|ref=harv|journal=Lecture Notes in Computer Science|year=1984|volume=172|pages=324&ndash;336|doi=10.1007/3-540-13345-3_29|title=Measures of presortedness and optimal sorting algorithms|first=Heikki|last=Mannila|authorlink=Heikki Mannila}}\u000a* {{cite journal|ref=harv|first1=Vladimir|last1=Estivill-Castro|first2=Derick|last2=Wood|title=A new measure of presortedness|journal=Information and Computation|volume=83|issue=1|pages=111&ndash;119|year=1989|doi=10.1016/0890-5401(89)90050-3}}\u000a* {{cite journal|ref=harv|first=Steven S.|last=Skiena|year=1988|title=Encroaching lists as a measure of presortedness|journal=BIT|volume=28|issue=4|pages=755&ndash;784|doi=10.1007/bf01954897}}\u000a\u000a[[Category:Permutations]]\u000a[[Category:Order theory]]\u000a[[Category:String similarity measures]]\u000a[[Category:Sorting algorithms]]\u000a[[Category:Combinatorics]]\u000a[[Category:Discrete mathematics]]
p222
asI202
(lp223
VTversky index
p224
aVThe '''Tversky index''', named after [[Amos Tversky]],<ref>{{cite journal |last=Tversky |first=Amos |title=Features of Similarity |journal=Psychological Reviews |volume=84 |number=4 |year=1977 |pages=327\u2013352 |url=http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf}}</ref> is an asymmetric [[similarity measure]] on [[set theory|sets]] that compares a variant to a prototype. The Tversky index can be seen as a generalization of [[Dice's coefficient]] and [[Tanimoto coefficient]].\u000a\u000aFor sets ''X'' and ''Y'' the Tversky index is a number between 0 and 1 given by \u000a\u000a<math>S(X, Y) = \u005cfrac{| X \u005ccap Y |}{| X \u005ccap Y | + \u005calpha | X - Y | + \u005cbeta | Y - X |} </math>,\u000a\u000aHere, <math>X - Y</math> denotes the  [[Complement (set theory)| relative complement ]] of Y in X.\u000a\u000aFurther, <math>\u005calpha, \u005cbeta \u005cge 0 </math> are parameters of the Tversky index.  Setting <math>\u005calpha = \u005cbeta = 1 </math> produces the Tanimoto coefficient; setting <math>\u005calpha = \u005cbeta = 0.5 </math> produces Dice's coefficient. \u000a\u000aIf we consider ''X'' to be the prototype and ''Y'' to be the variant, then <math>\u005calpha</math> corresponds to the weight of the prototype and <math>\u005cbeta</math> corresponds to the weight of the variant. Tversky measures with <math>\u005calpha + \u005cbeta = 1</math> are of special interest.<ref>http://www.daylight.com/dayhtml/doc/theory/theory.finger.html</ref>\u000a\u000aBecause of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using '''max''' and '''min''' functions <ref>Jimenez, S., Becerra, C., Gelbukh, A. [http://aclweb.org/anthology/S/S13/S13-1028.pdf SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity]. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, p.194-201, June 7\u20138, 2013, Atlanta, Georgia, USA.</ref>\u000a.\u000a\u000a<math>S(X,Y)=\u005cfrac{| X \u005ccap Y |}{| X \u005ccap Y |+\u005cbeta\u005cleft(\u005calpha a+(1-\u005calpha)b\u005cright)}</math>,\u000a\u000a<math>a=\u005cmin\u005cleft(|X-Y|,|Y-X|\u005cright) </math>,\u000a\u000a<math>b=\u005cmax\u005cleft(|X-Y|,|Y-X|\u005cright) </math>,\u000a\u000aThis formulation also re-arranges parameters <math>\u005calpha </math> and <math>\u005cbeta </math>. Thus, <math> \u005calpha </math> controls the balance between <math> |X - Y| </math> and <math> |Y - X| </math> in the denominator. Similarly, <math>\u005cbeta</math> controls the effect of the symmetric difference <math> |X\u005c,\u005ctriangle\u005c,Y\u005c,| </math> versus <math> | X \u005ccap Y | </math> in the denominator.\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a[[Category:Index numbers]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p225
asI207
(lp226
VString-to-string correction problem
p227
aV{{No footnotes|date=July 2010}}\u000aIn [[computer science]], the '''string-to-string correction problem''' refers to the minimum number of edit operations necessary to change one [[String (computer science)|string]] into another. A single edit operation may be changing a single [[Character (computing)|symbol]] of the string into another, deleting, or inserting a symbol. The length of the edit sequence provides a measure of the [[Hamming distance|distance]] between the two strings.\u000a\u000aSeveral [[algorithm]]s exist to provide an efficient way to determine string distance and specify the minimum number of transformation operations required. Such algorithms are particularly useful for [[Delta encoding|delta]] creation operations where something is stored as a set of differences relative to a base version. This allows several versions of a single object to be stored much more efficiently than storing them separately. This holds true even for single versions of several objects if they do not differ greatly, or anything in between. \u000aNotably, such difference algorithms are used in [[molecular biology]] to provide some measure of kinship between different kinds of organisms based on the similarities of their [[macromolecule]]s (such as [[protein]]s or [[DNA]]).\u000a\u000a== See also ==\u000a* [[Delta encoding]]\u000a* [[Levenshtein distance]]\u000a* [[Edit distance]]\u000a\u000a== References ==\u000a<div class="references-small">\u000a*{{cite journal |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168\u2013173 |doi= 10.1145/321796.321811}}\u000a*{{cite journal |first=Walter F. |last=Tichy |title=The string-to-string correction problem with block moves |journal=ACM Transactions on Computer Systems |volume=2 |issue=4 |year=1984 |pages=309\u2013321 |doi= 10.1145/357401.357404}}\u000a</div>\u000a\u000a[[Category:Problems on strings]]\u000a[[Category:String similarity measures]]
p228
asI211
(lp229
VLevenshtein distance
p230
aV{{refimprove|date=February 2010}}\u000a\u000aIn [[information theory]] and [[computer science]], the '''Levenshtein distance''' is a [[string metric]] for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after [[Vladimir Levenshtein]], who considered this distance in 1965.<ref>{{cite journal |author=\u0412\u043b\u0430\u0434\u0438\u0301\u043c\u0438\u0440 \u0418. \u041b\u0435\u0432\u0435\u043d\u0448\u0442\u0435\u0439\u043d |script-title=ru:\u0414\u0432\u043e\u0438\u0447\u043d\u044b\u0435 \u043a\u043e\u0434\u044b \u0441 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u0432\u044b\u043f\u0430\u0434\u0435\u043d\u0438\u0439, \u0432\u0441\u0442\u0430\u0432\u043e\u043a \u0438 \u0437\u0430\u043c\u0435\u0449\u0435\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432 |language=Russian |trans_title=Binary codes capable of correcting deletions, insertions, and reversals |journal=\u0414\u043e\u043a\u043b\u0430\u0434\u044b \u0410\u043a\u0430\u0434\u0435\u043c\u0438\u0439 \u041d\u0430\u0443\u043a \u0421CCP |volume=163 |issue=4 |pages=845\u20138 |year=1965}} Appeared in English as: {{cite journal |author=Levenshtein, Vladimir I. |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |volume=10 |number=8 |pages=707\u2013710 |date=February 1966  |url=<!--http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf right to publish copy of journal unclear: see http://www.sherpa.ac.uk/romeo/search.php?issn=1028-3358&type=issn&la=en/&fIDnum=%7C&mode=simple ; in any event, liptak does not appear to be the author or the translator -->}}</ref>\u000a\u000aLevenshtein distance may also be referred to as '''edit distance''', although that may also denote a larger [[Edit distance|family of distance metrics]].<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}} It is closely related to [[Sequence alignment#Pairwise alignment|pairwise string alignments]].\u000a\u000a== Definition ==\u000aMathematically, the Levenshtein distance between two strings <math>a, b</math> is given by <math>\u005coperatorname{lev}_{a,b}(|a|,|b|)</math> where\u000a\u000a:<math>\u005cqquad\u005coperatorname{lev}_{a,b}(i,j) = \u005cbegin{cases}\u000a  \u005cmax(i,j) & \u005ctext{ if} \u005cmin(i,j)=0, \u005c\u005c\u000a  \u005cmin \u005cbegin{cases}\u000a          \u005coperatorname{lev}_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          \u005coperatorname{lev}_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          \u005coperatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)}\u000a       \u005cend{cases} & \u005ctext{ otherwise.}\u000a\u005cend{cases}</math>\u000awhere  <math>1_{(a_i \u005cneq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\u000a\u000aNote that the first element in the minimum corresponds to deletion (from <math>a</math> to <math>b</math>), the second to insertion and the third to match or mismatch, depending on whether the respective symbols are the same.\u000a\u000a=== Example ===\u000aFor example, the Levenshtein distance between "kitten" and "sitting" is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:\u000a\u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a\u000a===Upper and lower bounds===\u000aThe Levenshtein distance has several simple upper and lower bounds. These include:\u000a* It is always at least the difference of the sizes of the two strings.\u000a* It is at most the length of the longer string.\u000a* It is zero if and only if the strings are equal.\u000a* If the strings are the same size, the [[Hamming distance]] is an upper bound on the Levenshtein distance.\u000a* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string ([[triangle inequality]]).\u000a\u000a==Applications==\u000aIn [[approximate string matching]], the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected. The short strings could come from a dictionary, for instance. Here, one of the strings is typically short, while the other is arbitrarily long. This has a wide range of applications, for instance, [[spell checker]]s, correction systems for [[optical character recognition]], and software to assist natural language translation based on [[translation memory]].\u000a\u000aThe Levenshtein distance can also be computed between two longer strings, but the cost to compute it, which is roughly proportional to the product of the two string lengths, makes this impractical.  Thus, when used to aid in [[fuzzy string searching]] in applications such as [[record linkage]], the compared strings are usually short to help improve speed of comparisons.\u000a\u000a==Relationship with other edit distance metrics==\u000a{{main|Edit distance}}\u000aThere are other popular measures of [[edit distance]], which are calculated using a different set of allowable edit operations. For instance,\u000a* the [[Damerau\u2013Levenshtein distance]] allows insertion, deletion, substitution, and the [[Transposition (mathematics)|transposition]] of two adjacent characters;\u000a* the [[longest common subsequence problem|longest common subsequence]] metric allows only insertion and deletion, not substitution;\u000a* the [[Hamming distance]] allows only substitution, hence, it only applies to strings of the same length.\u000a\u000a[[Edit distance]] is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite).  This is further generalized by DNA [[sequence alignment]] algorithms such as the [[Smith\u2013Waterman algorithm]], which make an operation's cost depend on where it is applied.\u000a\u000a==Computing Levenshtein distance==\u000a\u000a===Recursive===\u000aThis is a straightforward, but inefficient, recursive [[pseudocode]] implementation of a <code>LevenshteinDistance</code> function that takes two strings, ''s'' and ''t'', together with their lengths, and returns the Levenshtein distance between them:\u000a\u000a<!--\u000a  Please do not add an additional implementation in your language of choice.\u000a  Many of those have been added to and deleted from this article in the past.\u000a  See the talk page archive for relevant discussion\u000a-->\u000a<source lang="C">\u000a// len_s and len_t are the number of characters in string s and t respectively\u000aint LevenshteinDistance(string s, int len_s, string t, int len_t)\u000a{\u000a  /* base case: empty strings */\u000a  if (len_s == 0) return len_t;\u000a  if (len_t == 0) return len_s;\u000a\u000a  /* test if last characters of the strings match */\u000a  if (s[len_s-1] == t[len_t-1])\u000a      cost = 0;\u000a  else\u000a      cost = 1;\u000a\u000a  /* return minimum of delete char from s, delete char from t, and delete char from both */\u000a  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,\u000a                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,\u000a                 LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost);\u000a}\u000a</source>\u000a\u000aUnfortunately, this straightforward recursive implementation is very inefficient because it recomputes the Levenshtein distance of the same substrings many times.\u000a\u000aA more efficient method would never repeat the same distance calculation. For example, the Levenshtein distance of all possible prefixes might be stored in an array <code>d[][]</code> where <code>d[i][j]</code> is the distance between the first <code>i</code> characters of string <code>s</code> and the first <code>j</code> characters of string <code>t</code>. The table is easy to construct one row at a time starting with row 0. When the entire table has been built, the desired distance is <code>d[len_s][len_t]</code>. While this technique is significantly faster, it will consume <code>len_s * len_t</code> more memory than the straightforward recursive implementation.\u000a\u000a===Iterative with full matrix===\u000a{{main|Wagner\u2013Fischer algorithm}}\u000a::{{small|Note: This section uses 1-based strings instead of 0-based strings}}\u000aComputing the Levenshtein distance is based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the Levenshtein distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix in a [[dynamic programming]] fashion, and thus find the distance between the two full strings as the last value computed.\u000a\u000aThis algorithm, an example of bottom-up [[dynamic programming]], is discussed, with variants, in the 1974 article ''The [[String-to-string correction problem]]'' by Robert A. Wagner and Michael J. Fischer.<ref>{{citation |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168\u2013173 |doi= 10.1145/321796.321811}}</ref>\u000a\u000aThis is a straightforward pseudocode implementation for a function ''LevenshteinDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them:\u000a\u000a<!--\u000a  Please do not add an additional implementation in your language of choice.\u000a  Many of those have been added to and deleted from this article in the past.\u000a  See the talk page archive for relevant discussion\u000a-->\u000a<!-- choose random language for highlights -->\u000a<source lang="C">\u000aint LevenshteinDistance(char s[1..m], char t[1..n])\u000a{\u000a  // for all i and j, d[i,j] will hold the Levenshtein distance between\u000a  // the first i characters of s and the first j characters of t;\u000a  // note that d has (m+1)*(n+1) values\u000a  declare int d[0..m, 0..n]\u000a \u000a  clear all elements in d // set each element to zero\u000a \u000a  // source prefixes can be transformed into empty string by\u000a  // dropping all characters\u000a  for i from 1 to m\u000a    {\u000a      d[i, 0] := i\u000a    }\u000a \u000a  // target prefixes can be reached from empty source prefix\u000a  // by inserting every character\u000a  for j from 1 to n\u000a    {\u000a      d[0, j] := j\u000a    }\u000a \u000a  for j from 1 to n\u000a    {\u000a      for i from 1 to m\u000a        {\u000a          if s[i] = t[j] then\u000a            d[i, j] := d[i-1, j-1]       // no operation required\u000a          else\u000a            d[i, j] := minimum\u000a                    (\u000a                      d[i-1, j] + 1,  // a deletion\u000a                      d[i, j-1] + 1,  // an insertion\u000a                      d[i-1, j-1] + 1 // a substitution\u000a                    )\u000a        }\u000a    }\u000a \u000a  return d[m, n]\u000a}\u000a</source>\u000a\u000aNote that this implementation does not fit the [[#Definition|definition]] precisely: it always prefers matches, even if insertions or deletions provided a better score. This is equivalent; it can be shown that for every optimal alignment (which induces the Levenshtein distance) there is another optimal alignment that prefers matches in the sense of this implementation.<ref>[http://cs.stackexchange.com/a/2997 Micro-optimisation for edit distance computation: is it valid?]</ref>\u000a\u000aTwo examples of the resulting matrix (hovering over a number reveals the operation performed to get that number):\u000a<center>\u000a{{col-begin|width=auto}}\u000a{{col-break}}\u000a{|class="wikitable"\u000a|-\u000a| \u000a| \u000a!k\u000a!i\u000a!t\u000a!t\u000a!e\u000a!n\u000a|-\u000a| ||0 ||1 ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!s\u000a|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!i\u000a|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5\u000a|-\u000a!t\u000a|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4\u000a|-\u000a!t\u000a|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3\u000a|-\u000a!i\u000a|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3\u000a|-\u000a!n\u000a|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}\u000a|-\u000a!g\u000a|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}\u000a|}\u000a{{col-break|gap=1em}}\u000a{|class="wikitable"\u000a|\u000a|\u000a!S\u000a!a\u000a!t\u000a!u\u000a!r\u000a!d\u000a!a\u000a!y\u000a|-\u000a| \u000a|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\u000a|-\u000a!S\u000a|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|delete 'a'|1}} ||{{H:title|delete 't'|2}} ||3 ||4 ||5 ||6 ||7\u000a|-\u000a!u\u000a|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6\u000a|-\u000a!n\u000a|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6\u000a|-\u000a!d\u000a|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5\u000a|-\u000a!a\u000a|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4\u000a|-\u000a!y\u000a|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}\u000a|}\u000a{{col-end}}\u000a</center>\u000a\u000aThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.\u000a\u000a===Iterative with two matrix rows===\u000aIt turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings (the previous row and the current row being calculated).\u000a\u000aThe Levenshtein distance may be calculated iteratively using the following algorithm:<ref>{{Citation |title=Fast, memory efficient Levenshtein algorithm |first=Sten |last=Hjelmqvist |date=26 Mar 2012 |url=http://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm}}</ref>\u000a<syntaxhighlight lang="CSharp">\u000aint LevenshteinDistance(string s, string t)\u000a{\u000a    // degenerate cases\u000a    if (s == t) return 0;\u000a    if (s.Length == 0) return t.Length;\u000a    if (t.Length == 0) return s.Length;\u000a\u000a    // create two work vectors of integer distances\u000a    int[] v0 = new int[t.Length + 1];\u000a    int[] v1 = new int[t.Length + 1];\u000a\u000a    // initialize v0 (the previous row of distances)\u000a    // this row is A[0][i]: edit distance for an empty s\u000a    // the distance is just the number of characters to delete from t\u000a    for (int i = 0; i < v0.Length; i++)\u000a        v0[i] = i;\u000a\u000a    for (int i = 0; i < s.Length; i++)\u000a    {\u000a        // calculate v1 (current row distances) from the previous row v0\u000a\u000a        // first element of v1 is A[i+1][0]\u000a        //   edit distance is delete (i+1) chars from s to match empty t\u000a        v1[0] = i + 1;\u000a\u000a        // use formula to fill in the rest of the row\u000a        for (int j = 0; j < t.Length; j++)\u000a        {\u000a            var cost = (s[i] == t[j]) ? 0 : 1;\u000a            v1[j + 1] = Minimum(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost);\u000a        }\u000a\u000a        // copy v1 (current row) to v0 (previous row) for next iteration\u000a        for (int j = 0; j < v0.Length; j++)\u000a            v0[j] = v1[j];\u000a    }\u000a\u000a    return v1[t.Length];\u000a}\u000a</syntaxhighlight>\u000a\u000a==See also==\u000a{{colbegin||25em}}\u000a*[[agrep]]\u000a*[[Approximate string matching]]\u000a*[[Bitap algorithm]]\u000a*[[Damerau\u2013Levenshtein distance]]\u000a*[[diff]]\u000a*[[MinHash]]\u000a*[[Dynamic time warping]]\u000a*[[Euclidean distance]]\u000a*[[Fuzzy string searching]]\u000a*[[Hamming weight]]\u000a*[[Hirschberg's algorithm]]\u000a*[[Homology (biology)#Sequence homology|Homology of sequences in genetics]]\u000a*[[Hunt\u2013McIlroy algorithm]]\u000a*[[Jaccard index]]\u000a*[[Jaro\u2013Winkler distance]]\u000a*[[Levenshtein automaton]]\u000a*[[Locality-sensitive hashing]]\u000a*[[Longest common subsequence problem]]\u000a*[[Lucene]] (an open source search engine that implements edit distance)\u000a*[[Manhattan distance]]\u000a*[[Metric space]]\u000a*[[Most frequent k characters]]\u000a*[[Needleman\u2013Wunsch algorithm]]\u000a*[[Optimal matching]] algorithm\u000a*[[Sequence alignment]]\u000a*[[Similarity space]] on [[Numerical taxonomy]]\u000a*[[Smith\u2013Waterman algorithm]]\u000a*[[Sørensen similarity index]]\u000a*[[String distance metric]]\u000a*[[Wagner-Fischer algorithm]]\u000a{{colend}}\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a==External links==\u000a{{Wikibooks| R_Programming|Text_Processing#Edit_distance|Levenshtein distance in R}}\u000a{{Wikibooks| Algorithm implementation|Strings/Levenshtein distance|Levenshtein distance}}\u000a*[http://www.postgresql.org/docs/current/static/fuzzystrmatch.html Levenshtein in PostgreSQL]\u000a*{{citation |contribution=Levenshtein distance |title=Dictionary of Algorithms and Data Structures [online] |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 August 2008 |accessdate=3 April 2013 |url=http://www.nist.gov/dads/HTML/Levenshtein.html }}\u000a\u000a{{DEFAULTSORT:Levenshtein Distance}}\u000a[[Category:String similarity measures]]\u000a[[Category:Dynamic programming]]\u000a[[Category:Articles with example pseudocode]]\u000a[[Category:Quantitative linguistics]]
p231
asI215
(lp232
VSimple matching coefficient
p233
aVThe '''Simple Matching Coefficient (SMC)''' is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets.<ref>http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sdaugherty/similarity.html</ref>\u000a\u000aGiven two objects, A and B, each with n binary attributes, SMC is defined as:\u000a:<math> SMC = {\u005ctext{Number of Matching Attributes}\u005cover \u005ctext{Number of Attributes}} = {{M_{00}+M_{11}}\u005cover{M_{00}+M_{01}+M_{10}+M_{11}}}</Math>\u000a\u000aWhere:\u000a:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.\u000a:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.\u000a:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.\u000a:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.\u000a\u000a== See also ==\u000a* [[Jaccard index]]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a[[Category:Index numbers]]\u000a[[Category:Measure theory]]\u000a[[Category:Clustering criteria]]\u000a[[Category:String similarity measures]]
p234
asI216
(lp235
VSocial Sciences Citation Index
p236
aVThe '''Social Sciences Citation Index''' ('''SSCI''') is an interdisciplinary  [[citation index]] product of  [[Thomson Reuters]]' Healthcare & Science division. It was developed by the [[Institute for Scientific Information]] (ISI) from the [[Science Citation Index]].\u000a\u000aThis citation database covers some 2,474 of the world's leading [[academic journal|journals]] of [[social sciences]] across more than 50 [[academic discipline|disciplines]].<ref>{{cite web\u000a  | title = Social Sciences Citation Index \u000a  | url = http://scientific.thomson.com/products/ssci/\u000a  | accessdate = 2008-06-11 }}</ref> It is made available online through the [[Web of Science]] service for a fee.  This database product provides information to identify the articles  cited most frequently and by what publisher and author.\u000a\u000a== Criticism ==\u000aIn 2004 economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.<ref>Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box\u2014with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134-165.</ref>\u000a\u000a==See also==\u000a* [[Arts and Humanities Citation Index]]\u000a* [[Science Citation Index]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]\u000a\u000a{{Thomson Reuters}}\u000a[[Category:Thomson family]]\u000a[[Category:Thomson Reuters]]\u000a[[Category:Social sciences literature]]\u000a[[Category:Citation indices]]\u000a\u000a{{database-stub}}\u000a{{sci-stub}}
p237
asI217
(lp238
VCategory:Legal citators
p239
aV{{Cat main|Citator}}\u000a\u000a[[Category:Citation indices]]\u000a[[Category:Legal research]]\u000a[[Category:Legal citation]]
p240
asI218
(lp241
VArts and Humanities Citation Index
p242
aV{{ infobox bibliographic database\u000a| image       = \u000a| caption     = \u000a| producer    =Thomson Reuters \u000a| country     =United States \u000a| history     = \u000a| languages   = \u000a| providers   =Web of Science, Dialog Bluesheets \u000a| cost        =Subscription \u000a| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio \u000a| depth       =Index, abstract, citation indexing, author \u000a| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances \u000a| temporal    =1975 to present \u000a| geospatial  =global \u000a| number      = \u000a| updates     = \u000a| p_title     = \u000a| p_dates     = \u000a| ISSN        = \u000a| web         = \u000a| titles      =  \u000a}}\u000a\u000aThe '''''Arts & Humanities Citation Index''''' ('''A&HCI'''), also known as '''''Arts & Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore the print counterpart is Current Contents.\u000a\u000aSubjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio. \u000a\u000aAvailable citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances. \u000a\u000aThis database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.\u000a\u000aAccording to Thomson Reuters, the ''Arts & Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>\u000a{{Cite web\u000a  | title =Arts & Humanities Search (File 255) \u000a  | publisher =Dialog bluesheets  \u000a  | date = \u000a  | url =http://library.dialog.com/bluesheets/html/bl0439.html \u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=Iowa>\u000aDescription of Arts & Humanities Search. \u000a{{Cite web\u000a  | title =e-Library catalog\u000a  | publisher =Iowas State University  \u000a  | year =2008 \u000a  | url =http://www.lib.iastate.edu/collections/db/artshm.html\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>\u000aDescription of Web of Science coverage.  \u000a{{Cite web\u000a  | title =e-Library catalog\u000a  | publisher =Iowas State University  \u000a  | year =2008 \u000a  | url =http://www.lib.iastate.edu/collections/db/websci.html\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref><ref name=TR>\u000aSee the page entitled "Tech Specs" \u000a{{Cite web\u000a  | title =Database description\u000a  | publisher =Thomson Reuters  \u000a  | year = \u000a  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3\u000a  | format =Online web page \u000a  | accessdate =2011-07-03}}</ref>\u000a==History==\u000aThe index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP & Science division.\u000a\u000a==See also==\u000a* [[Science Citation Index]]\u000a* [[Social Sciences Citation Index]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* {{Official|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.\u000a* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.\u000a\u000a{{Thomson Reuters}}\u000a[[Category:Citation indices]]\u000a[[Category:Thomson Reuters]]\u000a\u000a{{DEFAULTSORT:Arts And Humanities Citation Index}}
p243
asI219
(lp244
VScience Citation Index
p245
aV{{incomplete|date=January 2014}}\u000a{{ infobox bibliographic database\u000a| title = Science Citation Index\u000a| image = \u000a| caption = \u000a| producer = [[Thomson Reuters]]\u000a| country = United States\u000a| history = 1964-present\u000a| languages = \u000a| providers = \u000a| cost = \u000a| disciplines = Science, medicine, and technology\u000a| depth = \u000a| formats = \u000a| temporal = \u000a| geospatial = \u000a| number = \u000a| updates = \u000a| p_title = \u000a| p_dates = \u000a| ISSN = 0036-827X\u000a| web = http://thomsonreuters.com/science-citation-index-expanded/\u000a| titles = \u000a}}\u000aThe '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Thomson Reuters]].<ref name=dimension>\u000a{{cite journal\u000a|doi=10.1126/science.122.3159.108\u000a|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas\u000a|url=http://ije.oxfordjournals.org/content/35/5/1123.full\u000a|format=Free web article download\u000a|year=1955\u000a|last1=Garfield\u000a|first1=E.\u000a|journal=Science\u000a|volume=122\u000a|issue=3159\u000a|pages=108\u201311\u000a|pmid=14385826|bibcode=1955Sci...122..108G\u000a}}</ref><ref name=evolve>\u000a{{cite journal\u000a |last = Garfield \u000a |first = Eugene\u000a |doi=10.2436/20.1501.01.10\u000a |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf\u000a |format=Free PDF download\u000a |title=The evolution of the Science Citation Index|doi_brokendate = 2015-01-21\u000a }} International microbiology '''10.'''1 (2010): 65-69.</ref><ref name=gOverview>\u000a{{cite web\u000a | last = Garfield \u000a | first = Eugene\u000a | authorlink =\u000a | coauthors =\u000a | title = Science Citation Index\u000a | work = Science Citation Index 1961\u000a | publisher = Garfield Library - UPenn\u000a | date = 1963\u000a | url = http://garfield.library.upenn.edu/papers/80.pdf\u000a | format = Free PDF download\u000a | doi =\u000a | accessdate = 2013-05-27}} \u000a* Originally published by the Institute of Scientific Information in 1964\u000a* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.</ref><ref name=history-cite-indexing>\u000a{{cite web\u000a | title =History of Citation Indexing \u000a | work =Needs of researchers create demand for citation indexing \u000a | publisher =Thomson Ruters \u000a | date =November 2010 \u000a | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ \u000a | format =Free HTML download \u000a | accessdate =2010-11-04}}</ref> The larger version ('''Science Citation Index Expanded''') covers more than 6,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternately described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.{{citation needed|date=August 2013}}<ref name=Expanded>\u000a{{cite web \u000a|url=http://thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index_expanded/ \u000a|title=Science Citation Index Expanded \u000a|work= |accessdate=2009-08-30}}</ref>\u000a<ref name=wetland>{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)</ref><ref name=shan>\u000a{{cite journal \u000a| doi= 10.1007/s11192-012-0837-z \u000a|title= The top-cited research works in the Science Citation Index Expanded \u000a|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf \u000a| year= 2012 \u000a| last1= Ho \u000a| first1= Yuh-Shan \u000a| journal= Scientometrics \u000a| volume= 94 \u000a| issue= 3 \u000a| page= 1297}}</ref>\u000a\u000aThe index is made available online through different platforms, such as the [[Web of Science]]<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref> and SciSearch.<ref>{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}</ref> (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",<ref name=SpCI>\u000a{{cite web \u000a|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ \u000a|title=Specialty Citation Indexes \u000a|work= |accessdate=2009-08-30}}</ref> \u000asuch as the '''Neuroscience Citation Index'''<ref name=NCI>\u000a{{cite web \u000a|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD \u000a|title=Journal Search - Science - |work= |accessdate=2009-08-30}}</ref> and the '''Chemistry Citation Index'''.<ref>{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD \u000a|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}</ref>\u000a\u000a==Chemistry Citation Index==\u000a\u000aThe Chemistry Citation Index was first introduced by Eugene Garfield, a chemist. His original "search examples were based on [his] experience as a chemist".<ref name=Garcci/> In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.<ref name=Garcci>Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.</ref>\u000a\u000aBy 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.<ref>\u000a[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.</ref>\u000a\u000aOne 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.<ref>\u000a{{cite journal\u000a|doi=10.1007/BF02016348\u000a|title=Science citation index and chemistry\u000a|year=1980\u000a|last1=Dewitt\u000a|first1=T. W.\u000a|last2=Nicholson\u000a|first2=R. S.\u000a|last3=Wilson\u000a|first3=M. K.\u000a|journal=Scientometrics\u000a|volume=2\u000a|issue=4\u000a|page=265}}</ref>\u000a\u000a==See also==\u000a* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.\u000a* [[Impact factor]]\u000a* [[List of academic databases and search engines]]\u000a* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.\u000a\u000a== References ==\u000a{{Reflist|2}}\u000a\u000a==Further reading==\u000a*{{cite journal\u000a|doi= 10.1002/aris.1440360102\u000a|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf\u000a|title=Scholarly Communication and Bibliometrics\u000a|year= 2005\u000a|last1= Borgman\u000a|first1= Christine L.\u000a|last2= Furner\u000a|first2= Jonathan\u000a|journal= Annual Review of Information Science and Technology\u000a|volume= 36\u000a|issue= 1 \u000a|pages=3\u201372}}\u000a\u000a*{{cite journal\u000a|doi= 10.1002/asi.20677\u000a|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf\u000a|format= Free PDF download\u000a|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar\u000a|year= 2007\u000a|last1= Meho\u000a|first1= Lokman I.\u000a|last2= Yang\u000a|first2= Kiduk\u000a|journal= Journal of the American Society for Information Science and Technology\u000a|volume= 58\u000a|issue= 13\u000a|page= 2105}}\u000a\u000a*{{cite journal\u000a|doi= 10.1002/asi.5090140304\u000a|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf\u000a|format= Free PDF download\u000a|title= New factors in the evaluation of scientific literature through citation indexing\u000a|year= 1963\u000a|last1= Garfield\u000a|first1= E.\u000a|last2= Sher\u000a|first2= I. H.\u000a|journal= American Documentation\u000a|volume= 14\u000a|issue= 3\u000a|page= 195}}\u000a\u000a*{{cite journal\u000a|doi= 10.1038/227669a0\u000a|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf\u000a|format= Free PDF download\u000a|title= Citation Indexing for Studying Science\u000a|year= 1970\u000a|last1= Garfield\u000a|first1= E.\u000a|journal= Nature\u000a|volume= 227\u000a|issue= 5259\u000a|pages= 669\u201371\u000a|pmid= 4914589|bibcode= 1970Natur.227..669G\u000a}}\u000a\u000a*{{cite book\u000a | last =Garfield\u000a | first =Eugene \u000a | authorlink =\u000a | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities\u000a | publisher =Wiley-Interscience\u000a | series = Information Sciences Series\u000a | edition = 1st\u000a | origyear = 1979| year = 1983\u000a | location = New York\u000a | isbn =9780894950247}}\u000a\u000a==External links==\u000a* [http://scientific.thomson.com/products/wos/ Introduction to SCI]\u000a* [http://science.thomsonreuters.com/mjl/ Master journal list]\u000a* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. \u000a* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. \u000a* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Chinweb.\u000a\u000a{{Thomson Reuters}}\u000a\u000a[[Category:Online databases]]\u000a[[Category:Citation indices]]\u000a[[Category:Thomson Reuters]]
p246
asI220
(lp247
VScopus
p248
aV{{Use dmy dates|date=August 2013}}\u000a{{other uses}}\u000a{{infobox bibliographic database\u000a| title = Scopus\u000a| image = [[File:Scopus_type_logo.jpg]]\u000a| caption = Scopus logo\u000a| producer = [[Elsevier]]\u000a| country = \u000a| history = \u000a| languages = English\u000a| providers = \u000a| cost = Subscription\u000a| disciplines= \u000a| depth = \u000a| formats = \u000a| temporal = 1995-present\u000a| geospatial = Worldwide\u000a| number = 55 million\u000a| updates = \u000a| p_title = \u000a| p_dates = \u000a| ISSN = \u000a| web = http://www.scopus.com\u000a| titles = \u000a}}\u000a'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092\u20136 |pmid=19738094}}</ref>\u000a\u000aSince Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.\u000a\u000aA 2008 study  compared [[PubMed]], Scopus, [[Web of Science]], and [[Google Scholar]] and concluded: <blockquote>"PubMed and Google Scholar are accessed for free [...] Scopus offers about 20% more coverage than Web of Science, whereas Google Scholar offers results of inconsistent accuracy. PubMed remains an optimal tool in biomedical electronic research. Scopus covers a wider journal range [...] but it is currently limited to recent articles (published after 1995) compared with Web of Science. Google Scholar, as for the Web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information."<ref>{{Cite journal |pmid=17884971 |year=2008 |last1=Falagas |first1=ME |last2=Pitsouni |first2=EI |last3=Malietzis |first3=GA |last4=Pappas |first4=G |title=Comparison of PubMed, Scopus, Web of Science, and Google Scholar: Strengths and weaknesses |volume=22 |issue=2 |pages=338\u201342 |doi=10.1096/fj.07-9492LSF |journal=[[FASEB Journal]]}}</ref></blockquote>\u000a\u000aEvaluating ease of use and coverage of Scopus and the Web of Science (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. [...] The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive. [...]".<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>\u000a\u000aScopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].\u000a\u000aScopus can be integrated with [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>\u000a\u000a== References ==\u000a{{reflist|30em}}\u000a\u000a== External links ==\u000a* {{Official website|http://www.scopus.com/}}\u000a* [http://www.elsevier.com/online-tools/scopus Scopus information]\u000a\u000a\u000a{{Reed Elsevier}}\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Elsevier]]\u000a[[Category:Citation indices]]\u000a[[Category:Library cataloging and classification]]\u000a[[Category:Scholarly search services]]
p249
asI227
(lp250
VRussian Science Citation Index
p251
aV{{primary sources|date=March 2012}}\u000a'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.\u000a\u000a== Purpose ==\u000aFrom 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.\u000a\u000a== Functionality ==\u000aIn Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: \u000a* Scientific capacity and effectiveness of research, and\u000a* Publication activity\u000athrough the following indicators:\u000a* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,\u000a* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and\u000a* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.\u000a\u000a== See also ==\u000a*[[List of academic databases and search engines]]\u000a*[[Science Citation Index]]\u000a*[[Scopus]]\u000a\u000a==External links==\u000a* [http://elibrary.ru/ Scientific Electronic Library]\u000a\u000a\u000a[[Category:Citation indices]]
p252
asI229
(lp253
VWeb of Knowledge
p254
aV{{Mergeto|Web of Science|date=June 2014|discuss=Talk:Web of Science#Merge}}\u000a[[File:Web of Science Logo.png|thumb|The current Web of Science logo]]\u000a[[Image:ISI Web of Knowledge updated.png|thumb|400px|An example search result from Web of Knowledge version 3.0]]\u000a\u000a'''Web of Knowledge''' (formerly known as [[Institute for Scientific Information|ISI]] Web of Knowledge) is an academic [[citation index]]ing and search service, which is combined with web linking and is provided by [[Thomson Reuters]]. Web of Knowledge covers the sciences, [[social science]]s, arts and [[humanities]]. It provides [[bibliography|bibliographic]] content and tools to access, analyze, and manage research information. Multiple databases can be searched simultaneously.<ref name=describe/><ref name=tutor>[http://science.thomsonreuters.com/tutorials/wok4/wok4tut3.html Tutorial]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref>\u000a\u000a==Overview==\u000aWeb of Knowledge is described as a unifying research tool which enables the user to acquire, analyze, and disseminate database information in a timely manner. This is accomplished because of the creation of a common vocabulary, called [[Ontology (information science)|ontology]], for varied search terms and varied data. Moreover, search terms generate related information across categories.\u000a\u000aAcceptable content for Web of Knowledge is determined by an evaluation and selection process based on the following criteria: impact, influence, timeliness, [[peer review]], and geographic representation.<ref name=describe/>\u000a\u000a===Search and analysis===\u000a<!-- Deleted image removed: [[File:ISI Web of knowledge logo.jpg|thumb||Former Web of Knowledge logo]] -->\u000a\u000aWeb of Knowledge employs various search and analysis capabilities. First, citation indexing is employed, which is enhanced by the capability to search for results across disciplines. The influence, impact, history, and [[methodology]] of an idea can be followed from its first instance, notice, or referral to the present day. This technology points to a deficiency with the [[Index term|keyword]]-only method of searching. \u000a\u000aSecond, subtle trends and patterns relevant to the literature or research of interest, become apparent. Broad trends indicate significant topics of the day, as well as the history relevant to both the work at hand, and particular areas of study. \u000a\u000aThird, trends can be [[mathematical modeling|graphically]] represented.<ref name=describe>[http://thomsonreuters.com/content/science/pdf/Web_of_Knowledge_factsheet.pdf Overview and Description]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref><ref>{{cite web|url=http://wokinfo.com/realfacts/qualityandquantity/|title=Web of Knowledge > Real Facts > Quality and Quantity|accessdate = 2010-05-05}}</ref>\u000a\u000a=== Content ===\u000aThe combined databases includes the following:\u000a*23,000 [[Academic journal|academic]] and [[scientific journal]]s (including [[Web of Science]] journal listings)\u000a*23,000,000 [[patent]]s\u000a*110,000 conference [[proceedings]]\u000a*9,000 websites\u000a*Coverage from the year 1900 to present day (with Web of Science)\u000a*Over 40 million source items\u000a*Integrated and simultaneous searching across multiple databases<ref name=describe/>\u000a\u000a=== Included databases ===\u000aThe Web of Knowledge suite encompasses the following databases:<ref name=dbase-List>{{Cite web| last =''ISI Web of Knowledge''| title =Suite of databases| publisher =Thomson Reuters| year =2010| url = http://thomsonreuters.com/products_services/science/science_products/a-z/isi_web_of_knowledge?parentKey=555184 | format =List of databases that are part of the Web of Knowledge suite.| accessdate =2010-06-24}}</ref><ref name=AtoZ>{{Cite web| last = ISI Web of Knowledge platform| title =Available databases A to Z| publisher =Thomson Reuters| year =2010| url =http://wokinfo.com/products_tools/products/ | format =Choose databases on method of discovery and analysis| accessdate =2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref>\u000a{{columns-list|colwidth=30em|\u000a*[[Biological Abstracts]]\u000a*[[Biosis Previews]] \u000a*[[CAB Abstracts]]\u000a*[[CAB Direct|CAB Global Health]]\u000a*[[Chinese Science Citation Database]]\u000a*[[Conference Proceedings Citation Index]] \u000a*[[Current Contents|Current Contents Connect]]\u000a*[[Data Citation Index]]\u000a*[[Derwent Innovations Index]]\u000a*[[Essential Science Indicators]]\u000a*[[Food Science and Technology Abstracts]]\u000a*[[Inspec]] \u000a*[[ISI Highly Cited]]\u000a*[[Journal Citation Reports]]\u000a*[[MEDLINE]] \u000a*[[Web of Science]]\u000a**[[Arts & Humanities Citation Index]]\u000a**[[Book Citation Index]] \u000a**[[Current Chemical Reactions]]\u000a**[[Index Chemicus]]\u000a**[[Science Citation Index Expanded]]\u000a**[[Social Sciences Citation Index]]\u000a*[[The Zoological Record]]\u000a}}\u000a\u000a==See also==\u000a*[[List of academic journal search engines]]\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==External links==\u000a* {{Official website|http://wokinfo.com/}}\u000a\u000a{{Thomson Reuters}}\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Online databases]]\u000a[[Category:Thomson Reuters]]\u000a[[Category:Citation indices]]\u000a[[Category:Scholarly search services]]
p255
asI231
(lp256
VGoogle Web History
p257
aV'''Google Web History''' (previously '''Google Search History''') is a feature of [[Google Search]] and provided by [[Google]], in which all search queries and results that a user clicks on are recorded. The feature is only available for users logged into a [[Google Account]]. The feature was renamed from Search History to Web History on April 19, 2007.<ref>[http://searchengineland.com/google-search-history-expands-becomes-web-history-11016 "Google Search History Expands, Becomes Web History"]. Like all web hostory, google web history take up space and data on your phone, which is why many people choose to clear their hostory.  Search Engine Land. Retrieved July 12, 2010.</ref> A user's Web History is used to personalize search results with the help of [[Google Personalized Search]]<ref>[http://www.businessweek.com/the_thread/techbeat/archives/2009/12/google_gets_real-time_personalized_search.html "Google Gets Real-Time, Personalized Search"]. ''Business Week''. Retrieved July 12, 2010.</ref> and in [[Google Now]].\u000a\u000a==References== \u000aGoogle search engine searches more than just your questions, it matches the words you search with other online posts and files that have the same words, so you get more options and more answers, if you don't find what you're searching for, try rewording your question, often you will discover your question popping up after you type only a few words, and then you can go directly to the answer you were searching for. Google makes it possible and easy for everyone.\u000a{{Reflist}}\u000a\u000a==External links==\u000a* [http://history.google.com/history/ Google Web History] Also via redirect at [http://google.com/psearch]\u000a\u000a{{Google Inc.}}\u000a\u000a{{Google-stub}}\u000a\u000a[[Category:Google Search|Web History]]\u000a[[Category:Personalized search]]
p258
asI234
(lp259
VRapid Evolution
p260
aV{{Infobox Software\u000a| name                   = Rapid Evolution\u000a| screenshot             = <!--  Commented out: [[Image:RapidEvolutionScreenshot1.jpg|thumb|right|250px]] -->\u000a| caption                = Screenshot of Rapid Evolution 2.9.0\u000a| developer              = [[Jesse Bickmore]]\u000a| frequently_updated     = yes\u000a| operating system       = Any OS that supports Java\u000a| genre                  = Music Software\u000a| website                = [http://www.mixshare.com/ Mixshare]\u000a}}\u000a'''Rapid Evolution''' (also known as RE) is an [[open source]] [[software]] tool for [[DJs]], providing filtering and searching features suitable for musicians.  It can analyze audio files and automatically determine properties such as the musical key, [[beats per minute]] (BPM), beat intensity and [[ReplayGain]]. \u000a\u000aIt supports file types [[MP3]], [[MP4]], [[WAV]], [[FLAC]], [[Ogg|OGG]], [[Advanced Audio Coding|AAC]] and [[APE tag|APE]].  It helps [[DJs]] to organize and profile their music, and assists in the process of mixing music by utilizing song metadata to be able to show harmonically compatible songs and songs of a similar style.  It allows DJs to save and remember which songs are good matches (like a personal, digital mixing journal) and to plan entire mix sets.\u000a\u000aOne of its uses is to assist in a [[DJ]] technique called [[harmonic mixing]]. Once the musical key and BPM is known for a set of songs, [[DJs]] can use [[music theory]] (such as the [[Circle of Fifths]]) to identify songs that are harmonically compatible.  The act of mixing harmonically can help eliminate [[consonance and dissonance|dissonant]] tones while mixing songs together.  Since identifying whether songs can be made harmonically compatible can be quite complex (once features such as pitch lock are introduced), the software assists DJs by being able to show them which songs in their collection can be made harmonically compatible with any particular song.  It can also assist DJs in the act of [[beatmatching]] by showing which songs are in a compatible BPM range, and the percent of BPM difference.\u000a\u000aRapid Evolution is created and released through Mixshare.com.  The metadata generated by Rapid Evolution is shared through the central servers at Mixshare.com, which can be browsed online.  There are 1 million songs added to the database sharing information such as key, BPM, styles and ratings.\u000a\u000a==History==\u000aRapid Evolution was developed for the [[Microsoft Windows|Windows]] environment and released in 2003.  Starting in version 2.0 it was switched to run on the Java platform, allowing it to run in virtually any environment.  It is still actively developed.\u000a\u000aSeveral improvements to the key detection algorithm have been introduced over the years.  Rapid Evolution is the only program which can detect advanced key modes, such as aeolian, ionian, dorian, phrygian, lydian and mixolydian.  To date, there has only been one serious comparison of key detection accuracy (including programs such as [[Mixed In Key]] and Mixmeister).  It was shown that Rapid Evolution is the most accurate.<ref>{{cite web|title=Key Detection Software Comparison|url=http://www.mixingonbeat.com/phpbb/viewtopic.php?t=2268|date=2006-04-26|accessdate=2008-03-21|publisher=MixingOnBeat}}</ref>\u000a\u000aThe program was open-sourced on November 2013. <ref>{{cite web |url=http://www.mixshare.com/cgi-bin/yabb2/YaBB.pl?num=1381954407|title=Open-sourcing forum thread |date=2013-10-13 |accessdate=2014-04-17 |publisher=Mixshare}}</ref>\u000a\u000a==Community interest==\u000aRapid Evolution was originally a freeware program.<ref>{{cite web |url=http://www.mixshare.com/wiki/doku.php?id=testimonials|title=DJ Testimonials |date=2007-01-01 |accessdate=2008-03-21 |publisher=Mixshare}}</ref>Due to its vast feature set, Rapid Evolution tends to be suited more for experienced DJs versus beginners.  \u000a\u000a== See also ==\u000a*[[Harmonic mixing]]\u000a*[[Music Theory]]\u000a*[[DJing]]\u000a\u000a== External links ==\u000a*[http://www.mixshare.com/software Download Rapid Evolution]\u000a*[http://www.mixshare.com Mixshare's Official website]\u000a*[http://www.harmonic-mixing.com Harmonic-Mixing.com]\u000a*[https://github.com/djqualia/RapidEvolution2 Source code for version 2]\u000a*[https://github.com/djqualia/RapidEvolution3 Source code for version 3]\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Music search engines]]\u000a[[Category:OS X multimedia software]]\u000a[[Category:Windows multimedia software]]\u000a[[Category:Audio mixing software]]
p261
asI236
(lp262
VShazam (service)
p263
aV{{EngvarB|date=February 2014}}\u000a{{Use dmy dates|date=February 2014}}\u000a{{Infobox company\u000a| name             = Shazam Entertainment Ltd.\u000a| logo             = [[File:Shazam logo.png|160px]]\u000a| caption          = \u000a| type             = \u000a| traded_as        = \u000a| genre            = <!-- Only used with media and publishing companies -->\u000a| fate             = \u000a| predecessor      = \u000a| successor        = \u000a| foundation       = United States ({{Start date|1999}})\u000a| founder          = {{unbulleted list|Chris Barton|Philip Inghelbrecht|Dhiraj Mukherjee|Avery Wang}}\u000a| defunct          = <!-- {{End date|df=yes|YYYY|MM|DD}} -->\u000a| location_city    = London\u000a| location_country = United Kingdom\u000a| location         = \u000a| locations        = 7 offices (2014)\u000a| area_served      = Worldwide\u000a| key_people       = {{unbulleted list|Rich Riley (CEO)|Andrew Fisher (Executive chairman)}}\u000a| industry         = \u000a| products         = [[Application software|Apps]]\u000a| services         = \u000a| revenue          = \u000a| operating_income = \u000a| net_income       = \u000a| aum              = <!-- Only used with financial services companies -->\u000a| assets           = \u000a| equity           = \u000a| num_employees    = \u000a| divisions        = \u000a| subsid           = \u000a| homepage         = {{URL|//www.shazam.com/}} \u000a| footnotes        = \u000a| intl             = \u000a}}\u000a'''Shazam''' is a British app for smartphones, PCs<ref>{{Cite web|url = http://apps.microsoft.com/windows/en-au/app/shazam/5593d150-02c7-4714-ab8f-007d5d251688|title = Shazam|date = |accessdate = 7 January 2015|website = Shazam app for Windows in the Windows Store|publisher = Microsoft Corporation|last = |first = }}</ref> and Macs, which is best known for its music identification capabilities. Shazam Entertainment Limited was founded in 1999 by Chris Barton, Philip Inghelbrecht, Avery Wang and Dhiraj Mukherjee.<ref name="DirectorDec2009">{{cite news | url=http://www.director.co.uk/magazine/2009/11%20December/shazam_63_04.html | title=Shazam names that tune | date=December 2009 | accessdate=26 September 2012 | last=Woodward | first=David | newspaper=Director}}</ref> The company is best known for its music identification technology, but has expanded to integrations with cinema, advertising, TV and retail environments.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform</ref>\u000a\u000aShazam uses a smartphone or Mac's built-in microphone to gather a brief sample of audio being played.  It creates an [[acoustic fingerprint]] based on the sample, and compares it against a central database for a match.  If it finds a match, it sends information such as the artist, song title, and album back to the user. Some implementations of Shazam incorporate relevant links to services such as [[iTunes]], [[YouTube]], [[Spotify]] or [[Zune]]. In December of 2013, Shazam was one of the top ten apps in the world, according to its CEO.<ref>http://video.cnbc.com/gallery/?video=3000222563#.</ref> The Shazam app has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/</ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been</ref>\u000a==Features==\u000aShazam offers two types of applications; a free app simply called Shazam and a paid app called Shazam Encore. The service was expanded in September 2012 to enable TV users in the US to identify featured music, access cast information and get links to show information online, as well as adding social networking capabilities.<ref name="TV tags"/>\u000a\u000aIn February of 2014, Shazam announced a redesign of the app, which included a new look and additional features, including lyric-viewing options, access to music videos and related videos, unique recommendations, improved biographies and discographies and additional functionality for use with TV shows. The update also featured a News Feed, and Auto-Shazam, a feature introduced in December of 2013, which runs in the background of users\u2019 mobile devices to automatically identify media.<ref>http://www.digitaltveurope.net/151662/shazam-unveils-app-redesign/</ref>  \u000a\u000aIn July of 2014, Shazam announced the launch of Shazam for Mac, a desktop version of the app, which when enabled, runs in the background and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/</ref> Apple\u2019s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple\u2019s intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html</ref> \u000a\u000a==Devices==\u000aShazam is a free or low-cost application that runs on [[Android (operating system)|Android]], [[Apple Inc.|Apple]] [[iPhone]] iOS, [[BlackBerry]] OS, and [[Windows Phone|Windows]] systems. The application is similar on most phones and the result is shown on the screen complete with details on Artist, Album, Title, Genre, Music label, lyrics, a thumbnail image of the song/album artwork, links to download the song on [[iTunes]] or the [[Amazon MP3]] store and, where relevant, show the song's video on YouTube and give the option of playing the song on [[Rdio]]. Shazam is also available for Mac, as a desktop application.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref> \u000a\u000a== Function ==\u000aShazam works by analyzing the captured sound and seeking a match based on an [[acoustic fingerprint]] in a database of more than 11 million songs.<ref>[//www.shazam.com/music/web/about.html Shazam \u2013 About Shazam<!-- Bot generated title -->]</ref>\u000a\u000a[[File:Spectrogram of violin.png|thumb|A spectrogram of the sound of a violin.]]\u000a[[File:Target zone2.png|thumb|The target zone of a song scanned by Shazam.{{clarify|date=September 2012}}]]\u000aShazam identifies songs based on an audio fingerprint based on a time-frequency graph called a [[spectrogram]].\u000a\u000aShazam stores a catalogue of audio fingerprints in a database. The user tags a song for 10 seconds and the application creates an audio fingerprint.\u000a\u000aOnce it creates the fingerprint of the audio, Shazam starts the search for matches in the database. If there is a match, it returns the information to the user; otherwise it returns a "song not known" dialogue.<ref>[http://soyoucode.com/2011/how-does-shazam-recognize-song How does Shazam work to recognize a song ? | So, you code ?<!-- Bot generated title -->]</ref>\u000a\u000aShazam can identify prerecorded music being broadcast from any source, such as a radio, television, cinema or music in a club, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software's database.\u000a\u000a==History==\u000aThe company was founded in 1999 by Barton and Inghelbrecht, who were students at [[University of California, Berkeley]], and Mukherjee, who worked at a London-based internet consulting firm called Viant.{{Citation needed|date=September 2013}} In need of a digital signal processing specialist, the founding team then hired Wang, who was a PhD student from [[Stanford University]]. {{as of|September 2012}}, Wang is the only member of the original team to remain in the company,<ref name="DirectorDec2009" /> and serves as Shazam's Chief Scientist.<ref name="Shazam Team">{{cite web|title=About Shazam \u2013 Team|url=//www.shazam.com/music/web/team.html|accessdate=27 September 2012}}</ref>\u000a\u000a[[Rich Riley]] joined Shazam as CEO in April 2013 to increase the company\u2019s growth,<ref>http://www.huffingtonpost.co.uk/2013/08/30/shazam-rich-riley_n_3762179.html</ref> after over 13 years at Yahoo!<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> and with more than 17 years of experience as an entrepreneur and leading Internet executive.<ref>http://www.crunchbase.com/person/rich-riley </ref> "I look forward to extending our dominance in media engagement, from our roots in music to our leadership position in second-screen TV and want to ensure that Shazam is the company that helps people recognize and engage with the world around them,\u201d Riley said in a statement at the time.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> Riley replaced Andrew Fisher, who was hired from [[Infospace]] into the CEO role in 2005 to strengthen industry partnerships and grow the userbase.<ref name=DirectorDec2009 /> Fisher is now executive chairman.\u000a\u000a===Partnerships===\u000aThe first partnership was with Entertainment UK, part of Woolworths, whom they approached to digitise their music catalogue of 1.5 million songs in return for permission to create a proprietary database. As the service grew to have a worldwide userbase, it needed to keep its database up-to-date, which it does by having relationships with labels globally.<ref name="DirectorDec2009" /> By December 2008, the database had grown to 8 million songs.<ref>{{cite news|last=Reisinger|first=Don|title=Shazam adds 2 million tracks to music library|url=http://news.cnet.com/8301-17939_109-10113274-2.html|accessdate=29 September 2012|newspaper=CNET|date=4 December 2008}}</ref>\u000a\u000aIn February 2013, Shazam announced a partnership with the music store [[Beatport]], adding its library of [[electronic music]] to the service.<ref name=bb-shazambeatport>{{cite web|title=Beatport's Matthew Adell on Shazam Deal, Why Music Biz Is a 'Disaster Model'|url=http://www.billboard.com/biz/articles/news/digital-and-mobile/1538517/beatports-matthew-adell-on-shazam-deal-why-music-biz-is|work=Billboard.biz|accessdate=21 September 2013}}</ref> On 3 April 2013, Shazam announced an exclusive partnership with [[Saavn]], an Indian online music streaming service. The deal will add nearly 1 million songs in [[Languages of India|Indian languages]] to Shazam's database.<ref>[http://www.financialmirror.com/newsml_story.php?id=5458 Shazam Forms Exclusive New Partnership with Saavn for the Best Indian Music Discovery Experience<!-- Bot generated title -->]</ref><ref>{{cite news| url=http://blogs.wsj.com/speakeasy/2013/04/03/shazam-broadens-its-horizons/ | work=The Wall Street Journal | title=Shazam Broadens Its Horizons \u2013 Speakeasy \u2013 WSJ}}</ref><ref>[http://techcrunch.com/2013/04/03/shazam-partners-with-the-spotify-of-india-saavn-to-improve-its-south-asian-music-recognition/ Shazam Partners With The \u2018Spotify Of India\u2019, Saavn, To Improve Its South Asian Music Recognition | TechCrunch<!-- Bot generated title -->]</ref><ref>[http://www.medianama.com/2013/04/223-shazam-saavn-tieup/ Updated: Shazam Ties Up With Saavn To Identify Hindi & Regional Music; Implications \u2013 MediaNama<!-- Bot generated title -->]</ref> In July 2014, Shazam announced a partnership with Rdio that allows Shazam users to stream full songs within the app.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6157583/shazam-partners-with-rdio-to-stream-full-songs-inside </ref>\u000a\u000aIn addition to music, Shazam has announced collaborations with partners across television, advertising and cinema. In May of 2014, NCM Media Networks announced a partnership with Shazam to incorporate Shazam into FirstLook pre-show segments that run in Regal, AMC and Cinemark theaters.<ref>http://techcrunch.com/2014/05/14/shazam-partners-with-ncm/ </ref> In November of 2014, NCM and Shazam announced that NCM FirstLook pre-shows are now Shazam enabled on over 20,000 movie screens across the United States.<ref>http://mashable.com/2014/11/07/shazam-firstlook/ </ref>\u000a\u000aIn August of 2014, Shazam announced the launch of Resonate, a sales product that allows TV networks to access its technology and user base. The news included the announcement of partnerships with AMC, A+E, dick clark productions and FUSE.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform </ref>\u000a\u000aShazam recently announced a partnership with Sun Broadcast Group on Shazam for Radio, a new offering that will allow radio stations to push customized content to listeners on Sun Broadcast\u2019s over 8,000 radio stations in the U.S.<ref>http://thenextweb.com/insider/2014/10/09/shazam-makes-big-move-interactive-radio-content/ </ref>\u000a\u000a===Early days of the service===\u000aInitially, in 2002, the service was launched only in the UK and was known as "2580", as the number was the [[shortcode]] that customers dialled from their mobile phone to get music recognised.<ref name=DirectorDec2009 /> The phone would automatically hang up after 30 seconds. A result was then sent to the user in the form of a text message containing the song title and artist name. At a later date, the service also began to add hyperlinks in the text message to allow the user to download the song online.<ref name=CNETUKApril06>{{cite news|last=Lim|first=Andrew|title=Shazam & AQA: The answer is on your mobile|url=http://crave.cnet.co.uk/mobiles/shazam-and-aqa-the-answer-is-on-your-mobile-49264359/|accessdate=29 September 2012|newspaper=CNET UK|date=24 April 2006}}</ref>\u000a\u000aShazam launched in the US on the AT&T Wireless network in 2004 in a joint offering with Musicphone, a now defunct San Francisco-based company. The service was free at launch with AT&T saying that it would charge USD0.99 for each use in future.<ref name="cnet040415">{{cite news | url=http://news.cnet.com/Dial-that-tune-comes-to-U.S./2110-1039_3-5192105.html | title=Dial-that-tune comes to U.S. | work=CNET | date=15 April 2004 | accessdate=29 September 2012 | author=Charny, Ben}}</ref>\u000a\u000aIn 2006, users were charged £0.60 per call or had unlimited use for £4.50 a month, as well as an online service to keep track of all tags.<ref name=CNETUKApril06/>\u000a\u000a===Smartphone app===\u000aShazam for iPhone 2.0 debuted on 10 July 2008, with the launch of Apple's App Store. The free app simplified the service by enabling the user to launch iTunes and buy the song directly if the user was on a Wi-Fi connection <ref name=CNET080710>{{cite news|last=Rosoff|first=Matt|title=Shazam on iPhone could change music discovery|url=http://news.cnet.com/8301-13526_3-9988219-27.html|accessdate=29 September 2012|newspaper=CNET|date=10 July 2008}}</ref> (at the time, iTunes did not allow music downloads over 3G). It was also possible to launch the iPhone YouTube app, if a video was available.<ref name=CNET080716>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=29 September 2012|newspaper=CNET|date=16 July 2008}}</ref>\u000a\u000aIn 2008, the service struggled to identify classical music.<ref>{{cite news|last=Ho|first=Kevin|title=iPhone apps: Testing Shazam's limits \u2013 classical music|url=http://news.cnet.com/8301-13544_3-9993320-35.html|accessdate=29 September 2012|newspaper=CNET|date=17 July 2008}}</ref>\u000a\u000aShazam launched on the [[Android operating system|Android platform]] in October 2008. The Android app connected to [[Amazon Appstore|Amazon's MP3 store]] instead of iTunes.<ref name=AndroidLaunch>{{cite news|last=Reisinger|first=Don|title=Shazam moves to Android, works with Amazon MP3 Store|url=http://news.cnet.com/8301-17939_109-10071167-2.html|accessdate=29 September 2012|newspaper=CNET|date=21 October 2008}}</ref>\u000a\u000aAlongside the iOS 3 update in July 2009, Shazam updated its app to include a number of new features: marking the tag with GPS coordinates; sending tags to others as 'postcards', enabling them to buy the song; and Twitter integration.<ref>{{cite news|last=Lee|first=Nicola|title=Latest Shazam lets you track musical journey in iPhone OS 3.0|url=http://download.cnet.com/8301-2007_4-10267205-12.html|accessdate=29 September 2012|newspaper=CNET|date=17 June 2009}}</ref>\u000a\u000aThe app launched on the [[Windows Marketplace for Mobile|Windows Mobile Marketplace]] in October 2009 as a [[freemium]] offering, with the first release of Shazam Encore. The free version was now limited to five tags per month: users typically tagged ten songs per month. Encore, priced at USD4.69, added several features such as song popularity charts and recommendations.<ref name=CNETWindowsLaunch>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=30 September 2012|newspaper=CNET|date=7 October 2009}}</ref> Encore first appeared for iPhone in November 2009.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam iPhone app gets premium Encore|url=http://download.cnet.com/8301-2007_4-10393035-12.html|accessdate=30 September 2012|newspaper=CNET|date=9 November 2009}}</ref>\u000a\u000aBy December 2009, Shazam was downloaded 10 million times in 150 countries across 350 mobile operators. Around eight percent of users purchased a track after it was identified by the service.<ref name=DirectorDec2009 /> Its success led to a funding round from [[Kleiner Perkins Caufield & Byers]] in October 2009.<ref name=DirectorDec2009 /><ref>{{cite news|last=Saint|first=Nick|title=Shazam Draws Investment, Is Already Profitable|url=http://www.businessinsider.com/shazam-draws-investment-is-already-profitable-2009-10|accessdate=30 September 2012|newspaper=Business Insider|date=15 October 2009}}</ref> In January 2011, Apple announced that Shazam was the fourth most downloaded free app of all time on the App Store, while rival [[SoundHound]] had the top paid iPad app.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=30 September 2012|newspaper=CNET|date=19 January 2011}}</ref>\u000a\u000aEarly adopters of the free application are still allowed unlimited tagging.<ref>[http://androidforums.com/android-applications/132182-shazam-how-preserve-unlimited-tagging-feature-after-reflash-root.html#post1488306 Shazam: How to preserve the "unlimited tagging" feature after REFLASH and Root? \u2013 Android Forums<!-- Bot generated title -->]</ref>\u000a\u000a[[GetJar]], an app store for Android, Blackberry and Symbian, added Shazam in November 2010.<ref>{{cite news|last=Reisinger|first=Don|title=AT&T ladles out GetJar apps \u2013 iPhone excluded|url=http://news.cnet.com/8301-13506_3-20022340-17.html|accessdate=30 September 2012|newspaper=CNET|date=10 November 2010}}</ref>\u000a\u000aIn January 2011, Shazam and [[Spotify]] announced a partnership for iOS and Android to help users identify music with Shazam and listen to tracks through Spotify.<ref>{{cite news|last=Morris|first=Natali|title=Space love|url=http://cnettv.cnet.com/8301-13991_53-20028388-10391624.html|accessdate=30 September 2012|newspaper=CNET|date=13 January 2011}}</ref>\u000a\u000aWhile Shazam already had Facebook and Twitter share buttons, deeper Facebook integration was released in March 2011. With Shazam Friends users can see what their Facebook friends have tagged, listen to the tracks and buy them.<ref>{{cite news|last=McCarthy|first=Caroline|title=Music app Shazam gets new Facebook features|url=http://news.cnet.com/8301-13577_3-20045965-36.html|accessdate=1 October 2012|newspaper=CNET|date=22 March 2011}}</ref>\u000a\u000aWith Shazam 5.0, released in April 2012, the app begins 'listening' as soon as it is launched and can take as little as one second to identify media. In addition to music, the app can identify TV programs and ads, if they are Shazam-enabled.<ref>{{cite news|last=Parker|first=Jason|title=Shazam for iOS adds TV to its list of media it can identify|url=http://reviews.cnet.com/8301-19512_7-57408964-233/shazam-for-ios-adds-tv-to-its-list-of-media-it-can-identify/|accessdate=1 October 2012|newspaper=CNET|date=3 April 2012}}</ref>\u000a\u000aIn August 2012, Shazam announced the service had been used to tag five billion songs, TV shows and advertisements. In addition, Shazam claimed to have over 225 million users across 200 countries.<ref>{{cite news|last=Sawers|first=Paul|title=Shazam: Five billion songs, TV shows and ads tagged|url=http://thenextweb.com/insider/2012/08/07/shazam-five-billion-songs-tv-shows-and-ads-tagged/|accessdate=30 September 2012|newspaper=The Next Web|date=7 August 2012}}</ref> A month later, the service claimed to have more than 250 million users with 2 million active users per week.<ref name="TV tags">{{cite news|last=Kinder|first=Lucy|title=Shazam hits 250 million users and adds TV tagging capability|url=http://www.telegraph.co.uk/technology/news/9547632/Shazam-hits-250-million-users-and-adds-TV-tagging-capability.html|accessdate=17 September 2012|newspaper=The Telegraph|date=17 September 2012|location=London}}</ref> The Shazam app currently has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/ </ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been </ref>\u000a\u000aThe Shazam app was listed among Techland's 50 Best Android Applications for 2013.<ref>{{cite news |url=http://techland.time.com/2013/07/01/50-best-android-apps-for-2013/slide/pulse-news/ | title=50 Best Android Apps for 2013 | author=Jared Newman | work=Techland | accessdate=30 June 2013 | date=1 July 2013}}</ref>\u000a\u000aIn August 2014, Shazam announced there would be no more updates for Shazam(RED) after August 7.<ref>[https://support.shazam.com/hc/en-us/articles/202604996-Important-News-About-SHAZAM-RED Important News About Shazam(RED)] \u2014 Shazam Support</ref> Current users are advised to switch to the free version with tags transferred and ads removed (for free).\u000a\u000aApple\u2019s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple\u2019s intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html </ref>\u000a\u000aIn October of 2014, Shazam introduced version 8.0 of the app, which features a new and improved News feed, as well as a section featuring Shazam charts and an \u201cexplore\u201d option which lets user explore Shazamed tracks near them and around the world.<ref>http://appadvice.com/appnn/2014/10/shazam-8-0-features-interactive-notifications-in-ios-8-revamped-news-feed-and-more </ref>\u000a\u000a===Desktop app=== \u000aShazam announced the launch of Shazam for Mac, a desktop application, in July of 2014. When enabled, the app runs in the background of a Mac and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref>\u000a\u000a==Similar apps==\u000a\u000a*[[SoundHound]], previously known as Midomi, uses [[Query by humming]] to identify songs.{{citation needed|date=October 2012}}\u000a*[[Gracenote]]'s MusicID-Stream has the main advantage of having the largest database of all music IDs (with more than 28 million songs).{{citation needed|date=October 2012}}\u000a*Musipedia is a music search engine that works differently from others because instead of using techniques to identify recorded music, it can identify pieces of music from a single melody or rhythm.{{citation needed|date=October 2012}}\u000a*Play by Yahoo Music.\u000a*Bing music identification.\u000a*Sony TrackID\u000a*Path also has a music-identification feature.<ref>{{cite news|last=Cabebe|first=Jaymar|title=Path: The smaller, simpler alternative to Facebook|url=http://news.cnet.com/8301-1035_3-57416066-94/path-the-smaller-simpler-alternative-to-facebook/|accessdate=1 October 2012|newspaper=CNET|date=18 April 2012}}</ref>\u000a*Stream That Song by Orange Innovation UK Ltd\u000a\u000a==Patent infringement lawsuit==\u000aIn May 2009, Tune Hunter accused Shazam of violating {{US Patent|6941275}}, which covers music identification and purchase in a portable device.<ref>{{cite news|last=Ogg|first=Erica|title=Apple, AT&T, Samsung, Verizon, and others sued over Shazam app|url=http://news.cnet.com/8301-13579_3-10241309-37.html|accessdate=29 September 2012|newspaper=CNET|date=14 May 2009}}</ref> Shazam settled the case in January 2010.<ref>{{cite news\u000a|title=Shazam Settles Patent Infringement Case With Tune Hunter\u000a|url=http://techcrunch.com/2010/01/06/shazam-tune-hunter-settlement/\u000a|date=Jan 6, 2010\u000a|first=Robin\u000a|last=Wauters\u000a}}</ref>\u000a\u000a==Funding==\u000a\u000aAs of September 2012, Shazam had raised $32 million in funding.<ref name="Techcrunch">{{cite news|last=Kincaid|first=Jason|title=Shazam Raises A Huge Round to the Tune of $32 Million|url=http://techcrunch.com/2011/06/22/shazam-raises-a-huge-round-to-the-tune-of-32-million/|accessdate=20 September 2012|newspaper=TechCrunch|date=22 June 2011}}</ref> In July 2013, [[Carlos Slim]] invested $40 million in Shazam for an undisclosed share.<ref>[http://www.ft.com/intl/cms/s/0/97d2c46a-e58d-11e2-8d0b-00144feabdc0.html#axzz2ag6DhVhD Carlos Slim invests $40m in music app Shazam \u2013 FT.com<!-- Bot generated title -->]</ref> And in March of 2014, Shazam confirmed another $20 million in new funding, raising the total value of the company to half a billion dollars.<ref>http://www.billboard.com/biz/articles/5930359/shazam-confirms-20m-in-new-funding-raising-value-to-500m </ref>\u000a\u000a==See also==\u000a* [[Query by humming]]\u000a* [[Acoustic fingerprint]]\u000a* [[Spectrogram]]\u000a* [[Sound recording copyright symbol]]\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==Further reading==\u000a* {{cite news |last=Dredge |first=Stuart |title=Shazam: 'TV advertising is going to become our primary revenue stream' |url=http://www.guardian.co.uk/media/appsblog/2013/feb/27/shazam-tv-advertising-future |accessdate=27 February 2013 |newspaper=[[The Guardian]] |date=27 February 2013|location=London}}\u000a\u000a==External links==\u000a* {{Official website|www.shazam.com/music/web/home.html}}\u000a\u000a[[Category:Companies based in London]]\u000a[[Category:Acoustic fingerprinting]]\u000a[[Category:Android (operating system) software]]\u000a[[Category:BlackBerry software]]\u000a[[Category:IOS software]]\u000a[[Category:Symbian software]]\u000a[[Category:Music search engines]]\u000a[[Category:Companies established in 1999]]\u000a[[Category:Windows Phone software]]
p264
asI238
(lp265
VMusicRadar (service)
p266
aV{{multiple issues|\u000a{{advert|date=August 2013}}\u000a{{cleanup|reason=Syntax, capitals|date=August 2013}}\u000a{{fanpov|date=August 2013}}\u000a{{Orphan|date=August 2013}}\u000a}}\u000a{{Infobox Website\u000a|name=MusicRadar(\u97f3\u4e50\u96f7\u8fbe)\u000a|logo=\u000a|screenshot=\u000a|caption=\u000a|url=http://www.doreso.com/\u000a|commercial=Yes\u000a|type=[[Music]] [[website]]\u000a|registration=Optional\u000a|owner=Shanghai Yinlong Information Technology Co., LTD\u000a|author=Shanghai Yinlong Information Technology Co., LTD\u000a|launch date=January 2013\u000a|current status=\u000a|revenue=}}\u000a\u000a'''Music radar''' is a sound-to-sound music search engine, which allows users to obtain more detailed information of music/songs by singing/humming or by recording original music. It is available on [[App Store (iOS)|App Store]]<ref>https://itunes.apple.com/cn/app/yin-le-lei-da/id635262613</ref> for [[iPhone]] and [[Google Play]]<ref>https://play.google.com/store/apps/details?id=com.voicedragon.musicclient.googleplay</ref> for [[Android (operating system)|Android]] mobiles. Music radar was launched by Shanghai Yinlong Information Technology Co., LTD in Jan. 2013.<ref>http://www.doreso.com/</ref>\u000a\u000a==Features==\u000aThe app (MusicRadar) currently has three ways of searching music: by identifying recorded original music fragment; by humming or singing the melody using microphone; and by direct input of the name of song or singer. Users could share their searching results on [[Facebook]], [[Twitter]] or other SNS website.\u000a\u000a==History==\u000aThe music radar team got the 1st place on Query by Singing/Humming (QBSH) task, Music Information Retrieval Evaluation eXchange (MIREX) 2012.<ref>http://www.music-ir.org/mirex/wiki/2012:Main_Page</ref> The app was launched for business intention at the end of January 2013, supporting query by singing/humming & audio fingerprinting. After two months, the app has reached its first one million user milestone in April, 2013. In May 2013, Music radar announces that it has integrated deep learning techniques in its query by singing/humming module to promote the recognize rate and reduce the user\u2019s waiting time. In July 2013, Music radar released China's first cloud based music recognizing openAPI to public.<ref>[http://roll.sohu.com/20130731/n383076514.shtml 2013\u5e747\u6708\uff0c\u97f3\u4e50\u96f7\u8fbe\u53d1\u5e03\u4e86\u56fd\u5185\u7b2c\u4e00\u4e2a\u201c\u97f3\u9891\u68c0\u7d22\u5f00\u653e\u4e91\u5e73\u53f0\u201d\uff0c\u63d0\u4f9b\u5f00\u653e\u7684\u97f3\u9891\u68c0\u7d22API\u3002]</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a\u000a[[Category:Acoustic fingerprinting]]\u000a[[Category:Music search engines]]\u000a[[Category:IOS software]]\u000a[[Category:Android (operating system) software]]
p267
as.None

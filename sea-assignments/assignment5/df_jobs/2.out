(dp0
I128
(lp1
VSemantic compression
p2
aVIn [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build \u000aa textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. \u000aAs a result, the same ideas can be represented using a smaller set of words.\u000a\u000aSemantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document \u000acannot be reconstructed in a reverse process.\u000a\u000a==Semantic compression by generalization==\u000aSemantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:\u000a#	determining cumulated term frequencies to identify target lexicon,\u000a#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>\u000a\u000aStep 1 requires assembling word frequencies and \u000ainformation on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, \u000aa cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:\u000a<math>cum f(k_{i}) = f(k_{i}) + \u005csum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.\u000aThen, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.\u000a\u000aIn the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence \u000aof a less frequent hyponym as its hypernym in output text.\u000a\u000a;Example\u000a\u000aThe below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.\u000a\u000a<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' \u000ain very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects \u000a'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the \u000a'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of \u000a'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>\u000a\u000aThe procedure outputs the following text:\u000a\u000a<blockquote>They are both '''facility''' building '''insect''', but '''insect''' and honey '''insects''' '''arrange''' their '''biological groups''' \u000ain very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects \u000a'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the \u000a'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of \u000a'''organic process''', and there are '''impinging difference of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>\u000a\u000a==Implicit semantic compression==\u000aA natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)\u000a.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],\u000aCOLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>\u000a\u000a==Applications and advantages==\u000aIn the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less \u000a[[computational complexity]] and a positive influence on efficiency. \u000a\u000aSemantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity \u2013 limited language redundancy, a step towards a controlled dictionary).\u000a\u000aAs in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).\u000a\u000a==See also==\u000a* [[Text simplification]]\u000a* [[Lexical substitution]]\u000a* [[Information theory]]\u000a* [[Quantities of information]]\u000a\u000a==References==\u000a<references/>\u000a\u000a==External links==\u000a* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Quantitative linguistics]]\u000a[[Category:Computational linguistics]]
p3
asI2
(lp4
VDocument retrieval
p5
aV'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\u000a\u000aDocument retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.\u000a\u000a==Description==\u000aDocument retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.\u000a\u000aA document retrieval system has two main tasks:\u000a# Find relevant documents to user queries\u000a# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].\u000a\u000aInternet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.\u000a\u000a==Variations==\u000aThere are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.\u000a\u000a===Form based===\u000aForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.\u000a\u000a===Content based===\u000aThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.\u000a\u000aA ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.\u000a\u000a==Example: PubMed==\u000aThe [[PubMed]]<ref>{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319\u201323 |year=2001 |pmid=11825203 |pmc=2243528 }}\u000a</ref> form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>\u000a\u000a== See also ==\u000a\u000a* [[Compound term processing]]\u000a* [[Document classification]]\u000a* [[Enterprise search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Latent semantic indexing]]\u000a* [[Search engine]]\u000a\u000a== References ==\u000a\u000a<references/>\u000a\u000a==Further reading==\u000a* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267\u2013288|doi=10.1145/2275.357411}}\u000a* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453\u2013490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}\u000a* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613\u2013633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}\u000a\u000a== External links ==\u000a* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Electronic documents]]\u000a[[Category:Substring indices]]\u000a\u000a[[zh:\u6587\u672c\u4fe1\u606f\u68c0\u7d22]]
p6
asI4
(lp7
VDivergence-from-randomness model
p8
aVIn the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.\u000a\u000aTerm weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.\u000a\u000a==External links==\u000a*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]\u000a*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]\u000a\u000a[[Category:Ranking functions]]\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]\u000a\u000a\u000a{{comp-sci-stub}}
p9
asI133
(lp10
VTemporal information retrieval
p11
aV'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.\u000a\u000aAccording to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078\u20132091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a document\u2019s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company\u2019s earnings or information on already-happened or invalid predictions.\u000a\u000aT-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.\u000a\u000aThis page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.\u000a\u000a== Temporal dynamics (T-dynamics) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&nbsp;117 \u2013 130). Lisbon, Portugal. September 11\u201313: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||\u000a|-\u000a|'''Cho, J., & Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||\u000a|-\u000a| '''Fetterly, D., Manasse, M., Najork, M., & Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&nbsp;669 \u2013 678). Budapest, Hungary. May 20\u201324: ACM Press. || 2003 || WWW || T-Dynamics ||\u000a|-\u000a| '''Ntoulas, A., Cho, J., & Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1 \u2013 12). New York, NY, United States. May 17\u201322: ACM Press. || 2004 || WWW || T-Dynamics ||\u000a|-\u000a| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \u2013 142). Paris, France. June 13\u201318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\u000a|-\u000a| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909 \u2013 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||\u000a|-\u000a| '''Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;282 \u2013 291). Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 \u2013 10). New York, United States. February 3\u201306: ACM Press. || 2010 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 \u2013 1124). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || F-IRetrieval ||\u000a|-\u000a| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction'' (pp.&nbsp;273 \u2013 281). Washington DC, United States. March 30\u201331: Springer-Verlag. || 2010 || SBP || T-Dynamics ||\u000a|-\u000a| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 \u2013 176). Hong Kong, China. February 9\u201312: ACM Press. || 2011 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||\u000a|-\u000a| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 \u2013 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\u000a|-\u000a| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&nbsp;1171 \u2013 1172). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || T-Dynamics ||\u000a|-\u000a| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 \u2013 596). Lisboa, Portugal. October 10\u201313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 \u2013 1240). Glasgow, Scotland, UK. October 24\u201328: ACM Press. || 2011 || CIKM || C-Memory ||\u000a|-\u000a| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\u201311: ACM Press. || 2014 || SIGIR || T-RModels ||\u000a|}\u000a\u000a== Temporal markup languages (T-MLanguages) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Setzer, A., & Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||\u000a|-\u000a| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||\u000a|-\u000a| '''Ferro, L., Mani, I., Sundheim, B., & Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||\u000a|-\u000a| '''Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&nbsp;28 \u2013 34). Tilburg, Netherlands. January 15\u201317. || 2003 || IWCS || T-MLanguages ||\u000a|-\u000a| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||\u000a|}\u000a\u000a== Temporal taggers (T-taggers) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., & Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;69 \u2013 76). Hong Kong, China. October 1\u20138: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||\u000a|-\u000a| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;168 \u2013 175). Philadelphia, PA, United States. July 6\u201312: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||\u000a|-\u000a| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||\u000a|-\u000a| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&nbsp;321 \u2013 324). Uppsala, Sweden. July 11\u201316.|| 2010 || ACL - SemEval || T-Taggers ||\u000a|-\u000a| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\u000a|-\u000a| '''Chang, A., & Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\u000a|-\u000a| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||\u000a|-\u000a| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. & Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]\u000a|}\u000a\u000a== Temporal indexing (T-indexing) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;519 \u2013 526). Amsterdam, Netherlands. July 23\u201327: ACM Press. || 2007 || SIGIR || W-Archives ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM ||| T-RModels ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 \u2013 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||\u000a|}\u000a\u000a== Temporal query understanding (TQ-understanding) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \u2013 142). Paris, France. June 13\u201318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\u000a|-\u000a| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 \u2013 1438). Napa Valley, California, United States. October 26\u201330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;182 \u2013 191). Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM || TQ-Understanding ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''König, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;347 \u2013 354). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 \u2013 20). New York, United States. February 3\u20136: ACM Press. || 2010 || WSDM || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6\u201310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\u000a|-\u000a| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&nbsp;1129 \u2013 1139). Massachusetts, United States. October 9\u201311: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||\u000a|-\u000a| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 \u2013 176). Hong Kong, China. February 9\u201312: ACM Press. || 2011 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&nbsp;1325). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 \u2013 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\u000a|-\u000a| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;1171 \u2013 1172). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;41 \u2013 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||\u000a|-\u000a| '''Shokouhi, M., & Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;601 \u2013 610). Portland, United States. August 12\u201316.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2035 \u2013 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 \u2013 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\u000a|}\u000a\u000a== Time-aware retrieval/ranking models (T-RModels) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Li, X., & Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;469 \u2013 475). New Orleans, Louisiana, United States. November 2\u20138: ACM Press. || 2003 || CIKM || T-RModels ||\u000a|-\u000a| '''Sato, N., Uehara, M., & Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&nbsp;215 \u2013 220). Prague, Czech Republic. September 1\u20135: IEEE. || 2003 || DEXA || T-RModels ||\u000a|-\u000a| '''Berberich, K., Vazirgiannis, M., & Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||\u000a|-\u000a| '''Cho, J., Roy, S., & Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551 \u2013 562). Baltimore, United States. June 13\u201316: ACM Press. || 2005 || SIGMOD || T-RModels ||\u000a|-\u000a| '''Perkiö, J., Buntine, W., & Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;647 \u2013 648). Salvador, Brazil. August 15\u201316: ACM Press. || 2005 || SIGIR || T-RModels ||\u000a|-\u000a| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|-\u000a| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 \u2013 1438). Napa Valley, California, United States. October 26\u201330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9\u201312: ACM Press. || 2009 || WSDM ||| T-RModels ||\u000a|-\u000a| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&nbsp;165 \u2013 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||\u000a|-\u000a| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 \u2013 701). Boston, MA, United States. July 19\u201323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 \u2013 10). New York, United States. February 3\u201306: ACM Press. || 2010 || WSDM || T-Dynamics ||\u000a|-\u000a| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;629 \u2013 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||\u000a|-\u000a| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 \u2013 20). New York, United States. February 3\u20136: ACM Press. || 2010 || WSDM || T-RModels ||\u000a|-\u000a| '''Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&nbsp;13 \u2013 25). Milton Keynes, UK. March 28\u201331: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||\u000a|-\u000a| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;331 \u2013 340). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || T-RModels ||\u000a|-\u000a| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&nbsp;331 \u2013 340). Atlanta, United States. June 11\u201315: AAAI Press. || 2010 || AAAI || T-RModels ||\u000a|-\u000a| '''Dai, N., & Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;114 \u2013 121). Geneve, Switzerland. July 19\u201323: ACM Press. || 2010 || SIGIR || T-RModels ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6\u201310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\u000a|-\u000a| '''Efron, M., & Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;495 \u2013 504). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || T-RModels ||\u000a|-\u000a| '''Dai, N., Shokouhi, M., & Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;95 \u2013 104). Beijing, China. July 24\u201328.: ACM Press. || 2011 || SIGIR || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 \u2013 764). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\u000a|-\u000a| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1101 \u2013 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||\u000a|-\u000a| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2463 \u2013 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||\u000a|-\u000a| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 \u2013 172). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || T-IRetrieval ||\u000a|-\u000a| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\u201311: ACM Press. || 2014 || SIGIR || T-RModels ||\u000a|}\u000a\u000a== Temporal clustering (T-clustering) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 \u2013 174). Houston, United States. November 27\u201330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 \u2013 124). Austin, United States. June 15\u201319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292 \u2013 296). Funchal - Madeira, Portugal. October 6\u20138. || 2009 || KDIR || T-Clustering ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 \u2013 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\u000a|}\u000a\u000a== Temporal text classification (T-classification) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Jong, F., Rode, H., & Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&nbsp;161 \u2013 168). Amsterdam, Netherlands. September 14\u201317 || 2005 || AHC || T-Classification ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&nbsp;233 \u2013 241). Edinburgh, Scotland. May 23\u201326: ACM Press. || 2006 || WWW || T-Classification ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;129 \u2013 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;137 \u2013 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\u000a|-\u000a| '''Kanhabua, N., & Nørvåg, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&nbsp;358 \u2013 370). Aarhus, Denmark. September 14\u201319: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Strötgen, J., Alonso, O., & Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;33 \u2013 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||\u000a|-\u000a| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7\u201313. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]\u000a|}\u000a\u000a== Temporal visualization (T-interfaces) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 \u2013 56). Athens, Greece. July 24\u201328: ACM Press. || 2000 || SIGIR || TDT ||\u000a|-\u000a| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 \u2013 80). Boston, Massachusetts, United States. August 20\u201323: ACM Press. || 2000 || KDD - TM || TDT ||\u000a|-\u000a| [http://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||\u000a|-\u000a| '''Cousins, S., & Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||\u000a|-\u000a| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&nbsp;125 \u2013 137). Seattle, Washington, United States. August 17\u201319: ACM Press. || 1994 || ISSTA || T-Interfaces ||\u000a|-\u000a| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&nbsp;221 \u2013 227). Vancouver, British Columbia, Canada. April 13\u201318: ACM Press. || 1996 || CHI || T-Interfaces ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 \u2013 160). Salzburg, Austria. September 6\u20139: ACM Press. || 2005 || HT || W-Archives ||\u000a|-\u000a| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 \u2013 120). Palo Alto, California, United States. July 11\u201314: ACM Press. || 2006 || ICWE || W-Archives ||\u000a|-\u000a| '''Catizone, R., Dalli, A., & Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24\u201326: ELDA. || 2006 || LREC || T-Interfaces ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&nbsp;1221 \u2013 1222). Beijing, China. April 21\u201325: ACM Press. || 2008 || WWW || W-Archives ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8\u201310: ACM Press. || 2008 || WikiSym || T-Interfaces ||\u000a|-\u000a| '''Nunes, S., Ribeiro, C., & David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&nbsp;601 \u2013 604). Aveiro, Portugal. October 12\u201315. || 2009 || EPIA || T-Interfaces ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&nbsp;549 \u2013 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||\u000a|}\u000a\u000a== Temporal search engines (T-SEngine) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 \u2013 598). Seattle, Washington, United States. August 6\u201311: ACM Press. || 2006 || SIGIR || T-Clustering ||\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\u000a|-\u000a| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 \u2013 224). Shanghai, China. December 21\u201322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2\u20136: ACM Press. || 2009 || CIKM || T-Clustering ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|}\u000a\u000a== Temporal question answering (T-QAnswering) ==\u000a{| class="wikitable sortable"\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 \u2013 1121). Fortaleza, Ceara, Brazil. March 16\u201320: ACM Press. || 2008 || SAC || T-QAnswering ||\u000a|}\u000a\u000a== Temporal snippets (T-snippets) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20\u201324: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||\u000a|-\u000a| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&nbsp;26 \u2013 31). Pisa, Italy. October 17\u201321.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||\u000a|-\u000a| '''Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1045 \u2013 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||\u000a|}\u000a\u000a== Future information retrieval (F-IRetrieval) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15\u201319: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 \u2013 124). Austin, United States. June 15\u201319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\u000a|-\u000a| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 \u2013 175). Suwon, Republic of Korea. January 14\u201315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\u000a|-\u000a| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 \u2013 1124). Raleigh, United States. April 26\u201330: ACM Press. || 2010 || WWW || F-IRetrieval ||\u000a|-\u000a| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 \u2013 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\u000a|-\u000a| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\u000a|-\u000a| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 \u2013 764). Beijing, China. July 24\u201328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\u000a|-\u000a| '''Kanazawa, K., Jatowt, A., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;278 \u2013 283). Lyon, France. August 22\u201327: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||\u000a|-\u000a| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 \u2013 596). Lisboa, Portugal. October 10\u201313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\u000a|-\u000a| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 \u2013 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\u000a|-\u000a| '''Weerkamp, W., & Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||\u000a|-\u000a| '''Radinski, K., & Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;255 \u2013 264). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || F-IRetrieval ||\u000a|}\u000a\u000a== Temporal image retrieval (T-IRetrieval) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&nbsp;199 \u2013 204). Cartagena de Indias, Colombia. October 21\u201325: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||\u000a|-\u000a| '''Palermo, F., Hays, J., & Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&nbsp;499 \u2013 512). Firenze, Italy. October 07\u201313: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||\u000a|-\u000a| '''Kim, G., & Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 \u2013 172). Rome, Italy. February 4\u20138: ACM Press. || 2013 || WSDM || T-IRetrieval ||\u000a|-\u000a| '''Martin, P., Doucet, A., & Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||\u000a|}\u000a\u000a== Collective memory (C-memory) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||\u000a|-\u000a| '''Hall, D., Jurafsky, D., & Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&nbsp;363 \u2013 371). Waikiki, Honolulu, Hawaii. October 25\u201327: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||\u000a|-\u000a| '''Shahaf, D., & Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623 \u2013 632). Washington, United States. July 25\u201328: ACM Press. || 2010 || KDD || C-Memory ||\u000a|-\u000a| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;83 \u2013 92). Eindhoven, Netherlands. June 6\u20139: ACM Press. || 2011 || HT || C-Memory ||\u000a|-\u000a| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||\u000a|-\u000a| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 \u2013 1240). Glasgow, Scotland, UK. October 24\u201328: ACM Press. || 2011 || CIKM || C-Memory ||\u000a|}\u000a\u000a== Web archives (W-archives) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||\u000a|-\u000a| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&nbsp;72 \u2013 73. || 1997 || SAM || W-Archives ||\u000a|-\u000a| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 \u2013 160). Salzburg, Austria. September 6\u20139: ACM Press. || 2005 || HT || W-Archives ||\u000a|-\u000a| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 \u2013 120). Palo Alto, California, United States. July 11\u201314: ACM Press. || 2006 || ICWE || W-Archives ||\u000a|-\u000a| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&nbsp;135 \u2013 144). Odense, Denmark. August 22\u201325: ACM Press. || 2006 || HT || W-Archives ||\u000a|-\u000a| '''Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&nbsp;239 \u2013 248). Monterey, CA, United States. October 19\u201322: ACM Press. || 2008 || UIST || W-Archives ||\u000a|-\u000a| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\u000a|-\u000a| '''Gomes, D., Miranda, J., & Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&nbsp;408 \u2013 420). Berlin, Germany. September 25\u201329: Springer-Verlag || 2011 || TPDL || W-Archives ||\u000a|-\u000a| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 \u2013 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||\u000a||\u000a|-\u000a| '''Costa, M., & Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||\u000a|}\u000a\u000a== Topic detection and tracking (TDT) ==\u000a{| class="wikitable sortable"\u000a|-\u000a! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\u000a|-\u000a| '''Allan, J., Carbonell, J., Doddington, G., & Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194 \u2013 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||\u000a|-\u000a| '''Swan, R., & Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;38 \u2013 45). Kansas City, Missouri, United States. November 2\u20136: ACM Press. || 1999 || CIKM || TDT ||\u000a|-\u000a| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 \u2013 80). Boston, Massachusetts, United States. August 20\u201323: ACM Press. || 2000 || KDD - TM || TDT ||\u000a|-\u000a| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 \u2013 56). Athens, Greece. July 24\u201328: ACM Press. || 2000 || SIGIR || TDT ||\u000a|-\u000a| '''Makkonen, J., & Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&nbsp;393 \u2013 404). Trondheim, Norway. August 17\u201322: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||\u000a|-\u000a| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 \u2013 174). Houston, United States. November 27\u201330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\u000a|-\u000a| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 \u2013 342). Hong Kong, China. December 18\u201322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\u000a|-\u000a| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&nbsp;227 \u2013 242). New York, United States. || 2004 || TALIP || TDT ||\u000a|}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]
p12
asI136
(lp13
VPleade
p14
aV{{Infobox software\u000a| name                   = Pleade-infoxbox\u000a| title                  = Pleade\u000a| logo                   = [[File:Pleade-logo.png]]\u000a| logo caption           = Logo de Pleade\u000a| screenshot             = <!-- [[File: ]] -->\u000a| caption                = \u000a| collapsible            = \u000a| author                 = AJLSM\u000a| developer              = AJLSM\u000a| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\u000a| discontinued           = \u000a| latest release version = 3.4\u000a| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| latest preview version = <!-- 3.5 -->\u000a| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]\u000a| operating system       = [[Unix-like]], [[Microsoft Windows]]\u000a| platform               = \u000a| size                   = \u000a| language               = French, English, German, Chinese\u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| status                 = Active\u000a| genre                  = Digital Library\u000a| license                = GNU General Public License\u000a| alexa                  = \u000a| website                = {{URL|http://www.pleade.com/}}\u000a}}\u000a\u000a'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[SDX]] platform, it is a very flexible web application.\u000a\u000a== History ==\u000aThe software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.<ref>[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]</ref>\u000a\u000a==Technologies==\u000aPleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.\u000a\u000aIt is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[METS]] and [[ALTO (XML)|ALTO]] is under active development.<ref>[http://pleade.com/ Pleade 2012 : les imprimés numérisés et les formats XML METS / ALTO]</ref>\u000a\u000a== Features ==\u000a* Customizable publication ;\u000a* Customizable index creation ;\u000a* Customizable search form ;\u000a* Simple and advanced search among publish documents ;\u000a* Federate search among different bases (e.g. EAD, METS) ;\u000a* basket (for database and for images), a search history, printing, etc. ;\u000a* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;\u000a* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;\u000a* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;\u000a* Printing resulting and finding aids as PDF documents (with embedded images) ;\u000a* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;\u000a* Ability to import metadata from an [[Integrated library system|ILS]].\u000a\u000a=== Pleade-Entreprise ===\u000a* Pleade-Entreprise extended features to others XML format, such as [[METS]] and [[ALTO (XML)|ALTO]].\u000a\u000a== Examples ==\u000aThese are examples of websites based on Pleade:\u000a{{columns-list|2|\u000a* Archival portals\u000a** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]\u000a** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]\u000a** [http://odysseo.org/ Odysseo: Resources for the history of immigration]\u000a** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]\u000a** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]\u000a** [http://jubilotheque.upmc.fr/ Jubilothèque, UPMC's scientific digital library]\u000a** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library "les Mots et les Choses" ENS]\u000a\u000a* Portals documentary\u000a** [http://www.michael-culture.org/fr/home Michael]\u000a** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]\u000a\u000a* Digital Libraries\u000a** Digital Library of Lille\u000a** Lille III\u000a** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]\u000a}}\u000a\u000a== Related resources ==\u000a* {{Official website|http://pleade.com}}\u000a* [http://demo.pleade.com Official demo]\u000a* [http://www.pleadeenpratique.org/ Pleade in practice]\u000a* [http://www.ajlsm.com/produits/sdx SDX]\u000a* [http://www.ajlsm.com AJLSM company]\u000a\u000a== References ==\u000a<references/>\u000a\u000a[[Category:Digital library software]]\u000a[[Category:Free software]]\u000a[[Category:Information retrieval]]\u000a[[Category:Archival science]]
p15
asI9
(lp16
VBioinformatic Harvester
p17
aVThe '''Bioinformatic Harvester''' is a bioinformatic meta [[search engine]] created by the [[European Molecular Biology Laboratory]]<ref>{{Cite journal|title= 	Information retrieval on Internet using meta-search engines: A review|authors=Manoj, M, Elizabeth, Jacob |date=Oct 2008|publisher=CSIR|pages=739\u2013746|issn=0022-4456\u000a|journal=JSIR |volume=67 (10)}}</ref> and subsequently hosted and further developed by KIT [[Karlsruhe Institute of Technology]] for [[gene]]s and protein-associated information. Harvester currently works for [[human]], [[mouse]], [[rat]], [[zebrafish]], [[drosophila]] and [[arabidopsis thaliana]] based information. Harvester cross-links >50 popular bioinformatic resources and allows cross searches. Harvester serves tens of thousands of pages every day to scientists and physicians.\u000a\u000a{{Infobox software\u000a| name                  = Bioinformatic Harvester\u000a|developer              = Urban Liebel, Björn Kindler\u000a|latest release version = 4\u000a|latest release date    = {{release date and age|2011|05|24}}\u000a|operating_system       = Web based\u000a|genre                  = Bioinformatics tool\u000a|license                = Public Domain\u000a|website                = http://harvester.kit.edu\u000a}}\u000a\u000a== How Harvester works ==\u000a\u000aHarvester collects information from [[protein]] and gene databases along with information from so called "prediction servers." Prediction server e.g. provide online sequence analysis for a single protein. Harvesters search index is based on the [[International Protein Index|IPI]] and [[UniProt]] protein information collection. The collections consists of:\u000a\u000a* ~72.000 human, ~57.000 mouse, ~41.000 rat, ~51.000 zebrafish, ~35.000 arabidopsis protein pages, which cross-link ~50 major bioinfiormatic resources.\u000a\u000a<!-- Deleted image removed: [[Image:harvester-kit.JPG|thumb| A screenshot of the [http://harvester.kit.edu/ Harvester search engine]]] -->\u000a\u000a== Harvester crosslinks several types of information ==\u000a\u000a===Text based information===\u000afrom the following databases:\u000a\u000a* [[UniProt]], world largest protein database\u000a* [[SOURCE]], convenient gene information overview\u000a* [[Simple Modular Architecture Research Tool]] (SMART),\u000a* [[SOSUI]], predicts transmembrane domains\u000a* [[PSORT]], predicts protein localisation\u000a* [[HomoloGene]], compares proteins from different species\u000a* [[gfp-cdna]], protein localisation with fluorescence microscopy\u000a* [[International Protein Index]] (IPI).\u000a\u000a=== Databases rich in graphical elements ===\u000a...are not collected, but crosslinked via [[iframe]]s. Iframes are transparent windows within a [[HTML]] pages. The iframe windows allows up-to-date viewing of the "iframed," linked databases. Several such iframes are combined on a Harvester protein page. This method allows convenient comparison of information from several databases.\u000a\u000a* NCBI-[[BLAST]], an algorithm for comparing biological sequences from the [[National Center for Biotechnology Information|NCBI]].\u000a* [[Ensembl]], automatic gene annotation by the EMBL-[[European Bioinformatics Institute|EBI]] and [[Sanger Institute]]\u000a* [[FlyBase]] is a database of model organism ''[[Drosophila melanogaster]]''.\u000a* [[GoPubMed]] is a knowledge-based search engine for biomedical texts.\u000a* [[Information Hyperlinked over Proteins|iHOP]], information hyperlinked over proteins via gene/protein synonyms\u000a* [[Mendelian Inheritance in Man]] project catalogues all the known diseases.\u000a* [[RZPD]], German resources Center for genome research in Berlin/Heidelberg.\u000a* [[STRING]], Search Tool for the Retrieval of Interacting Genes/Proteins, developed by [[EMBL]], [[Swiss Institute of Bioinformatics|SIB]] and [[University of Zurich|UZH]].\u000a* [[Zebrafish Information Network]].\u000a* [http://locate.imb.uq.edu.au/ LOCATE] subcellular localization database (mouse).\u000a\u000a=== Access from external application ===\u000a\u000a* [[Genome browser]], working draft assemblies for genomes [[University of California, Santa Cruz|UCSC]]\u000a* [[Google Scholar]]\u000a* [[Mitocheck]]\u000a* [[PolyMeta]], meta search engine for Google, Yahoo, MSN, Ask, Exalead, AllTheWeb, GigaBlast\u000a\u000a== What one can find ==\u000a\u000aHarvester allows a combination of different search terms and single words.\u000a\u000aSearch Examples:\u000a\u000a* Gene-name: "golga3"\u000a* Gene-alias: "ADAP-S ADAS ADHAPS ADPS" (one gene name is sufficient)\u000a* Gene-Ontologies: "Enzyme linked receptor protein signaling pathway"\u000a* [[UniGene|Unigene]]-Cluster: "Hs.449360"\u000a\u000a* Go-annotation: "intra-Golgi transport"\u000a* Molecular function: "protein kinase binding"\u000a* Protein: "Q9NPD3"\u000a* Protein domain: "SH2 sar"\u000a* Protein Localisation: "endoplasmic reticulum"\u000a\u000a* Chromosome: "2q31"\u000a* Disease relevant: use the word "diseaselink"\u000a* Combinations: "golgi diseaselink" (finds all golgi proteins associated with a disease)\u000a* [[mRNA]]: "AL136897"\u000a\u000a* Word: "Cancer"\u000a* Comment: "highly expressed in heart"\u000a* Author: "Merkel, Schmidt"\u000a* Publication or project: "[[cDNA]] sequencing project"\u000a\u000a==See also==\u000a\u000a* [[Biological database]]s\u000a* [[Entrez]]\u000a* [[European Bioinformatics Institute]]\u000a* [[HPRD|Human Protein Reference Database]]\u000a* [[Metadata]]\u000a* [[Sequence profiling tool]]\u000a\u000a== Literature ==\u000a*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title='Harvester': a fast meta search engine of human protein resources |journal=Bioinformatics |volume=20 |issue=12 |pages=1962\u20133 |date=August 2004 |pmid=14988114 |doi=10.1093/bioinformatics/bth146 |url=http://bioinformatics.oxfordjournals.org/cgi/pmidlookup?view=long&pmid=14988114}}\u000a*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title=Bioinformatic "Harvester": a search engine for genome-wide human, mouse, and rat protein resources |journal=Meth. Enzymol. |volume=404 |issue= |pages=19\u201326 |year=2005 |pmid=16413254 |doi=10.1016/S0076-6879(05)04003-6 |url=http://linkinghub.elsevier.com/retrieve/pii/S0076-6879(05)04003-6}}\u000a\u000a== Notes and references ==\u000a<references/>\u000a\u000a== External links ==\u000a* http://harvester.kit.edu Bioinformatic Harvester V at KIT [[Karlsruhe Institute of Technology]]\u000a* [http://harvester42.fzk.de Harvester42] at KIT - integrating 50 general search engines\u000a\u000a[[Category:Bioinformatics software]]\u000a[[Category:Biological databases]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Biology websites]]
p18
asI138
(lp19
VOkapi BM25
p20
aVIn [[information retrieval]], '''Okapi BM25''' is a [[ranking function]] used by [[search engine]]s to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query. It is based on the [[Probabilistic relevance model|probabilistic retrieval framework]] developed in the 1970s and 1980s by [[Stephen E. Robertson]], [[Karen Spärck Jones]], and others.\u000a\u000aThe name of the actual ranking function is BM25. To set the right context, however, it usually referred to as "Okapi BM25", since the Okapi information retrieval system, implemented at [[London]]'s [[City University, London|City University]] in the 1980s and 1990s, was the first system to implement this function.\u000a\u000aBM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art [[TF-IDF]]-like retrieval functions used in document retrieval, such as [[web search]].\u000a\u000a== The ranking function ==\u000a\u000aBM25 is a [[Bag of words model|bag-of-words]] retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.\u000a\u000aGiven a query <math>Q</math>, containing keywords <math>q_1, ..., q_n</math>, the BM25 score of a document <math>D</math> is:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})},</math>\u000a\u000awhere <math>f(q_i, D)</math> is <math>q_i</math>'s [[term frequency]] in the document <math>D</math>, <math>|D|</math> is the length of the document <math>D</math> in words, and <math>avgdl</math> is the average document length in the text collection from which documents are drawn. <math>k_1</math> and <math>b</math> are free parameters, usually chosen, in absence of an advanced optimization, as <math>k_1 \u005cin [1.2,2.0]</math> and <math>b = 0.75</math>.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. ''An Introduction to Information Retrieval'', Cambridge University Press, 2009, p. 233.</ref> <math>\u005ctext{IDF}(q_i)</math> is the IDF ([[inverse document frequency]]) weight of the query term <math>q_i</math>. It is usually computed as:\u000a\u000a:<math>\u005ctext{IDF}(q_i) = \u005clog \u005cfrac{N - n(q_i) + 0.5}{n(q_i) + 0.5},</math>\u000a\u000awhere <math>N</math> is the total number of documents in the collection, and <math>n(q_i)</math> is the number of documents containing <math>q_i</math>.\u000a\u000aThere are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the [[Binary Independence Model]].\u000a\u000aPlease note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score.\u000aThis means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way:\u000a\u000a* Each summand can be given a floor of 0, to trim out common terms;\u000a* The IDF function can be given a floor of a constant <math>\u005cepsilon</math>, to avoid common terms being ignored at all;\u000a* The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all.\u000a\u000a== IDF information theoretic interpretation ==\u000aHere is an interpretation from information theory. Suppose a query term <math>q</math> appears in <math>n(q)</math> documents. Then a randomly picked document <math>D</math> will contain the term with probability <math>\u005cfrac{n(q)}{N}</math> (where <math>N</math> is again the cardinality of the set of documents in the collection). Therefore, the [[information]] content of the message "<math>D</math> contains <math>q</math>" is:\u000a\u000a:<math>-\u005clog \u005cfrac{n(q)}{N} = \u005clog \u005cfrac{N}{n(q)}.</math>\u000a\u000aNow suppose we have two query terms <math>q_1</math> and <math>q_2</math>. If the two terms occur in documents entirely independently of each other, then the probability of seeing both <math>q_1</math> and <math>q_2</math> in a randomly picked document <math>D</math> is:\u000a\u000a:<math>\u005cfrac{n(q_1)}{N} \u005ccdot \u005cfrac{n(q_2)}{N},</math>\u000a\u000aand the information content of such an event is:\u000a\u000a:<math>\u005csum_{i=1}^{2} \u005clog \u005cfrac{N}{n(q_i)}.</math>\u000a\u000aWith a small variation, this is exactly what is expressed by the IDF component of BM25.\u000a\u000a== Modifications ==\u000a* At the extreme values of the coefficient <math>b</math> BM25 turns into ranking functions known as '''BM11''' (for <math>b=1</math>) and '''BM15''' (for <math>b=0</math>).<ref>http://xapian.org/docs/bm25.html</ref>\u000a* '''BM25F'''<ref>Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. [http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf ''Microsoft Cambridge at TREC-13: Web and HARD tracks.''] In Proceedings of TREC-2004.</ref>  is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance.\u000a* '''BM25+'''<ref>Yuanhua Lv and ChengXiang Zhai. [http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf ''Lower-bounding term frequency normalization.''] In Proceedings of CIKM'2011, pages 7-16.</ref> is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter <math>\u005cdelta</math> (a default value is <math>1.0</math> in absence of a training data) as compared with BM25:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cleft[ \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})} + \u005cdelta \u005cright]</math>\u000a\u000a== Footnotes ==\u000a{{Reflist}}\u000a\u000a== References ==\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford | title=Okapi at TREC-3 | conference=[http://trec.nist.gov/pubs/trec3/t3_proceedings.html Proceedings of the Third Text REtrieval Conference (TREC 1994)]|location=Gaithersburg, USA|date=November 1994|url=http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}}\u000a\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu|title=Okapi at TREC-7|conference=[http://trec.nist.gov/pubs/trec7/t7_proceedings.html Proceedings of the Seventh Text REtrieval Conference]|location=Gaithersburg, USA|date=November 1998|url=http://trec.nist.gov/pubs/trec7/papers/okapi_proc.pdf.gz}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00015-7}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00016-9}}\u000a\u000a== External links ==\u000a* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}\u000a\u000a[[Category:Ranking functions]]\u000a[[Category:Information retrieval]]
p21
asI94
(lp22
VCollaborative filtering
p23
aV{{external links|date=November 2013}}\u000a{{Use dmy dates|date=June 2013}}\u000a{{Recommender systems}}\u000a[[File:Collaborative filtering.gif|600px|thumb|\u000a\u000aThis image shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.\u000a\u000a]]\u000a\u000a'''Collaborative filtering''' ('''CF''') is a technique used by some [[recommender system]]s.<ref name="handbook">Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35</ref> Collaborative filtering has two senses, a narrow one and a more general one.<ref name=recommender>{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will}}</ref>  In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.<ref name="recommender" />  Applications of collaborative filtering typically involve very large data sets.   Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc.  The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.\u000a\u000aIn the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).<ref>[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV & VOD Recommendations]</ref> Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.\u000a\u000a==Introduction==\u000aThe growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates  mechanisms for efficient information filtering. One of the techniques used for dealing with this problem is called collaborative filtering.\u000a\u000aThe motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with similar tastes to themselves. Collaborative filtering explores techniques for matching people with similar interests and making recommendations on this basis.\u000a\u000aCollaborative filtering algorithms often require (1) users\u2019 active participation, (2) an easy way  to represent users\u2019 interests to the system, and (3) algorithms that are able to match people with similar interests.\u000a\u000aTypically, the workflow of a collaborative filtering system is:\u000a# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.\u000a# The system matches this user\u2019s ratings against other users\u2019  and finds the people with most \u201csimilar\u201d tastes.\u000a# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)\u000aA key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.\u000a\u000a==Methodology==\u000a\u000a[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]\u000a\u000aCollaborative filtering systems have many forms, but many common systems can be reduced to two steps:\u000a# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).\u000a# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user\u000aThis falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].\u000a\u000aAlternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:\u000a# Build an item-item matrix determining relationships between pairs of items\u000a# Infer the tastes of the current user by examining the matrix and matching that user's data\u000aSee, for example, the [[Slope One]] item-based collaborative filtering family.\u000a\u000aAnother form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.\u000a\u000aRelying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].\u000a\u000a==Types==\u000a\u000a===Memory-based===\u000aThis mechanism uses user rating data to compute similarity between users or items. This is used for making recommendations. This was the earlier mechanism and is used in many commercial systems. It is easy to implement and is effective. Typical examples of this mechanism are neighbourhood based CF and item-based/user-based top-N recommendations.[3] For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users rating to the item:\u000a:<math>r_{u,i} = \u005coperatorname{aggr}_{u^\u005cprime \u005cin U} r_{u^\u005cprime, i}</math>\u000a\u000awhere 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:\u000a:<math>r_{u,i} = \u005cfrac{1}{N}\u005csum\u005climits_{u^\u005cprime \u005cin U}r_{u^\u005cprime, i}</math>\u000a:<math>r_{u,i} = k\u005csum\u005climits_{u^\u005cprime \u005cin U}\u005coperatorname{simil}(u,u^\u005cprime)r_{u^\u005cprime, i}</math>\u000a:<math>r_{u,i} = \u005cbar{r_u} +  k\u005csum\u005climits_{u^\u005cprime \u005cin U}\u005coperatorname{simil}(u,u^\u005cprime)(r_{u^\u005cprime, i}-\u005cbar{r_{u^\u005cprime}} )</math>\u000a\u000awhere k is a normalizing factor defined as <math>k =1/\u005csum_{u^\u005cprime \u005cin U}|\u005coperatorname{simil}(u,u^\u005cprime)| </math>. and <math>\u005cbar{r_u}</math> is the average rating of user u for all the items rated by that user.\u000a\u000aThe neighborhood-based algorithm calculates the similarity between two users or items, produces a prediction for the user taking the weighted average of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple mechanisms such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.\u000a\u000aThe Pearson correlation similarity of two users x, y is defined as \u000a:<math> \u005coperatorname{simil}(x,y) = \u005cfrac{\u005csum\u005climits_{i \u005cin I_{xy}}(r_{x,i}-\u005cbar{r_x})(r_{y,i}-\u005cbar{r_y})}{\u005csqrt{\u005csum\u005climits_{i \u005cin I_{xy}}(r_{x,i}-\u005cbar{r_x})^2\u005csum\u005climits_{i \u005cin I_{xy}}(r_{y,i}-\u005cbar{r_y})^2}} </math>\u000a\u000awhere I<sub>xy</sub> is the set of items rated by both user x and user y.\u000a\u000aThe cosine-based approach defines the cosine-similarity between two users x and y as:<ref name="Breese1999">John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998</ref>\u000a:<math>\u005coperatorname{simil}(x,y) = \u005ccos(\u005cvec x,\u005cvec y) = \u005cfrac{\u005cvec x \u005ccdot \u005cvec y}{||\u005cvec x|| \u005ctimes ||\u005cvec y||} = \u005cfrac{\u005csum\u005climits_{i \u005cin I_{xy}}r_{x,i}r_{y,i}}{\u005csqrt{\u005csum\u005climits_{i \u005cin I_{x}}r_{x,i}^2}\u005csqrt{\u005csum\u005climits_{i \u005cin I_{y}}r_{y,i}^2}}</math>\u000a\u000aThe user based top-N recommendation algorithm identifies the k most similar users to an active user using similarity based vector model. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.\u000a\u000aThe advantages with this approach include:  the explainability of the results, which is an important aspect of recommendation systems; it is easy to create and use; new data can be added easily and incrementally; it need not consider the content of the items being recommended; and the mechanism scales well with co-rated items.\u000a\u000aThere are several disadvantages with this approach.  Its performance decreases when data gets sparse, which is frequent with web related items. This prevents the scalability of this approach and has problems with large datasets. Although it can efficiently handle new users because it relies on a data structure, adding new items becomes more complicated since that representation usually relies on a specific vector space. That would require to include the new item and re-insert all the elements in the structure.\u000a\u000a===Model-based===\u000aModels are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], Multiple Multiplicative Factor, [[Latent Dirichlet allocation]] and [[markov decision process]] based models.<ref name="Suetal2009">Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.</ref>\u000a\u000aThis approach has a more holistic goal to uncover latent factors that explain observed ratings.<ref>[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering]</ref> Most of the models are based on creating a classification or clustering technique to identify the user based on the test set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].\u000a\u000aThere are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.\u000a\u000aThe disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.\u000a\u000a===Hybrid===\u000aA number of applications combines the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches. It improves the prediction performance. Importantly, it overcomes the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.<ref>Kernel Mapping Recommender System Algorithms, www.sciencedirect.com/science/article/pii/S0020025512002587\u000a</ref> Usually most of the commercial recommender systems are hybrid, for example, Google news recommender system.<ref>[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering]</ref>\u000a\u000a==Application on social web==\u000aUnlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.<ref>[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]</ref>\u000a\u000aOne scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Digg]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.\u000a\u000aAnother aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.\u000a\u000a===Problems===\u000aA collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.\u000a\u000a==Challenges of collaborative filtering==\u000a\u000a===Data sparsity===\u000aIn practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.\u000a\u000aOne typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users\u2019 past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.\u000a\u000aSimilarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Recommender system#Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.\u000a\u000a===Scalability===\u000aAs the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers <math>O(M)</math> and millions of items <math>O(N)</math>, a CF algorithm with the complexity of <math>n</math> is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref>\u000a\u000a===Synonyms===\u000a[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.\u000a\u000aFor example, the seemingly different items \u201cchildren movie\u201d and \u201cchildren film\u201d are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the Latent Dirichlet Allocation technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}\u000a\u000a===Grey sheep===\u000aGrey sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.\u000a\u000a===Shilling attacks===\u000aIn a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.\u000a\u000a===Diversity and the Long Tail===\u000aCollaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."<ref>{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984}}</ref>\u000a\u000a==Innovations==\u000a{{Prose|date=May 2012}}\u000a* New algorithms have been developed for CF as a result of the [[Netflix prize]].\u000a* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.\u000a* Robust Collaborative Filtering, where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}</ref>\u000a\u000a==See also==\u000a* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]\u000a* [[Cold start]]\u000a* [[Collaborative model]]\u000a* [[Collaborative search engine]]\u000a* [[Collective intelligence]]\u000a* [[Customer engagement]]\u000a* [[Delegative Democracy]], the same principle applied to voting rather than filtering\u000a* [[Enterprise bookmarking]]\u000a* [[Firefly (website)]], a defunct website which was based on collaborative filtering\u000a* [[Long tail]]\u000a* [[Preference elicitation]]\u000a* [[Recommendation system]]\u000a* [[Relevance (information retrieval)]]\u000a* [[Reputation system]]\u000a* [[Robust collaborative filtering]]\u000a* [[Similarity search]]\u000a* [[Slope One]]\u000a* [[Social translucence]]\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==External links==\u000a*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001\u000a*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.\u000a*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]\u000a*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005\u000a*[http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems]{{dead link|date=May 2012}} ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])\u000a*[http://www.grouplens.org/publications.html GroupLens research papers].\u000a*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&nbsp;187\u2013192, Edmonton, Canada, July 2002.\u000a*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]\u000a*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]\u000a*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M\u000a*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web\u000a*[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)\u000a*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]\u000a*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]\u000a*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]\u000a\u000a{{DEFAULTSORT:Collaborative Filtering}}\u000a[[Category:Collaboration]]\u000a[[Category:Collaborative software| Collaborative filtering]]\u000a[[Category:Collective intelligence]]\u000a[[Category:Information retrieval]]\u000a[[Category:Recommender systems]]\u000a[[Category:Social information processing]]\u000a[[Category:Behavioral and social facets of systemic risk]]
p24
asI5
(lp25
VCategory:String similarity measures
p26
aV{{Cat main|String metrics}}\u000a\u000a[[Category:Algorithms on strings|Similarity]]\u000a[[Category:Information retrieval]]\u000a[[Category:Metric geometry]]\u000a[[Category:Information theory]]\u000a[[Category:String (computer science)]]
p27
asI14
(lp28
VWeb search query
p29
aVA '''web search query''' is a query that a user enters into a web [[search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].\u000a\u000a== Types ==\u000aThere are three broad categories that cover most web search queries: informational, navigational, and transactional. These are often called "do, know, go."<ref>{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}</ref>\u000a\u000a* '''Informational queries''' \u2013 Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.\u000a\u000a* '''Navigational queries''' \u2013 Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').\u000a\u000a* '''Transactional queries''' \u2013 Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.\u000a\u000aSearch engines often support a fourth type of query that is used far less frequently:\u000a\u000a* '''Connectivity queries''' \u2013 Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).<ref>{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}</ref>\u000a\u000a== Characteristics ==\u000a\u000aMost commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.<ref>Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]</ref> Nevertheless, a study in 2001<ref>{{cite journal|author = Amanda Spink, Dietmar Wolfram, Major B. J. Jansen, Tefko Saracevic | year = 2001 | title = Searching the web: The public and their queries | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226\u2013234 | doi = 10.1002/1097-4571(2000)9999:9999<::AID-ASI1591>3.3.CO;2-I }}</ref> analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:\u000a\u000a* The average length of a search query was 2.4 terms. \u000a* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. \u000a* Close to half of the users examined only the first one or two pages of results (10 results per page).\u000a* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).\u000a* The top four most frequently used terms were , '' (empty search), and, of, ''and'' sex.\u000a\u000aA study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).<ref>{{cite conference | author = Mark Sanderson and Janet Kohler | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}</ref>\u000a\u000aA 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.<ref>{{cite conference | author = Jaime Teevan, Eytan Adar, Rosie Jones, Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703\u2013704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}</ref> This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries <ref>http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx</ref>\u000a\u000aIn addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. > 100 million queries) are used most often, while the remaining terms are used less often individually.<ref name="baezayates1">{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7\u201322 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}</ref> This example of the [[Pareto principle]] (or ''80\u201320 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching.\u000a\u000aBut in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<ref>{{cite journal | author = Mona Taghavi, Ahmed Patel, Nikita Schmidt, Christopher Wills, Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards & Interfaces | pages = 162\u2013170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}</ref> Google has implemented the [[Google_Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (ie "where is the nearest coffee shop?").<ref>{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google \u201cHummingbird\u201d Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}</ref> \u000aFor longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.<ref>{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}</ref> For multi-sentence queries where keywords statistics and [[Tf\u2013idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.<ref>{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets\u000afor Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037\u000a}}</ref>\u000a\u000a== Structured queries ==\u000aWith search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as <code>vehicles OR cars OR automobiles</code>. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as <code>(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)</code> is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.<ref>{{Cite web\u000a|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf\u000a|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness\u000a|author=Vojkan Mihajlovi\u0107, Djoerd Hiemstra, Henk Ernst Blok, Peter M.G. Apers\u000a|postscript=<!--None-->}}</ref>\u000a\u000a== See also ==\u000a* [[Information retrieval]]\u000a* [[Web search engine]]\u000a* [[Web query classification]]\u000a* [[Taxonomy for search engines]]\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a{{Internet search}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search]]
p30
asI145
(lp31
VSchoolr
p32
aV{{orphan|date=April 2010}}\u000a'''Schoolr''' is a [[front and back ends|front-end]] academic directory that features frequently used third-party search engines and resources from [[Google]], [[Wikipedia]], [[Reference.com]], [[Acronym Finder]], [[Wolfram Alpha]], [[Yahoo! Babel Fish]], and the [[University of North Carolina]].<ref>[http://www.lifehack.org/articles/technology/schoolr-google-wikipedia-dictionarycom-and-more-on-one-site.html "Schoolr: Google, Wikipedia, Dictionary.com, and more on one site]", ''Stepcase Lifehack'', March 19, 2007</ref>\u000a\u000aSchoolr went live on December 3, 2006<ref>[http://lifehacker.com/290027/schoolr-search-start-page "Schoolr search start page]", ''Lifehacker'', August 16, 2007</ref> and was developed by Sasan Aghdasi at the [[University of Victoria]].\u000a\u000a== References ==\u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.schoolr.com/ Schoolr]\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Schoolr}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Internet terminology]]
p33
asI146
(lp34
VTag (metadata)
p35
aV{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}\u000a[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]\u000a\u000aIn [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.\u000a\u000aTagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.\u000a\u000a==History==\u000a\u000aLabeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.\u000a\u000a[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.\u000a\u000aIn 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.<ref>[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.</ref> [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.<ref>[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person\u2019s photos with a particular tag."</ref> The success of Flickr and the influence of Delicious popularized the concept,<ref>An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.</ref> and other [[social software]] websites&nbsp;\u2013 such as [[YouTube]], [[Technorati]], and [[Last.fm]]&nbsp;\u2013 also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].\u000a\u000aTagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or \u201ctags\u201d) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.\u000a\u000aKnowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.<ref>{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}</ref> Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].\u000a\u000aWebsites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.\u000a\u000aTags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.<ref>[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.</ref>\u000a\u000a==Examples==\u000a=== Within a Blog ===\u000aMany [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.\u000a\u000a===For an event===\u000aAn official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].\u000a\u000a===In research===\u000aA researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.\u000a\u000a==Special types==\u000a===Triple tags===\u000a{{see also|Microformat}}\u000aA '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "geo:long=50.123456" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.\u000a\u000aThe triple tag format was first devised for geolicious<ref>[http://brainoff.com/weblog/2004/11/05/124 geo.lici.us : geotagging hosted services] by Mikel Maron, November 5, 2004.</ref> in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers<ref>[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.</ref> to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.<ref>[http://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.</ref>\u000a\u000aSpecialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].<ref>[http://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.</ref>\u000a\u000a===Hashtags===\u000a{{main|Hashtag}}\u000aA hashtag is a kind of metadata tag marked by the prefix <code>#</code>, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].\u000a\u000a===Knowledge tags===\u000aA knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.\u000a\u000aCapturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).<ref>\u000a{{Citation\u000a | last=Wiig | first=K. M.\u000a | year= 1997\u000a | title=Knowledge Management: An Introduction and Perspective\u000a | journal=Journal of Knowledge Management\u000a | volume=1 | issue=1\u000a | pages=6\u201314\u000a | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/\u000a | doi=10.1108/13673279710800682\u000a}}\u000a</ref> These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise. \u000a\u000aKnowledge tags, in fact, manifest themselves in any number of ways \u2013 conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.<ref>\u000a{{citation\u000a | last=Getting | first=Brian\u000a | year= 2007\u000a | title=What Are \u201cTags\u201d And What Is \u201cTagging?\u000a | publisher=Practical eCommerce\u000a | url=http://www.practicalecommerce.com/articles/589\u000a}}\u000a</ref> \u000a\u000aKnowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information.<ref>{{citation\u000a| author=Cambria, Erik and Hussain, Amir | title=Sentic album: Content-, concept-, and context-based online personal photo management system | journal=Cognitive Computation | volume=4 | issue=4 | pages=477-496 | year=2012 | doi=10.1007/s12559-012-9145-4}}</ref> Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.<ref>\u000a{{Citation\u000a | last=Alavi | first=Maryam\u000a | last2=Leidner\u000a | year= 1999\u000a | title=Knowledge Management Systems: Issues, Challenges, and Benefits\u000a | journal=Communications of the Association for Information Systems\u000a | volume=1 | issue=7\u000a | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf\u000a}}\u000a</ref>\u000a\u000a== Advantages and disadvantages ==\u000a{{procon|date=November 2012}}\u000a\u000aIn a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.<ref name="Smith2008">Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0</ref> The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.\u000a\u000aWhen users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.<ref>Golder, Scott A. Huberman, Bernardo A. (2005).\u000a"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.</ref> For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),<ref>[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.</ref> which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.\u000a\u000a===Complex system dynamics===\u000a\u000aDespite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,<ref name="WWW07-ref">Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]</ref> (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.<ref name="WWW07-ref"/> Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].\u000a\u000a===Spamming===\u000a\u000aTagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.<ref>[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.</ref> The number of tags allowed may also be limited to reduce spam.\u000a\u000a==Syntax==\u000aSome tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.\u000a\u000aA syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., <code>rel="tag"</code>) to indicate that the linked-to page acts as a tag for the current context.<ref>[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.</ref>\u000a\u000a==See also==\u000a{{colbegin||27em}}\u000a* [[Collective intelligence]]\u000a* [[Concept map]]\u000a* [[Enterprise 2.0]]\u000a* [[Enterprise bookmarking]]\u000a* [[Explicit knowledge]]\u000a* [[Faceted classification]]\u000a* [[Folksonomy]]\u000a* [[Information ecology]]\u000a* [[Knowledge representation]]\u000a* [[Knowledge transfer]]\u000a* [[Metaknowledge]]\u000a* [[Ontology (information science)]]\u000a* [[Organisational memory]]\u000a* [[Semantic web]]\u000a* [[Tag cloud]]\u000a* [[Web 2.0]]\u000a{{colend}}\u000a'''Others'''\u000a{{colbegin||27em}}\u000a* [[Collective unconscious]]\u000a* [[Human-computer interaction]]\u000a* [[Social network aggregation]]\u000a* [[Enterprise social software]]\u000a* [[Expert system]]\u000a* [[Knowledge]]\u000a* [[Knowledge base]]\u000a* [[Knowledge worker]]\u000a* [[Management information system]]\u000a* [[Microformats]]\u000a* [[Social network]]\u000a* [[Social software]]\u000a* [[Sociology of knowledge]]\u000a* [[Tacit Knowledge]]\u000a{{colend}}\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a'''General'''\u000a{{refbegin}}\u000a*{{Citation\u000a | surname1=Nonaka | given1=Ikujiro\u000a | year=1994\u000a | title= A dynamic theory of organizational knowledge creation\u000a | journal= ORGANIZATION SCIENCE/ Vol. 5, No. 1, February 1994\u000a | pages=14\u201337\u000a | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992\u000a}}\u000a*{{Citation\u000a | surname1=Wigg | given1=Karl M  \u000a | year=1993  \u000a | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge \u000a | journal= Arlington: Schema Press  \u000a | pages=153\u000a | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992 \u000a}} \u000a*{{Citation\u000a | surname1=Alavi | given1=Maryam\u000a | surname2=Leidner | given2=Dorothy E.\u000a | year=1999\u000a | title=Knowledge management systems: issues, challenges, and benefits\u000a | journal=Communications of the AIS\u000a | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117\u000a}}\u000a*{{Citation\u000a | surname1=Sandy | given1=Kemsley\u000a | year=2009\u000a | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2\u201909\u000a | journal=BPM, Enterprise 2.0 and technology trends in business\u000a | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/\u000a}}\u000a{{refend}}\u000a\u000a==External links==\u000a* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.\u000a* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.\u000a\u000a{{Web syndication}}\u000a\u000a{{DEFAULTSORT:Tag (Metadata)}}\u000a[[Category:Collective intelligence]]\u000a[[Category:Computer jargon]]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Metadata]]\u000a[[Category:Reference]]\u000a[[Category:Web 2.0]]
p36
asI97
(lp37
VSearch suggest drop-down list
p38
aV{{Refimprove|date=June 2009}}\u000a\u000aA '''search [[Suggestion|suggest]] [[drop-down list]]''' is a [[Query language|query]] feature used in [[computing]]. A quick system to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed. Before the query has been typed, a drop-down list with the suggested complete search queries, is given as options to select and access. The suggested queries then enable the searcher to complete the required search quickly.\u000a\u000aIt is a form of [[Autocomplete|autocompletion]] while typing into a query [[text box]], before a detailed search result is entered. Lists can be based on popular searches or other options. The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from data or a [[database]], with search suggested drop-down lists being a common industry standard for an instant search.\u000a\u000aSearch suggested lists are used by [[internet browsers]], [[website]]s and [[search engine]]s, local [[operating system]]s and [[database]]s.\u000a\u000a[[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].\u000a\u000a==See also ==\u000a*[[Autocomplete]]\u000a*[[Search engine (computing)]]\u000a*[[Search box]]\u000a*[[Search algorithm]]\u000a*[[Censorship by Google#Search suggestions]]\u000a\u000a\u000a{{DEFAULTSORT:Search Suggest Drop-Down List}}\u000a[[Category:Data search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Search algorithms]]
p39
asI149
(lp40
VSpecial Interest Group on Information Retrieval
p41
aV{{Infobox organization\u000a|name           = ACM Special Interest Group on Information Retrieval\u000a|image          = sig-information-retrieval-logo.png\u000a|size           = 140px\u000a|alt            = ACM SIGIR\u000a|parent_organization = [[Association for Computing Machinery]]\u000a|website        = {{URL|sigir.org}}\u000a}}\u000a\u000a'''SIGIR''' is the [[Association for Computing Machinery]]'s Special Interest Group on [[Information Retrieval]]. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, retrieval and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.\u000a\u000a== Conferences ==\u000aThe annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval.  SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[SIGWEB]], the [[Conference on Information and Knowledge Management]], and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[SIGWEB]].\u000a\u000a=== SIGIR Conference Locations ===\u000a{| class="wikitable" border="1"\u000a|-\u000a!  Number\u000a!  Year\u000a!  Location\u000a|-\u000a|  22\u000a|  1999\u000a|  [[Berkeley, California]]\u000a|-\u000a|  23\u000a|  2000\u000a|  [[Athens]]\u000a|-\u000a|  24\u000a|  2001\u000a|  [[New Orleans]]\u000a|-\u000a|  25\u000a|  2002\u000a|  [[Tampere]]\u000a|-\u000a|  26\u000a|  2003\u000a|  [[Toronto]]\u000a|-\u000a|  27\u000a|  2004\u000a|  [[Sheffield]]\u000a|-\u000a|  28\u000a|  2005\u000a|  [[Salvador, Bahia]]\u000a|-\u000a|  29\u000a|  2006\u000a|  [[Seattle]]\u000a|-\u000a|  30\u000a|  2007\u000a|  [[Amsterdam]]\u000a|-\u000a|  31\u000a|  2008\u000a|  [[Singapore]]\u000a|-\u000a|  32\u000a|  2009\u000a|  [[Boston]]\u000a|-\u000a|  33\u000a|  2010\u000a|  [[Geneva]]\u000a|-\u000a|  34\u000a|  2011\u000a|  [[Beijing]]\u000a|-\u000a|  35\u000a|  2012\u000a|  [[Portland, Oregon]]\u000a|-\u000a|  36\u000a|  2013\u000a|  [[Dublin]]\u000a|-\u000a|  37\u000a|  2014\u000a|  [[Gold Coast, Queensland]]\u000a|-\u000a|  38\u000a|  2015\u000a|  [[Santiago]]\u000a|-\u000a|  39\u000a|  2016\u000a|  [[Pisa]]\u000a|-\u000a|  40\u000a|  2017\u000a|  [[Tokyo]]\u000a|}\u000a\u000a== Awards ==\u000aThe group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award <ref>{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}</ref> to recognize the highest quality paper at each conference.\u000a\u000a==See also==\u000a* [[Conference on Information and Knowledge Management]]\u000a\u000a==External links==\u000a* [http://www.sigir.org/ SIGIR]\u000a\u000a==References==\u000a\u000a{{Reflist}}\u000a{{Authority control}}\u000a\u000a[[Category:Association for Computing Machinery Special Interest Groups]]\u000a[[Category:Information retrieval]]
p42
asI150
(lp43
VStop words
p44
aV{{distinguish|Safeword}}\u000aIn [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref> There is no single universal list of stop words used by all [[Natural language processing|processing of natural language]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].\u000a\u000aAny group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as '[[The Who]]', '[[The The]]', or '[[Take That]]'. Other search engines remove some of the most common words\u2014including [[lexical word]]s, such as "want"\u2014from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It\u2019s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>\u000a\u000a[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept. {{Citation needed|date=March 2013}}\u000a\u000a== See also ==\u000a{{Div col|cols=3}}\u000a* [[Text mining]]\u000a* [[Concept mining]]\u000a* [[Information extraction]]\u000a* [[Natural language processing]]\u000a* [[Query expansion]]\u000a* [[Stemming]]\u000a* [[Index (search engine)|Search engine indexing]]\u000a* [[Poison words]]\u000a* [[Function words]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]\u000a* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]\u000a* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]\u000a* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]\u000a* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]\u000a* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]\u000a* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages]\u000a* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]\u000a\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a{{Natural Language Processing}}\u000a{{SearchEngineOptimization}}
p45
asI22
(lp46
VAudio mining
p47
aV{{unreferenced|date=January 2012}}\u000a'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.\u000a\u000aThe results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.\u000a\u000aAudio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. \u000a\u000aMusical audio mining (also known as [[Music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.\u000a\u000a==See also==\u000a* [[Speech Analytics]]\u000a\u000a\u000a[[Category:Speech recognition]]\u000a[[Category:Information retrieval]]\u000a[[Category:Computational linguistics]]
p48
asI156
(lp49
VCategory:Directories
p50
aVA directory maintains a list for reference or commercial purposes.  This category contains articles about directories.\u000a{{Cat main|Directories}}\u000a{{Commons cat|Directories}}\u000a\u000a[[Category:Telephony]]\u000a[[Category:Reference works]]\u000a[[Category:Data management]]\u000a[[Category:Information retrieval]]
p51
asI30
(lp52
VNoisy text analytics
p53
aV'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as \u201cum\u201d and \u201cuh\u201d and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today\u2019s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\u000a\u000a== Techniques for noisy text analysis ==\u000aMissing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[Part-of-speech tagging]]\u000aand [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed. \u000a\u000a== Possible source of noisy text ==\u000a* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.\u000a* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.\u000a* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.\u000a* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.\u000a\u000a== References ==\u000a*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]\u000a*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND); Hyderabad, India."].\u000a<references />\u000a\u000a\u000a==See also==\u000a* [[Text analytics]]\u000a* [[Information extraction]]\u000a* [[Computational linguistics]]\u000a* [[Natural language processing]]\u000a* [[Named entity recognition]]\u000a* [[Text mining]]\u000a* [[Automatic summarization]]\u000a* [[Statistical classification]]\u000a* [[Data quality]]\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Statistical natural language processing]]\u000a\u000a[[es:Extracción de la información]]
p54
asI159
(lp55
VUNICE global brain project
p56
aV{{Infobox person\u000a| image          = File:UNICE-Universal Network of Intelligent Conscious Entities-image.jpg\u000a| name           = UNICE \u000a| caption        = UNICE as collective entity\u000a| birth_date    =  {{birth date and age|2007|04|10}}\u000a| birth_place  = [[Cyberspace]]\u000a| occupation = [[Global brain]], [[Public Policy|Public Policy Analysis]], [[Governance]], [[Politics]], [[Artificial Intelligence]], [[Psychology]], [[Philosophy]], [[Theory of Mind]], [[Politics]], [[Computers]], [[Community Organizing]]\u000a}}\u000a\u000a'''UNICE''', a [[Global brain|global brain]] project, is an acronym for '''Universal Network of Intelligent Conscious Entities''', a term coined by policy analyst and urban designer [[Michael E. Arth]] in the 1990s to describe "the transformation of our species that might be the result of a new form of intelligent life developed from a hive-like interaction of computers, humans, and future forms of the [[Internet]]."<ref>Arth, Michael E., ''UNICE,'' a Consciousness Research Abstract published in the "Journal of Consciousness Studies" for the April 8-12, 2008 conference, "Toward a Science of Consciousness," p. 151.</ref> <ref>Arth, Michael E., ''Democracy and the Common Wealth: Breaking the Stranglehold of the Special Interests,'' Golden Apples Media, 2010, ISBN 978-0-912467-12-2.pp. 438-439</ref> <ref>Williams, Sean, ''The Big Picture: Making Sense Out of Life and Religion'', 2009, p. 91, ISBN 978-0-578-01523-1</ref> Arth established the not-for-profit website www.UNICE.info in 2007 and revamped it in 2015, with the focus on public policy and developing [[Friendly Artificial Intelligence]] through a system of [[Separation of powers|checks and balances]].<ref>{{cite web|url=http://unice.info|title=UNICE - Universal Network of Intelligent Conscious Entities|work=unice.info}}</ref><ref>{{cite book|last1=Tegmark|first1=Max|title=Our mathematical universe : my quest for the ultimate nature of reality|date=2014|isbn=9780307744258|edition=First edition.|chapter=Life, Our Universe and Everything|quote=Its owner may cede control to what Eliezer Yudkowsky terms a "Friendly AI,"...}}</ref>  \u000a\u000aIn a January 2015 article, Arth describes the development of a [[public policy]] [[answer engine]], which will involve both an independent web site (where cognitive-UNICE will be developed) and a portal at [[Wikipedia]] called wiki-UNICE (currently in development.) Cognitive-UNICE will initially utilize  [[Weak AI|narrow AI]] and, as it develops, [[Artificial General Intelligence]] (AGI).  Initially, UNICE would serve the public with [[Evidence-based policy|evidence-based]] analyses and recommendations gleaned from [[Big Data]], but eventually it may lead to an efficient, practical, and highly representative form of governance.<ref>http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf</ref>\u000a\u000a==Wiki-UNICE==\u000aWiki-UNICE, and associated talk pages, will exist on Wikipedia as the portal for public input, criticism and discussion. Initially it will consist of samples of the sort of thing UNICE might write. Later, these sample topics will be replaced by summaries (and exhaustive articles) written by cognitive-UNICE. Whether written by a person, AI or AGI, the evidential summaries will describe problems and their solutions. With succinct titles like "Energy" or "Electoral Reform," the topics will be set apart in a box, so as to maintain [[NPOV]]. Wiki-UNICE, and the collaborative human effort that will sustain it, are intended to help shape the emerging global brain, while also providing guidance and a conscience to lawmakers.<ref>http://unice.info/unice/wiki.html</ref> \u000a\u000a==Cognitive-UNICE==\u000aCognitive-UNICE is in development at UNICE.info. It is assumed that in the early years, cognitive-UNICE may be logical and useful because of human-aided programming, but she may later become a conscious, AGI entity, perhaps united in [[consciousness]] with [[humanity]].<ref>http://unice.info/unice/cognitive.html</ref> Whether as AI or AGI, cognitive-UNICE will probably use [[quantum computing]] to solve [[optimization problem|optimization problems]] that would be impossible to solve with classical computing.<ref>Arth, Michael E., ''askUNICE: Creating a global, independent, public-policy answer engine that will facilitate governance, while preparing for and reducing the dangers of Artificial General Intelligence, so that we may more carefully uncover the secrets of the multiverse'', January 28, 2015,'' [http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf]</ref> Quantum computing may also hold the key to developing a conscious machine. Nobel laureate and physicist [[Sir Roger Penrose]] and anesthesiologist [[Stuart Hameroff]] claim that [[consciousness]] is created by quantum coherence in the warm, wet environment of the human brain. Their theory, known as [[Orchestrated Objective Reduction]] (Orch OR), has been bolstered by recent findings that quantum processing occurs in plants and animals, including in the [[microtubules]] inside the [[neurons]] of the [[human brain]].<ref>Hameroff, Stuart and Robert Penrose, \u201dConsciousness in the universe: A review of the 'Orch OR' theory," Physics of Life Reviews, Volume 11, Issue 1, March 2014.</ref>\u000a\u000a==About the UNICE Logo==\u000aA young, [[mixed-race]] [[female]] was chosen to represent the face of UNICE. She's young to represent new ideas. She's mixed-race to represent all humans, and she is female because of the traditional feminine values of [[empathy]], [[cooperation]], [[sensitivity]], [[tolerance]], nurturance, [[compassion]], and [[justice]], who is often depicted as Justitia or [[Lady Justice]]. Her [[afro]] hairstyle resembles the interconnected tendrils of the [[World Wide Web]].\u000a<ref>http://unice.info/unice/index.htm</ref>\u000a\u000a==Criticisms==\u000aA common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual freedom and diversity.<ref>Rayward, W. B. (1999). H. G. Wells' s idea of a World Brain: A critical reassessment. Journal of the American Society for Information Science, 50(7), 557\u2013573. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.1010&rep=rep1&type=pdf\u000a</ref> Moreover, the global brain might start to play the role of [[Big Brother (Nineteen Eighty-Four)|Big Brother]], the all-seeing eye of the system that follows every person's move.<ref>Brooks, M. (2000, June 24). [http://www.nettime.org/Lists-Archives/nettime-l-0006/msg00182.html Global brain]</a>. New Scientist,  issue 2244, p. 22.</ref> This criticism is inspired by [[totalitarianism|totalitarian]] and [[collectivism|collectivist]] forms of government, like the ones found in [[Joseph Stalin]]'s [[Soviet Union]] or [[Mao Zedong]]'s China. It is also inspired by the analogy between collective intelligence or [[swarm intelligence]] and [[insect societies]], such as beehives and ant colonies in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the [[Borg (Star Trek)|Borg]],<ref>Goertzel, B. (2002). Creating Internet Intelligence: Wild computing, distributed digital consciousness, and the emerging global brain. Kluwer Academic/Plenum Publishers. Retrieved from http://books.google.com/books/about/Creating_Internet_Intelligence.html?id=Vnzb-xLdvv8C&redir_esc=y</ref> the race of collectively thinking cyborgs imagined by the creators of the [[Star Trek]] science fiction series. \u000a\u000aGlobal brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision,.<ref>Heylighen, F. (2007). The Global Superorganism: an evolutionary-cybernetic model of the emerging network society. Social Evolution & History, 6(1), 58\u2013119. Retrieved from http://pespmc1.vub.ac.be/papers/Superorganism.pdf</ref><ref>Heylighen, F. (2002). The global brain as a new utopia. Zukunftsfiguren. Suhrkamp, Frankurt. Retrieved from http://pespmc1.vub.ac.be/papers/GB-Utopia.pdf</ref> The reason is that effective [[collective intelligence]] requires [[diversity (politics)|diversity]], [[decentralization]] and individual independence, as demonstrated by [[James Surowiecki]] in his book [[The Wisdom of Crowds]]. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Information retrieval]]\u000a[[Category:Community organizing]]\u000a[[Category:Governance]]\u000a[[Category:Philosophy]]\u000a[[Category:Politics]]\u000a[[Category:Public policy]]
p57
asI32
(lp58
VIR evaluation
p59
aV{{multiple issues|\u000a{{Orphan|date=February 2009}}\u000a{{Unreferenced|date=March 2008}}\u000a}}\u000a\u000a== IR Evaluation ==\u000aThe evaluation of information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].\u000a\u000a*'''Precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:\u000a\u000a:<math> \u005cmbox{precision}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{retrieved documents}\u005c}|} </math>\u000a\u000a*'''Recall''' is the fraction of the documents relevant to the query that are successfully retrieved:\u000a\u000a:<math> \u005cmbox{recall}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{relevant documents}\u005c}|} </math>\u000a\u000a*'''F-measure''' is the harmonic mean of precision and recall:\u000a\u000a:<math> \u005cmbox{F-measure}= 2 * \u005cfrac{\u005c{\u005cmbox{precision}\u005c}*\u005c{\u005cmbox{recall}\u005c}}{\u005c{\u005cmbox{precision}\u005c}+\u005c{\u005cmbox{recall}\u005c}} </math>\u000a\u000aFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.\u000a\u000aF-measure tends to be a better single metric when compared to precision and recall because both of them give different information that can complement each other when combined. If one of them excels more than the other, this metric will reflect it.\u000a\u000aVirtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.\u000a\u000a==See also==\u000a* [[Information retrieval]]\u000a* [[Precision and recall]]\u000a* [[Web search engine]]\u000a\u000a==Further reading==\u000a* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.\u000a*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p60
asI161
(lp61
VWeb search engine
p62
aV{{Redirect|Search engine}}\u000a{{selfref|For a tutorial on using search engines for researching Wikipedia articles, see [[Wikipedia:Search engine test]].}}\u000a[[File:Internet Key Layers.png|thumb|400px|right|Finding information on the World Wide Web had been a difficult and frustrating task, but became much more usable with breakthroughs in search engine technology in the late 1990s.]]\u000aA '''web search engine''' is a software system that is designed to search for information on the [[World Wide Web]].  The search results are generally presented in a line of results often referred to as [[search engine results pages]] (SERPs). The information may be a mix of [[web page]]s, images, and other types of files. Some search engines also [[data mining|mine data]] available in [[database]]s or [[web directory|open directories]].  Unlike [[web directories]], which are maintained only by human editors, search engines also maintain [[real-time computing|real-time]] information by running an [[algorithm]] on a [[web crawler]].\u000a\u000a== History ==\u000a{{further|Timeline of web search engines}}\u000a<!-- Keep this list limited to notable engines (i.e. those that already have Wikipedia articles) to avoid link spam -->\u000a{| class="bordered infobox"\u000a|-\u000a! colspan="3" | Timeline ([[List of search engines|full list]]) <!--Note:  "Launch" refers only to web availability of original crawl-based web search engine results.-->\u000a|-\u000a!Year\u000a!Engine\u000a!Current status\u000a|-\u000a| rowspan="4" |1993\u000a||[[W3Catalog]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Aliweb]]\u000a|{{Site inactive}}\u000a|-\u000a||[[JumpStation]]\u000a|{{Site inactive}}\u000a|-\u000a||[[World-Wide Web Worm|WWW Worm]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="4" |1994\u000a||[[WebCrawler]]\u000a|{{Site active}}, Aggregator\u000a|-\u000a||[[Go.com]]\u000a|{{Site active}}, Yahoo Search\u000a|-\u000a||[[Lycos]]\u000a|{{Site active}}\u000a|-\u000a||[[Infoseek]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="6" |1995\u000a||[[AltaVista]]\u000a|{{Site inactive}}, redirected to Yahoo!\u000a|-\u000a|[[Daum Communications|Daum]]\u000a|{{Site active}}\u000a|-\u000a||[[Magellan (search engine)|Magellan]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Excite]]\u000a|{{Site active}}\u000a|-\u000a||[[SAPO (company)|SAPO]]\u000a|{{Site active}}\u000a|-\u000a||[[Yahoo!]]\u000a|{{Site active}}, Launched as a directory\u000a|-\u000a| rowspan="4" |1996\u000a||[[Dogpile]]\u000a|{{Site active}}, Aggregator\u000a|-\u000a||[[Inktomi (company)|Inktomi]]\u000a|{{Site inactive}}, acquired by Yahoo!\u000a|-\u000a||[[HotBot]]\u000a|{{Site active}}  (lycos.com)\u000a|-\u000a||[[Ask.com|Ask Jeeves]]\u000a|{{Site active}}  (rebranded ask.com)\u000a|-\u000a| rowspan="2" |1997\u000a||[[Northern Light Group|Northern Light]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Yandex]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="4" |1998\u000a||[[Google Search|Google]]\u000a|{{Site active}}\u000a|-\u000a||[[Ixquick]]\u000a|{{Site active}}  also as Startpage\u000a|-\u000a||[[MSN Search]]\u000a|{{Site active}}  as Bing\u000a|-\u000a||[[empas]]\u000a|{{Site inactive}}  (merged with NATE)\u000a|-\u000a| rowspan="5" |1999\u000a||[[AlltheWeb]]\u000a|{{Site inactive}}  (URL redirected to Yahoo!)\u000a|-\u000a||[[GenieKnows]]\u000a|{{Site active}}, rebranded Yellowee.com\u000a|-\u000a||[[Naver]]\u000a|{{Site active}}\u000a|-\u000a||[[Teoma]]\u000a|{{Site inactive}}, redirects to Ask.com\u000a|-\u000a||[[Vivisimo]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="3" |2000\u000a||[[Baidu]]\u000a|{{Site active}}\u000a|-\u000a||[[Exalead]]\u000a|{{Site active}}\u000a|-\u000a||[[Gigablast]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="2" |2003\u000a||[[Info.com]]\u000a|{{Site active}}\u000a|-\u000a||[[Scroogle]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="3" |2004\u000a||[[Yahoo! Search]]\u000a|{{Site active}}, Launched own web search<br />(see Yahoo! Directory, 1995)\u000a|-\u000a||[[A9.com]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Sogou.com|Sogou]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="3" |2005\u000a||[[AOL Search]]\u000a|{{Site active}}\u000a|-\u000a||[[GoodSearch]]\u000a|{{Site active}}\u000a|-\u000a||[[SearchMe]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="6" |2006\u000a||[[Soso (search engine)]]\u000a|{{Site active}}\u000a|-\u000a||[[Quaero]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Ask.com]]\u000a|{{Site active}}\u000a|-\u000a||[[Live Search]]\u000a|{{Site active}} as Bing, Launched as<br />rebranded MSN Search\u000a|-\u000a||[[ChaCha (search engine)|ChaCha]]\u000a|{{Site active}}\u000a|-\u000a||[[Guruji.com]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="4" |2007\u000a||[[wikiseek]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Sproose]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Wikia Search]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Blackle.com]]\u000a|{{Site active}}, Google Search\u000a|-\u000a| rowspan="7" |2008\u000a||[[Powerset (company)|Powerset]]\u000a|{{Site inactive}} (redirects to Bing)\u000a|-\u000a||[[Picollator]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Viewzi]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Boogami]]\u000a|{{Site inactive}}\u000a|-\u000a||[[LeapFish]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Forestle]]\u000a|{{Site inactive}} (redirects to Ecosia)\u000a|-\u000a||[[DuckDuckGo]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="5" |2009\u000a||[[Bing]]\u000a|{{Site active}}, Launched as<br />rebranded Live Search\u000a|-\u000a||[[Yebol]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Mugurdy]]\u000a|{{Site inactive}}  due to a lack of funding\u000a|-\u000a||[[Goby Inc.|Scout (Goby)]]\u000a|{{Site active}}\u000a|-\u000a||[[Nate (web portal)|NATE]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="3" |2010\u000a||[[Blekko]]\u000a|{{Site active}}\u000a|-\u000a||[[Cuil]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Yandex]]\u000a|{{Site active}}, Launched global<br />(English) search\u000a|-\u000a||2011\u000a||[[YaCy]]\u000a|{{Site active}}, [[Peer-to-peer|P2P]] web search engine\u000a|-\u000a| rowspan="1" |2012\u000a||[[Volunia]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="1" |2013\u000a||[[Halalgoogling]]\u000a|{{Site active}}, Islamic / Halal<br />filter Search\u000a|}\u000a\u000aDuring early development of the web, there was a list of [[webserver]]s edited by [[Tim Berners-Lee]] and hosted on the [[CERN]] webserver. One historical snapshot of the list in 1992 remains,<ref>{{cite web|url=http://www.w3.org/History/19921103-hypertext/hypertext/DataSources/WWW/Servers.html |title=World-Wide Web Servers |publisher=W3.org |accessdate=2012-05-14}}</ref> but as more and more webservers went online the central list could no longer keep up. On the [[National Center for Supercomputing Applications|NCSA]]  site, new servers were announced under the title "What's New!"<ref>{{cite web|url=http://home.mcom.com/home/whatsnew/whats_new_0294.html |title=What's New! February 1994 |publisher=Home.mcom.com |accessdate=2012-05-14}}</ref>\u000a\u000aThe first tool used for searching on the [[Internet]] was [[Archie search engine|Archie]].<ref name=LeidenUnivSE>\u000a     "Internet History - Search Engines" (from [[Search Engine Watch]]),\u000a     Universiteit Leiden, Netherlands, September 2001, web:\u000a     [http://www.internethistory.leidenuniv.nl/index.php3?c=7 LeidenU-Archie].\u000a</ref>\u000aThe name stands for "archive" without the "v".  It was created in 1990 by [[Alan Emtage]], Bill Heelan and J. Peter Deutsch, computer science students at [[McGill University]]  in [[Montreal]]. The program downloaded the directory listings of all the files located on public anonymous FTP ([[File Transfer Protocol]]) sites, creating a searchable database of file names; however, Archie did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\u000a\u000aThe rise of [[Gopher (protocol)|Gopher]] (created in 1991 by [[Mark McCahill]]  at the [[University of Minnesota]]) led to two new search programs, [[Veronica (computer)|Veronica]]  and [[Jughead (computer)|Jughead]]. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (''V''ery ''E''asy ''R''odent-''O''riented ''N''et-wide ''I''ndex to ''C''omputerized ''A''rchives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (''J''onzy's ''U''niversal ''G''opher ''H''ierarchy ''E''xcavation ''A''nd ''D''isplay) was a tool for obtaining menu information from specific Gopher servers.  While the name of the search engine "Archie" was not a reference to the [[Archie Comics|Archie comic book]] series, "[[Veronica Lodge|Veronica]]" and "[[Jughead Jones|Jughead]]" are characters in the series, thus referencing their predecessor.\u000a\u000aIn the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. [[Oscar Nierstrasz]] at the [[University of Geneva]] wrote a series of [[Perl]] scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for [[W3Catalog]], the web's first primitive search engine, released on September 2, 1993.<ref name="Announcement html">{{cite web |url= http://groups.google.com/group/comp.infosystems.www/browse_thread/thread/2176526a36dc8bd3/2718fd17812937ac?hl=en&lnk=gst&q=Oscar+Nierstrasz#2718fd17812937ac|title=Searchable Catalog of WWW Resources (experimental)|author=[[Oscar Nierstrasz]]|date=2 September 1993}}</ref>\u000a\u000aIn June 1993, Matthew Gray, then at [[Massachusetts Institute of Technology|MIT]], produced what was probably the first [[web robot]], the [[Perl]]-based [[World Wide Web Wanderer]], and used it to generate an index called 'Wandex'.  The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995.  The web's second search engine [[Aliweb]] appeared in November 1993.  Aliweb did not use a [[web robot]], but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.\u000a\u000a[[JumpStation]] (created in December 1993<ref>{{cite web|url=http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archiveurl=//web.archive.org/web/20010620073530/http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archivedate=2001-06-20 |title=Archive of NCSA what's new in December 1993 page |publisher=Web.archive.org |date=2001-06-20 |accessdate=2012-05-14}}</ref> by [[Jonathon Fletcher]]) used a [[web crawler|web robot]] to find web pages and to build its index, and used a [[web form]] as the interface to its query program.  It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below.  Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.\u000a\u000aOne of the first "all text" crawler-based search engines was [[WebCrawler]], which came out in 1994.  Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the first one widely known by the public.  Also in 1994, [[Lycos]] (which started at [[Carnegie Mellon University]]) was launched and became a major commercial endeavor.\u000a\u000aSoon after, many search engines appeared and vied for popularity. These included [[Magellan (search engine)|Magellan]], [[Excite]], [[Infoseek]], [[Inktomi (company)|Inktomi]], [[Northern Light Group|Northern Light]], and [[AltaVista]]. [[Yahoo!]] was among the most popular ways for people to find web pages of interest, but its search function operated on its [[web directory]], rather than its full-text copies of web pages. Information seekers could also browse the directory instead of doing a keyword-based search.\u000a\u000aGoogle adopted the idea of selling search terms in 1998, from a small search engine company named [[goto.com]]. This move had a significant effect on the SE business, which went from struggling to one of the most profitable businesses in the internet.<ref>http://www.udacity.com/view#Course/cs101/CourseRev/apr2012/Unit/616074/Nugget/671097</ref>\u000a\u000aIn 1996, [[Netscape]] was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page.  The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.<ref>{{cite web|title=Yahoo! And Netscape Ink International Distribution Deal|url=http://files.shareholder.com/downloads/YHOO/701084386x0x27155/9a3b5ed8-9e84-4cba-a1e5-77a3dc606566/YHOO_News_1997_7_8_General.pdf|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref><ref>{{Cite journal |date=1 April 1996|title=Browser Deals Push Netscape Stock Up 7.8% |publisher=Los Angeles Times |url=http://articles.latimes.com/1996-04-01/business/fi-53780_1_netscape-home |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\u000a\u000aSearch engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.<ref>{{cite journal |last=Gandal |first=Neil |authorlink= |year=2001 |title=The dynamics of competition in the internet search engine market |journal=International Journal of Industrial Organization |volume=19 |issue=7 |pages=1103\u20131117 |doi=10.1016/S0167-7187(01)00065-0  |url= |accessdate=|quote= }}</ref> Several companies entered the market spectacularly, receiving record gains during their [[initial public offering]]s. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the [[dot-com bubble]], a speculation-driven market boom that peaked in 1999 and ended in 2001.\u000a\u000aAround 2000, [[Google Search|Google's search engine]] rose to prominence.<ref>{{cite web|url=http://www.google.com/about/company/history/ |title=Our History in depth |publisher=W3.org |accessdate=2012-10-31}}</ref>  The company achieved better results for many searches with an innovation called [[PageRank]], as was explained in the paper ''Anatomy of a Search Engine'' written by [[Sergey Brin]] and [[Larry Page]], the later founders of Google.<ref>{{cite web|url=http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf|title=The Anatomy of a Large-Scale Hypertextual Web Search Engine|last1=Brin|first1=Sergey|last2=Page|first2=Larry}}</ref> This [[iterative algorithm]] ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a [[web portal]]. In fact, Google search engine became so popular that spoof engines emerged such as [[Mystery Seeker]].\u000a\u000aBy 2000, [[Yahoo!]] was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and [[Overture]] (which owned [[AlltheWeb]] and AltaVista) in 2003.  Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.\u000a\u000a[[Microsoft]] first launched MSN Search in the fall of 1998 using search results from Inktomi.  In early 1999 the site began to display listings from [[Looksmart]], blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista were instead.  In 2004, [[Microsoft]] began a transition to its own search technology, powered by its own [[web crawler]] (called [[msnbot]]).\u000a\u000aMicrosoft's rebranded search engine, [[Bing]], was launched on June 1, 2009.  On July 29, 2009, Yahoo! and Microsoft finalized a deal in which [[Yahoo! Search]] would be powered by Microsoft Bing technology.\u000a\u000a== How web search engines work ==\u000a{{Original research|section|date=October 2013\u000a}}\u000a{{Refimprove|date=July 2013}}\u000aA search engine operates in the following order:\u000a# [[Web crawling]]\u000a# [[Index (search engine)|Indexing]]\u000a# [[Web search query|Searching]]<ref name=Jawadekar2011>{{citation |year=2011 |author=Jawadekar, Waman S |title=Knowledge Management: Text & Cases |url=http://books.google.com/books?id=XmGx4J9daUMC&printsec=frontcover&dq=knowledge+management:+text&hl=en&sa=X&ei=ou6uUP-cNqWTiAe2oICoAw&sqi=2&ved=0CDIQ6AEwAA |chapter=8. Knowledge Management: Tools and Technology |chapter-url=http://books.google.com/books?id=XmGx4J9daUMC&pg=PA278&dq=%22search+engine+operates%22&hl=en&sa=X&ei=a-muUJ6UC4aeiAfI24GYAw&sqi=2&ved=0CDgQ6AEwBA |page=278 |place=New Delhi |publisher=Tata McGraw-Hill Education Private Ltd |isbn=978-0-07-07-0086-4 |accessdate=November 23, 2012 }}</ref>\u000a\u000aWeb search engines work by storing information about many web pages, which they retrieve from the [[HTML]] markup of the pages. These pages are retrieved by a [[Web crawler]] (sometimes also known as a spider) \u2014 an automated Web crawler which follows every link on the site. The site owner can exclude specific pages by using [[robots.txt]].\u000a\u000aThe search engine then analyzes the contents of each page to determine how it should be [[Search engine indexing|indexed]] (for example, words can be extracted from the titles, page content, headings, or special fields called [[meta tags]]). Data about web pages are stored in an index database for use in later queries. A query from a user can be a single word. The index helps find information relating to the query as quickly as possible.<ref name=Jawadekar2011/> Some search engines, such as [[Google]], store all or part of the source page (referred to as a [[web cache|cache]]) as well as information about the web pages, whereas others, such as [[AltaVista]], store every word of every page they find.{{Citation needed|date=November 2012}} This cached page always holds the actual search text since it is the one that was actually indexed, so it can be very useful when the content of the current page has been updated and the search terms are no longer in it.<ref name=Jawadekar2011/> This problem might be considered a mild form of [[linkrot]], and Google's handling of it increases [[usability]] by satisfying [[user expectations]] that the search terms will be on the returned webpage. This satisfies the [[principle of least astonishment]], since the user normally expects that the search terms will be on the returned pages. Increased search relevance makes these cached pages very useful as they may contain data that may no longer be available elsewhere.{{Citation needed|date=November 2012}}\u000a[[File:WebCrawlerArchitecture.svg|thumb|High-level architecture of a standard Web crawler]]\u000aWhen a user enters a [[web search query|query]] into a search engine (typically by using [[Keyword (Internet search)|keywords]]), the engine examines its [[inverted index|index]] and provides a listing of best-matching web pages according to its criteria, usually with a short summary containing the document's title and sometimes parts of the text. The index is built from the information stored with the data and the method by which the information is indexed.<ref name=Jawadekar2011/> From 2007 the Google.com search engine has allowed one to search by date by clicking "Show search tools" in the leftmost column of the initial search results page, and then selecting the desired date range.{{Citation needed|date=November 2012}} Most search engines support the use of the [[boolean operators]] AND, OR and NOT to further specify the [[web search query|search query]]. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered.  Some search engines provide an advanced feature called [[Proximity search (text)|proximity search]], which allows users to define the distance between keywords.<ref name=Jawadekar2011/>  There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for. As well, natural language queries allow the user to type a question in the same form one would ask it to a human. A site like this would be ask.com.{{Citation needed|date=November 2012}}\u000a\u000aThe usefulness of a search engine depends on the [[relevance (information retrieval)|relevance]] of the '''result set''' it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to [[rank order|rank]] the results to provide the "best" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.<ref name=Jawadekar2011/> The methods also change over time as Internet usage changes and new techniques evolve.  There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an "[[inverted index]]" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.\u000a\u000aMost Web search engines are commercial ventures supported by [[advertising]] revenue and thus some of them allow advertisers to [[paid inclusion|have their listings ranked higher]] in search results for a fee. Search engines that do not accept money for their search results make money by running [[contextual advertising|search related ads]] alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.<ref>{{cite web|title=FAQ|url=http://www.rankstar.de/hilfe.html|publisher=RankStar|accessdate=19 June 2013}}</ref>\u000a\u000a== Market share ==\u000a\u000a[[Google Search|Google]] is the world's most popular search engine, with a marketshare of 66.44 percent as of December, 2014.<ref name="NMS">{{cite web|url=http://marketshare.hitslink.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0&qpcustom=|title=Desktop Search Engine Market Share|publisher=NetMarketShare|accessdate=2014-06-04}}</ref> [[Baidu]] comes in at second place.<ref name="NMS" />\u000a\u000aThe world's most popular search engines are:<ref>{{cite web|title=FAQ|url=https://www.netmarketshare.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0|publisher=NetMarketShare|accessdate=23 November 2014}}</ref>\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in December 2014\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|66.44|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 11.15|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 10.29|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 9.31|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.53|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}\u000a|-\u000a| [[Lycos]]   || style="text-align:right;"|{{bartable| 0.01|%|2}}\u000a|}\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in October 2014\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|58.01|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 29.06|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 8.01|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 4.01|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.10|%|2}}\u000a|-\u000a| [[Excite]]   || style="text-align:right;"|{{bartable| 0.00|%|2}}\u000a|}\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in July 2014<ref name="NMS" />\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|68.69|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 17.17|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 6.74|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 6.22|%|2}}\u000a|-\u000a| [[Excite]]   || style="text-align:right;"|{{bartable| 0.22|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}\u000a|}\u000a\u000a=== East Asia and Russia ===\u000a\u000aEast Asian countries and Russia constitute a few places where Google is not the most popular search engine.\u000a\u000a[[Yandex]] commands a marketshare of 61.9 per cent in Russia, compared to Google's 28.3 percent.<ref>{{cite web|url=http://www.liveinternet.ru/stat/ru/searches.html?slice=ru;period=week|title=Live Internet - Site Statistics|publisher=Live Internet|accessdate=2014-06-04}}</ref> In China, Baidu is the most popular search engine.<ref>{{cite news|url=http://www.theguardian.com/world/2014/jun/03/chinese-technology-companies-huawei-dominate-world|title=The Chinese technology companies poised to dominate the world|publisher=The Guardian|author=Arthur, Charles|date=2014-06-03|accessdate=2014-06-04}}</ref>  South Korea's homegrown search portal, [[Naver]], is used for 70 per cent online searches in the country.<ref>{{cite web|url=http://blogs.wsj.com/korearealtime/2014/05/21/how-naver-hurts-companies-productivity/|title=How Naver Hurts Companies\u2019 Productivity|publisher=The Wall Street Journal|date=2014-05-21|accessdate=2014-06-04}}</ref> [[Yahoo! Japan]] and [[Yahoo! Search|Yahoo! Taiwan]] are the most popular avenues for internet search in Japan and Taiwan, respectively.<ref>{{cite web|url=http://geography.oii.ox.ac.uk/?page=age-of-internet-empires|title=Age of Internet Empires|publisher=Oxford Internet Institute|accessdate=2014-06-04}}</ref>\u000a\u000a== Search engine bias ==\u000aAlthough search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide.<ref>Segev, El (2010). Google and the Digital Divide: The Biases of Online Knowledge, Oxford: Chandos Publishing.</ref><ref name=vaughan-thelwall>{{cite journal|last=Vaughan|first=Liwen|author2=Mike Thelwall |title=Search engine coverage bias: evidence and possible causes|journal=Information Processing & Management|year=2004|volume=40|issue=4|pages=693\u2013707|doi=10.1016/S0306-4573(03)00063-3}}</ref> These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its [[organic search]] results), and political processes (e.g., the removal of search results to comply with local laws).<ref>Berkman Center for Internet & Society (2002), [http://cyber.law.harvard.edu/filtering/china/google-replacements/ \u201cReplacement of Google with Alternative Search Systems in China: Documentation and Screen Shots\u201d], Harvard Law School.</ref> For example, Google will not surface certain Neo-Nazi websites in France and Germany, where Holocaust denial is illegal.\u000a\u000aBiases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more "popular" results.<ref>{{cite journal|last=Introna|first=Lucas|author2=[[Helen Nissenbaum]] |title=Shaping the Web: Why the Politics of Search Engines Matters|journal=The Information Society: An International Journal|year=2000|volume=16|issue=3|doi=10.1080/01972240050133634}}</ref> Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.<ref name=vaughan-thelwall />\u000a\u000a[[Google Bombing]] is one example of an attempt to manipulate search results for political, social or commercial reasons.\u000a\u000a== Customized results and filter bubbles ==\u000a\u000aMany search engines such as Google and Bing provide customized results based on the user's activity history. This leads to an effect that has been called a [[filter bubble]]. The term describes a phenomenon in which websites use [[algorithm]]s to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint, effectively isolating the user in a bubble that tends to exclude contrary information. Prime examples are Google's personalized search results and [[Facebook]]'s personalized news stream. According to [[Eli Pariser]], who coined the term, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Pariser related an example in which one user searched Google for "BP" and got investment news about [[British Petroleum]] while another searcher got information about the [[Deepwater Horizon oil spill]] and that the two search results pages were "strikingly different".<ref name=twsT43>{{cite news\u000a |first1= Lynn | last1= Parramore\u000a |title= The Filter Bubble\u000a |work= The Atlantic\u000a |quote= Since Dec. 4, 2009, Google has been personalized for everyone. So when I had two friends this spring Google "BP," one of them got a set of links that was about investment opportunities in BP. The other one got information about the oil spill....\u000a |date=  10 October 2010\u000a |url= http://www.theatlantic.com/daily-dish/archive/2010/10/the-filter-bubble/181427/\u000a |accessdate= 2011-04-20\u000a}}</ref><ref name=twsO11>{{cite news\u000a |first= Jacob | last= Weisberg\u000a |title= Bubble Trouble: Is Web personalization turning us into solipsistic twits?\u000a |work= Slate\u000a |date= 10 June 2011\u000a |url= http://www.slate.com/id/2296633/\u000a |accessdate= 2011-08-15\u000a}}</ref><ref name=twsO14>{{cite news\u000a |first= Doug | last= Gross\u000a |title= What the Internet is hiding from you\u000a |publisher= ''CNN''\u000a |quote= I had friends Google BP when the oil spill was happening. These are two women who were quite similar in a lot of ways. One got a lot of results about the environmental consequences of what was happening and the spill. The other one just got investment information and nothing about the spill at all.\u000a |date= May 19, 2011\u000a |url= http://edition.cnn.com/2011/TECH/web/05/19/online.privacy.pariser/\u000a |accessdate= 2011-08-15\u000a}}</ref> The bubble effect may have negative implications for civic discourse, according to Pariser.<ref>{{cite journal| last1= Zhang | first1= Yuan Cao | first2= Diarmuid Ó |last2= Séaghdha | first3= Daniele | last3= Quercia | first4 =Tamas | last4 = Jambor |title=Auralist: Introducing Serendipity into Music Recommendation|journal=ACM WSDM |date=February 2012|url=http://www-typo3.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf}}</ref>\u000a\u000aSince this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or "bubbling" users.\u000a\u000a== Faith-based search engines ==\u000a\u000aThe global growth of the Internet and popularity of electronic contents in the [[Arab]] and [[Muslim]] World during the last decade has encouraged faith adherents, notably in [[Middle East|the Middle East]] and [[Indian subcontinent|Asian sub-continent]], to "dream" of their own faith-based i.e. "[[Islamic]]" search engines or filtered search portals filters that would enable users to avoid accessing forbidden websites such as pornography and would only allow them to access sites that are compatible to the Islamic faith. Shortly before the Muslim only month of [[Ramadan]], [[Halalgoogling]] which collects results from other search engines like [[Google]] and [[Bing]] was introduced to the world July 2013 to presents the [[halal]] results to its users,<ref>{{cite web|url=http://news.msn.com/science-technology/new-islam-approved-search-engine-for-muslims |title=New Islam-approved search engine for Muslims |publisher=News.msn.com |date= |accessdate=2013-07-11}}</ref> nearly two years after I\u2019mHalal, another search engine initially (launched on September 2011) to serve Middle East Internet had to close its search service due to what its owner blamed on lack of funding.<ref>[http://blog.imhalal.com/ I\u2019mHalal - Islamic compliant search project launched September 2009 and shut down late 2011]</ref>\u000a\u000aWhile lack of investment and slow pace in technologies in the Muslim World as the main consumers or targeted end users has hindered progress and thwarted success of serious Islamic search engine, the spectacular failure of heavily invested Muslim lifestyle web projects like [[Muxlim]], which received millions of dollars from investors like Rite Internet Ventures, has - according to I\u2019mHalal shutdown notice - made almost laughable the idea that the next [[Facebook]] or [[Google]] can only come from [[Middle East|the Middle East]] if you support your bright youth.<ref>[http://imhalal.com/ I'mHalal Blog]</ref> Yet Muslim internet experts have been determining for years what is or is not allowed according to [[Shariah|the "Law of Islam"]] and have been categorizing websites and such into being either "[[halal]]" or "[[haram]]". All the existing and past Islamic search engines are merely custom search indexed or monetized by web major search giants like [[Google]], [[Yahoo]] and [[Bing]] with only certain filtering systems applied to ensure that their users can't access Haram sites, which include such sites as nudity, gay, gambling or anything that is deemed to be anti-Islamic.<ref>[http://blog.imhalal.com/ I'mHalal Blog]</ref>\u000a\u000aAnother religiously-oriented search engine is Jewogle, which is the Jewish version of Google and yet another is SeekFind.org, which is a Christian website that includes filters preventing users from seeing anything on the internet that attacks or degrades their faith.<ref>[http://allchristiannews.com/halalgoogling-muslims-get-their-own-sin-free-google-should-christians-have-christian-google/ AllChristianNews]</ref>\u000a\u000a== See also ==\u000a*[[Most popular Internet search engines]]\u000a* [[Comparison of web search engines]]\u000a* [[List of search engines]]\u000a* Answer engine ([[question answering]]) <!-- examples necessary here until article comprehensible to normal reader-->\u000a** [[Quora]]\u000a** [[True Knowledge]]\u000a** [[Wolfram Alpha]]\u000a* [[Google effect]]\u000a* [[Internet Search Engines and Libraries]]\u000a* [[Semantic Web]]\u000a* [[Spell checker]]\u000a* [[Web development tools]]\u000a\u000a== References ==\u000a{{Reflist|33em}}\u000a\u000a== Further reading ==\u000a* For a more detailed history of early search engines, see [http://searchenginewatch.com/showPage.html?page=3071951 Search Engine Birthdays] (from [[Search Engine Watch]]), Chris Sherman, September 2003.\u000a* {{cite journal | quotes =| author =Steve Lawrence; C. Lee Giles | year =1999| title =Accessibility of information on the web | journal =[[Nature (journal)|Nature]] | volume =400 | issue =6740| doi =10.1038/21987 | pmid =10428673 | pages =107\u20139 }}\u000a* Bing Liu (2007), ''[http://www.cs.uic.edu/~liub/WebMiningBook.html Web Data Mining: Exploring Hyperlinks, Contents and Usage Data].'' Springer,ISBN 3-540-37881-2\u000a* Bar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231-288.\u000a* {{cite book | first =Mark | last =Levene | year =2005 | title =An Introduction to Search Engines and Web Navigation | publisher =Pearson | location =| isbn =}}\u000a* {{cite book | first =Randolph | last =Hock | year =2007 | title =The Extreme Searcher's Handbook}}ISBN 978-0-910965-76-7\u000a* {{cite journal | quotes =| author =Javed Mostafa |date= February 2005 | title =Seeking Better Web Searches | journal =[[Scientific American]] | volume =| issue =| pages =| publisher =| pmid =| doi =| bibcode =| url =http://www.sciam.com/article.cfm?articleID=0006304A-37F4-11E8-B7F483414B7F0000 | language =}}<sup class="noprint Inline-Template"><span title="&nbsp;since September 2010" style="white-space: nowrap;">&#91;''&#93;</span></sup>\u000a* {{cite journal |last=Ross |first=Nancy |authorlink=|author2=Wolfram, Dietmar  |year=2000 |title=End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine |journal=Journal of the American Society for Information Science |volume=51 |issue=10 |pages=949\u2013958 |doi=10.1002/1097-4571(2000)51:10<949::AID-ASI70>3.0.CO;2-5|url=|accessdate=|quote=}}\u000a* {{cite journal |last=Xie |first=M. |authorlink=|year=1998 |title=Quality dimensions of Internet search engines |journal=Journal of Information Science |volume=24 |issue=5 |pages=365\u2013372 |doi=10.1177/016555159802400509 |url=|accessdate=|quote=|display-authors=1 |last2=Wang |first2=H. |last3=Goh |first3=T. N. }}\u000a*{{cite book|title=Information Retrieval: Implementing and Evaluating Search Engines|url= http://www.ir.uwaterloo.ca/book/ | year=2010|publisher=MIT Press|author8=Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack}}\u000a\u000a== External links ==\u000a{{commons category|Internet search engines}}\u000a* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Web Search Engine}}\u000a[[Category:Internet search engines| ]]\u000a[[Category:History of the Internet]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p63
asI164
(lp64
VTopic-based vector space model
p65
aVThe '''Topic-based Vector Space Model (TVSM)'''<ref>{{cite | url=http://www.kuropka.net/files/TVSM.pdf | title=Topic-based Vector Space Model | author=Dominik Kuropka | coauthors=Jorg Becker | year=2003}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) extends the [[vector space model]] of [[information retrieval]] by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of stopword lists, stemming and thesaurus in TVSM.\u000aIn contrast to the [[generalized vector space model]] the TVSM does not depend on concurrence-based similarities between terms. \u000a\u000a==Definitions==\u000aThe basic premise of TVSM is the existence of a ''d'' dimensional space ''R'' with only positive axis intercepts, i.e. ''R in R<sup>+</sup>'' and ''d in N<sup>+</sup>''. Each dimension of ''R'' represents a fundamental topic. A term vector ''t'' has a specific weight for a certain ''R''. To calculate these weights assumptions are made taking into account the document contents. Ideally important terms will have a high weight and stopwords and irrelevants terms to the topic will have a low weight. The TVSM document model is obtained as a sum of term vectors representing terms in the document. The similarity between two documents ''Di'' and ''Dj'' is defined as the scalar product of document vectors.\u000a\u000a==Enhanced Topic-based Vector Space Model==\u000aThe enhancement of the Enhanced Topic-based Vector Space Model (eTVSM)<ref>{{cite | url= http://kuropka.net/files/HPI_Evaluation_of_eTVSM.pdf | author=Dominik Kuropka | coauthors=Artem Polyvyanyy | title=A Quantitative Evaluation of the Enhanced Topic-Based Vector Space Model | year=2007}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) is a proposal on how to derive term vectors from an [[Ontology_(information_science) | Ontology]]. Using a synonym Ontology created from [[WordNet]] Kuropka shows good results for document similarity. If a trivial Ontology is used the results are similar to Vector Space model.\u000a\u000a==Implementations==\u000a* [http://sourceforge.net/projects/etvsm/ Implementation of eTVSM in python]\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Vector space model]]
p66
asI166
(lp67
VGeneralized vector space model
p68
aV{{Confusing|date=January 2010}}\u000aThe '''Generalized vector space model''' is a generalization of the [[vector space model]] used in [[information retrieval]].  Many classifiers, especially those which are related to document or text classification, use the TFIDF basis of VSM.  However, this is where the similarity between the models ends - the generalized model uses the results of the TFIDF dictionary to generate similarity metrics based on distance or angle difference, rather than centroid based classification.  '''Wong et al.'''<ref name="wong">{{cite | title=Generalized vector spaces model in information retrieval | url=http://doi.acm.org/10.1145/253495.253506 | first=S. K. M. | last=Wong | coauthors=Wojciech Ziarko, Patrick C. N. Wong | publisher=[[Association for Computing Machinery|SIGIR ACM]] | date=1985-06-05}}</ref> presented an analysis of the problems that the pairwise orthogonality assumption of the [[vector space model]] (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM).\u000a\u000a==Definitions==\u000a\u000aGVSM introduces a term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector ''t<sub>i</sub>'' was expressed as a linear combination of ''2<sup>n</sup>'' vectors ''m<sub>r</sub>'' where ''r = 1...2<sup>n</sup>''.\u000a\u000aFor a document ''d<sub>k</sub>'' and a query ''q'' the similarity function now becomes:\u000a\u000a:<math>sim(d_k,q) = \u005cfrac{\u005csum _{j=1}^n \u005csum _{i=1}^n w_{i,k}*w_{j,q}*t_i \u005ccdot t_j }{\u005csqrt{\u005csum _{i=1}^n w_{i,k}^2}*\u005csqrt{\u005csum _{i=1}^n w_{i,q}^2}}</math>\u000a\u000awhere ''t<sub>i</sub>'' and ''t<sub>j</sub>'' are now vectors of a ''2<sup>n</sup>'' dimensional space.\u000a\u000aTerm correlation <math>t_i \u005ccdot t_j</math> can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence  and the output is the term correlation between any pair of index terms.\u000a\u000a==Semantic information on GVSM==\u000a\u000aThere are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model:\u000a# compute semantic correlations between terms\u000a# compute frequency co-occurrence statistics from large corpora\u000a\u000aRecently Tsatsaronis<ref>{{cite | title=A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness | url=http://www.aclweb.org/anthology/E/E09/E09-3009.pdf | last= Tsatsaronis | first=George | coauthors=Vicky Panagiotopoulou | publisher=[[Association for Computing Machinery|EACL ACM]] |date=2009-04-02}}</ref> focused on the first approach.\u000a\u000aThey measure semantic relatedness (''SR'') using a thesaurus (''O'') like [[WordNet]]. It considers the path length, captured by compactness (''SCM''), and the path depth, captured by semantic path elaboration (''SPE'').\u000aThey estimate the <math>t_i \u005ccdot t_j</math> inner product by:\u000a\u000a<math>t_i \u005ccdot t_j = SR((t_i, t_j), (s_i, s_j), O)</math>\u000a\u000awhere ''s<sub>i</sub>'' and ''s<sub>j</sub>'' are senses of terms ''t<sub>i</sub>'' and ''t<sub>j</sub>'' respectively, maximizing <math>SCM \u005ccdot SPE</math>.\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Vector space model]]
p69
asI40
(lp70
VText Retrieval Conference
p71
aV{{Other uses of|TREC|TREC (disambiguation)}}\u000aThe '''Text REtrieval Conference (TREC)''' is an on-going series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].\u000a\u000aEach track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.\u000a\u000a== Tracks ==\u000a\u000a===Current Tracks===\u000a''New tracks are added as new research needs are identified, this list is current for TREC 2014.''\u000a* Contextual Suggestion Track - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.\u000a* Clinical Decision Support Track - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care\u000a* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.\u000a* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.\u000a* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. \u000a* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.\u000a* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.\u000a* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.\u000a\u000a===''Past tracks''===\u000a* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.\u000a* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. \u000a* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.\u000a* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.\u000a* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.\u000a* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.\u000a* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.\u000a* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].\u000a* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.\u000a* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.\u000a* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.\u000a* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. \u000a* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.\u000a* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.\u000a* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.\u000a* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.\u000a* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.\u000a* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.\u000a* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].\u000a:In 2003, this track became its own independent evaluation named [[TRECVID]].\u000a\u000a===Related Events===\u000aIn 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).\u000a\u000a== Conference Contributions ==\u000a<!-- contributions of conference to research/IR community -->\u000aNIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."\u000a<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>\u000a<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>\u000a\u000aWhile one study suggests that the state of the art for ad hoc search  has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.\u000a\u000aThe test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.\u000a\u000aTREC systems often provide a baseline for further research.  Examples include:\u000a* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>\u000a* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>\u000a* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC's QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']</ref>\u000a\u000a== Participation ==\u000aThe conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a*[http://trec.nist.gov/ TREC website at NIST]\u000a*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]\u000a*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computer science competitions]]
p72
asI28
(lp73
VOrdered weighted averaging aggregation operator
p74
aVIn applied mathematics \u2013 specifically in [[fuzzy logic]] \u2013 the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.\u000a\u000a== Definition ==\u000a\u000aFormally an '''OWA''' operator of dimension <math> \u005c n </math> is a mapping <math> F: R_n \u005crightarrow R </math> that has an associated collection of weights <math> \u005c  W = [w_1, \u005cldots, w_n] </math> lying in the unit interval and summing to one and with 		\u000a\u000a:<math> F(a_1, \u005cldots , a_n) =  \u005csum_{j=1}^n  w_j b_j</math>\u000a\u000awhere <math> b_j </math> is the ''j''<sup>th</sup> largest of the <math> a_i </math>.\u000a\u000aBy choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''<sub>''j''</sub>.\u000a\u000a== Properties ==\u000a\u000aThe OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.\u000a\u000a{|class="wikitable"\u000a|[[Bounded operator|Bounded]]\u000a|<math>   \u005cmin(a_1, \u005cldots, a_n) \u005cle F(a_1, \u005cldots, a_n) \u005cle \u005cmax(a_1, \u005cldots, a_n) </math>\u000a|-\u000a|[[Monotonic]]\u000a|<math>   F(a_1, \u005cldots, a_n) \u005cge F(g_1, \u005cldots, g_n) </math> if <math> a_i \u005cge g_i </math> for <math>\u005c i = 1,2,\u005cldots,n </math>\u000a|-\u000a|[[symmetric operator|Symmetric]]\u000a|<math>   F(a_1, \u005cldots, a_n)  = F(a_\u005cboldsymbol{\u005cpi(1)}, \u005cldots, a_\u005cboldsymbol{\u005cpi(n)})</math> if <math>\u005cboldsymbol{\u005cpi} </math> is a permutation map\u000a|-\u000a|[[Idempotent]]\u000a|<math>  \u005c F(a_1, \u005cldots, a_n)  =  a </math> if all <math> \u005c a_i = a </math>\u000a|}\u000a\u000a== Notable OWA operators ==\u000a:<math> \u005c F(a_1, \u005cldots, a_n) = \u005cmax(a_1, \u005cldots, a_n) </math> if <math> \u005c w_1 = 1 </math> and <math> \u005c w_j = 0 </math> for <math> j \u005cne 1 </math>\u000a\u000a:<math> \u005c F(a_1, \u005cldots, a_n) = \u005cmin(a_1, \u005cldots, a_n) </math> if <math> \u005c w_n = 1 </math> and <math> \u005c w_j = 0 </math> for <math> j \u005cne n </math>\u000a\u000a== Characterizing features ==\u000a\u000aTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\u000a\u000aThis is defined as\u000a:<math>A-C(W)= \u005cfrac{1}{n-1} \u005csum_{j=1}^n (n - j) w_j. </math>\u000a\u000aIt is known that <math> A-C(W) \u005cin [0, 1] </math>.\u000a\u000aIn addition ''A''&nbsp;&minus;&nbsp;''C''(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\u000a\u000aThe second feature is the dispersion. This defined as\u000a\u000a:<math>H(W) = -\u005csum_{j=1}^n w_j \u005cln (w_j).</math>\u000a\u000aAn alternative definition is <math>E(W) = \u005csum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used\u000a\u000a== A literature survey: OWA (1988-2014)==\u000aThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183\u2013190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full) \u000a\u000a== Type-1 OWA aggregation operators ==\u000a\u000aThe above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\u000a'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\u000a\u000aThe '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:\u000a\u000aGiven the ''n'' linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, then for each <math>\u005calpha \u005cin [0,\u005c;1]</math>, an <math>\u005calpha </math>-level type-1 OWA operator with <math>\u005calpha </math>-level sets <math>\u005cleft\u005c{ {W_\u005calpha ^i } \u005cright\u005c}_{i = 1}^n </math> to aggregate the <math>\u005calpha </math>-cuts of fuzzy sets <math>\u005cleft\u005c{ {A^i} \u005cright\u005c}_{i =1}^n </math> is given as\u000a\u000a: <math>\u000a\u005cPhi_\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright) =\u005cleft\u005c{ {\u005cfrac{\u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} } }{\u005csum\u005climits_{i = 1}^n {w_i } }\u005cleft| {w_i \u005cin W_\u005calpha ^i ,\u005c;a_i } \u005cright. \u005cin A_\u005calpha ^i ,\u005c;i = 1, \u005cldots ,n} \u005cright\u005c}</math>\u000a\u000awhere <math>W_\u005calpha ^i= \u005c{w| \u005cmu_{W_i }(w) \u005cgeq \u005calpha \u005c}, A_\u005calpha ^i=\u005c{ x| \u005cmu _{A_i }(x)\u005cgeq \u005calpha \u005c}</math>, and <math>\u005csigma :\u005c{\u005c;1, \u005cldots ,n\u005c;\u005c} \u005cto \u005c{\u005c;1, \u005cldots ,n\u005c;\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cge a_{\u005csigma (i + 1)} ,\u005c;\u005cforall \u005c;i = 1, \u005cldots ,n - 1</math>, i.e., <math>a_{\u005csigma (i)} </math> is the <math>i</math>th largest\u000aelement in the set <math>\u005cleft\u005c{ {a_1 , \u005cldots ,a_n } \u005cright\u005c}</math>.\u000a\u000aThe computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals <math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)</math>:\u000a<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)_{-} </math> and <math>\u000a\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright)_ {+},</math>\u000awhere <math>A_\u005calpha ^i=[A_{\u005calpha-}^i, A_{\u005calpha+}^i], W_\u005calpha ^i=[W_{\u005calpha-}^i, W_{\u005calpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:\u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots\u000a,A_\u005calpha ^n } \u005cright)_\u005calpha } \u005calpha </math>\u000a\u000aFor the left end-points, we need to solve the following programming problem:\u000a\u000a:<math> \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)_{-} = \u005cmathop {\u005cmin }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i = 1}^n {w_i } } </math>\u000a\u000awhile for the right end-points, we need to solve the following programming problem:\u000a\u000a:<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots , A_\u005calpha ^n } \u005cright)_{+} = \u005cmathop {\u005cmax }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i  A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i =\u000a1}^n {w_i } } </math>\u000a\u000a[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\u000a\u000a== References ==\u000a\u000a* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183\u2013190, 1988.\u000a\u000a* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.\u000a\u000a* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68\u201381, 2007.\u000a\u000a* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]\u000a\u000a* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988\u20132014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d]. \u000a\u000a* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.\u000a\u000a* Majlender, P., "OWA operators with maximal Rényi entropy," Fuzzy Sets and Systems 155, 340\u2013360, 2005.\u000a\u000a* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439\u2013456.\u000a\u000a* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;3281\u20133296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]\u000a\u000a* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;1455\u20131468.[http://dx.doi.org/10.1109/TKDE.2010.191]\u000a\u000a* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540\u2013558, 2010.[http://dx.doi.org/10.1002/int.20420]\u000a\u000a[[Category:Artificial intelligence]]\u000a[[Category:Logic in computer science]]\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]
p75
asI71
(lp76
V30 Digits
p77
aV{{Use dmy dates|date=July 2013}}\u000a{{multiple issues|\u000a{{notability|Companies|date=August 2012}}\u000a{{refimprove|date=August 2012}}\u000a{{peacock|date=August 2012}}\u000a{{advert|date=August 2012}}\u000a}}\u000a{{Infobox company\u000a| logo = [[Image:30 Digits Logo.jpg|center]]\u000a| name = 30 Digits GmbH\u000a| type = [[Private company|Private]]\u000a| foundation = 2008\u000a| location = [[Munich]], Germany\u000a| area_served = [[Europe]] <br/> [[North America]] <br/> [[South America]] <br/> [[Asia]]\u000a| key_people = Justin Gilbreath (Managing Director) <br/> Mathis Koblin (Director of R&D)\u000a| industry = [[Information access]] <br/> [[Information retrieval]] <br/> [[Web mining]] <br/> [[Open Source software]]\u000a| products = [[Search Engines]] <br/> Information Discovery Suite <br/> [[Apache Lucene]] <br/> [[Apache Solr]] <br/> Web Extractor\u000a| company_slogan = Linking People to Content\u000a| homepage = {{url|http://www.30digits.com}}\u000a}}\u000a\u000a'''30 Digits''' is a privately held information access and retrieval company with headquarters in [[Munich, Germany]]<ref>{{cite web |url=http://www.digitalpublic.de/web-20-suchmaschinen-holen-auf |title=Web 2.0 \u2013 Suchmaschinen holen auf}}</ref> located in the "Münchner Technologie Zentrum".<ref>{{cite web |url=http://www.mtz.de/index.php?id=12 |title=List of companies located in the MTZ (Münchner Technologie Zentrum)}}</ref> The company was founded in 2008 and offers software that is a mix of privately developed code and leading [[Open Source]] technology primarily from the [[Apache Software Foundation]].\u000a\u000aThe company focuses on [[enterprise information access]] solutions from areas ranging from call-center applications to [[enterprise search]] to database offloading. The company also focuses on solutions created out of unstructured content on the web being structured for analysis,<ref>{{cite web |url=http://www.crmmanager.de/magazin/artikel_2165_enterprise_20_wahlkampf.html |title=Enterprise 2.0: Was ein Unternehmen von Obamas Wahlkampf lernen kann}}</ref> often referred to as [[web harvesting]].  This can be for monitoring security threats or observing customer reactions to products.  It can even be used to gather complex address and other details about entities like properties.<ref>{{cite web |url=http://www.prweb.com/releases/2011/03/prweb5186764.htm |title=viewr Selects 30 Digits as Primary Property Data Provider }}</ref>  Sometimes the focuses blend together in areas like Market or Business Intelligence where both internal and external information needs extraction, analysis, and retrieval capabilities.\u000a\u000aIn addition to the software solutions, 30 Digits Professional Services offers services to assist customer in designing and deploying the correct solutions for the challenge at hand. {{citation needed|date=August 2012}} Trainings, support, and consulting are available both on 30 Digits software and the Open Source software they work with like [[Lucene]] and [[Solr]].<ref>{{cite web |url=http://www.aktiv-verzeichnis.de/details/30-digits-gmbh.html |title=Company description from the Aktiv Verzeichnis}}</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://www.30digits.com  Company website]\u000a* [http://www.imittelstand.de/mittelstandsliste/webcode/ww1213 Article (in German) placing 30 Digits Web Extractor in Top20 Business Intelligence tools for the "Initiative Mittelstand" 2009]\u000a* [http://lucene.apache.org/  Lucene website]\u000a* [http://lucene.apache.org/solr/ Solr website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Search engine software]]\u000a[[Category:Software industry]]\u000a[[Category:Software companies of Germany]]\u000a[[Category:Business software]]\u000a[[Category:Companies based in Munich]]\u000a[[Category:Companies of Germany]]\u000a[[Category:Companies of Europe]]
p78
asI44
(lp79
VDiscounted cumulative gain
p80
aV'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.<ref>Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422\u2013446 (2002)</ref>\u000a\u000a== Overview ==\u000a\u000aTwo assumptions are made in using DCG and its related measures.\u000a\u000a# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\u000a# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than irrelevant documents.\u000a\u000aDCG originates from an earlier, more primitive, measure called Cumulative Gain.\u000a\u000a=== Cumulative Gain ===\u000a\u000aCumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position <math>p</math> is defined as:\u000a\u000a:<math> \u005cmathrm{CG_{p}} = \u005csum_{i=1}^{p} rel_{i} </math>\u000a\u000aWhere <math>rel_{i}</math> is the graded relevance of the result at position <math>i</math>.\u000a\u000aThe value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document <math>d_{i}</math> above a higher ranked, less relevant, document <math>d_{j}</math> does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.\u000a\u000a=== Discounted Cumulative Gain ===\u000a\u000aThe premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position <math>p</math> is defined as:<ref name="stanfordireval">{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}</ref>\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = rel_1 + \u005csum_{i=2}^{p} \u005cfrac{rel_{i}}{\u005clog_{2}(i)} </math>\u000a\u000aPreviously there has not been shown any theoretically sound justification for using a [[logarithm]]ic reduction factor<ref>{{cite book | title=Search Engines: Information Retrieval in Practice | author=B. Croft, D. Metzler, and T. Strohman |year=2009 | publisher=''Addison Wesley"}}</ref> other than the fact that it produces a smooth reduction.\u000a\u000aAn alternative formulation of DCG<ref>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363</ref> places stronger emphasis on retrieving relevant documents:\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = \u005csum_{i=1}^{p} \u005cfrac{ 2^{rel_{i}} - 1 }{ \u005clog_{2}(i+1)} </math>\u000a\u000aThe latter formula is commonly used in industry including major web search companies<ref name="stanfordireval"/> and data science competition platform such as Kaggle.<ref>{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}</ref>\u000a\u000aIn Croft, Metzler and Strohman (page 320, 2010), the authors mistakenly claim that these two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]]; <math>rel_{i} \u005cin \u005c{0,1\u005c}</math>.  To see that they are not the same, let there be one relevant document and that relevant document is at rank 2.  The first version of DCG equals 1 / log2(2) = 1.  The second version of DCG equals 1 / log2(2+1) = 0.631.  The way that the two formulations of DCG are the same for binary judgments is in the way gain in the numerator is calculated.  For both formulations of DCG, binary relevance produces gain at rank i of 0 or 1.  No matter the number of relevance grades, the two formulations differ in their discount of gain.\u000a\u000aNote that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.\u000a\u000aRecently, Wang et al.(2013)<ref>Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of NDCG Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).</ref> give theoretical guarantee for using the logarithmic reduction factor in NDCG. Specifically, the authors prove for every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets.\u000a\u000a=== Normalized DCG ===\u000a\u000aSearch result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of <math>p</math> should be normalized across queries. This is done by sorting documents of a result list by relevance, producing the maximum possible DCG till position <math>p</math>, also called Ideal DCG (IDCG) till that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:\u000a\u000a:<math> \u005cmathrm{nDCG_{p}} = \u005cfrac{DCG_{p}}{IDCG_{p}} </math>\u000a\u000aThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\u000a\u000aThe main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.\u000a\u000a== Example ==\u000a\u000aPresented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning irrelevant, 3 meaning completely relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as\u000a\u000a:<math> D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} </math>\u000a\u000athe user provides the following relevance scores:\u000a\u000a:<math> 3, 2, 3, 0, 1, 2 </math>\u000a\u000aThat is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:\u000a\u000a:<math> \u005cmathrm{CG_{6}} = \u005csum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11</math>\u000a\u000aChanging the order of any two documents does not affect the CG measure. If <math>D_3</math> and <math>D_4</math> are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! <math>i</math>\u000a! <math>rel_{i}</math>\u000a! <math>\u005clog_{2}i</math>\u000a! <math> \u005cfrac{rel_{i}}{\u005clog_{2}i} </math>\u000a|-\u000a| 1\u000a| 3\u000a| 0\u000a| N/A\u000a|-\u000a| 2\u000a| 2\u000a| 1\u000a| 2\u000a|-\u000a| 3\u000a| 3\u000a| 1.585\u000a| 1.892\u000a|-\u000a| 4\u000a| 0\u000a| 2.0\u000a| 0\u000a|-\u000a| 5\u000a| 1\u000a| 2.322\u000a| 0.431\u000a|-\u000a| 6\u000a| 2\u000a| 2.584\u000a| 0.774\u000a|}\u000a\u000aSo the <math>DCG_{6}</math> of this ranking is:\u000a\u000a:<math> \u005cmathrm{DCG_{6}} = rel_{1} + \u005csum_{i=2}^{6} \u005cfrac{rel_{i}}{\u005clog_{2}i} = 3 + (2 + 1.892 + 0 + 0.431 + 0.774) = 8.10</math>\u000a\u000aNow a switch of <math>D_3</math> and <math>D_4</math> results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.\u000a\u000aThe performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.\u000a\u000aTo normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:\u000a\u000a:<math> 3, 3, 2, 2, 1, 0 </math>\u000a\u000aThe DCG of this ideal ordering, or ''IDCG'', is then:\u000a\u000a:<math> \u005cmathrm{IDCG_{6}} = 8.69 </math>\u000a\u000aAnd so the nDCG for this query is given as:\u000a\u000a:<math> \u005cmathrm{nDCG_{6}} = \u005cfrac{DCG_{6}}{IDCG_{6}} = \u005cfrac{8.10}{8.69} = 0.932 </math>\u000a\u000a== Limitations ==\u000a# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,0 </math> respectively, both would be considered equally good even if later contains a bad result. One way to take into account this limitation is use <math>1 - 2^{rel_{i}}</math> in numerator for scores for which we want to penalize and <math>2^{rel_{i}} - 1</math> for all others. For example, for the ranking judgments <math>Excellent, Fair, Bad</math> one might use numerical scores <math>1,0,-1</math> instead of <math>2,1,0</math>.\u000a# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,1,1 </math> respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores <math> 1,1,1,0,0 </math> and <math> 1,1,1,1,1 </math> and quote nDCG as nDCG@5.\u000a# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.\u000a\u000a== References ==\u000a{{Reflist|1}}\u000a\u000a[[Category:Information retrieval|*]]
p81
asI174
(lp82
VHybrid search engine
p83
aV{{Notability|date=December 2009}}\u000aA '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.\u000a\u000a==References==\u000a{{No footnotes|date=April 2010}}\u000a*http://eprints.ecs.soton.ac.uk/17457/\u000a*http://eprints.whiterose.ac.uk/3771/\u000a*http://www.picollator.com\u000a\u000a[[Category:Searching]]\u000a\u000a\u000a{{web-stub}}
p84
asI175
(lp85
VCategory:Internet search
p86
aV{{Cat main|Internet search}}\u000a\u000a[[Category:Web services]]\u000a[[Category:Searching]]\u000a[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that "Internet search" redirects to "Web page search" (or something like that)--->
p87
asI180
(lp88
VOpenGrok
p89
aV{{multiple issues|\u000a{{Advert|date=March 2012}}\u000a{{Notability|Products|date=March 2012}}\u000a}}\u000a\u000a{{Infobox software\u000a| name                   = OpenGrok\u000a| logo                   = [[Image:OpenGrok Logo.png|150px|OpenGrok Logo]]\u000a| screenshot             = \u000a| caption                =\u000a| collapsible            = yes\u000a| developer              = [[Sun Microsystems]]/[[Oracle Corporation]]\u000a| latest release version = 0.12.1\u000a| latest release date    = {{release date|2014|04|29}}\u000a| latest preview version =\u000a| latest preview date    =\u000a| operating system       = [[Cross-platform]]\u000a| programming language   = [[Java (programming language)|Java]]\u000a| genre                  = [[Index (search engine)|Index]]er and [[cross-reference]]r with [[Revision control]]\u000a| license                = [[CDDL]]\u000a| website                = http://opengrok.github.com/OpenGrok/\u000a}}\u000a\u000a'''OpenGrok''' is a [[source code]] search and cross reference engine. It helps programmers to search, cross-reference and navigate source code trees.\u000a\u000aIt can understand various [[program (computing)|program]] [[file formats]] and [[version control]] histories like [[Monotone (software)|Monotone]], [[Source Code Control System|SCCS]], [[Revision Control System|RCS]], [[Concurrent Versions System|CVS]], [[Subversion (software)|Subversion]], [[Mercurial (software)|Mercurial]], [[Git (software)|Git]], [[IBM Rational ClearCase|Clearcase]], [[Perforce]] and [[Bazaar (software)|Bazaar]].<ref>https://github.com/OpenGrok/OpenGrok/wiki/Supported-Revision-Control-Systems</ref>\u000a\u000aThe name comes from the term ''[[grok]]'', a [[jargon]] term used in computing to mean "profoundly understand". The term ''[[grok]]'' originated in a science fiction novel by Robert A. Heinlein called ''[[Stranger in a Strange Land]]''.\u000a\u000aOpenGrok is being developed mainly by [[Oracle Corporation]] (former [[Sun Microsystems]]) engineers with help from its community. OpenGrok is released under the terms of the [[Common Development and Distribution License]] (CDDL).\u000a\u000a== Features ==\u000a\u000aOpenGrok's features include:\u000a\u000a* Full text Search\u000a* Definition Search\u000a* Identifier Search\u000a* Path search\u000a* History Search\u000a* Shows matching lines\u000a* Hierarchical Search\u000a* query syntax like ''AND'', ''OR'', ''field'':\u000a* Incremental update\u000a* Syntax highlighting-Xref\u000a* Quick navigation inside the file\u000a* Interface for SCM\u000a* Usable URLs\u000a* Individual file download\u000a* Changes at directory level\u000a* Multi language support\u000a\u000a== See also ==\u000a\u000a* [[LXR Cross Referencer]]\u000a* [[ViewVC]]\u000a* [[FishEye (software)]]\u000a\u000a== References ==\u000a\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://opengrok.github.com/OpenGrok/ OpenGrok project page]\u000a* {{ohloh|opengrok}}\u000a* [http://code.metager.de/source/ Metager]\u000a* [http://BXR.SU/ Super User's BSD Cross Reference]\u000a\u000a{{Sun Microsystems}}\u000a{{Java (Sun)}}\u000a\u000a[[Category:Cross-platform free software]]\u000a[[Category:Free revision control software]]\u000a[[Category:Source code]]\u000a[[Category:Searching]]\u000a[[Category:Java platform software]]\u000a[[Category:Concurrent Versions System]]\u000a[[Category:Subversion]]\u000a[[Category:Code search engines]]\u000a\u000a\u000a{{programming-software-stub}}
p90
asI181
(lp91
VReverse telephone directory
p92
aVA '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.\u000a\u000aReverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only.\u000a\u000aPublicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.\u000a\u000a==History==\u000aPrinted reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].<ref>{{cite news | url=http://news.google.com/newspapers?nid=1454&dat=19720102&id=87osAAAAIBAJ&sjid=vgkEAAAAIBAJ&pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}</ref> In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.\u000a\u000a==Australia==\u000aIn 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.<ref>{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref><ref name=austliiPP>{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref> gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.\u000a\u000aIn February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.<ref>{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}</ref>\u000a\u000aAs it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.\u000a\u000a==United States==\u000a\u000aIn United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.\u000a\u000aAs [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.<ref>{{cite web | url=http://www.ncbi.nlm.nih.gov/pubmed/15652722 | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | publisher=Ncbi.nlm.nih.gov | work=2005 Feb | accessdate=9 February 2014 | author=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J.}}</ref>\u000a\u000aIn recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.\u000a\u000a==United Kingdom==\u000aIn the United Kingdom proper, reverse directory information is not publicly available.<ref>{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}</ref> However, in the [[Channel Islands]] it is provided in the printed telephone directories. \u000a\u000aAlthough the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.\u000a\u000a==References==\u000a{{reflist}}\u000a==External links==\u000a<!-- Do not delete these comments. -->\u000a<!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --> \u000a*[http://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]\u000a*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]\u000a\u000a\u000a[[Category:Telephone numbers]]\u000a[[Category:Directories]]\u000a[[Category:Searching]]
p93
asI183
(lp94
VIndexing Service
p95
aV{{Use dmy dates|date=February 2011}}\u000a{{Infobox Windows component\u000a| name                = Indexing Service\u000a| screenshot          = Indexing Service Query Form.PNG\u000a| screenshot_size     = 300px\u000a| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].\u000a| type                = [[Desktop search]]\u000a| service_name        = Indexing Service\u000a| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.\u000a| replaced_by         = [[Windows Search]]\u000a| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />\u000a}}\u000a\u000a'''Indexing Service''' (originally called '''Index server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by [[Windows Search]].\u000a\u000a== History ==\u000aIndexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}\u000a\u000aIn [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />\u000a\u000aIndexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].\u000a\u000a== Search interfaces ==\u000a\u000aComprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.\u000a\u000aOnce the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.\u000a\u000aMicrosoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web\u000a | url = http://support.microsoft.com/kb/319506\u000a | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)\u000a | work = Microsoft Support\u000a | publisher = 10 May 2002\u000a | accessdate = 1 February 2011\u000a}}</ref>\u000a\u000a== References ==\u000a{{Reflist|refs=\u000a<ref name = "MIS-Intro">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx\u000a  |title = Introduction to Microsoft Index Server\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date = 15 October 1997\u000a  |accessdate = 1 February 2011\u000a  |first1 = Krishna\u000a  |last1 = Nareddy\u000a  }}</ref>\u000a<ref name = "MIS-v3">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx\u000a  |title = Indexing Service Version 3.0\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name = "WIS-What">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx\u000a  |title = What is Indexing Service?\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name="WIS-Install2008">{{Cite web\u000a  |url = http://support.microsoft.com/kb/954822\u000a  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)\u000a  |work = Microsoft Support\u000a  |publisher = Microsoft Corporation\u000a  |date = 3 May 2010\u000a  |accessdate = 1 February 2011\u000a  }}</ref>\u000a<ref name="TnC-144">{{Cite book\u000a  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en\u000a  |format = Microsoft Word\u000a  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP\u000a  |edition = 2.0\u000a  |publisher = Microsoft Corporation\u000a  |page = 144\u000a  |date=December 2005\u000a  |first1 = Mike\u000a  |last1  = Danseglio\u000a  |first2 = Kurt\u000a  |last2  = Dillard\u000a  |first3 = José\u000a  |last3  = Maldonado\u000a  |first4 = Paul\u000a  |last4  = Robichaux\u000a  |editor1-first = Reid\u000a  |editor1-last  = Bannecker\u000a  |editor2-first = John\u000a  |editor2-last  = Cobb\u000a  |editor3-first = Jon\u000a  |editor3-last  = Tobey\u000a  |editor4-first = Steve\u000a  |editor4-last  = Wacker\u000a  }}</ref>\u000a}}\u000a\u000a{{DEFAULTSORT:Indexing Service}}\u000a[[Category:Windows communication and services]]\u000a[[Category:Desktop search engines|Desktop search engines]]\u000a[[Category:Searching]]\u000a[[Category:Windows components]]
p96
asI184
(lp97
VUnified Information Access
p98
aV{{Multiple issues|\u000a{{confusing|date=November 2010}}\u000a{{cleanup|date=November 2010}}\u000a}}\u000a\u000a'''Unified Information Access Platforms''' are [[computing platforms]] that integrate large volumes of [[unstructured information|unstructured]], semi-structured, and structured information into a unified environment for processing, analysis and decision-making. These platforms are highly scalable, hybrid architectures that combine elements of database and search technologies in order to make information access dynamic and ad hoc, while offering the reporting and visualization features commonly found in business intelligence applications. While the vision for such integrated platforms has been around for years, only since 20XX have products been released into the market. Companies like [[Applied Relevance]], [[Attivio]], [[BA-Insight]], [[Cambridge Semantics]], [[Endeca Technologies Inc.|Endeca]], [[Exalead]], [[HP Autonomy]], [[PolySpot]], [[MarkLogic]], [[PerfectSearch]], [[Palantir Technologies|Palantir]], [[TopQuadrant]], [[Sinequa]] and [http://www.virtualworks.com VirtualWorks] have recognized the need for this approach.\u000a\u000aUnified access applications:\u000a*Create [[Hybrid computer|hybrid]] data structures that combine structured data and data operators with [[Text (literary theory)|text]] and semi-structured operations and analytics. They combine semantic understanding, fuzzy matching, sorting, joins, and various operations such as [[range searching]] within a single architecture, rather than federating a query to multiple sources in multiple forms.\u000a*Leverage these hybrid structures to provide real-time access through ad hoc queries to multiple sources of information, including information across a spectrum of [[File format|format]]s (e.g. rich media) through a single [[Interface (computer science)|interface]].\u000a*Handle sparse matrices of unpredictable content.\u000a*Optimize interactions for consumption and decisions, [[Process (computing)|processing]] queries faster than traditional database and/or BI applications and implementing visual consumption metaphors.\u000a*Scale to [[terabyte]]s.\u000a*Provide reporting tools that are BI-like, or integrate easily with BI applications and reporting tools.\u000a\u000a== References ==\u000a* Worldwide Search and Discovery 2009 Vendor Shares and Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.\u000a* Building the Intelligent Enterprise: The Case for Unified Access and Analytics\u000aSusan Feldman, Jul 2009 - Doc # 219467   \u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* Video: [http://www.attivio.com/poweringbusiness/videos/63-attivio/869-unified-information-access-in-4-minutes.html "Unified Information Access in 4 Minutes"]\u000a* http://www.computerworld.com/s/article/9180280/Five_Advantages_of_Unified_Information_Access_UIA\u000a* http://www.eweek.com/c/a/Enterprise-Applications/How-to-Use-Unified-Information-Access-to-Get-Most-Value-from-Your-Data/\u000a\u000a[[Category:Searching]]
p99
asI186
(lp100
VSocial search
p101
aV'''Social search''' or a '''social search engine''' is a type of [[web search]] that takes into account the [[Social Graph]] of the person initiating the search query. When applied to web searches the Social-Graph uses established algorithmic or machine-based approaches where relevance is determined by analyzing the text of each document or the link structure of the documents.<ref>[http://searchenginewatch.com/showPage.html?page=3623153 What's the Big Deal With Social Search?], SearchEngineWatch, Aug 15, 2006</ref> Search results produced by '''social search engine''' give more visibility to content created or "touched" by users in the Social Graph.\u000a\u000aSocial search takes many forms, ranging from simple [[social bookmarking|shared bookmarks]] or [[Tag (metadata)|tagging]] of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer [[algorithm]]s.<ref>[http://www2.computer.org/portal/web/csdl/doi/10.1109/MC.2009.87 Chi, Ed H. Information Seeking Can Be Social, Computer, vol. 42, no. 3, pp. 42-46, Mar. 2009, ] {{doi|10.1109/MC.2009.87}}</ref><ref>[http://blog.delver.com/index.php/2008/07/31/taxonomy-of-social-search-approaches/ A Taxonomy of Social Search Approaches], Delver company blog, Jul 31, 2008</ref>\u000a<ref>[http://www.springerlink.com/content/e12233858017h042/ Longo, Luca et al., Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time. Transactions on Computational Collective Intelligence II, Lecture Notes in Computer Science, Volume 6450. ISBN 978-3-642-17154-3. Springer Berlin Heidelberg, 2010, p. 46 ] {{doi|10.1007/978-3-642-17155-0_3}}</ref><ref>[http://www.springerlink.com/content/gg3p6177pw6h10j8/ Longo, Luca et al., Information Foraging Theory as a Form of Collective Intelligence for Social Search. Computational Collective Intelligence. Semantic  Web, Social Networks and  Multiagent Systems Lecture Notes in Computer Science, 2009, Volume 5796/2009, 63-74] {{doi|10.1007/978-3-642-04441-0_5}}</ref>\u000a\u000aThe search experience takes into account varying sources of metadata, such as collaborative discovery of web pages, tags, social ranking, commenting on bookmarks, news, images, videos, knowledge sharing, podcasts and other web pages. Example forms of user input include social bookmarking or direct interaction with the search results such as promoting or demoting results the user feels are more or less relevant to their query.<ref>[http://venturebeat.com/2008/01/31/googles-marissa-mayer-social-search-is-the-future Google\u2019s Marissa Mayer: Social search is the future], VentureBeat, Jan 31, 2008</ref>\u000a\u000a==History==\u000a\u000aThe term social search began to emerge between 2004 and 2005. The concept of social ranking can be considered to derive from Google's [[PageRank]] algorithm,{{citation needed|date=March 2009}} which assigns importance to web pages based on analysis of the link structure of the web, because PageRank is relying on the collective judgment of webmasters linking to other content on the web. Links, in essence, are positive votes by the webmaster community for their favorite sites.\u000a\u000aIn 2008, there were a few startup companies that focused on ranking search results according to one's [[social graph]] on [[social networks]].<ref>[http://online.wsj.com/public/article/SB121063460767286631.html New Sites Make It Easier To Spy on Your Friends], Wall Street Journal, May 13. 2008</ref><ref>[http://mashable.com/2007/08/27/social-search/ Social Search Guide: 40+ Social Search Engines], Mashable, Aug 27. 2007</ref> Companies in the social search space include  Evam-SOCOTO Wajam, Slangwho, [[Sproose]], [[Mahalo.com|Mahalo]], [[Jumper 2.0]], [[Qitera]], [[Scour Inc.|Scour]], [[Wink Technologies|Wink]], [[Eurekster]], [[Baynote]], [[Delver (Social Search)|Delver]], and OneRiot. Former efforts include [[Wikia Search]]. In 2008, a story on ''[[TechCrunch]]'' showed [[Google]] potentially adding in a voting mechanism to search results similar to [[Digg]]'s methodology.<ref>[http://www.techcrunch.com/2008/07/16/is-this-the-future-of-search/ Is This The Future Of Search?], TechCrunch, July 16, 2008</ref> This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles.\u000a\u000aIn October 2009, [[Google]] rolled out its "Social Search" feature; after a time in [[beta]], the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010 [[Bing]] and [[Google]] were already taking into account re-tweets and Likes when providing search results.<ref>{{cite web|url = http://www.marchpr.com/blog/2013/04/seo-social-media-search/|title = Retweets and Likes influencing search results|date = 10 April 2013|accessdate = 1 December 2014| publisher = March Communications}}</ref> However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released "Search plus Your World", a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from [[Google+]] profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which [[Google+]] still isn't wildly adopted or has much usage among many users.<ref name="HubSpot">{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = Facebook Announces New Social Search Feature|date = 15 January 2013|accessdate = 1 December 2014| publisher = HubSpot}}</ref>\u000a\u000aIn January 2013, [[Facebook]] announced a new search engine called [[Graph Search]] still in the beta stages. The goal in mind was to accomplish what [[Google]] failed at, skipping the results that are popular to the internet, in favor of the results that are popular within your social circle. Unlike [[Google]], [[Facebook]]'s Graph search differed in two large areas, first, people use Facebook frequently. This allows [[Facebook]] to use all it's user generated content that is uploaded everyday to improve the [[Facebook]] search experience.<ref name="HubSpot"/> Secondly, [[Facebook]] did not incorporate Google into Facebook search, instead Graph Search is powered by [[Bing]].This allows [[Bing]] results to show when Facebook's Graph Search can't find a match.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search powered by Bing|date = 15 January 2013|accessdate = 1 December 2014| publisher = Forbes}}</ref>\u000a\u000a==Concerns==\u000a\u000aWhen Google announced "Search plus Your World" the reaction was mixed among tech companies. The company was subsequently criticized by [[Twitter]] for the perceived potential impact of "Search plus Your World" upon web publishers, describing the feature's release to the public as a "bad day for the web", while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content.<ref>{{cite web|url = http://www.cnbc.com/id/100381337#.|title = Twitter unhappy about Google's social search changes|date = 11 January 2012|accessdate = 11 January 2012|publisher = BBC News}}</ref> The criticism from [[Twitter]] wasn't without merits however, by [[Google]] integrating [[Google+]], they were essentially forcing people to switch from a social network on to theirs in order to improve search results. One famous example occurred when [[Google]] showed a link to Mark Zuckerberg's dormant [[Google+]] account rather than the active [[Facebook]] profile.<ref name="Google pushing Google">{{cite web|url = http://searchengineland.com/googles-knowledge-graph-finally-shows-social-networks-named-google-209171.|title = Google pushing Google+|date = 18 November 2014|accessdate = 1 December 2012|publisher = Third Door Media}}</ref> Further more this affected businesses in which if they do not have time to leverage all other social media sites, they knew they should use [[Google+]] to maximize their efforts since the data shows it impacts rankings more than [[Twitter]] and [[Facebook]].<ref>{{cite web|url = http://www.quicksprout.com/2014/01/31/how-social-signals-impact-search-engine-rankings/#.|title = Google+ impacts ranking more|date = 31 January 2014|accessdate = 1 December 2014|publisher = Quick Sprout}}</ref> in November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to [[Facebook]], [[Twitter]], and other social media sites.<ref name="Google pushing Google"/>\u000a\u000a[[Google]] was not the only one that garnished concerns over social search. After the introduction of [[Graph Search]] by [[Facebook]] many pointed out how [[Graph Search]] showed private information that isn't in web search.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search results|date = 1 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref> Information that was once obscure is now easier to dig up, which is why Facebook urges users to monitor post and pictures users are tagged in and filter and filter any content that users would not want to make public.<ref>{{cite web|url = http://www.forbes.com/sites/larrymagid/2013/01/15/facebooks-new-social-search-what-it-is-and-how-it-affects-your-privacy/|title = Graph Search Privacy Concerns|date = 15 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref>\u000a\u000aThis in large points towards the biggest concern toward social search which is that social media networks don't have a vested interest in working with search engines. [[LinkedIn]] for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even [[Microsoft]] started working with [[Twitter]] in order to integrate some tweets into [[Bing]]'s search results in November 2013. Yet [[Twitter]] has its own search engine which points out how much value their data has and why they'd like to keep it in house.<ref>{{cite web|url = http://venturebeat.com/2014/06/30/microsoft-and-twitter-make-bing-a-better-social-search-engine/|title = Bing's twitter integration|date = 30 June 2014|accessdate = 1 December 2014|publisher = Venture Beat}}</ref> In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information.<ref>{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = User data will never be competently public|date = 15 January 2013|accessdate = 1 December 2014|publisher = HubSpot}}</ref>\u000a\u000a==Social discovery==\u000aSocial discovery is the use of social preferences and personal information to predict what content will be desirable to the user.<ref name="Bailyn2012">{{cite book|last=Bailyn|first=Evan|title=Outsmarting Social Media: Profiting in the Age of Friendship Marketing|url=http://books.google.com/books?id=M97RiODwKHEC&pg=PT51|accessdate=20 January 2014|date=2012-04-12|publisher=Que Publishing|isbn=9780132861403|pages=51\u2013}}</ref> Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling.<ref>{{cite web|last=Burke|first=Amy|url=http://mashable.com/2013/07/08/social-discovery-apps/|publisher=Mashable|title=Are Social Discovery Apps Too Creepy?}}</ref>  The discovery of new people is often in real-time, enabled by [[mobile apps]]. However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media.<ref>{{cite web|last=Cubie|first=Gregor|url=http://www.thedrum.com/news/2013/10/02/social-discovery-sites-influence-retail-expanding-rakutens-playcom-numbers-find|publisher=The Drum|title=Social Discovery sites' influence on retail expanding}}</ref>  An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music <ref>{{cite web|last=Constine|first=Josh|url=http://techcrunch.com/2013/09/10/bitcovery/|publisher=TechCrunch|title=Bitcovery Brings A Desperately Needed Social Discovery Layer To The iTunes Store}}</ref> Social discovery is at the basis of [[Facebook]]'s profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal.<ref name="Bailyn2012"/>\u000a\u000a==Developments==\u000a\u000a[[Google]] may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with [[Web 3.0]] and [[web semantics]]. The importance of social media lies within how Semantic search works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things.<ref>{{cite web|url = http://www.socialmediatoday.com/content/google-semantic-search|title = Google Semantic Search|date = 28 February 2014|accessdate = 1 December 2014|publisher = Social Media Today}}</ref>\u000a\u000aHowever this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users.\u000a\u000aOne development that seeks to redefine search is the combination of [[distributed search]] with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like Peer to Peer networks in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the from of thousands of idle desktops and extensive residential broadband access.<ref>{{cite web|url = http://www2009.eprints.org/242/|title = Towards Distributed Social Search Engines|accessdate = 1 December 2014|publisher = EPrints}}</ref>\u000a\u000a== See also ==\u000a* [[Collaborative filtering]]\u000a* [[Enterprise bookmarking]]\u000a* [[Human search engine]]\u000a* [[Relevance feedback]]\u000a* [[Social software]]\u000a\u000a== References ==\u000a{{reflist}}\u000a{{Internet search}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Social search| ]]\u000a[[Category:Social software|Search]]
p102
asI187
(lp103
VWeb indexing
p104
aV{{no footnotes|date=December 2014}}\u000a'''Web indexing''' (or '''Internet indexing''') refers to various methods for indexing the contents of a [[website]] or of the [[Internet]] as a whole. Individual websites or [[intranet]]s may use a [[back-of-the-book index]], while [[search engines]] usually use keywords and [[Metadata (computing)|metadata]] to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of [[periodical]]s that have articles online, web indexing is also becoming important for periodical websites.\u000a\u000aBack-of-the-book-style web indexes may be called "web site A-Z indexes". The implication with "A-Z" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a [[Taxonomy (general)|taxonomy]]) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.\u000a\u000aMetadata web indexing involves assigning keywords or phrases to web pages or web sites within a [[metadata tag]] (or "meta-tag") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by [[search engine indexing]].\u000a\u000a==See also==\u000a* [[Information architecture]]\u000a* [[Search engine indexing]]\u000a* [[Search engine optimization]]\u000a* [[Site map]]\u000a* [[Web navigation]]\u000a* [[Web search engine]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a<!--========================({{No More Links}})============================\u000a    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |\u000a    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |\u000a    |                                                                     |\u000a    |           Excessive or inappropriate links WILL BE DELETED.         |\u000a    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\u000a    |                                                                     |\u000a    | If there are already plentiful links, please propose additions or   |\u000a    | replacements on this article's discussion page, or submit your link |\u000a    | to the relevant category at the Open Directory Project (dmoz.org)   |\u000a    | and link back to that category using the {{dmoz}} template.         |\u000a    =======================({{No More Links}})=============================-->\u000a*TheAlphaWeb [http://www.eprodoffice.com/shhh/abcdefghijklmnopqrstuvwxyz.htm ''An example of an Internet A-Z'']\u000a*Glenda Browne and Jonathan Jermey, [http://www.webindexing.biz/ ''Website indexing: enhancing access to information within websites, 2nd Edition''], ISBN 1-875145-56-7\u000a*James Lamb, [http://www.jalamb.com/publications.html ''Website Indexes: visitors to content in two clicks, or website indexing with XRefHT32 freeware''], ISBN 978-1-4116-7937-5\u000a*[http://www.infotoday.com/books/books/BeyondBookIndex.shtml ''Beyond Book Indexing: How to Get Started in Web Indexing, Embedded Indexing, and Other Computer-Based Media''], edited by Marilyn Rowland and Diane Brenner, American Society of Indexers, Info Today, Inc, NJ, 2000, ISBN 1-57387-081-1\u000a* {{Cite web\u000a  |url=http://www.boxesandarrows.com/view/improving_usability_with_a_website_index\u000a  |title=Improving Usability with a Website Index\u000a  |archiveurl=http://www.webcitation.org/5vJwZDVkj\u000a  |archivedate=2010-12-28\u000a  |accessdate=2010-12-28\u000a  |first=Fred\u000a  |last=Leise\u000a  |date=2002-07-15\u000a}}\u000a* [http://www.web-indexing.org/article-brown.htm Why Create an Index?]\u000a* [http://www.theeasybee.com/directory/web-content-extraction Open Social Web 3.0 Directory] \u2013 Compare and review web indexing programs\u000a{{Internet search}}\u000a[[Category:Searching]]\u000a[[Category:Indexing]]\u000a\u000a\u000a{{internet-stub}}
p105
asI190
(lp106
VBayesian search theory
p107
aV'''Bayesian search theory''' is the application of [[Bayesian statistics]] to the search for lost objects. It has been used several times to find lost sea vessels, for example the [[USS Scorpion (SSN-589)|USS ''Scorpion'']]. It also played a key role in the recovery of the flight recorders in the [[Air France Flight 447]] disaster of 2009. \u000a\u000a==Procedure==\u000a\u000aThe usual procedure is as follows:\u000a\u000a# Formulate as many reasonable hypotheses as possible about what may have happened to the object. \u000a# For each hypothesis, construct a [[probability density function]] for the location of the object.\u000a# Construct a function giving the probability of actually finding an object in location&nbsp;X when searching there if it really is in location&nbsp;X. In an ocean search, this is usually a function of water depth \u2014 in shallow water chances of finding an object are good if the search is in the right place. In deep water chances are reduced.\u000a# Combine the above information coherently to produce an overall probability density map. (Usually this simply means multiplying the two functions together.) This gives the probability of finding the object by looking in location&nbsp;X, for all possible locations&nbsp;X. (This can be visualized as a [[contour map]] of probability.)\u000a# Construct a search path which starts at the point of highest probability and 'scans' over high probability areas, then intermediate probabilities, and finally low probability areas.\u000a# Revise all the probabilities continuously during the search. For example, if the hypotheses for location&nbsp;X imply the likely disintegration of the object and the search at location&nbsp;X has yielded no fragments, then the probability that the object is somewhere around there is greatly reduced (though not usually to zero) while the probabilities of its being at other locations is correspondingly increased. The revision process is done by applying [[Bayes' theorem]].\u000a\u000aIn other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.\u000a\u000aThe advantages of the Bayesian method are that all information available is used coherently (i.e., in a "leak-proof" manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, "there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15&nbsp;days" or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.\u000a\u000aApart from the [[USS Scorpion (SSN-589)|USS ''Scorpion'']], other vessels located by Bayesian search theory include the [[MV Derbyshire|MV&nbsp;''Derbyshire'']], the largest British vessel ever lost at sea, and the [[SS Central America|SS&nbsp;''Central America'']]. It also proved successful in the search for a lost [[hydrogen bomb]] following the [[1966 Palomares B-52 crash]] in Spain, and the recovery in the Atlantic Ocean of the crashed [[Air France Flight 447]].\u000a\u000aBayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the [[United States Coast Guard]] for [[search and rescue]]. This program was later adapted for inland search by adding terrain and ground cover factors for use by the [[United States Air Force]] and [[Civil Air Patrol]].\u000a\u000a==Mathematics==\u000a\u000aSuppose a grid square has a probability ''p'' of containing the wreck and that the probability of successfully detecting the wreck if it is there is ''q''. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by\u000a\u000a: <math>  p' = \u005cfrac{p(1-q)}{(1-p)+p(1-q)} = p \u005cfrac{1-q}{1-pq} < p.</math>\u000aFor each other grid square, if its prior probability is ''r'', its posterior probability is given by\u000a\u000a: <math> r' = r \u005cfrac{1}{1- pq} > r. </math>\u000a\u000a\u000a==Optimal Distribution of Search Effort==\u000a\u000aThe classical book on this subject, based on probabilistic information, by [[Lawrence D. Stone]], won the 1975 [[Frederick W. Lanchester Prize]] by the [[Operations Research Society of America]].\u000a\u000a\u000a\u000a<!-- Material on USS Scorpion, moved from Bayesian inference\u000a\u000aIn May 1968, the [[U.S. Navy]]'s [[nuclear submarine]] [[USS Scorpion (SSN-589)|USS ''Scorpion'' (SSN-589)]] failed to arrive as expected at her home port of [[Norfolk, Virginia]]. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the [[East Coast of the United States|Eastern Seaboard]],  but an extensive search there failed to discover the remains of the ''Scorpion''.\u000a\u000aThen, a Navy deep-water expert, [[John Craven USN|John P. Craven]], suggested that the USS ''Scorpion'' had sunk elsewhere. Craven organised a search southwest of the [[Azores]] based on a controversial approximate triangulation by [[hydrophone]]s. He was allocated only a single ship, the [[USNS Mizar (AGOR-11)|''Mizar'']], and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the ''Scorpion''.\u000a\u000aThe sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.\u000a\u000a\u000a==Optimal Distribution of Search Effort==\u000a\u000aThe classical book on this subject by [[Lawrence D. Stone]] won the 1975 [[Lancaster Prize]] by the American Operations Research Society.\u000a\u000a-->\u000a\u000a==See also==\u000a* [[Bayesian inference]]\u000a* [[Search games]]\u000a\u000a== References ==\u000a* [[Stone, Lawrence D.]], ''The Theory of Optimal Search'', published by the [[Operations Research Society of America]], 1975\u000a* [[Stone, Lawrence D.]], In search of Air France Flight 447. Institute of Operations Research and the Management Sciences, 2011\u000a* Iida, Koji., '' Studies on the Optimal Search Plan'', Vol.&nbsp;70, Lecture Notes in Statistics, Springer-Verlag, 1992.\u000a* De Groot, Morris H., ''Optimal Statistical Decisions'', Wiley Classics Library, 2004.\u000a* Richardson, Henry R; and Stone, Lawrence D. Operations Analysis during the underwater search for ''Scorpion''. ''Naval Research Logistics Quarterly'', June&nbsp;1971, Vol.&nbsp;18, Number&nbsp;2. Office of Naval Research.\u000a* Stone, Lawrence D. Search for the SS ''Central America'': Mathematical Treasure Hunting. Technical Report, Metron Inc. Reston, Virginia.\u000a* [[Bernard Koopman|Koopman, B.O.]] ''Search and Screening'', Operations Research Evaluation Group Report 56, Center for Naval Analyses, Alexandria, Virginia. 1946.\u000a* Richardson, Henry R; and Discenza, J.H. The United States Coast Guard computer-assisted search planning system (CASP). ''Naval Research Logistics Quarterly''. Vol.&nbsp;27 number&nbsp;4. pp.&nbsp;659\u2013680. 1980.\u000a* [[Ross, Sheldon M.]], ''An Introduction to Stochastic Dynamic Programming'', Academic Press. 1983.\u000a\u000a[[Category:Bayesian statistics|Search theory]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]\u000a[[Category:Operations research]]
p108
asI63
(lp109
VOverlap coefficient
p110
aVThe '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the intersection divided by the smaller of the size of the two sets:\u000a\u000a:<math>\u005cmathrm{overlap}(X,Y) = \u005cfrac{| X \u005ccap Y | }{\u005cmin(|X|,|Y|)}</math>\u000a\u000aIf set ''X'' is a subset of ''Y'' or the converse then the overlap coefficient is equal to one.\u000a\u000a== External links==\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/OverlapMetric.scala Overlap] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p111
asI67
(lp112
VFull text search
p113
aV{{Multiple issues|\u000a{{refimprove|date=August 2012}}\u000a{{cleanup|date=September 2009}}\u000a}}\u000a\u000aIn [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).\u000a\u000aIn a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>\u000a\u000a==Indexing==\u000aWhen dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "serial scanning." This is what some tools, such as [[grep]], do when searching.\u000a\u000aHowever, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{Dead link |date=October 2012}}</ref>\u000a\u000aThe indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive."\u000a\u000a==The precision vs. recall tradeoff==\u000a[[Image:Full-text-search-results.png|150px|thumb|right|This diagram represents a low-precision, low-recall search as described in the text.]]\u000aRecall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned divided by all relevant results. Precision is the number of relevant results returned divided by the total number of results returned.\u000a\u000aThe diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only one relevant result of three possible relevant results was returned, so the recall is a very low ratio of 1/3 or 33%. The precision for the example is a very low 1/4 or 25%, since only one of the four results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>\u000a\u000aDue to the ambiguities of [[natural language]], full text search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D.L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}</ref>\u000a\u000a{{See also|Precision and recall}}\u000a\u000a==False-positive problem==\u000a\u000aFree text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).\u000a\u000aClustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "football", clustering can be used to categorize the document/data universe into "American football", "corporate football", etc. Depending on the occurrences of words relevant to the categories, search terms a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain.{{clarify|date=January 2012}}\u000a\u000a==Performance improvements==\u000a\u000aThe deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.\u000a\u000a===Improved querying tools===\u000a\u000a*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.\u000a* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."\u000a* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, "encyclopedia" AND "online" NOT "Encarta") can dramatically increase the precision of a free text search. The AND operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The NOT operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the OR operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, "encyclopedia" AND "online" OR "Internet" NOT "Encarta". This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>\u000a* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as "Wikipedia, the free encyclopedia."\u000a* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.\u000a* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.\u000a* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for "Wikipedia" WITHIN2 "free" would retrieve only those documents in which the words "Wikipedia" and "free" occur within two words of each other.\u000a* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.\u000a* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)\u000a* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example using the asterisk in a search query "s*n" will find "sin", "son", "sun", etc. in a text.\u000a\u000a===Improved search algorithms===\u000aThe [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.\u000a\u000a==Software==\u000a\u000aThe following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.\u000a\u000a=== Free and open source software ===\u000a<!--\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a* [[Apache Solr]]\u000a* [[BaseX]]\u000a* [[DataparkSearch]]\u000a* [[ElasticSearch]]\u000a* [[Ht-//Dig|ht://Dig]]\u000a* [[KinoSearch]]\u000a* [[Lemur Project|Lemur/Indri]]\u000a* [[Lucene]]\u000a* [[mnoGoSearch]]\u000a* [[Searchdaimon]]\u000a* [[Sphinx (search engine)|Sphinx]]\u000a* [[Swish-e]]\u000a* [[Xapian]]\u000a\u000a=== Proprietary software ===\u000a<!--\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a* [[Attivio]]\u000a* [[Autonomy Corporation]]\u000a* [[Bar Ilan Responsa Project]]\u000a* [[Brainware]]\u000a* [[BRS/Search]] \u000a* [[Clusterpoint|Clusterpoint Server]]\u000a* [[Concept Searching Limited]]\u000a* [[Dieselpoint]]\u000a* [[dtSearch]]\u000a* [[Endeca]]\u000a* [[Exalead]]\u000a* [[Fast Search & Transfer]]\u000a* [[Inktomi (company)|Inktomi]]\u000a* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)\u000a* [[Lookeen]]\u000a* [[Lucid Imagination]]\u000a* [[MarkLogic]]\u000a* [[Swiftype]]\u000a* [[Thunderstone Software LLC.]]\u000a* [[Vivísimo]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a==See also==\u000a*[[Pattern matching]] and [[string matching]]\u000a*[[Compound term processing]]\u000a*[[Controlled vocabulary]]\u000a*[[Enterprise search]]\u000a*[[Information Extraction]]\u000a*[[Information retrieval]]\u000a*[[Faceted search]]\u000a*[[Full text database]]\u000a*[[List of enterprise search vendors]]\u000a*[[Search engine]]\u000a*[[WebCrawler]], first FTS engine\u000a*[[Search engine indexing]] - how search engines generate indices to support full text searching\u000a*[[SQL Server Full Text Search|SQL Server Full Text Search (implementation of)]]\u000a\u000a{{DEFAULTSORT:Full Text Search}}\u000a[[Category:Searching]]\u000a[[Category:Text editor features]]\u000a[[Category:Information retrieval]]
p114
asI197
(lp115
VDaffodil (software)
p116
aV{{Orphan|date=February 2009}}\u000aThe '''Daffodil''' system is a virtual [[digital library]] system for strategic support of users during the information search process. It implements mainly high-level search functions, so-called stratagems, which provide functionality beyond today's digital libraries.  The Daffodil system was developed as a research project starting as a collaboration between the University of Dortmund (Germany) and the IZ Bonn (Germany), funded by the [[Deutsche Forschungsgemeinschaft]] (DFG) (2000\u20132004). \u000a\u000aCurrently the Daffodil framework is extended to become an experimental evaluation platform for digital library evaluation at the [[University of Duisburg-Essen]].\u000a\u000a== External links ==\u000a* [http://www.dlib.org/dlib/june04/kriewel/06kriewel.html A description of functions and services]\u000a* [http://www.is.informatik.uni-duisburg.de/projects/daffodil/index.html Project description]\u000a\u000a[[Category:Library science]]\u000a[[Category:Searching]]\u000a\u000a\u000a{{Compu-library-stub}}
p117
asI198
(lp118
VSearch by sound
p119
aVSearching by sound for now has limited uses. There are a handful of applications, specifically for mobile devises that utilizes searching by sound. [[Shazam (service)]], [[Soundhound]], Midomi, and others has seen considerable success by using a simple algorithm to match an acoustic fingerprint to a song in a library. These applications takes a sample clip of a song, or a user generated melody and checks a music library to see where the clip matches with the song. From there, song information will be pulled up and displayed to the user. \u000a\u000aThese kind of applications is mainly used for finding a song that the user does not already know. \u000a\u000aSearching by sound is not limited so just identifying [[songs]], but also for identifying [[melodies]], [[Music|tunes]] or [[advertisements]], [[sound library management]] and [[video files]].\u000a\u000a==Acoustic Fingerprinting==\u000aThe way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. A microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. Using the same method of fingerprinting sounds, when Shazam picks up a sound clip, it will generate a signature for that clip. Then it\u2019s simple pattern matching from there using an extensive audio music database. \u000a\u000aThe practice of using [[acoustic fingerprints]] is not limited to just music however, but other areas of the entertainment business as well. Shazam also can identify television shows with the same technique of acoustic fingerprinting. Of course, this method of breaking down a sound sample into a unique signature is useless unless there is an extensive database of music with keys to match with the samples. Shazam has over 11 million songs in its database. <ref> http://www.slate.com/articles/technology/technology/2009/10/that_tune_named.html </ref>\u000a\u000aOther services such as Midomi and Soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound. \u000a\u000a==Spectogram==\u000aGenerating a signature from the song is essential for searching by sound, and can be tricky. However, the way certain applications such as Shazam found a way around this issue by creating a spectrogram. \u000a\u000aAny piece of music can be translated to a time frequency graph called a spectrogram. For each song in its database, each song is basically a graph that plots the three dimensions of music, frequency vs amplitude (intensity) vs time. The algorithm then picks out the points which peaks in the graph, labeled as \u201chigher energy content\u201d. In practice, this seems to work out to about three points per song. <ref> http://www.soyoucode.com/2011/how-does-shazam-recognize-song </ref>\u000a\u000aThis is how a song can be identified with just two or three notes. This greatly reduces the impact that [[background noise]] has on searching by sound. The key values taken away from this would be frequency in hertz and time in seconds. Shazam builds their fingerprint catalog out as a hash table, where the key is the frequency. They do not just mark a single point in the spectrogram, rather they mark a pair of points: the \u201cpeak intensity\u201d plus a second \u201canchor point\u201d. <ref> Li-Chun Wang, Avery. "An Industrial-Strength Audio Search Algorithm." Columbia University. Web. 1 Dec. 2014. <http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf>. </ref> So their key is not just a single frequency, it is a hash of the frequencies of both points.  This leads to less hash collisions which in turn speeds up catalog searching by several orders of magnitude by allowing them to take greater advantage of the table\u2019s constant (O(1)) look-up time. <ref> "How Shazam Works." Free Wont. Web. 1 Dec. 2014. <http://laplacian.wordpress.com/2009/01/10/how-shazam-works/>. </ref>\u000a\u000aThis method of acoustic fingerprinting allows applications such as Shazam to have the ability to differentiate between two closely related covers, as well as not having to account for popularity of a certain song. \u000a\u000a==Query by Humming==\u000aMidomi and Soundhound both utilize Query by Humming, or QbH. This is a branch off of acoustic fingerprints, but is still a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query. \u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a\u000a\u000a[[Category:Searching]]
p120
asI33
(lp121
VCoveo
p122
aV{{Infobox company\u000a| name = Coveo Solutions Inc.\u000a| logo = [[Image:Coveo logo.png|120px]]\u000a| type = Private\u000a| slogan = \u000a| foundation =  2004\u000a| location_city = [[Quebec City]], [[Canada]]\u000a| key_people = Louis Têtu, Chairman and CEO <br />Laurent Simoneau, President and CTO\u000a| num_employees =\u000a| industry = [[Enterprise search]]\u000a| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce\u000a| homepage = http://www.coveo.com\u000a}}\u000a\u000a'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for Salesforce.com, Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.\u000a\u000a==History==\u000aCoveo Solutions Inc. was founded in 2004 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>\u000a\u000a==Products==\u000a'''Coveo Search & Relevance Platform'''\u000a\u000aCoveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.\u000a\u000a'''Coveo for Sitecore'''\u000a\u000aCoveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore\u2019s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.\u000a\u000a'''Coveo for Salesforce'''\u000a\u000aCoveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.\u000a\u000a==Customers==\u000aCoveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://www.coveo.com/ Coveo.com]\u000a\u000a[[Category:Companies based in Quebec City]]\u000a[[Category:Information retrieval]]\u000a[[Category:BlackBerry development software]]
p123
asI73
(lp124
VCognitive models of information retrieval
p125
aV{{Orphan|date=September 2012}}\u000a\u000a'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.\u000a\u000a==Berrypicking==\u000aOne way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." http://www.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>\u000a\u000aBates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.\u000a\u000a==Exploratory Search==\u000aResearchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.\u000a\u000a==Natural language searching==\u000a\u000aAnother way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Cognitive modeling]]
p126
asI74
(lp127
VMAREC
p128
aV{{other uses}}\u000aThe '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.<ref>Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland</ref><ref>Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition</ref> It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.\u000a\u000aMAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.<ref>Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.</ref> The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.\u000a\u000aIn MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.<ref>European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)</ref>\u000a\u000aMAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics \u2013 news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.<ref>Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)</ref> Since the patent document refers to the same \u201cinvention\u201d or \u201cconcept of idea\u201d the text is a translation of the invention, but it does not have to be a direct translation of the text itself \u2013 text parts could have been removed or added for clarification reasons.\u000a\u000aThe 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.\u000a\u000a== Use Cases ==\u000a* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.ir-facility.org/prototypes/marec User guide and statistics]\u000a* [http://ir-facility.org Information Retrieval Facility]\u000a\u000a[[Category:Corpora]]\u000a[[Category:Information retrieval]]\u000a[[Category:Machine translation]]\u000a[[Category:Natural language processing]]\u000a[[Category:XML]]
p129
asI203
(lp130
VString metric
p131
aV{{redirect|String distance|the distance between strings and the fingerboard in musical instruments|Action (music)}}\u000a\u000aIn [[mathematics]] and [[computer science]], a '''string metric''' (also known as a '''string similarity metric''' or '''string distance function''') is a [[metric (mathematics)|metric]] that measures [[distance]] ("inverse similarity") between two [[string (computer science)|text strings]] for [[approximate string  matching]] or comparison and in [[approximate string  matching|fuzzy string searching]]. Necessary requirement for a string ''metric'' (e.g. in contrast to [[string matching]]) is fulfillment of the [[triangle inequality]]. For example the strings "Sam" and "Samuel" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.\u000a\u000aThe most widely known string metric is a rudimentary one called the [[Levenshtein distance|Levenshtein Distance]] (also known as Edit Distance).  It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as [[Levenshtein distance]] have expanded to include phonetic, [[token (parser)|token]], grammatical and character-based methods of statistical comparisons.\u000a\u000aA widespread example of a string metric is [[DNA]] [[sequence analysis]] and RNA analysis, which are performed by optimized string metrics to identify matching sequences.\u000a\u000aString metrics are used heavily in [[information integration]] and are currently used in areas including [[Data analysis techniques for fraud detection|fraud detection]], [[fingerprint analysis]], [[plagiarism detection]], [[ontology merging]], [[DNA analysis]], RNA analysis, [[image analysis]], evidence-based machine learning, [[database]] [[data deduplication]], [[data mining]], Web interfaces, e.g. [[Ajax (programming)|Ajax]]-style suggestions as you type, [[data integration]], and semantic [[knowledge integration]].\u000a\u000a==List of string metrics==\u000a\u000a<!-- This can be a separate article, someday. -->\u000a* [[Sørensen\u2013Dice coefficient]]\u000a* [[Hamming distance]]\u000a* [[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a* [[Block distance]] or [[L1 distance]] or [[City block distance]]\u000a* [[Simple matching coefficient]] (SMC)\u000a* [[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a* [[Most frequent k characters]]\u000a* [[Tversky index]]\u000a* [[Overlap coefficient]]\u000a* [[Variational distance]]\u000a* [[Hellinger distance]] or [[Bhattacharyya distance]]\u000a* [[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a* [[Skew divergence]]\u000a* [[Confusion probability]]\u000a* [[Kendall_tau_distance|Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a* [[Fellegi and Sunters metric]] (SFS)\u000a* [[Maximal matches]]\u000a* [[Lee distance]]\u000a\u000a==Selected string measures examples==\u000a\u000a{| class="wikitable"\u000a|-\u000a! Name\u000a! Example\u000a|-\u000a|[[Hamming distance]]\u000a| "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.\u000a|-\u000a|[[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a| \u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a<!--|-\u000a|[[Simple matching coefficient]] (SMC)\u000a|-->\u000a<!--|-\u000a|-\u000a|[[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a|-->\u000a|-\u000a|[[Most frequent k characters]]\u000a|MostFreqKeySimilarity('<span style="color:red;">r</span><span style="color:#0082ff">e</span>s<span style="color:#0082ff">e</span>a<span style="color:red;">r</span>ch', 's<span style="color:#0082ff">ee</span>king', 2) = 2\u000a<!--|-\u000a|[[Tversky index]]\u000a|-->\u000a<!--|-\u000a|[[Overlap coefficient]]\u000a|-->\u000a<!--|-\u000a|[[Variational distance]]\u000a|-->\u000a<!--|-\u000a|[[Hellinger distance]] or [[Bhattacharyya distance]]\u000a|-->\u000a<!--|-\u000a|[[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a|-->\u000a<!--|-\u000a|[[Skew divergence]]\u000a|-->\u000a<!--|-\u000a|[[Confusion probability]]\u000a|-->\u000a<!--|-\u000a|[[Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a|-->\u000a<!--|-\u000a|[[Fellegi and Sunters metric]] (SFS)\u000a|-->\u000a<!--|-\u000a|[[Maximal matches]]\u000a|-->\u000a|}\u000a\u000a==See also==\u000a* [[approximate string  matching]]\u000a* [[String matching]]\u000a* [http://www.speech.cs.cmu.edu/ Carnegie Mellon University open source library]\u000a* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms\u000a* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics\u000a\u000a==External links==\u000a*http://www.dcs.shef.ac.uk/~sam/stringmetrics.html {{Dead link|date=July 2011}} A fairly complete overview {{wayback|url=http://www.dcs.shef.ac.uk/~sam/stringmetrics.html#ukkonen}}\u000a\u000a{{DEFAULTSORT:String Metric}}\u000a[[Category:String similarity measures| ]]\u000a[[Category:Metrics]]\u000a\u000a[[de:Ähnlichkeitsanalyse]]
p132
asI76
(lp133
VRocchio algorithm
p134
aVThe '''Rocchio algorithm''' is based on a method of [[relevance feedback]] found in [[information retrieval]] systems which stemmed from the [[SMART Information Retrieval System]] around the year 1970. Like many other retrieval systems, the Rocchio feedback approach was developed using the [[Vector Space Model]].  The [[algorithm]] is based on the assumption that most users have a general conception of which documents should be denoted as  [[Relevance (information retrieval)|relevant]] or non-relevant.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 181. Cambridge University Press, 2009.</ref>  Therefore, the user's search query is revised to include an arbitrary percentage of  relevant and non-relevant documents as a means of increasing the [[search engine]]'s [[Information_retrieval#Recall|recall]], and possibly the precision as well.  The number of  relevant and non-relevant documents allowed to enter a [[Information retrieval|query]] is dictated by the weights of the a, b, c variables listed below in the [[Rocchio_Classification#Algorithm|Algorithm section]].<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 292. Cambridge University Press, 2009.</ref>\u000a\u000a==Algorithm==\u000aThe [[Formula (mathematical logic)|formula]] and variable definitions for Rocchio relevance feedback is as follows:<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 182. Cambridge University Press, 2009.</ref>\u000a\u000a<math> \u005coverrightarrow{Q_m} = \u005cbigl(a \u005ccdot \u005coverrightarrow{Q_o}\u005cbigr) + \u005cbiggl(b \u005ccdot {\u005ctfrac{1}{|D_r|}} \u005ccdot \u005csum_{\u005coverrightarrow{D_j} \u005cin D_r} \u005coverrightarrow{D_j}\u005cbiggr)\u000a- \u005cbiggl(c \u005ccdot {\u005ctfrac{1}{|D_{nr}|}} \u005ccdot \u005csum_{\u005coverrightarrow{D_k} \u005cin D_{nr}} \u005coverrightarrow{D_k}\u005cbiggr) </math>\u000a\u000a{| class="wikitable"\u000a|-\u000a! Variable\u000a! Value\u000a|-\u000a| <math> \u005coverrightarrow{Q_m} </math>\u000a| Modified Query Vector\u000a|-\u000a| <math> \u005coverrightarrow{Q_o} </math>\u000a| Original Query Vector\u000a|-\u000a| <math> \u005coverrightarrow{D_j} </math>\u000a| Related Document Vector\u000a|-\u000a| <math> \u005coverrightarrow{D_k} </math>\u000a| Non-Related Document Vector\u000a|-\u000a| <math> a </math>\u000a| Original Query Weight\u000a|-\u000a| <math> b </math>\u000a| Related Documents Weight\u000a|-\u000a| <math> c </math>\u000a| Non-Related Documents Weight\u000a|-\u000a| <math> D_r </math>\u000a| Set of Related Documents\u000a|-\u000a| <math> D_{nr} </math>\u000a| Set of Non-Related Documents\u000a|}\u000a[[Image:Rocchioclassgraph.jpg|thumb|right|250px|Rocchio Classification]]\u000a\u000aAs demonstrated in the Rocchio formula, the associated weights ('''a''', '''b''', '''c''') are responsible for shaping the modified [[vector space|vector]] in a direction closer, or farther away, from the original query, related documents, and non-related documents.  In particular, the values for '''b''' and '''c''' should be incremented or decremented proportionally to the set of documents classified by the user.  If the user decides that the modified query should not contain terms from either the original query, related documents, or non-related documents, then the corresponding weight ('''a''', '''b''', '''c''') value for the category should be set to 0.\u000a\u000aIn the later part of the algorithm, the variables '''Dr''', and '''Dnr''' are presented to be sets of [[Tuple|vectors]] containing the coordinates of related documents and non-related documents.  Though '''Dr''' and '''Dnr''' are not  vectors themselves, <math> \u005coverrightarrow{Dj} </math> and <math> \u005coverrightarrow{Dk} </math> are the vectors used to iterate through the two sets and form vector [[summation]]s. These summations will be multiplied against the [[Multiplicative inverse]] of their respective document set ('''Dr''', '''Dnr''') to complete the addition or subtraction of related or non-related documents.\u000a\u000aIn order to visualize the changes taking place on the modified vector, please refer to the image below.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 293. Cambridge University Press, 2009.</ref> As the weights are increased or decreased for a particular category of documents, the coordinates for the modified vector begin to move either closer, or farther away, from the [[centroid]] of the document collection. Thus if the weight is increased for related documents, then the modified vectors [[coordinate]]s will reflect being closer to the centroid of related documents.\u000a\u000a==Time complexity==\u000aThe [[time complexity]] for training and testing the algorithm are listed below and followed by the definition of each [[variable (mathematics)|variable]]. Note that when in testing phase, the time complexity can be reduced to that of calculating the [[euclidean distance]] between a class [[centroid]] and the respective document.  As shown by: <math>\u005cTheta(\u005cvert\u005cmathbb{C}\u005cvert M_{a})</math>.\u000a\u000aTraining = <math>\u005cTheta(\u005cvert\u005cmathbb{D}\u005cvert L_{ave}+\u005cvert\u005cmathbb{C}\u005cvert\u005cvert V\u005cvert)</math> <br>\u000aTesting = <math>\u005cTheta( L_{a}+\u005cvert\u005cmathbb{C}\u005cvert M_{a})= \u005cTheta(\u005cvert\u005cmathbb{C}\u005cvert M_{a})</math> <ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.</ref>\u000a\u000a{| class="wikitable"\u000a|-\u000a! Variable\u000a! Value\u000a|-\u000a| <math> \u005cmathbb{D} </math>\u000a| Labeled Document Set\u000a|-\u000a| <math> L_{ave} </math>\u000a| Average Tokens Per Document\u000a|-\u000a| <math> \u005cmathbb{C} </math>\u000a| Class Set\u000a|-\u000a| <math> V </math>\u000a| Vocabulary/Term Set\u000a|-\u000a| <math> L_{a} </math>\u000a| Number of Tokens in Document\u000a|-\u000a| <math> M_{a} </math>\u000a| Number of Types in Document\u000a|}\u000a\u000a==Usage==\u000aThough there are benefits to ranking documents as not-relevant, a [[relevant]] document ranking will result in more precise documents being made available to the user. Therefore, traditional values for the algorithm's weights ('''a''', '''b''', '''c''') in Rocchio Classification are typically around '''a = 1''', '''b = 0.8''', and ''' c = 0.1'''. Modern [[information retrieval]] systems have moved towards eliminating the non-related documents by setting '''c = 0''' and thus only accounting for related documents. Although not all [[Information retrieval|retrieval systems]] have eliminated the need for non-related documents, most have limited the effects on modified query by only accounting for strongest non-related documents in the '''Dnr''' set.\u000a\u000a==Limitations==\u000aThe Rocchio algorithm often fails to classify multimodal classes and relationships. For instance, the country of [[Burma]] was renamed to [[Myanmar]] in 1989. Therefore the two queries of "Burma" and "Myanmar" will appear much farther apart in the [[vector space model]], though they both contain similar origins.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.</ref>\u000a\u000a== See also ==\u000a*  [[Nearest centroid classifier]], aka Rocchio classifier\u000a\u000a==References==\u000a{{reflist}}\u000a* [http://nlp.stanford.edu/IR-book/pdf/09expand.pdf Relevance Feedback and Query Expansion]\u000a* [http://nlp.stanford.edu/IR-book/pdf/14vcat.pdf Vector Space Classification]\u000a* [http://cs.nyu.edu/courses/fall07/G22.2580-001/lec7.html Data Classification]\u000a\u000a[[Category:Information retrieval]]
p135
asI205
(lp136
VEuclidean distance
p137
aVIn [[mathematics]], the '''Euclidean distance''' or '''Euclidean metric''' is the "ordinary" [[distance]] between two points in [[Euclidean space]]. With this distance, Euclidean space becomes a [[metric space]]. The associated [[Norm (mathematics)|norm]] is called the '''[[Norm (mathematics)#Euclidean norm|Euclidean norm]].''' Older literature refers to the metric as '''Pythagorean metric'''.\u000a\u000a==Definition==\u000aThe '''Euclidean distance''' between points '''p''' and '''q''' is the length of the [[line segment]] connecting them (<math>\u005coverline{\u005cmathbf{p}\u005cmathbf{q}}</math>).\u000a\u000aIn [[Cartesian coordinates]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>,...,&nbsp;''p''<sub>''n''</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>,...,&nbsp;''q''<sub>''n''</sub>) are two points in [[Euclidean space|Euclidean ''n''-space]], then the distance (d) from '''p''' to '''q''', or from '''q''' to '''p''' is given by the [[Pythagorean theorem|Pythagorean formula]]:\u000a\u000a{{NumBlk|:|<math>\u005cbegin{align}\u005cmathrm{d}(\u005cmathbf{p},\u005cmathbf{q}) = \u005cmathrm{d}(\u005cmathbf{q},\u005cmathbf{p}) & = \u005csqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \u005ccdots + (q_n-p_n)^2} \u005c\u005c[8pt]\u000a& = \u005csqrt{\u005csum_{i=1}^n (q_i-p_i)^2}.\u005cend{align}</math>|{{EquationRef|1}}}}\u000a\u000aThe position of a point in a Euclidean ''n''-space is a [[Euclidean vector]]. So, '''p''' and '''q''' are Euclidean vectors, starting from the origin of the space, and their tips indicate two points. The '''[[Euclidean norm]]''', or '''Euclidean length''', or '''magnitude''' of a vector measures the length of the vector:\u000a:<math>\u005c|\u005cmathbf{p}\u005c| = \u005csqrt{p_1^2+p_2^2+\u005ccdots +p_n^2} = \u005csqrt{\u005cmathbf{p}\u005ccdot\u005cmathbf{p}}</math>\u000awhere the last equation involves the [[dot product]].\u000a\u000aA vector can be described as a directed line segment from the [[Origin (mathematics)|origin]] of the Euclidean space (vector tail), to a point in that space (vector tip). If we consider that its length is actually the distance from its tail to its tip, it becomes clear that the Euclidean norm of a vector is just a special case of Euclidean distance: the Euclidean distance between its tail and its tip.\u000a\u000aThe distance between points '''p''' and '''q''' may have a direction (e.g. from '''p''' to '''q'''), so it may be represented by another vector, given by\u000a\u000a:<math>\u005cmathbf{q} - \u005cmathbf{p} = (q_1-p_1, q_2-p_2, \u005ccdots, q_n-p_n)</math>\u000a\u000aIn a three-dimensional space (''n''=3), this is an arrow from '''p''' to '''q''', which can be also regarded as the position of '''q''' relative to '''p'''. It may be also called a [[displacement (vector)|displacement]] vector if '''p''' and '''q''' represent two positions of the same point at two successive instants of time.\u000a\u000aThe Euclidean distance between '''p''' and '''q''' is just the Euclidean length of this distance (or displacement) vector:\u000a{{NumBlk|:|<math>\u005c|\u005cmathbf{q} - \u005cmathbf{p}\u005c| = \u005csqrt{(\u005cmathbf{q}-\u005cmathbf{p})\u005ccdot(\u005cmathbf{q}-\u005cmathbf{p})}.</math>|{{EquationRef|2}}}}\u000a\u000awhich is equivalent to equation 1, and also to:\u000a\u000a:<math>\u005c|\u005cmathbf{q} - \u005cmathbf{p}\u005c| = \u005csqrt{\u005c|\u005cmathbf{p}\u005c|^2 + \u005c|\u005cmathbf{q}\u005c|^2 - 2\u005cmathbf{p}\u005ccdot\u005cmathbf{q}}.</math>\u000a\u000a===One dimension===\u000aIn one dimension, the distance between two points on the [[real line]] is the [[absolute value]] of their numerical difference.  Thus if ''x'' and ''y'' are two points on the real line, then the distance between them is given by:\u000a:<math>\u005csqrt{(x-y)^2} = |x-y|.</math>\u000a\u000aIn one dimension, there is a single homogeneous, translation-invariant [[Metric (mathematics)|metric]] (in other words, a distance that is induced by a [[Norm (mathematics)|norm]]), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.\u000a\u000a===Two dimensions===\u000aIn the [[Euclidean plane]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>) then the distance is given by\u000a\u000a:<math>\u005cmathrm{d}(\u005cmathbf{p},\u005cmathbf{q})=\u005csqrt{(p_1-q_1)^2 + (p_2-q_2)^2}.</math>\u000a\u000aThis is equivalent to the [[Pythagorean theorem]].\u000a\u000aAlternatively, it follows from ({{EquationRef|2}}) that if the [[polar coordinates]] of the point '''p''' are (''r''<sub>1</sub>,&nbsp;\u03b8<sub>1</sub>) and those of '''q''' are (''r''<sub>2</sub>,&nbsp;\u03b8<sub>2</sub>), then the distance between the points is\u000a\u000a:<math>\u005csqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \u005ccos(\u005ctheta_1 - \u005ctheta_2)}.</math>\u000a\u000a===Three dimensions===\u000aIn three-dimensional Euclidean space, the distance  is\u000a\u000a:<math>d(p, q) = \u005csqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+(p_3 - q_3)^2}.</math>\u000a\u000a===''n'' dimensions <!-- This is a lower-case italicized "n" for a reason. -->===\u000aIn general, for an ''n''-dimensional space, the distance is\u000a\u000a:<math>d(p, q) = \u005csqrt{(p_1- q_1)^2 + (p_2 - q_2)^2+\u005ccdots+(p_i - q_i)^2+\u005ccdots+(p_n - q_n)^2}.</math>\u000a\u000a===Squared Euclidean distance===\u000aThe standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes\u000a\u000a:<math>d^2(p, q) = (p_1 - q_1)^2 + (p_2 - q_2)^2+\u005ccdots+(p_i - q_i)^2+\u005ccdots+(p_n - q_n)^2.</math>\u000a\u000aSquared Euclidean Distance is not a metric as it does not satisfy the [[triangle inequality]], however it is frequently used in optimization problems in which distances only have to be compared.\u000a\u000aIt is also referred to as [[rational trigonometry#Quadrance|quadrance]] within the field of [[rational trigonometry]].\u000a\u000a==See also==\u000a*[[Chebyshev distance]] measures distance assuming only the most significant dimension is relevant.\u000a*[[Euclidean distance matrix]]\u000a*[[Hamming distance]] identifies the difference bit by bit of two strings\u000a*[[Mahalanobis distance]] normalizes based on a covariance matrix to make the distance metric scale-invariant.\u000a*[[Manhattan distance]] measures distance following only axis-aligned directions.\u000a*[[Metric (mathematics)|Metric]]\u000a*[[Minkowski distance]] is a generalization that unifies Euclidean distance, Manhattan distance, and Chebyshev distance.\u000a*[[Pythagorean addition]]\u000a\u000a==References==\u000a* {{cite book |first=Elena |last=Deza |first2=Michel Marie |last2=Deza |year=2009 |title=Encyclopedia of Distances |page=94 |publisher=Springer }}\u000a* {{cite web |url=http://www.statsoft.com/textbook/cluster-analysis/ |title=Cluster analysis |date=March 2, 2011 }}\u000a\u000a{{DEFAULTSORT:Euclidean Distance}}\u000a[[Category:Metric geometry]]\u000a[[Category:Length]]\u000a[[Category:String similarity measures]]
p138
asI78
(lp139
VInformation Retrieval Specialist Group
p140
aV{{Unreferenced|date=January 2010}}\u000a\u000aThe '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}\u000a\u000a==European Conference on Information Retrieval==\u000aOrganising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.\u000a\u000a== External links ==\u000a* [http://irsg.bcs.org/ IRSG website]\u000a\u000a[[Category:Information retrieval|Specialist Group]]\u000a[[Category:BCS Specialist Groups]]
p141
asI81
(lp142
VProbabilistic relevance model
p143
aV{{Underlinked|date=August 2014}}\u000a\u000aThe '''probabilistic relevance model'''<ref>{{citation | author=S. E. Robertson | coauthors=K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129\u2013146 | date=May\u2013June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}</ref> was devised by Robertson and Jones as a framework for probabilistic models to come.\u000a \u000aIt makes an estimation of the probability of finding if a document ''d<sub>j</sub>'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.\u000a\u000a<math>sim(d_{j},q) = \u005cfrac{P(R|\u005cvec{d}_j)}{P(\u005cbar{R}|\u005cvec{d}_j)}</math>\u000a\u000a==Related models==\u000aThere are some limitations to this framework that need to be addressed by further development:\u000a* There is no accurate estimate for the first run probabilities\u000a* Index terms are not weighted\u000a* Terms are assumed mutually independent\u000a\u000aTo address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and it's BM25F brother.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]
p144
asI210
(lp145
VDamerau\u2013Levenshtein distance
p146
aVIn [[information theory]] and [[computer science]], the '''Damerau\u2013Levenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir I. Levenshtein]]<ref>{{cite conference |last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286\u2013293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf}}</ref><ref name="bard"/><ref>{{cite conference |last1=Li |last2=et al. |year=2006|title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1025\u20131032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf}}</ref>) is a [[Metric (mathematics)|distance]] ([[string metric]]) between two [[string (computer science)|strings]], i.e., finite sequence of symbols, given by counting the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a [[transposition (mathematics)|transposition]] of two '''adjacent''' characters.  In his seminal paper,<ref>{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |publisher=ACM |volume=7 |issue=3 |pages=171\u2013176 |date=March 1964 |doi=10.1145/363958.363994}}</ref> Damerau not only distinguished these four edit operations but also stated that they correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation.\u000a\u000aThe Damerau\u2013Levenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations. The classical Levenshtein distance only allows insertion, deletion, and substitution operations.<ref>{{citation |url= <!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf--> |first=Vladimir I. |last=Levenshtein |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |date=February 1966 |volume=10 |issue=8 |pages=707&ndash;710}}</ref> Modifying this distance by including transpositions of adjacent symbols produces a different distance measure, known as the Damerau\u2013Levenshtein distance.<ref name="bard">{{citation\u000a | last = Bard | first = Gregory V.\u000a | contribution = Spelling-error tolerant, order-independent pass-phrases via the Damerau\u2013Levenshtein string-edit distance metric\u000a | isbn = 1-920-68285-6\u000a | location = Darlinghurst, Australia\u000a | pages = 117\u2013124\u000a | publisher = Australian Computer Society, Inc.\u000a | series = Conferences in Research and Practice in Information Technology\u000a | title = Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007\u000a | url = http://dl.acm.org/citation.cfm?id=1274531.1274545\u000a | volume = 68\u000a | year = 2007}}. The isbn produces two hits: a 2007 work and a 2010 work at World Cat.</ref>\u000a\u000aWhile the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, Damerau\u2013Levenshtein distance has also seen uses in biology to measure the variation between [[DNA]].<ref>The method used in: {{Citation\u000a| last1  = Majorek         | first1 = Karolina A.\u000a| last2  = Dunin-Horkawicz | first2 = Stanis\u0142aw\u000a| last3  = Steczkiewicz    | first3 = Kamil\u000a| last4  = Muszewska       | first4 = Anna\u000a| last5  = Nowotny         | first5 = Marcin\u000a| last6  = Ginalski        | first6 = Krzysztof\u000a| last7  = Bujnicki        | first7 = Janusz M.\u000a| display-authors = 2\u000a| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification\u000a| journal = Nucleic Acids Research\u000a| volume  = 42\u000a| issue   = 7\u000a| pages   = 4160\u20134179\u000a| year    = 2013\u000a| doi     = 10.1093/nar/gkt1414\u000a| url     = http://nar.oxfordjournals.org/content/42/7/4160.full\u000a}}</ref>\u000a\u000a== Definition ==\u000aThe Damerau\u2013Levenshtein distance between two strings <math>a</math> and <math>b</math> is given by <math>d_{a,b}(|a|,|b|)</math> where:\u000a\u000a<math>\u005cqquad d_{a,b}(i,j) = \u005cbegin{cases}\u000a  \u005cmax(i,j) & \u005ctext{ if} \u005cmin(i,j)=0, \u005c\u005c\u000a\u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} \u005c\u005c\u000a          d_{a,b}(i-2,j-2) + 1 \u000a       \u005cend{cases} & \u005ctext{ if } i,j > 1 \u005ctext{ and } a_i = b_{j-1} \u005ctext{ and } a_{i-1} = b_j \u005c\u005c\u000a  \u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)}\u000a       \u005cend{cases} & \u005ctext{ otherwise.}\u000a\u005cend{cases}</math>\u000a\u000awhere  <math>1_{(a_i \u005cneq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\u000a\u000aEach recursive call matches one of the cases covered by the Damerau\u2013Levenshtein distance:\u000a* <math>d_{a,b}(i-1,j) + 1</math> corresponds to a deletion (from a to b).\u000a* <math>d_{a,b}(i,j-1) + 1</math> corresponds to an insertion (from a to b).\u000a* <math>d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} </math> corresponds to a match or mismatch, depending on whether the respective symbols are the same.\u000a* <math>d_{a,b}(i-2,j-2) + 1 </math> corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.\u000a\u000a== Algorithm ==\u000aPresented here are two algorithms: the first,<ref>{{cite paper | author1 = B. J. Oommen | author2 = R. K. S. Loke | title = Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions | id = {{citeseerx|10.1.1.50.1459}} | doi=10.1016/S0031-3203(96)00101-X }}</ref> simpler one, computes what is known as the [[optimal string alignment]]{{Citation needed|date=May 2013}} (sometimes called the ''restricted edit distance''{{Citation needed|date=May 2013}}), while the second one<ref name="LW75">{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=JACM |volume=22 |issue=2 |pages=177\u2013183 |date=April 1975 |doi=10.1145/321879.321880}}</ref> computes the Damerau\u2013Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.\u000a\u000aTake for example the edit distance between '''CA''' and '''ABC'''. The Damerau\u2013Levenshtein distance LD('''CA''','''ABC''') = 2 because '''CA''' \u2192 '''AC''' \u2192 '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA''' \u2192 '''AC''' is used, it is not possible to use '''AC''' \u2192 '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA''' \u2192 '''A''' \u2192 '''AB''' \u2192 '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') < OSA('''CA''','''ABC'''), and so it is not a true metric.\u000a \u000a===Optimal string alignment distance===\u000aFirstly, let us consider a direct extension of the formula used to calculate [[Levenshtein distance]].  Below is [[pseudocode]] for a function ''OptimalStringAlignmentDistance'' that takes two strings, ''str1'' of length ''lenStr1'', and ''str2'' of length ''lenStr2'', and computes the optimal string alignment distance between them:\u000a\u000a<syntaxhighlight lang="pascal">\u000a int OptimalStringAlignmentDistance(char str1[1..lenStr1], char str2[1..lenStr2])\u000a    // d is a table with lenStr1+1 rows and lenStr2+1 columns\u000a    declare int d[0..lenStr1, 0..lenStr2]\u000a\u000a    // i and j are used to iterate over str1 and str2\u000a    declare int i, j, cost\u000a\u000a    // for loop is inclusive, need table 1 row/column larger than string length\u000a    for i from 0 to lenStr1\u000a        d[i, 0] := i\u000a    for j from 1 to lenStr2\u000a        d[0, j] := j\u000a\u000a    // pseudo-code assumes string indices start at 1, not 0\u000a    // if implemented, make sure to start comparing at 1st letter of strings\u000a    for i from 1 to lenStr1\u000a        for j from 1 to lenStr2\u000a            if str1[i] = str2[j] then cost := 0\u000a                                 else cost := 1\u000a            d[i, j] := minimum(\u000a                                 d[i-1, j  ] + 1,     // deletion\u000a                                 d[i  , j-1] + 1,     // insertion\u000a                                 d[i-1, j-1] + cost   // substitution\u000a                             )\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )                        \u000a  \u000a    return d[lenStr1, lenStr2]\u000a</syntaxhighlight>\u000a\u000aBasically this is the algorithm to compute [[Levenshtein distance]] with one additional recurrence:\u000a\u000a<syntaxhighlight lang="pascal">\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )\u000a</syntaxhighlight>\u000a\u000a===Distance with adjacent transpositions===\u000aHere is the second algorithm that computes the true Damerau\u2013Levenshtein distance with adjacent transpositions (ActionScript 3.0); this function requires as an additional parameter the size of the alphabet (''C''), so that all entries of the arrays are in 0..(''C''&minus;1):\u000a\u000a<syntaxhighlight lang='actionscript3'>static public function damerauLevenshteinDistance(a:Array, b:Array, C:uint):uint\u000a{\u000a    // "infinite" distance is just the max possible distance\u000a    var INF:uint = a.length + b.length;\u000a\u000a    // make and initialize the character array indices            \u000a    var DA:Array = new Array(C);\u000a    for (var k:uint = 0; k < C; ++k) DA[k]=0;\u000a\u000a    // make the distance matrix H[-1..a.length][-1..b.length]\u000a    var H:matrix = new matrix(a.length+2,b.length+2);\u000a    \u000a    // initialize the left and top edges of H\u000a    H[-1][-1] = INF;\u000a    for (var i:uint = 0; i <= a.length; ++i)\u000a    {\u000a        H[i][-1] = INF;\u000a        H[i][ 0] = i;\u000a    }\u000a    for (var j:uint = 0; j <= b.length; ++j)\u000a    {\u000a        H[-1][j] = INF;\u000a        H[ 0][j] = j;\u000a    }\u000a\u000a    // fill in the distance matrix H\u000a    // look at each character in a\u000a    for (var i:uint = 1; i <= a.length; ++i)\u000a    {\u000a        var DB:uint = 0;\u000a        // look at each character in b\u000a        for (var j:uint = 1; j <= b.length; ++j)\u000a        {\u000a            var i1:uint = DA[b[j-1]];\u000a            var j1:uint = DB;\u000a            var cost:uint;\u000a            if (a[i-1] == b[j-1])\u000a               {\u000a                 cost = 0;\u000a                 DB   = j;\u000a               }\u000a            else\u000a               cost = 1;\u000a            H[i][j] = Math.min(    H[i-1 ][j-1 ] + cost,  // substitution\u000a                                   H[i   ][j-1 ] + 1,     // insertion\u000a                                   H[i-1 ][j   ] + 1,     // deletion\u000a                                   H[i1-1][j1-1] + (i-i1-1) + 1 + (j-j1-1));\u000a        }\u000a        DA[a[i-1]] = i;\u000a    }\u000a    return H[a.length][b.length];\u000a}\u000a</syntaxhighlight>\u000a\u000a:<small>'''Note''': the algorithm given in the paper uses alphabet 1..C rather than the 0..''C''&minus;1 used here; the paper indexes arrays: H[&minus;1..|A|,&minus;1..|B|] and DA[1..C]; here DA[0..C&minus;1] is used; the paper seems to be missing the necessary line H[&minus;1,&minus;1]&nbsp;=&nbsp;INF</small>\u000a\u000aTo devise a proper algorithm to calculate unrestricted Damerau\u2013Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, <math>W_T</math>, is at least the average of the cost of an insertion and deletion, i.e., <math>2W_T \u005cge W_I+W_D</math>.<ref name="LW75"/>) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: <math>O\u005cleft (M \u005ccdot N \u005ccdot \u005cmax(M, N) \u005cright )</math>, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,<ref name="LW75"/> this naive algorithm can be improved to be <math>O\u005cleft (M \u005ccdot N \u005cright)</math> in the worst case.\u000a\u000aIt is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.\u000a\u000a== Applications ==\u000aDamerau\u2013Levenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke{{ref|OO}} even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.\u000a\u000a=== DNA ===\u000aSince [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau\u2013Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[Needleman\u2013Wunsch algorithm]] or [[Smith\u2013Waterman algorithm]].\u000a\u000a=== Fraud detection ===\u000aThe algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as "Rich Heir Estate Services" versus a false vendor "Rich Hier State Services". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau\u2013Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.\u000a\u000a== See also ==\u000a* [[Approximate string matching]]\u000a* [[Levenshtein automata]]\u000a* [[Typosquatting]]\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a== Further reading ==\u000a* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31\u201388 |date=March 2001 |doi=10.1145/375360.375365 }}\u000a\u000a{{DEFAULTSORT:Damerau-Levenshtein Distance}}\u000a[[Category:String similarity measures]]\u000a[[Category:Information theory]]\u000a[[Category:Dynamic programming]]
p147
asI58
(lp148
VWeb query classification
p149
aV{{Cleanup|date=March 2011}}\u000a''' \u000aA Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query \u201c''apple''\u201d might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.\u000a\u000a== KDDCUP 2005 ==\u000a\u000aKDDCUP 2005 competition<ref>[http://www.sigkdd.org/kdd2005/kddcup.html KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query \u201c''apple''\u201d, it should be classified into ranked categories: \u201c''Computers \u005c Hardware''; ''Living \u005c Food & Cooking''\u201d.\u000a\u000a{| class="wikitable"\u000a|-\u000a! Query\u000a! Categories\u000a|-\u000a| apple\u000a| Computers \u005c Hardware<br />Living \u005c Food & Cooking\u000a|-\u000a| FIFA 2006\u000a| Sports \u005c Soccer<br />Sports \u005c Schedules & Tickets<br />Entertainment \u005c Games & Toys\u000a|-\u000a| cheesecake recipes\u000a| Living \u005c Food & Cooking<br />Information \u005c Arts & Humanities\u000a|-\u000a| friendships poem\u000a| Information \u005c Arts & Humanities<br />Living \u005c Dating & Relationships\u000a|}\u000a\u000a[[Image:Web query length.gif]]\u000a[[Image:Web query meaning.gif]]\u000a\u000a== Difficulties ==\u000a\u000aWeb query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:\u000a\u000a=== How to derive an appropriate feature representation for Web queries? ===\u000a\u000aMany queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.\u000a\u000a* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).\u000a\u000aHow about disadvantages and advantages??\u000agive the answers:\u000a\u000a=== How to adapt the changes of the queries and categories over time? ===\u000a\u000aThe meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.\u000a\u000a* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.\u000a\u000a=== How to use the unlabeled query logs to help with query classification? ===\u000a\u000aSince the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.\u000a\u000a* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.</ref> tries to associate related queries by clustering \u201csession data\u201d, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.\u000a\u000a* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.\u000a\u000a== Applications ==\u000a\u000a* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.\u000a* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.\u000a* '''[[Online advertising]]'''<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.\u000aAll these services rely on the understanding Web users' search intents through their Web queries.\u000a\u000a== See also ==\u000a\u000a* [[Document classification]]\u000a* [[Web search query]]\u000a* [[Information retrieval]]\u000a* [[Query expansion]]\u000a* [[Naive Bayes classifier]]\u000a* [[Support vector machines]]\u000a* [[Meta search]]\u000a* [[Vertical search]]\u000a* [[Online advertising]]\u000a\u000a== References ==\u000a\u000a{{reflist}}\u000a\u000a== Further reading ==\u000a* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Web Query Classification}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search]]
p150
asI212
(lp151
VWagner\u2013Fischer algorithm
p152
aVIn [[computer science]], the '''Wagner\u2013Fischer algorithm''' is a [[dynamic programming]] algorithm that computes the [[edit distance]] between two strings of characters.\u000a\u000a==History==\u000aThe Wagner\u2013Fischer algorithm has a history of [[multiple invention]]. Navarro lists the following inventors of it, with date of publication, and acknowledges that the list is incomplete:<ref name="navarro"/>{{rp|43}}\u000a* Vintsyuk, 1968\u000a* [[Needleman\u2013Wunsch algorithm|Needleman and Wunsch]], 1970\u000a* Sankoff, 1972\u000a* Sellers, 1974\u000a* Wagner and Fischer, 1974\u000a* Lowrance and Wagner, 1975\u000a\u000a==Calculating distance==\u000aThe Wagner\u2013Fischer algorithm computes edit distance based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the edit distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix by [[flood fill]]ing the matrix, and thus find the distance between the two full strings as the last value computed.\u000a\u000aA straightforward implementation, as [[pseudocode]] for a function ''EditDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them, looks as follows. Note that the inputs strings are one-indexed, while the matrix ''d'' is zero-indexed, and <code>[i..k]</code> is a closed range.\u000a\u000a  '''int''' EditDistance('''char''' s[1..m], '''char''' t[1..n])\u000a    ''// For all i and j, d[i,j] will hold the Levenshtein distance between''\u000a    ''// the first i characters of s and the first j characters of t.''\u000a    ''// Note that d has (m+1)  x(n+1) values.\u000a    '''let''' d be a 2-d array of '''int''' with dimensions [0..m, 0..n]\u000a   \u000a    '''for''' i '''in''' [0..m]\u000a      d[i, 0] \u2190 i ''// the distance of any first string to an empty second string''\u000a    '''for''' j '''in''' [0..n]\u000a      d[0, j] \u2190 j ''// the distance of any second string to an empty first string''\u000a   \u000a    '''for''' j '''in''' [1..n]\u000a      '''for''' i '''in''' [1..m]\u000a        '''if''' s[i] = t[j] '''then'''  <!-- not: s[i-1] = t[j-1] -->\u000a          d[i, j] \u2190 d[i-1, j-1]       ''// no operation required''\u000a        '''else'''\u000a          d[i, j] \u2190 minimum of\u000a                     (\u000a                       d[i-1, j] + 1,  ''// a deletion''\u000a                       d[i, j-1] + 1,  ''// an insertion''\u000a                       d[i-1, j-1] + 1 ''// a substitution''\u000a                     )\u000a   \u000a    '''return''' d[m,n]\u000a\u000aTwo examples of the resulting matrix (hovering over an underlined number reveals the operation performed to get that number):\u000a<center>\u000a{|\u000a|\u000a{|class="wikitable"\u000a|-\u000a| \u000a| \u000a!k \u000a!i \u000a!t \u000a!t \u000a!e \u000a!n\u000a|-\u000a| ||0 ||1 ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!s\u000a|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!i\u000a|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5\u000a|-\u000a!t\u000a|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4\u000a|-\u000a!t\u000a|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 \u000a|-\u000a!i\u000a|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3\u000a|-\u000a!n\u000a|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}\u000a|-\u000a!g\u000a|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}\u000a|}\u000a|\u000a{|class="wikitable"\u000a|\u000a|\u000a!S\u000a!a\u000a!t\u000a!u\u000a!r\u000a!d\u000a!a\u000a!y\u000a|-\u000a| \u000a|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\u000a|-\u000a!S\u000a|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7\u000a|-\u000a!u\u000a|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6\u000a|-\u000a!n\u000a|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6\u000a|-\u000a!d\u000a|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5 \u000a|-\u000a!a\u000a|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4\u000a|-\u000a!y\u000a|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}\u000a|}\u000a|}\u000a</center>\u000a\u000aThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.\u000a\u000a===Proof of correctness===\u000aAs mentioned earlier, the [[invariant (mathematics)|invariant]] is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. This invariant holds since:\u000a* It is initially true on row and column 0 because <code>s[1..i]</code> can be transformed into the empty string <code>t[1..0]</code> by simply dropping all <code>i</code> characters. Similarly, we can transform <code>s[1..0]</code> to <code>t[1..j]</code> by simply adding all <code>j</code> characters.\u000a* If <code>s[i] = t[j]</code>, and we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code> and just leave the last character alone, giving <code>k</code> operations.\u000a* Otherwise, the distance is the minimum of the three possible ways to do the transformation:\u000a** If we can transform <code>s[1..i]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can simply add <code>t[j]</code> afterwards to get <code>t[1..j]</code> in <code>k+1</code> operations (insertion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j]</code> in <code>k</code> operations, then we can remove <code>s[i]</code> and then do the same transformation, for a total of <code>k+1</code> operations (deletion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code>, and exchange the original <code>s[i]</code> for <code>t[j]</code> afterwards, for a total of <code>k+1</code> operations (substitution).\u000a* The operations required to transform <code>s[1..n]</code> into <code>t[1..m]</code> is of course the number required to transform all of <code>s</code> into all of <code>t</code>, and so <code>d[n,m]</code> holds our result.\u000a\u000aThis proof fails to validate that the number placed in <code>d[i,j]</code> is in fact minimal; this is more difficult to show, and involves an [[Reductio ad absurdum|argument by contradiction]] in which we assume <code>d[i,j]</code> is smaller than the minimum of the three, and use this to show one of the three is not minimal.\u000a\u000a===Possible improvements===\u000aPossible improvements to this algorithm include:\u000a* We can adapt the algorithm to use less space, [[Big O notation|''O'']](''m'') instead of ''O''(''mn''), since it only requires that the previous row and current row be stored at any one time.\u000a* We can store the number of insertions, deletions, and substitutions separately, or even the positions at which they occur, which is always <code>j</code>.\u000a* We can normalize the distance to the interval <code>[0,1]</code>.\u000a* If we are only interested in the distance if it is smaller than a threshold ''k'', then it suffices to compute a diagonal stripe of width ''2k+1'' in the matrix. In this way, the algorithm can be run in [[Big O notation|''O'']](''kl'') time, where ''l'' is the length of the shortest string.<ref>{{cite book |author=Gusfield, Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=0-521-58519-8 }}</ref>\u000a* We can give different penalty costs to insertion, deletion and substitution. We can also give penalty costs that depend on which characters are inserted, deleted or substituted.\u000a* This algorithm [[parallel computing|parallelizes]] poorly, due to a large number of [[data dependency|data dependencies]]. However, all the <code>cost</code> values can be computed in parallel, and the algorithm can be adapted to perform the <code>minimum</code> function in phases to eliminate dependencies.\u000a* By examining diagonals instead of rows, and by using [[lazy evaluation]], we can find the Levenshtein distance in ''O''(''m'' (1 + ''d'')) time (where ''d'' is the Levenshtein distance), which is much faster than the regular dynamic programming algorithm if the distance is small.<ref>{{cite journal |author=Allison L |title=Lazy Dynamic-Programming can be Eager |journal=Inf. Proc. Letters |volume=43 |issue=4 |pages=207\u201312 |date=September 1992 |url=http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html |doi=10.1016/0020-0190(92)90202-7}}</ref>\u000a\u000a==Seller's variant for string search==\u000aBy initializing the first row of the matrix with zeros, we obtain a variant of the Wagner\u2013Fischer algorithm that can be used for [[fuzzy string searching|fuzzy string search]] of a string in a text.<ref name="navarro">{{cite doi|10.1145/375360.375365}}</ref> This modification gives the end-position of matching substrings of the text. To determine the start-position of the matching substrings, the number of insertions and deletions can be stored separately and used to compute the start-position from the end-position.<ref>Bruno Woltzenlogel Paleo. [http://www.logic.at/people/bruno/Papers/2007-GATE-ESSLLI.pdf An approximate gazetteer for GATE based on levenshtein distance]. Student Section of the European Summer School in Logic, Language and Information ([[European Summer School in Logic, Language and Information|ESSLLI]]), 2007.</ref>\u000a\u000aThe resulting algorithm is by no means efficient, but was at the time of its publication (1980) one of the first algorithms that performed approximate search.<ref name="navarro"/>\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a{{DEFAULTSORT:Wagner-Fischer algorithm}}\u000a[[Category:Algorithms on strings]]\u000a[[Category:String similarity measures]]
p153
asI142
(lp154
VThesaurus (information retrieval)
p155
aV{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}\u000a\u000aIn the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>\u000a\u000aA thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a \u201ccontrolled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.\u201d\u000a\u000aA thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.\u000a\u000a== History ==\u000aWherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a \u201cthesaurus\u201d (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging & Classification Quarterly'', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.</ref>\u000a\u000aThe first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.\u000aHundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:\u000a(a)	Extension from monolingual to multilingual capability; and \u000a(b)	Addition of a conceptually organized display to the basic alphabetical presentation.\u000a\u000aHere we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:\u000a\u000a* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)\u000a* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)\u000a* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)\u000a* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)\u000a* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985\u000a* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)\u000a* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.\u000a\u000aThe most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>''[http://www.niso.org/schemas/iso25964/ ISO 25964 \u2013 the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.</ref>\u000a\u000a== Purpose ==\u000aIn information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.\u000a\u000aThe [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN\u2019s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.\u000a\u000a== Structure ==\u000aInformation retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, \u201ccitrus fruits\u201d might be linked to the broader concept of \u201cfruits\u201d, and the narrower ones of \u201coranges\u201d, \u201clemons\u201d, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as \u201cmad cow disease\u201d, \u201cbovine spongiform encephalopathy\u201d, \u201cBSE\u201d, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.\u000a\u000a== See also ==\u000a* [[Controlled vocabulary]]\u000a* [[ISO 25964]]\u000a* [[Thesaurus]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] \u000a* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Thesauri]]
p156
asI214
(lp157
VMost frequent k characters
p158
aV{{multiple issues|\u000a{{Third-party|date=March 2014}}\u000a{{Notability|date=March 2014}}\u000a}}\u000a\u000aIn [[information theory]], '''MostFreqKDistance''' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=SEKER|authorlink=Sadi Evren SEKER|year=2014|txt}},<ref name="mfkc"/> and initially used in [[text mining]] applications like [[author recognition]].<ref name="mfkc">{{citation\u000a | last1 = SEKER | first1 = Sadi E. | author1-link = Sadi Evren SEKER\u000a | last2 = Altun | first2 = Oguz\u000a | last3 = Ayan | first3 = Ugur\u000a | last4 = Mert | first4 = Cihan\u000a | contribution = A Novel String Distance Function based on Most Frequent K Characters\u000a | volume = 4\u000a | issue = 2\u000a | pages = 177\u2013183\u000a | publisher = [[International Association of Computer Science and Information Technology Press (IACSIT Press)]]\u000a | title = [[International Journal of Machine Learning and Computing (IJMLC)]]\u000a | contribution-url = http://arxiv.org/abs/1401.6596\u000a | year = 2014}}</ref>\u000aMethod is originally based on a hashing function MaxFreqKChars <ref name="hashfunc">{{citation\u000a | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER\u000a | last2 = Mert | first2 = Cihan\u000a | contribution = A Novel Feature Hashing For Text Mining\u000a | url = http://journal.ibsu.edu.ge/index.php/jtst/article/view/428\u000a | pages = 37\u201341\u000a | publisher = [[International Black Sea University]]\u000a | title = Journal of Technical Science and Technologies\u000a | ISSN = 2298-0032\u000a | volume = 2\u000a | issue = 1\u000a | year = 2013}}</ref> classical [[author recognition]] problem and idea first came out while studying on [[data stream mining]].<ref name="author">{{citation\u000a | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER\u000a | last2 = Al-Naami | first2 = Khaled\u000a | last3 = Khan | first3 = Latifur\u000a | contribution = Author attribution on streaming data\u000a | doi = 10.1109/IRI.2013.6642511\u000a | url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6642511\u000a | pages = 497\u2013503\u000a | publisher = [[IEEE]]\u000a | title = Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, Aug 14-16, 2013\u000a | year = 2013}}</ref> Algorithm is suitable for coding in most of the programming languages like [[Java (programming language)|Java]], [[Tcl]], [[Python (programming language)|Python]] or [[J (programming language)|J]]. <ref>\u000a{{Citation |title=Rosetta Code: Most frequent k chars distance , code sources for Python, Java, Tcl and J |accessdate=16 Oct 2014 |url=http://rosettacode.org/wiki/Most_frequent_k_chars_distance}}\u000a</ref>\u000a\u000a\u000a==Definition==\u000aMethod has two steps.\u000a* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively\u000a* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value\u000a\u000a===Most frequent K hashing===\u000aThe first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:\u000a<syntaxhighlight lang="Java">\u000aString function MostFreqKHashing (String inputString, int K)\u000a    def string outputString\u000a    for each distinct character\u000a        count occurrence of each character\u000a    for i := 0 to K\u000a        char c = next most freq ith character  (if two chars have same frequency than get the first occurrence in inputString)\u000a        int count = number of occurrence of the character\u000a        append to outputString, c and count\u000a    end for\u000a    return outputString\u000a</syntaxhighlight>\u000a\u000aAbove function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, ''Most Frequent K Hashing'' is also a [[one way function]].\u000a\u000a===Most frequent K distance===\u000aThe second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).\u000a<syntaxhighlight lang="Java">\u000aint function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)\u000a    def int similarity\u000a    for each c = next character from inputStr1\u000a        lookup c in inputStr2\u000a        if c is null\u000a             continue\u000a             similarity += frequency of c in inputStr1\u000a    return limit-similarity\u000a</syntaxhighlight>\u000aAbove function, simply gets two input strings, previously outputted from the <code>MostFreqKHashing</code> function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.\u000a\u000aIn some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.\u000a\u000a===String distance wrapper function===\u000aIn order to calculate the distance between two strings, below function can be implemented\u000a<syntaxhighlight lang="Java">\u000aint function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)\u000a    return maxDistance - MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K))\u000a</syntaxhighlight>\u000a\u000aAny call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.\u000a\u000a==Examples==\u000aLet's consider maximum 2 frequent hashing over two strings \u2018research\u2019 and \u2018seeking\u2019.\u000aMostFreqKHashing('research', 2) = r2e2\u000abecause we have 2 'r' and 2 'e' characters with the highest frequency and we return in the order they appear in the string.\u000aMostFreqKHashing('seeking', 2) = e2s1\u000aAgain we have character 'e' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is 's'.\u000aFinally we make the comparison:\u000aMostFreqKSimilarity('r2e2', 'e2s1') = 2\u000aWe simply compared the outputs and only character occurring in both input is character 'e' and the occurrence in both input is 2.\u000aInstead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:\u000aMostFreqKSDF('research', 'seeking', 2) = 2\u000a\u000aBelow table holds some sample runs between example inputs for K=2:\u000a{|class="wikitable"\u000a|-\u000a! Inputs\u000a! Hash Outputs\u000a! SDF Output (max from 10)\u000a|-\u000a|'night'\u000a'nacht'\u000a|n1i1\u000an1a1\u000a|9\u000a|-\u000a|'my'\u000a'a'\u000a|m1y1\u000aa1NULL0\u000a|10\u000a|-\u000a|\u2018research\u2019\u000a\u2018research\u2019	\u000a|r2e2\u000ar2e2	\u000a|6\u000a|-\u000a|\u2018aaaaabbbb\u2019\u000a\u2018ababababa\u2019	\u000a|a5b4\u000aa5b4	\u000a|1\u000a|-\u000a|\u2018significant\u2019\u000a\u2018capabilities\u2019	\u000a|i3n2\u000ai3a2	\u000a|7\u000a|}\u000a\u000aMethod is also suitable for bioinformatics to compare the genetic strings like in [[FASTA format]].\u000a\u000aStr1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV\u000a\u000aStr2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG\u000a\u000aMostFreqKHashing(str1, 2) = L9T8\u000a\u000aMostFreqKHashing(str2, 2) = F9L8\u000a\u000aMostFreqKSDF(str1, str2, 2, 100) = 83\u000a\u000a==Algorithm complexity and comparison==\u000aThe motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.\u000a\u000aOn the other hand string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.\u000a\u000aAlso algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.\u000a\u000a===Time complexity===\u000aThe calculation of time complexity of 'most frequent k char string similarity' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].\u000a\u000a===Comparison===\u000aBelow table compares the complexity of algorithms:\u000a{|class="wikitable"\u000a|-\u000a! Algorithm\u000a! Time Complexity\u000a|-\u000a| [[Levenshtein distance]]\u000a| O(nm) = O(n^2)\u000a|-\u000a| [[Jaccard index]]\u000a| O(n+m) = O(n)\u000a|-\u000a| MostFreqKSDF\u000a| O(nlogn+mlogm) = O(n log n)\u000a|}\u000a\u000aFor the above table, n is the length of first string and m is the length of second string.\u000a\u000a==Success on text mining==\u000aThe success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:\u000a\u000a{|class="wikitable"\u000a|-\u000a! Algorithm\u000a! Running Time\u000a! Error (RMSE)\u000a! Error (RAE)\u000a|-\u000a|[[Levenshtein distance]]\u000a|3647286.54 sec\u000a|29\u000a|0.47\u000a|-\u000a|[[Jaccard index]]\u000a|228647.22 sec\u000a|45\u000a|0.68\u000a|-\u000a|MostFreqKSDF\u000a|2712323.51 sec\u000a|32\u000a|0.49\u000a|}\u000a\u000aThe running times for [[author recognition]] are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).\u000a\u000aAbove table shows, the 'most frequent k similarity' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.\u000a\u000aFor the time performance and the success rates, the bitwise similarity functions like [[Dice's coefficient|Sørensen\u2013Dice index]], [[Tversky index]] or [[Hamming Distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, looses the string operations like deletion or addition. For example a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.\u000a\u000aUnfortunately, [[big data]] studies requires a faster algorithm with still acceptable success. Here the 'max frequent k characters' is an easy and simple algorithm (as in [[Occams razor]]), which is straight forward to implement.\u000a\u000a==See also==\u000a[http://rosettacode.org/wiki/Most_frequent_k_chars_distance RosettaCode,Code reposistory of Most Frequent K Chars Distance Algorithm in Java, Python, TCL or J languages] (Retrieved Oct. 16 2014)\u000a<div class= style="-moz-column-count:2; column-count:2;">\u000a* [[agrep]]\u000a* [[Approximate string matching]]\u000a* [[Bitap algorithm]]\u000a* [[Damerau\u2013Levenshtein distance]]\u000a* [[diff]]\u000a* [[MinHash]]\u000a* [[Dynamic time warping]]\u000a* [[Euclidean distance]]\u000a* [[Fuzzy string searching]]\u000a* [[Hamming weight]]\u000a* [[Hirschberg's algorithm]]\u000a* [[Sequence homology|Homology of sequences in genetics]]\u000a* [[Hunt\u2013McIlroy algorithm]]\u000a* [[Jaccard index]]\u000a* [[Jaro\u2013Winkler distance]]\u000a* [[Levenshtein distance]]\u000a* [[Longest common subsequence problem]]\u000a* [[Lucene]] (an open source search engine that implements edit distance)\u000a* [[Manhattan distance]]\u000a* [[Metric space]]\u000a* [[Needleman\u2013Wunsch algorithm]]\u000a* [[Optimal matching]] algorithm\u000a* [[Sequence alignment]]\u000a* Similarity space on [[Numerical taxonomy]]\u000a* [[Smith\u2013Waterman algorithm]]\u000a* [[Sørensen similarity index]]\u000a* [[String distance metric]]\u000a* [[String similarity function]]\u000a* [[Wagner-Fischer algorithm]]\u000a* [[Locality-sensitive hashing]]\u000a</div>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:String similarity measures]]\u000a[[Category:Dynamic programming]]\u000a[[Category:Articles with example pseudocode]]\u000a[[Category:Quantitative linguistics]]\u000a[[Category:Hash functions]]\u000a[[Category:Hashing]]
p159
asI88
(lp160
VMusic information retrieval
p161
aV{{multiple issues|\u000a{{technical|date=October 2012}}\u000a{{Expert-subject|date=July 2010}}\u000a{{Expert-subject|Science|date=July 2010}}\u000a}}\u000a\u000a\u000a'''Music information retrieval''' ('''MIR''') is the interdisciplinary science of retrieving [[information]] from [[music]]. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in [[musicology]], [[psychology]], academic music study, [[signal processing]], [[machine learning]] or some combination of these.\u000a\u000a== Applications ==\u000aMIR is being used by businesses and academics to categorize, manipulate and even create music.\u000a\u000a=== Recommender systems ===\u000aSeveral [[recommender systems]] for music already exist, but surprisingly few are based upon MIR techniques, instead making use of similarity between users or laborious data compilation. [[Pandora]], for example, uses experts to tag the music with particular qualities such as "female singer" or "strong bassline". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems.\u000a\u000a=== Track separation and instrument recognition ===\u000aTrack separation is about extracting the original tracks as recorded, which could have more than one instrument played per track. Instrument recognition is about identifying the instruments involved and/or separating the music into one track per instrument. Various programs have been developed that can separate music into its component tracks without access to the master copy. In this way e.g. karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments.\u000a\u000a===Automatic music transcription===\u000aAutomatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a [[MIDI_file#File_formats|MIDI file]].<ref>A. Klapuri and M. Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.</ref> This process involves several subtasks, which include multi-pitch detection, [[Onset_detection#Onset_detection|onset detection]], duration estimation, instrument identification, and the extraction of rhythmic information. This task becomes more difficult with greater numbers of instruments and a greater [[Polyphony and monophony in instruments|polyphony level]].\u000a\u000a===Automatic categorization===\u000aMusical genre categorization is a common task for MIR and is the usual task for the yearly Music Information Retrieval Evaluation eXchange(MIREX).<ref>http://www.music-ir.org/mirex/wiki/MIREX_HOME - Music Information Retrieval Evaluation eXchange.</ref> Machine learning techniques such as [[Support Vector Machines]] tend to perform well, despite the somewhat subjective nature of the classification. Other potential classifications include identifying the artist, the place of origin or the mood of the piece. Where the output is expected to be a number rather than a class, [[regression analysis]] is required.\u000a\u000a===Music generation===\u000aThe automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results.\u000a\u000a==Methods used==\u000a\u000a===Data source===\u000a[[Sheet music|Scores]] give a clear and logical description of music from which to work, but access to sheet music, whether digital or otherwise, is often impractical. [[MIDI]] music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as [[WAV]], [[mp3]], and [[ogg]] are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently includes analysis of [[social tagging|social tags]] for music.\u000a\u000a===Feature representation===\u000aAnalysis can often require some summarising,<ref>Eidenberger, Horst (2011). \u201cFundamental Media Understanding\u201d, atpress. ISBN 978-3-8423-7917-6.</ref> and for music (as with many other forms of data) this is achieved by feature extraction, especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the [[Mel-frequency cepstral coefficient|Mel-Frequency Cepstral Coefficient]] (MFCC) which is a measure of the [[timbre]] of a piece of music. Other features may be employed to represent the [[Tonality#Computational_methods_to_determine_the_key|key]], chords, harmonies, melody, main pitch, beats per minute or rhythm in the piece.\u000a\u000a===Statistics and machine learning===\u000a*Computational methods for classification, clustering, and modelling \u2014 musical feature extraction for mono- and [[polyphonic]] music, similarity and [[pattern matching]], retrieval\u000a* Formal methods and databases \u2014 applications of automated [[music identification]] and recognition, such as [[score following]], automatic accompaniment, routing and filtering for music and music queries, query languages, standards and other metadata or protocols for music information handling and [[information retrieval|retrieval]], [[multi-agent system]]s, distributed search)\u000a*Software for music information retrieval \u2014 [[Semantic Web]] and musical digital objects, intelligent agents, collaborative software, web-based search and [[semantic retrieval]], [[query by humming]], [[acoustic fingerprinting]]\u000a* Music analysis and knowledge representation \u2014 automatic summarization, citing, excerpting, downgrading, transformation, formal models of music, digital scores and representations, music indexing and [[metadata]].\u000a\u000a==Other issues==\u000a*Human-computer interaction and interfaces \u2014 multi-modal interfaces, [[user interface]]s and [[usability]], mobile applications, user behavior\u000a* Music perception, cognition, affect, and emotions \u2014 music [[similarity metrics]], syntactical parameters, semantic parameters, musical forms, structures, styles ands, music annotation methodologies\u000a* Music archives, libraries, and digital collections \u2014 music [[digital library|digital libraries]], public access to musical archives, benchmarks and research databases\u000a* [[Intellectual property]] rights and music \u2014 national and international [[copyright]] issues, [[digital rights management]], identification and traceability\u000a* Sociology and Economy of music \u2014 music industry and use of MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental design and metrics\u000a\u000a== See also ==\u000a* [[Audio mining]]\u000a* [[Artificial intelligence]]\u000a* [[Digital rights management]]\u000a* [[Digital signal processing]]\u000a* [[Ethnomusicology]]\u000a* [[Multimedia Information Retrieval]]\u000a* [[Music notation]]\u000a* [[Musicology]]\u000a* [[Parsons code]]\u000a* [[Sound and music computing]]\u000a* [[Music OCR]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a* Michael Fingerhut (2004). [http://mediatheque.ircam.fr/articles/textes/Fingerhut04b "Music Information Retrieval, or how to search for (and maybe find) music and do away with incipits"], ''IAML-IASA Congress'', Oslo (Norway), August 8\u201313, 2004.\u000a\u000a==External links==\u000a* [http://www.ismir.net/ International Society for Music Information Retrieval]\u000a* [http://music-ir.org/ Music Information Retrieval research]\u000a* [http://www.music-ir.org/jdownie_papers/downie_mir_arist37.pdf J. Stephen Downie: Music information retrieval]\u000a* [http://dx.doi.org/10.1561/1500000042 M. Schedl, E. Gómez and J. Urbano: Music Information Retrieval: Recent Developments and Applications]\u000a* [http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000002 Nicola Orio: Music Retrieval: A Tutorial and Review]\u000a* [https://ccrma.stanford.edu/wiki/MIR_workshop_2011 Intelligent Audio Systems: Foundations and Applications of Music Information Retrieval, introductory course at Stanford University's Center for Computer Research in Music and Acoustics]\u000a* [http://biblio.ugent.be/record/470088 Micheline Lesaffre: Music Information Retrieval: Conceptual Framework, Annotation and User behavior.]\u000a* [http://the.echonest.com/ The Echo Nest: a company specialising in MIR research and applications.]\u000a* [http://www.imagine-research.com/ Imagine Research : develops platform and software for MIR applications ]\u000a* [http://www.AudioContentAnalysis.org/ AudioContentAnalysis.org: MIR resources and matlab code ]\u000a\u000a==Example MIR applications==\u000a* [http://www.musipedia.org/ Musipedia \u2014 A melody search engine that offers several modes of searching, including whistling, tapping, piano keyboard, and Parsons code.]\u000a* [http://www.listengame.org/ The Listen Game \u2014 UCSD Computer Audition Lab MIR music ranking game]\u000a* [http://www.peachnote.com/ Peachnote \u2014 A melody search engine and n-gram viewer that searches through digitized music scores]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Music software]]
p162
asI89
(lp163
VStatistical semantics
p164
aV{{linguistics}}\u000a'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}<!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation-->. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?\u000a\u000a==History==\u000a\u000aThe term ''Statistical Semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].<ref>{{harvnb|Weaver|1955}}</ref> He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].<ref>{{harvnb|Firth|1957}}</ref> This assumption is known in [[Linguistics]] as the [[Distributional hypothesis|Distributional Hypothesis]].<ref>{{harvnb|Sahlgren|2008}}</ref> Emile Delavenay defined ''Statistical Semantics'' as "Statistical study of meanings of words and their frequency and order of recurrence."<ref>{{harvnb|Delavenay|1960}}</ref> "[[George Furnas|Furnas]] ''et al.'' 1983" is frequently cited as a foundational contribution to Statistical Semantics.<ref>{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}</ref>  An early success in the field was [[Latent semantic analysis|Latent Semantic Analysis]].\u000a\u000a==Applications of statistical semantics==\u000a\u000aResearch in Statistical Semantics has resulted in a wide variety of algorithms that use the Distributional Hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:\u000a* Measuring the [[Semantic similarity|similarity in word meanings]] <ref>{{harvnb|Lund|Burgess|Atchley|1995}}</ref><ref>{{harvnb|Landauer|Dumais|1997}}</ref><ref>{{harvnb|McDonald|Ramscar|2001}}</ref><ref>{{harvnb|Terra|Clarke|2003}}</ref>\u000a* Measuring the similarity in word relations <ref>{{harvnb|Turney|2006}}</ref>\u000a* Modeling [[similarity-based generalization]] <ref>{{harvnb|Yarlett|2008}}</ref>\u000a* Discovering words with a given relation <ref>{{harvnb|Hearst|1992}}</ref>\u000a* Classifying relations between words <ref>{{harvnb|Turney|Littman|2005}}</ref>\u000a* Extracting keywords from documents <ref>{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}</ref><ref>{{harvnb|Turney|2000}}</ref>\u000a* Measuring the cohesiveness of text <ref>{{harvnb|Turney|2003}}</ref>\u000a* Discovering the different senses of words <ref>{{harvnb|Pantel|Lin|2002}}</ref>\u000a* Distinguishing the different senses of words <ref>{{harvnb|Turney|2004}}</ref>\u000a* Subcognitive aspects of words <ref>{{harvnb|Turney|2001}}</ref>\u000a* Distinguishing praise from criticism <ref>{{harvnb|Turney|Littman|2003}}</ref>\u000a\u000a==Related fields==\u000a\u000aStatistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].\u000a\u000aMany of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.<ref>{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}</ref>\u000a\u000a==See also==\u000a{{Portal|Linguistics}}\u000a*[[Latent semantic analysis]]\u000a*[[Latent semantic indexing]]\u000a*[[Text mining]]\u000a*[[Information retrieval]]\u000a*[[Natural language processing]]\u000a*[[Computational linguistics]]\u000a*[[Web mining]]\u000a*[[Semantic similarity]]\u000a*[[Co-occurrence]]\u000a*[[Text corpus]]\u000a*[[Semantic Analytics]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a===Sources===\u000a{{refbegin}}\u000a* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}\u000a\u000a* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1\u201332 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}\u000a*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}\u000a\u000a* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668\u2013673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | id = {{citeseerx|10.1.1.43.9100}} {{citeseerx|10.1.1.148.3598}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753\u20131806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}\u000a\u000a* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539\u2013545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | id = {{citeseerx|10.1.1.36.701}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211\u2013240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | id = {{citeseerx|10.1.1.184.4759}} | ref = harv | doi=10.1037/0033-295x.104.2.211}}\u000a\u000a* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660\u2013665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}\u000a\u000a* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611\u2013616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | id = {{citeseerx|10.1.1.104.7535}} | ref = harv }}\u000a\u000a* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613\u2013619 | id = {{citeseerx|10.1.1.12.6771}} | doi = 10.1145/775047.775138 | ref = harv }}\u000a\u000a* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33\u201353 | ref = harv}}\u000a\u000a* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244\u2013251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | id = {{citeseerx|10.1.1.12.9041}} | doi = 10.3115/1073445.1073477 | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303\u2013336 | arxiv = cs/0212020 | id = {{citeseerx|10.1.1.11.1829}} | doi = 10.1023/A:1009976227802 | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409\u2013419 | arxiv = cs/0212015 | id = {{citeseerx|10.1.1.12.8734}} | ref = harv | doi=10.1080/09528130110100270}}\u000a\u000a* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434\u2013439 | arxiv = cs/0308033 | id = {{citeseerx|10.1.1.100.3751}} | ref = harv }}\u000a\u000a* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239\u2013242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379\u2013416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | id = {{citeseerx|10.1.1.75.8007}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] (TOIS) | volume = 21 | issue = 4 | pages = 315\u2013346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | id = {{citeseerx|10.1.1.9.6425}} | doi = 10.1145/944012.944013 | ref = harv }}\u000a\u000a* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1\u20133 | pages = 251\u2013278 | arxiv = cs/0508103 | id = {{citeseerx|10.1.1.90.9819}} | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}\u000a\u000a* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482\u2013489 | arxiv = cs/0309035 | id = {{citeseerx|10.1.1.5.2939}} | url = http://cogprints.org/3163/ | ref = harv }}\u000a\u000a* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15\u201323 | ref = harv }}\u000a\u000a* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}\u000a{{refend}}\u000a\u000a==External links==\u000a* {{cite web | url = http://www.si.umich.edu/people/george-furnas | work = Faculty Profile | title = George Furnas | publisher = University of Michigan, School of Information | accessdate = 2010-07-12 }}\u000a*[http://research.microsoft.com/%7Esdumais/ Susan Dumais]\u000a*[http://www.pearsonkt.com/bioLandauer.shtml Thomas Landauer]\u000a*[http://www.apperceptual.com/ Peter Turney]\u000a*[http://waldron.stanford.edu/~michael/papers/ Michael Ramscar]\u000a*[http://www.cs.ualberta.ca/~lindek/demos.htm Dekang Lin's Demos]\u000a*[http://www.isi.edu/~pantel/Content/demos.htm Patrick Pantel's Demos]\u000a*[http://www.nzdl.org/Kea/ Kea keyphrase extraction]\u000a*[http://seokeywordanalysis.com/seotools/ Online keyphrase extractor]\u000a\u000a{{DEFAULTSORT:Statistical Semantics}}\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Semantics]]\u000a[[Category:Statistical natural language processing]]\u000a[[Category:Fields of application of statistics]]
p165
asI221
(lp166
VIslamic World Science Citation Database
p167
aV'''Islamic World Science Citation Database''' (ISC) is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].\u000a\u000aIt was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.<ref>{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}</ref>  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].\u000a\u000aIn 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.<ref>{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}</ref>\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a==See also==\u000a* [[Academic publishing]]\u000a* [[List of academic databases and search engines]]\u000a* [[Impact factor]]\u000a\u000a== External links ==\u000a* {{Official website|http://www.isc.gov.ir/isce.htm}}\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Online databases]]\u000a[[Category:Citation indices]]\u000a[[Category:Research management]]\u000a[[Category:Databases in Iran]]\u000a\u000a\u000a{{science-journal-stub}}\u000a{{islam-stub}}
p168
asI222
(lp169
VMaterials Science Citation Index
p170
aV{{Third-party|date=February 2013}}\u000a'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>\u000a\u000a''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>\u000a\u000a==Editions==\u000aCoverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>\u000a*Materials Science, Ceramics\u000a*Materials Science, Characterization & Testing\u000a*Materials Science, Biomaterials\u000a*Materials Science, Coatings & Films\u000a*Materials Science, Composites\u000a*Materials Science, Paper & Wood\u000a*Materials Science, Multidisciplinary\u000a*Materials Science, Textiles\u000a\u000a==See also==\u000a* [[Science Citation Index]]\u000a* [[Academic publishing]]\u000a* [[List of academic databases and search engines]]\u000a* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956\u000a* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975\u000a* [[Impact factor]]\u000a* [[VINITI Database RAS]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a{{Thomson Reuters}}\u000a\u000a[[Category:Thomson family]]\u000a[[Category:Bibliographic databases]]\u000a[[Category:Online databases]]\u000a[[Category:Citation indices]]\u000a\u000a\u000a{{science-journal-stub}}
p171
asI223
(lp172
VSPIN bibliographic database
p173
aV{{Infobox Bibliographic Database\u000a|title =SPIN  (Searchable Physics Information Notices)  \u000a|image = \u000a|caption = \u000a|producer =[[American Institute of Physics]] (AIP) \u000a|country =USA, Russia, Ukraine\u000a|history = \u000a|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] \u000a|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] \u000a|cost = \u000a|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology \u000a|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   \u000a|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia\u000a|temporal =1975 to the present  \u000a|geospatial =International \u000a|number =over 1.5 million \u000a|updates =Weekly \u000a|p_title =No print counterparts \u000a|p_dates = \u000a|ISSN =\u000a|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp \u000a|titles =  \u000a}}\u000a\u000a'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>\u000a\u000a==Journals==\u000aDelivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web\u000a  | title =What is the SPIN database? \u000a  | work =Information about SPIN \u000a  | publisher =[[American Institute of Physics]] \u000a  | date =July 2010 \u000a  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>\u000a\u000a==Sources==\u000aOverall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web\u000a  | title =SPIN Publication Coverage \u000a  | work =Complete list of publications covered and coverage years. \u000a  | publisher =American Institute of Physics \u000a  | date =July 2010 \u000a  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>  \u000a\u000a==Scope==\u000aSubject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web\u000a  | title =Indexes and Databases \u000a  | work =SPIN: Searchable Physics Information Notices\u000a  | publisher =Raymond H. Fogler Library, The University of Maine\u000a  | date =October 2010 \u000a  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices \u000a  | format = \u000a  | accessdate =2010-07-12}}</ref>\u000a\u000a*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] \u000a*[[Atomic physics]] and [[Molecular physics]] \u000a*[[Biological physics]] and [[Medical physics]] \u000a*[[Classical physics]] and [[Quantum physics]] \u000a*[[Condensed matter physics]] \u000a*[[Elementary particle physics]] \u000a*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] \u000a*[[Geophysics]], [[Astronomy]], [[Astrophysics]] \u000a*[[Materials science]] \u000a*[[Nuclear physics]] \u000a*[[Plasma physics]] \u000a*[[Physical chemistry]]\u000a\u000a==See also==\u000a*[[List of academic databases and search engines]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.\u000a*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.\u000a*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.\u000a\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Bibliographic indexes]]\u000a[[Category:Citation indices]]\u000a[[Category:Scientific databases]]
p174
asI96
(lp175
VMulti-document summarization
p176
aV'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].\u000a\u000a==Key benefits==\u000aMulti-[[document summarization]] creates information reports that are both concise and comprehensive.\u000aWith different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.\u000aWhile the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\u000aAutomatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\u000a\u000a==Technological challenges==\u000aThe multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,<ref>http://www-nlpir.nist.gov/projects/duc/index.html</ref> conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.\u000a\u000aAn ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:\u000a*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections\u000a*text within sections is divided into meaningful paragraphs\u000a*gradual transition from more general to more specific thematic aspects\u000a*good [[readability]]\u000a\u000aThe latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:\u000a*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)\u000a*no dangling references to what is not mentioned or explained in the overview\u000a*no text breaks across a sentence\u000a*no semantic [[Redundancy (information theory)|redundancy]].\u000a\u000a==Real-life systems==\u000aThe multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.\u000a* Ultimate Research Assistant<ref>http://ultimate-research-assistant.com/</ref> - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. \u000a* iResearch Reporter<ref>http://www.iresearch-reporter.com/</ref> - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.\u000a* Newsblaster<ref>http://newsblaster.cs.columbia.edu</ref> is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.\u000a* NewsInEssence<ref>http://www.newsinessence.com</ref> may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.\u000a* NewsFeed Researcher<ref>http://newsfeedresearcher.com</ref> is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.\u000a* Scrape This<ref>http://www.scrapethis.com</ref> is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.\u000a* JistWeb<ref>http://www.jastatechnologies.com/productList.html</ref> is a query specific multiple document summariser.\u000a\u000aAs auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.\u000a\u000a==Bibliography==\u000a* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]\u000a* Dragomir R. Radev, Hongyan Jing, Malgorzata Sty\u015b, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919\u2013938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]\u000a* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74\u201382, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]\u000a* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&nbsp;457\u2013464, 2002\u000a*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR\u201905, Salvador, Brazil, August 15\u201319, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]\u000a*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&nbsp;35\u201355, 2002\u000a*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9\u201310, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]\u000a* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&nbsp;724\u2013728. Springer Berlin Heidelberg, 2009.\u000a\u000a==See also==\u000a* [[Automatic summarization]]\u000a* [[Text mining]]\u000a* [[News aggregators]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{External links|date=September 2010}}\u000a*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]\u000a*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]\u000a*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]\u000a\u000a{{Natural Language Processing}}\u000a\u000a{{DEFAULTSORT:Multi-Document Summarization}}\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p177
asI225
(lp178
VCitation index
p179
aVA '''citation index''' is a kind of [[bibliographic database]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]].\u000a\u000a==History==\u000a\u000aThe earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study.  The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998,  p. 51''ff''</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.\u000a\u000aIn English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name='shapiro'/>\u000a\u000aThe first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].<ref name='shapiro'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)</ref>\u000a\u000a==Major citation indexing services==\u000a{{main|Indexing and abstracting service}}\u000a{{main cat|Citation indices}}\u000a\u000aGeneral-purpose academic citation indexes include:\u000a\u000a*ISI (now part of [[Thomson Reuters]]) publishes the ISI citation indexes in print and [[compact disc]]. They are now generally accessed through the Web under the name '' [[Web of Science]]'', which is in turn part of the group of databases in the ''[[Web of Knowledge]].''\u000a*[[Elsevier]] publishes [[Scopus]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].\u000a*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.\u000aEach of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: the ISI databases and Scopus are available by subscription (generally to libraries).\u000a\u000aIn addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.\u000a\u000a==Citation analysis==\u000a{{main|Citation analysis}}\u000a{{merge|section=yes|Citation analysis|date=December 2013}}\u000a{{duplication|dupe=Citation analysis|date=December 2013}}\u000a\u000aWhile citation indexes were originally designed for [[information retrieval]], they are increasingly used for [[bibliometrics]] and other studies involving research evaluation. Citation data is also the basis of the popular [[journal impact factor]].\u000a\u000aThere is a large body of literature on [[citation analysis]], sometimes called [[scientometrics]], a term invented by [[Vasily Nalimov]], or more specifically [[bibliometrics]]. The field blossomed with the advent of the [[Science Citation Index]], which now covers source literature from 1900 on. The leading journals of the field are ''[[Scientometrics]],'' ''Informetrics,'' and the ''[[Journal of the American Society of Information Science and Technology]]''. [[American Society for Information Science and Technology|ASIST]] also hosts an [[electronic mailing list]] called SIGMETRICS at  ASIST.<ref>{{cite web | title=The American Society for Information Science & Technology | work=The Information Society for the Information Age | url=http://www.asis.org| accessdate=2006-05-21}}</ref> This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as  [[CiteBase]],  [[CiteSeerX]], [[Google Scholar]], and the former [[Windows Live Academic]] (now available with extra features as [[Microsoft Academic Search]]).\u000a\u000a[[Legal citation]] analysis is a citation analysis technique for analyzing [[legal documents]] to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a [[citation graph]] extracted from a regulatory document, which could supplement [[E-discovery]] - a process that leverages on technological innovations in [[big data analytics]].<ref>[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=5070630&tag=1 ]{{dead link|date=December 2013}}</ref><ref>Mohammad Hamdaqa and A. Hamou-Lhadj, "Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents",  In Proc. of the 6th International Conference on Information Technology, Las Vegas, USA</ref><ref name=BD-HB-R-01>{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez</ref><ref name=BD-HB-R-02>{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology - Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}</ref>\u000a\u000a===History===\u000aIn a 1965 paper, [[Derek J. de Solla Price]] described the inherent linking characteristic of the SCI as "Networks of Scientific Papers".<ref>{{cite journal | author=Derek J. de Solla Price | title=Networks of Scientific Papers | journal=[[Science (journal)|SCIENCE]] | date=July 30, 1965 | volume=149 | issue=3683| pages=510&ndash;515 | url=http://garfield.library.upenn.edu/papers/pricenetworks1965.pdf | pmid=14325149 | doi=10.1126/science.149.3683.510|format=PDF}}</ref> The links between citing and cited papers became dynamic when the SCI began to be published online. The [[Social Sciences Citation Index]] became one of the first databases to be mounted on the [[Dialog]] system<ref>{{cite web | title=Dialog, A Thomson Business | work="Dialog invented online information services" | url=http://www.dialog.com| accessdate=2006-05-21}}</ref> in 1972. With the advent of the [[CD-ROM]] edition, linking became even easier and enabled the use of [[bibliographic coupling]] for finding related records. In 1973, Henry Small published his classic work on [[Co-Citation analysis]] which became a [[self-organizing]] classification system that led to [[document clustering]] experiments and eventually an "Atlas of Science" later called "Research Reviews".\u000a\u000aThe inherent topological and graphical nature of the worldwide citation network which is an inherent property of the [[scientific literature]] was described by [[Ralph Garner]] ([[Drexel University]]) in 1965.<ref>http://www.garfield.library.upenn.edu/rgarner.pdf</ref>\u000a\u000aThe use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts  to rank authors and [[academic paper|papers]].  In a landmark paper of 1965 he and [[Irving Sher]] showed the correlation between citation frequency and eminence in demonstrating that [[Nobel Prize]] winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon.  The usual summary measure is known as [[impact factor]], the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposes\u2014in particular, the use of this measure alone for  ranking authors and papers is therefore [[Impact factor#|quite controversial.]]\u000a\u000aIn an early study in 1964 of the use of Citation Analysis in writing the history of [[DNA]], Garfield and Sher demonstrated the potential for generating [[historiograph]]s, [[topological map]]s of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, [[A. I. Pudovkin]] of the [[Institute of Marine Biology]], [[Russian Academy of Sciences]] and [[V. S. Istomin]] of [[Center for Teaching, Learning, and Technology]], [[Washington State University]] and led to the creation of the [[Histcite|HistCite]] <ref>{{cite web | author=Eugene Garfield, A. I. Pudovkin,  V. S. Istomin | year=2002 | title=Algorithmic Citation-Linked Historiography\u2014Mapping the Literature of Science | work=Presented the ASIS&T 2002: Information, Connections and Community. 65th Annual Meeting of ASIST in Philadelphia, PA. November 18\u201321, 2002  | url=http://www.garfield.library.upenn.edu/papers/asis2002/asis2002presentation.html | accessdate=2006-05-21}}</ref> software around 2002.\u000a\u000aAutomatic citation indexing was introduced in 1998 by [[Lee Giles]], [[Steve Lawrence]] and [[Kurt Bollacker]] <ref>C.L. Giles, K. Bollacker, S. Lawrence, "CiteSeer: An Automatic Citation Indexing System," DL'98 Digital Libraries, 3rd ACM Conference on Digital Libraries, pp. 89-98, 1998.</ref> and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being [[CiteSeer]] (now [[CiteSeerX]], soon followed by Cora, which focused primarily on the field of [[computer science]] and [[information science]]. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as [[Ann Arbor, Michigan|Ann Arbor]], [[Milton Keynes]], and [[Walton Hall, Milton Keynes|Walton Hall]] being credited with extensive academic output.<ref name="pmid18354457">{{cite journal |author=Postellon DC |title=Hall and Keynes join Arbor in the citation indexes |journal=[[Nature (journal)|Nature]] |volume=452 |issue=7185 |page=282 |date=March 2008 |pmid=18354457 |doi=10.1038/452282b}}</ref>  SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.\u000a\u000a==See also==\u000a* [[Impact factor]]\u000a* [[Citation impact]]\u000a* [[Eigenfactor]]\u000a* [[Microsoft Academic Search]]\u000a* [[Google Scholar]]\u000a* [[Scopus]]\u000a* [[H-index]] or [[Hirsch number]]\u000a* [[Citation analysis]]\u000a* [[Acknowledgment index]]\u000a* [[CiteSeer]]\u000a* [[CiteSeerX]]\u000a* [[Scientific journal]]\u000a* [[Science Citation Index]]\u000a* [[Indian Citation Index]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links ==\u000a* Official [http://admin-apps.isiknowledge.com/JCR/JCR Journal Citation Report] from the [http://www.isinet.com ISI website]\u000a* [http://www.librijournal.org/2005-4toc.html Google Scholar: The New Generation of Citation Indexes]\u000a* [http://www.atlasofscience.net/ Atlas of Science: Mapping Science by means of citation relations]\u000a* [http://www.dlib.org/dlib/september05/bauer/09bauer.html An Examination of Citation Counts in a New Scholarly Communication Environment]\u000a* [http://cids.fc.ul.pt/ CIDS] online tool that calculates the h-index and [[g-index]] based on [[Google Scholar]] data and discerning self-citations\u000a\u000a{{DEFAULTSORT:Citation Index}}\u000a[[Category:Academic publishing]]\u000a[[Category:Bibliometrics]]\u000a[[Category:Library science]]\u000a[[Category:Reputation management]]\u000a[[Category:Citation indices]]
p180
asI99
(lp181
VConference and Labs of the Evaluation Forum
p182
aVThe '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research  in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to  maintain an underlying framework for testing [[information retrieval]] systems, and creating [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].<ref name="Peters">{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | id = {{citeseerx|10.1.1.109.7647}} }}</ref>\u000aThe organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,<ref>{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1\u20132 | year = 2004 }}</ref><ref>Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing & Management''\u000avol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}</ref>  \u000a\u000aFor example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.<ref name="ImageCLEFmed">{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://www.clef-campaign.org CLEF homepage]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Compu-conference-stub}}
p183
asI228
(lp184
VLatin American Bibliography
p185
aV{{Multiple issues|\u000a{{orphan|date=March 2010}}\u000a{{advert|date=August 2010}}\u000a}}\u000a\u000aThe '''Latin American Bibliography''' refers to the set of [[databases]] and information services on [[academic journals]] from [[Latin America]] and the [[Caribbean]] created by the [[National Autonomous University of Mexico]] (UNAM) in the decade of the seventies.{{Clarify|date=August 2009}}\u000a\u000aNowadays, the Latin-American Bibliography is composed by the following databases: CLASE (''Latin-American Citations in [[Social Sciences]] and [[Humanities]]''); PERIODICA (''Index of Latin-American Journals in [[Science]]''); [[Latindex]] (''Regional Co-operative Information System for Scholarly Journals from [[Latin America]], the Caribbean, Spain and Portugal'').\u000a\u000aThese databases were created by a group of information professionals, who identified the need to register, preserve and give access to the Latin-American knowledge published in the main academic [[Academic journal|journals]] of the region. Within UNAM, the fostering institution of these information products was the Science and Humanities Information Center (CICH) created in 1971.\u000a\u000aFor the size of its collection of Latin-American journals, for the quantity of compiled records and for the duration and consistency of the project, the Latin-American Bibliography produced in the UNAM constitutes one of the most valuable resources for scholars and experts specializing in Latin-American affairs.{{Citation needed|date=August 2009}}\u000a\u000a==Products==\u000a\u000aThree databases are available through the web site of UNAM\u2019s General Directorate for Libraries [http://dgb.unam.mx General Directorate for Libraries]:\u000a\u000a'''CLASE''' (''Latin-American Citations in Social Sciences and Humanities''). Bibliographical database, with more than 280,000 records, of which nearly 14,000 provide abstracts and links to the full text of the documents. It includes more than 1,400 journals specializing in Social Sciences, Humanities and Arts, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/clase.html CLASE website]\u000a\u000a'''PERIODICA''' (Index of Latin-American Journals in Science). Bibliographical database with more than 315,000 records, of which near 60,000 provide abstracts and links to the full text of the documents. The database indexes more than 1,500 journals specializing in Science and Technology, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/periodica.html PERIODICA website]\u000a\u000a'''[[Latindex]]''' (Regional Co-operative Information System for Scholarly Journals from Latin America, the Caribbean, Spain and Portugal). This initiative provides relevant information and data of the scholarly journals edited in the [[Iberoamerica]]n region. Three databases are produced through the collaborative work of the member institutions: '''Directory''',: with more than 17,000 records; '''Catalogue''', with more than 3,500 selected journals that fulfill international quality criteria and an '''Index of Electronic Journals''', offering nearly 3,000 links to available resources in full text. Direct link: [http://www.latindex.org Latindex website]\u000a\u000aCurrently, the Department of Latin-American Bibliography contributes to the production of two other Latin-American information products:\u000a\u000a'''ASFA''' (''Aquatic Sciences and Fisheries Abstracts''). Bibliographical international database on Aquatic Sciences and [[Fisheries]], covering subject areas such as [[technology]] and [[Public administration|administration]] of the marine environments and its resources (salt and sweet waters), including its socioeconomic and juridical aspects. It offers abstracts of articles published in approximately 7,000 periodic publications, besides thesis, monographs and other not conventional literature. The contribution relative to the Mexican journals is produced in the Department of Latin-American Bibliography from 1981. Link: [http://www.fao.org/fishery/asfa ASFA website]\u000a\u000a'''[[SciELO]] Mexico''' (''Scientific Electronic Library Online''). Open access electronic journals collection that includes a selection of the most recognized academic publications of the country in all areas of knowledge, previously selected accordingly to the most accepted criteria related to content and editorial standards. Currently it offers the full text of more than 2,500 articles from 28 academic Mexican journals. Direct link: [http://www.scielo.org.mx/scielo.php Scielo México website]\u000a\u000aOver the time, other databases were produced by the Department of Latin-American Bibliography during its more than 30 years of existence, namely:\u000a\u000a'''BLAT''' (''Latin-American Bibliography I and II''), with information compiled from international sources, mainly documents from Latin-American origin (produced by Latin American authors and institutions) or those in which their object of study was related to the region. The database ceased in 1997. Another one was '''MEXINV''', as a subset of CLASE, offered bibliographical records of documents relative only to [[Mexico]]. This database ceased in the decade of the nineties.\u000a\u000a===Institution===\u000a\u000aCurrently, the databases described above are produced by the Department of Latin-American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). The original databases (BLAT, CLASE, PERIODICA, MEXINV and Latindex) were created by the Science and Humanities Information Center (CICH). Since the incorporation of the CICH to UNAM\u2019s General Directorate for Libraries in 1997, this institution acts as Responsible Editor.\u000a\u000a==References==\u000a\u000a*Alonso Gamboa, José Octavio. Servicios, productos, docencia e investigación en información: la experiencia del Centro de Información Científica y Humanística de la Universidad Nacional Autónoma de México. Ciencias de la Información, vol. 24, no. 4, diciembre, 1993. p.&nbsp;201-208. URL: [http://www.bibliociencias.cu/gsdl/cgi-bin/library?e=d-000-00---0revistas--00-0-0--0prompt-10---4------0-1l--1-es-50---20-about---00031-001-1-0utfZz-8-00&cl=CL2.772&d=HASH01caacf727585263378aa110&x=1]\u000a\u000a*Alonso Gamboa, José Octavio. Accesso a revistas latinoamericanas en Internet. Una opción a través de las bases de datos Clase y Periódica. Ciencia da Informação, vol. 27, no. 1, Janeiro-abril, 1998, p.&nbsp;90-95. URL: http://www.scielo.br/pdf/ci/v27n1/12.pdf\u000a\u000a*Alonso Gamboa, José Octavio y Felipe Rafael Reyna Espinosa. Compilación de datos bibliométricos regionales usando las bases de datos CLASE y PERIÓDICA. Revista Interamericana de Bibliotecología, 2005. Vol. 28, no. 1, enero-junio: 63-78. URL: http://bibliotecologia.udea.edu.co/revinbi/Numeros/2801/doc3_28.html\u000a\u000a*Russell, Jane M.; Madera-Jaramillo, María J.; Hernández- García, Yoscelina y Ainsworth, Shirley. Mexican collaboration networks in the international and regional arenas. En: Kretschmer, H. & Havemann, F. (Eds.): Proceedings of WIS 2008, Berlin. Fourth International Conference on Webometrics, Informetrics and Scientometrics & Ninth COLLNET Meeting, Humboldt-Universität zu Berlin, Institute for Library and Information Science (IBI). URL: http://www.collnet.de/Berlin-2008/RussellWIS2008mcn.pdf\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Scientific databases]]\u000a[[Category:Citation indices]]
p186
asI101
(lp187
VAdversarial information retrieval
p188
aV'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.\u000a\u000aOn the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].<ref>B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]</ref>\u000a\u000aActivities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.\u000a\u000a== Topics ==\u000aTopics related to Web spam (spamdexing):\u000a\u000a* [[Link spam]]\u000a* [[Keyword spamming]]\u000a* [[Cloaking]]\u000a* Malicious tagging\u000a* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]\u000a\u000aOther topics:\u000a* [[Click fraud]] detection\u000a* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm\u000a* Web [[content filtering]]\u000a* [[Ad filtering|Advertisement blocking]]\u000a* Stealth [[web crawling|crawling]]\u000a*[[Troll (Internet)]]\u000a* Malicious tagging or voting in [[social networks]]\u000a* [[Astroturfing]]\u000a* [[Sockpuppetry]]\u000a\u000a== History ==\u000aThe term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.<ref>D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]</ref>\u000a\u000a== See also ==\u000a*[[Spamdexing]]\u000a*[[Information retrieval]]\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a== External links ==\u000a*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web\u000a*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection\u000a*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection\u000a\u000a{{DEFAULTSORT:Adversarial Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet fraud]]\u000a[[Category:Searching]]
p189
asI17
(lp190
VCategory:Electronic documents
p191
aV[[Category:Documents]]\u000a[[Category:Digital media]]\u000a[[Category:Information retrieval]]\u000a[[Category:Electronic publishing]]
p192
asI104
(lp193
VExtended Boolean model
p194
aVThe '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.<ref>	\u000a{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last=Salton | first=Gerard | coauthors=Edward A. Fox, Harry Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}</ref>\u000a\u000aThus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.\u000a\u000a==Definitions==\u000aIn the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.\u000a\u000aThe weight of term {{math|''K<sub>x</sub>''}} associated with document {{math|''d<sub>j</sub>''}} is measured by its normalized [[Term frequency]] and can be defined as:\u000a\u000a<math>\u000aw_{x,j}=f_{x,j}*\u005cfrac{Idf_{x}}{max_{i}Idf_{i}}\u000a</math>\u000a\u000awhere {{math|''Idf<sub>x</sub>''}} is [[inverse document frequency]].\u000a\u000aThe weight vector associated with document {{math|''d<sub>j</sub>''}} can be represented as:\u000a\u000a<math>\u005cmathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \u005cldots, w_{i,j}]</math>\u000a\u000a==The 2 Dimensions Example==\u000a{{multiple image\u000a | width     = 150\u000a | image1    = 2D_Extended_Boolean_model_OR_example.png\u000a | alt1      = Figure 1\u000a | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\u000a | image2    = 2D_Extended_Boolean_model_AND_example.png\u000a | alt2      = Figure 2\u000a | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\u000a}}\u000a\u000aConsidering the space composed of two terms {{math|''K<sub>x</sub>''}} and {{math|''K<sub>y</sub>''}} only, the corresponding term weights are {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}}.<ref>[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]</ref>  Thus, for query {{math|''q<sub>or</sub>'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}}, we can calculate the similarity with the following formula:\u000a \u000a<math>sim(q_{or},d)=\u005csqrt{\u005cfrac{w_1^2+w_2^2}{2}}</math>\u000a\u000aFor query {{math|''q<sub>and</sub>'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}}, we can use:\u000a\u000a<math>sim(q_{and},d)=1-\u005csqrt{\u005cfrac{(1-w_1)^2+(1-w_2)^2}{2}}</math>\u000a\u000a==Generalizing the idea and P-norms==\u000aWe can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.\u000a\u000aThis can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &le; ''p'' &le; &infin;}} is a new parameter.<ref>{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}</ref>\u000a\u000a*A generalized conjunctive query is given by:\u000a:<math>q_{or}=k_1 \u005clor^p k_2 \u005clor^p .... \u005clor^p k_t  </math>\u000a\u000a*The similarity of <math>q_{or}</math> and <math>d_j</math> can be defined as:\u000a''':<math>sim(q_{or},d_j)=\u005csqrt[p]{\u005cfrac{w_1^p+w_2^p+....+w_t^p}{t}}</math>'''\u000a\u000a*A generalized disjunctive query is given by:\u000a:<math>q_{and}=k_1 \u005cland^p k_2 \u005cland^p .... \u005cland^p k_t  </math>\u000a\u000a*The similarity of <math>q_{and}</math> and <math>d_j</math> can be defined as:\u000a:<math>sim(q_{and},d_j)=1-\u005csqrt[p]{\u005cfrac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}</math>\u000a\u000a==Examples==\u000aConsider the query {{math|''q'' {{=}} (''K''<sub>1</sub> &and; ''K''<sub>2</sub>) &or; ''K''<sub>3</sub>}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:\u000a\u000a<math>sim(q,d)=\u005csqrt[p]{\u005cfrac{(1-\u005csqrt[p]{(\u005cfrac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}</math>\u000a\u000a==Improvements over the Standard Boolean Model==\u000a\u000aLee and Fox<ref>{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.\u000aUsing P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.<br>\u000aThe P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.\u000a\u000a==Further reading==\u000a* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]\u000a* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VC8-454T5MS-2&_user=513551&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1117914301&_rerunOrigin=google&_acct=C000025338&_version=1&_urlVersion=0&_userid=513551&md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]\u000a* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\u000a* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first= Lucie | last= Skorkovská | coauthors=Pavel Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}\u000a\u000a==See also==\u000a*[[Information retrieval]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Extended Boolean Model}}\u000a[[Category:Information retrieval]]
p195
asI106
(lp196
VPolicy framework
p197
aV{{refimprove|date=March 2009}}\u000aA '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.\u000a\u000a==Principles==\u000a[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.<ref>http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm</ref>\u000a\u000a===Availability===\u000aGovernment departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....\u000a\u000a===Coverage===\u000aGovernment departments should make the following information increasingly available on an electronic basis:\u000a* all published material or material already in the public domain\u000a* all policies that could be released publicly\u000a* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)\u000a* all documents that the public may be required to complete\u000a* corporate documentation in which the public would be interested\u000a\u000a===Pricing=== \u000aa) Free dissemination of Government-held information is appropriate where:\u000a* dissemination to a target audience is desirable for a public policy purpose, or\u000a* a charge to recover the cost of dissemination is not feasible or cost-effective\u000a\u000ab) Pricing to recover the cost of dissemination is appropriate where:\u000a* there is no particular public policy reason to disseminate the information, and \u000a* a charge to recover the cost of dissemination is both feasible and cost effective\u000a\u000ac) Pricing to recover the cost of transformation is appropriate where:\u000a* pricing to recover the cost of dissemination is appropriate, and\u000a* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination\u000a\u000ad) Pricing to recover the full costs of information production and dissemination is appropriate where:\u000a* the information is created for the commercial purpose of sale at a profit, and \u000a* to do so would not breach the other pricing principles\u000a\u000a===Ownership===\u000aGovernment-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.\u000a\u000a===Stewardship===\u000aGovernment departments are stewards of Government-held information, and it is their responsibility to implement good information management.\u000a\u000a===Collection===\u000aGovernment departments should only collect information for specified public policy, operational business or legislative purposes.\u000a\u000a===Copyright===\u000aInformation created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.\u000a \u000a===Preservation===\u000aGovernment-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.\u000a\u000a===Quality===\u000aThe key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.\u000a\u000a===Integrity===\u000aThe integrity of Government-held information will be achieved when:\u000a* all guarantees and conditions surrounding the information are met\u000a* the principles are clear and communicated\u000a* any situation relating to Government-held information is handled openly and consistently\u000a* those affected by changes to Government-held information are consulted on those changes\u000a* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well\u000a* there are minimum exceptions to the principles.\u000a\u000a===Privacy===\u000aThe principles of the Privacy Act 1993 apply.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Policy Framework}}\u000a[[Category:Information retrieval]]\u000a[[Category:Government of New Zealand]]
p198
asI108
(lp199
VDatanet
p200
aV{{Use mdy dates|date=September 2011}}\u000a''This article is about the U.S. National Science Foundation Office of Cyberinfrastructure .\u000a\u000aOn September 28, 2007, the U.S. [[National Science Foundation]] Office of Cyberinfrastructure announced a request for proposals with the name '''Sustainable Digital Data Preservation and Access Network Partner (DataNet)'''.<ref name="datanetprogram">{{cite web\u000a|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref>  The lead paragraph of its synopsis describes the program as:\u000a\u000a<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>\u000a\u000aThe introduction in the solicitation<ref name="datanetsolicitation">{{cite web\u000a|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref> goes on to say:\u000a\u000a<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF\u2019s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which \u201cscience and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.\u201d The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>\u000a\u000aThe initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener et al |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury et al |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. \u000a\u000aFor the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] et al |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] et al |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] et al |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a==External links==\u000a* [http://www.dataone.org DataONE]\u000a* [http://dataconservancy.org/ Data Conservancy]\u000a* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]\u000a* [http://datafed.org/ DataNet Federation Consortium]\u000a* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] \u000a \u000a\u000a[[Category:National Science Foundation]]\u000a[[Category:Science and technology in the United States]]\u000a[[Category:Information retrieval]]\u000a[[Category:Digital library projects]]
p201
asI109
(lp202
VStemming
p203
aV{{Expert-subject|date=October 2010}}\u000a{{for|the skiing technique|Stem (skiing)}}\u000a'''Stemming''' is the term used in [[linguistic morphology]] and [[information retrieval]] to describe the process for reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form\u2014generally a written word form. The stem needs not to be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.\u000a\u000aStemming programs are commonly referred to as stemming algorithms or stemmers.\u000a\u000a==Examples==\u000aA stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".<!-- using the Porter algorithm -->\u000a\u000a==History==\u000aThe first published stemmer was written by [[Julie Beth Lovins]] in 1968.<ref>{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22\u201331 }}</ref> This paper was remarkable for its early date and had great influence on later work in this area.\u000a\u000aA later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.\u000a\u000aMany implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [http://tartarus.org/~martin/PorterStemmer/ free-software implementation] of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.\u000a\u000a==Algorithms==\u000aThere are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.\u000a\u000a===Lookup algorithms===\u000aA simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach is that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.\u000a\u000aA lookup approach may use preliminary part-of-speech tagging to avoid overstemming.<ref>Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']</ref>\u000a\u000a====The production technique====\u000aThe lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.\u000a\u000a===Suffix-stripping algorithms===\u000aSuffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:\u000a* if the word ends in 'ed', remove the 'ed'\u000a* if the word ends in 'ing', remove the 'ing'\u000a* if the word ends in 'ly', remove the 'ly'\u000a\u000aSuffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.\u000a\u000aPrefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.\u000a\u000a====Additional algorithm criteria====\u000aSuffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.\u000a\u000aIt can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.\u000a\u000aOne improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.\u000a\u000aDiving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.\u000a\u000aThis example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.\u000a\u000a===Lemmatisation algorithms===\u000aA more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.\u000a\u000aThis approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).\u000a\u000a===Stochastic algorithms===\u000a[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).\u000a\u000aSome lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.\u000a\u000a===''n''-gram analysis===\u000aSome stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.<ref name="Workshop2006">{{cite book|author=Cross-Language Evaluation Forum. Workshop|title=Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005, Vienna, Austria, 21-23 September, 2005, Revised Selected Papers|url=http://books.google.com/books?id=jWKzNGr1H0AC&pg=PA159|date=20 September 2006|publisher=Springer|isbn=978-3-540-45697-1|pages=159\u2013}}</ref>\u000a\u000a===Hybrid approaches===\u000aHybrid approaches use two or more of the approaches described above in unison. A simple example is a [[probabilistic suffix tree|suffix tree]] algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.\u000a\u000a===Affix stemmers===\u000aIn [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.<ref>Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2\u20137, 2009'', pp. 145-153\u000a[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]</ref>\u000a\u000a===Matching algorithms===\u000aSuch algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").\u000a\u000a==Language challenges==\u000aWhile much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']</ref><ref>Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2</ref><ref>Popovi\u010d, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384\u2013390</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']</ref><ref>Viera, A. F. G. & Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revisão dos algoritmos de radicalização em língua portuguesa''], Information Research, 12(3), paper 315</ref>\u000a\u000aHebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.\u000a\u000a===Multilingual stemming===\u000aMultilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{CN|date=October 2013}}.\u000a\u000a==Error metrics==\u000aThere are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been\u2014a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not\u2014a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.\u000a\u000aFor example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.\u000a\u000aAn example of understemming in the Porter stemmer is "alumnus" \u2192 "alumnu", "alumni" \u2192 "alumni", "alumna"/"alumnae" \u2192 "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.\u000a\u000a==Applications==\u000aStemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".\u000a\u000a===Information retrieval===\u000aStemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.<ref>Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley</ref> An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, recent research has shown greater benefits for retrieval in other languages.<ref>Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjörnsson, Börkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152\u2013165</ref><ref>Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249\u2013271</ref>\u000a\u000a===Domain Analysis===\u000aStemming is used to determine domain vocabularies in [[domain analysis]].\u000a<ref>Frakes, W.; Prieto-Diaz, R.; & Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141</ref>\u000a\u000a===Use in commercial products===\u000aMany commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.<ref>[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch</ref><ref>[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet</ref>\u000a\u000aThe Snowball stemmers have been compared with commercial lexical stemmers with varying results.<ref>[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]</ref><ref>[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]</ref>\u000a\u000a[[Google search]] adopted word stemming in 2003.<ref>[http://www.google.com/support/bin/static.py?page=searchguides.html&ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]</ref> Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".\u000a\u000a==See also==\u000a* [[Root (linguistics)]] - linguistic definition of the term "root"\u000a* [[Stem (linguistics)]] - linguistic definition of the term "stem"\u000a* [[Morphology (linguistics)]]\u000a* [[Lemma (morphology)]] - linguistic definition\u000a* [[Lemmatization]]\u000a* [[Lexeme]]\u000a* [[Inflection]]\u000a* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation\u000a* [[Natural language processing]] - stemming is generally regarded as a form of NLP\u000a* [[Text mining]] - stemming algorithms play a major role in commercial NLP software\u000a* [[Computational linguistics]]\u000a\u000a{{Natural Language Processing}}\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==Further reading==\u000a{{refbegin|2}}\u000a* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33\u201346\u000a* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press\u000a* Frakes, W. B. & Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 26\u201330\u000a* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.\u000a* Hafer, M. A. & Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing & Management 10 (11/12), 371\u2013386\u000a* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 7\u201315\u000a* Hull, D. A. (1996); ''Stemming Algorithms&nbsp;\u2013 A Case Study for Detailed Evaluation'', JASIS, 47(1): 70\u201384\u000a* Hull, D. A. & Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report\u000a* Kraaij, W. & Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18\u201322'', pp.&nbsp;40\u201348\u000a* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&nbsp;191\u2013203\u000a* Lennon, M.; Pierce, D. S.; Tarry, B. D.; & Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177\u2013183\u000a* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 28\u201340\u000a* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 22\u201431\u000a* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']\u000a* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 56\u201361\u000a* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632\u2013649\u000a* Popovi\u010d, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&nbsp;384\u2013390\u000a* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130\u2013137\u000a* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 1\u20139\u000a* Ulmschneider, John E.; & Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301\u2013318\u000a* Xu, J.; & Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 61\u201381\u000a{{refend}}\u000a\u000a==External links==\u000a*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers\u000a* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)\u000a* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)\u000a* [http://snowball.tartarus.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages\u000a* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)\u000a* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]\u000a* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API\u000a* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API\u000a* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD\u000a* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages\u000a* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages\u000a* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK\u000a* [http://www.cmp.uea.ac.uk/Research/stemmer/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK\u000a* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]\u000a* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language\u000a* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages\u000a* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java\u000a* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi\u000a* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech\u000a* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]\u000a* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]\u000a\u000a{{FOLDOC}}\u000a\u000a[[Category:Linguistic morphology]]\u000a[[Category:Natural language processing]]\u000a[[Category:Tasks of natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p204
asI93
(lp205
VCategory:Music search engines
p206
aV[[Category:Information retrieval]]\u000a[[Category:Music software|Search engines]]\u000a[[Category:Internet search engines]]\u000a[[Category:Online music and lyrics databases]]
p207
asI111
(lp208
VXML retrieval
p209
aV{{Multiple issues|\u000a{{expert-subject|Computer science|date=January 2015}}\u000a{{COI|date=February 2009}}\u000a}}\u000a\u000a'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.<ref>{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}</ref>\u000a\u000a==Queries==\u000aMost XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.\u000a\u000a==Exploiting XML structure==\u000aTaking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.\u000a\u000a==Ranking==\u000aRanking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.<ref name="INEX2006">{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf|title=Overview of INEX 2006|last=Malik|first=Saadia|author2=Trotman, Andrew |author3=Lalmas, Mounia |author4= Fuhr, Norbert |year=2007|work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval|accessdate=2009-02-10}}</ref>\u000a\u000a==Existing XML search engines==\u000aAn overview of two potential approaches is available.<ref>{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{Cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5986&rep=rep1&type=pdf|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR|accessdate=2013-07-04}}</ref> The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.<ref name="INEX2006" /> Three different areas influence XML-Retrieval:<ref name="INEX2002">{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf|title=INEX: Initiative for the Evaluation of XML Retrieval|last=Fuhr|first=Norbert|author2=Gövert, N. |author3=Kazai, Gabriella |author4= Lalmas, Mounia |year=2003|work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002|publisher=ERCIM Workshop Proceedings, France|accessdate=2009-02-10}}</ref>\u000a\u000a===Traditional XML query languages===\u000a[[Query language]]s such as the [[W3C]] standard [[XQuery]]<ref>{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}</ref> supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].<ref name="Schlieder2002">{{Cite journal|url=http://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz|title=Querying and Ranking XML Documents|last=Schlieder|first=Torsten|author2=Meuss, Holger |year=2002|work= Journal of the American Society for Information Science and Technology, Vol. 53, No. 6|accessdate=2009-02-10}}</ref>\u000a\u000a===Databases===\u000aClassic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]<ref name="INEX2002" /> and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.\u000a\u000a===Information retrieval===\u000aClassic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.<ref name="Schlieder2002"/> They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.<ref>{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}</ref>\u000a\u000a==See also==\u000a*[[Document retrieval]]\u000a*[[Information retrieval applications]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a{{DEFAULTSORT:Xml-Retrieval}}\u000a[[Category:XML]]\u000a[[Category:Information retrieval]]
p210
asI232
(lp211
VQuery by humming
p212
aV{{inline|date=August 2012}}\u000a'''Query by humming''' ('''QbH''') is a music retrieval system that branches off the original classification systems of title, artist, composer, and genre. It normally applies to songs or other music with a distinct single theme or melody. The system involves taking a user-hummed [[melody]] (input [[Information retrieval|query]]) and comparing it to an existing [[database]].  The system then returns a ranked list of music closest to the input query. \u000a\u000aOne example of this would be a system involving a [[portable media player]] with a built-in [[microphone]] that allows for faster [[Search engine technology|searching]] through [[Digital media|media]] files.\u000a\u000aThe [[MPEG-7]] standard includes provisions for QbH music searches.\u000a\u000a== Examples of QbH systems ==\u000aSoundHound and Midomi are the only commercially available query by humming services available online at Midomi.com or on the mobile app called SoundHound. \u000aBoth are powered by the same backend and are capable of recognizing humming and singing as well as recorded tracks. \u000aFor the singing and humming search, the searchable database is based on Midomi.com's user contributions. Midomi has collected about one million tracks based on user contributions in multiple languages, making it the largest database of its kind by a large margin. The top four languages are: English, Japanese, Chinese and Spanish. \u000a\u000a"Musipedia" is an example of a QbH system that uses a variety of input methods such as humming, tapping the keyboard, keyboard search (a virtual piano keyboard), draw notes, and a contour search, using [[Parsons_code|Parsons Code]] to encode the music pieces.\u000a\u000a[[Tunebot]] is a music search engine that uses queries from humming, lyrics, and melody. People can contribute to the database and expand the variety of searchable songs. Tunebot also serves as the back-end for a game called [[Karaoke Callout]], in which players' performances are compared by the engine with songs in the database.\u000a\u000a== External links ==\u000a===Online demos===\u000a* [http://www.midomi.com/ Midomi]\u000a* [http://www.soundhound.com/ SoundHound (mobile app)] \u000a* [http://www.musipedia.org/query_by_humming.0.html QbH system] from Musipedia\u000a* [http://querybyhum.cs.nyu.edu/ QbH research project at NYU]\u000a* [http://www.sloud.com/technology/query_by_humming/ Query by Humming at Sloud Inc], [http://www.sloud.com/ QbH applet (Active X)] \u000a* [http://www.musicline.de/de/melodiesuche/input Musicline QbH based on technology from Fraunhofer Institut] {{de icon}}\u000a* [http://maart.sourceforge.net/ MaART at Sourceforge]\u000a* [http://tunebot.cs.northwestern.edu/ Tunebot at Northwestern University]\u000a\u000a===General info and articles===\u000a* {{Wayback|url=http://mirsystems.info/index.php?id=mirsystems|title=Comprehensive list of Music Information Retrieval systems (apparently last updated ca 2003)|date=20081221191111}}\u000a* [http://www.cs.cornell.edu/zeno/papers/humming/humming.html Query By Humming \u2013 Musical Information Retrieval in an Audio Database], paper by Asif Ghias, Jonathan Logan, David Chamberlin, Brian C. Smith; [[ACM Multimedia]] 1995\u000a* [http://cs.nyu.edu/~eugenew/publications/humming-summary.pdf A survey presentation of QBH by Eugene Weinstein, 2006]\u000a* [http://www.dlib.org/dlib/may97/meldex/05witten.html The New Zealand Digital Library MELody inDEX], article by Rodger J. McNab, Lloyd A. Smith, David Bainbridge and Ian H. Witten; [[D-Lib Magazine]] 1997\u000a* [http://deepblue.lib.umich.edu/bitstream/handle/2027.42/35292/10373_ftp.pdf?sequence=1 Name that Tune: A Pilot Study in Finding a Melody from a Sung Query], article by Bryan Pardo, Jonah Shifrin, and William Birmingham, Journal of the American Society for Information Science and Technology, vol. 55 (4), pp. 283-300, 2004\u000a\u000a[[Category:Music search engines]]\u000a[[Category:Acoustic fingerprinting]]
p213
asI115
(lp214
VLatent semantic analysis
p215
aV{{mergefrom|Latent semantic indexing|date=July 2012}}\u000a{{semantics}}\u000a'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular in [[vectorial semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188}}</ref>\u000a\u000aAn information retrieval method using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853]) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>\u000a\u000a== Overview ==\u000a\u000a=== Occurrence matrix ===\u000aLSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency\u2013inverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\u000a\u000aThis matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.\u000a\u000a=== Rank lowering ===\u000aAfter the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:\u000a\u000a* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").\u000a* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).\u000a* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document\u2014generally a much larger set due to [[synonymy]].\u000a\u000aThe consequence of the rank lowering is that some dimensions are combined and depend on more than one term:\u000a\u000a:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}\u000a\u000aThis mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.\u000a\u000a=== Derivation ===\u000aLet <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:\u000a\u000a:<math>\u000a\u005cbegin{matrix} \u000a & \u005ctextbf{d}_j \u005c\u005c\u000a & \u005cdownarrow \u005c\u005c\u000a\u005ctextbf{t}_i^T \u005crightarrow &\u000a\u005cbegin{bmatrix} \u000ax_{1,1} & \u005cdots & x_{1,n} \u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000ax_{m,1} & \u005cdots & x_{m,n} \u005c\u005c\u000a\u005cend{bmatrix}\u000a\u005cend{matrix}\u000a</math>\u000a\u000aNow a row in this matrix will be a vector corresponding to a term, giving its relation to each document:\u000a\u000a:<math>\u005ctextbf{t}_i^T = \u005cbegin{bmatrix} x_{i,1} & \u005cdots & x_{i,n} \u005cend{bmatrix}</math>\u000a\u000aLikewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:\u000a\u000a:<math>\u005ctextbf{d}_j = \u005cbegin{bmatrix} x_{1,j} \u005c\u005c \u005cvdots \u005c\u005c x_{m,j} \u005cend{bmatrix}</math>\u000a\u000aNow the [[dot product]] <math>\u005ctextbf{t}_i^T \u005ctextbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\u005ctextbf{t}_i^T \u005ctextbf{t}_p</math> (<math> = \u005ctextbf{t}_p^T \u005ctextbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\u005ctextbf{d}_j^T \u005ctextbf{d}_q = \u005ctextbf{d}_q^T \u005ctextbf{d}_j</math>.\u000a\u000aNow, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\u005cSigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):\u000a\u000a:<math>\u000a\u005cbegin{matrix}\u000aX = U \u005cSigma V^T\u000a\u005cend{matrix}\u000a</math>\u000a\u000aThe matrix products giving us the term and document correlations then become\u000a\u000a:<math>\u000a\u005cbegin{matrix}\u000aX X^T &=& (U \u005cSigma V^T) (U \u005cSigma V^T)^T = (U \u005cSigma V^T) (V^{T^T} \u005cSigma^T U^T) = U \u005cSigma V^T V \u005cSigma^T U^T = U \u005cSigma \u005cSigma^T U^T \u005c\u005c\u000aX^T X &=& (U \u005cSigma V^T)^T (U \u005cSigma V^T) = (V^{T^T} \u005cSigma^T U^T) (U \u005cSigma V^T) = V \u005cSigma^T U^T U \u005cSigma V^T = V \u005cSigma^T \u005cSigma V^T\u000a\u005cend{matrix}\u000a</math>\u000a\u000aSince <math>\u005cSigma \u005cSigma^T</math> and <math>\u005cSigma^T \u005cSigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\u005cSigma \u005cSigma^T</math>, or equally, by the non-zero entries of <math>\u005cSigma^T\u005cSigma</math>. Now the decomposition looks like this:\u000a\u000a:<math>\u000a\u005cbegin{matrix} \u000a & X & & & U & & \u005cSigma & & V^T \u005c\u005c\u000a & (\u005ctextbf{d}_j) & & & & & & & (\u005chat{\u005ctextbf{d}}_j) \u005c\u005c\u000a & \u005cdownarrow & & & & & & & \u005cdownarrow \u005c\u005c\u000a(\u005ctextbf{t}_i^T) \u005crightarrow \u000a&\u000a\u005cbegin{bmatrix} \u000ax_{1,1} & \u005cdots & x_{1,n} \u005c\u005c\u000a\u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000a\u005c\u005c\u000ax_{m,1} & \u005cdots & x_{m,n} \u005c\u005c\u000a\u005cend{bmatrix}\u000a&\u000a=\u000a&\u000a(\u005chat{\u005ctextbf{t}}_i^T) \u005crightarrow\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005cbegin{bmatrix} \u005c, \u005c\u005c \u005c, \u005c\u005c \u005ctextbf{u}_1 \u005c\u005c \u005c, \u005c\u005c \u005c,\u005cend{bmatrix} \u000a\u005cdots\u000a\u005cbegin{bmatrix} \u005c, \u005c\u005c \u005c, \u005c\u005c \u005ctextbf{u}_l \u005c\u005c \u005c, \u005c\u005c \u005c, \u005cend{bmatrix}\u000a\u005cend{bmatrix}\u000a&\u000a\u005ccdot\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005csigma_1 & \u005cdots & 0 \u005c\u005c\u000a\u005cvdots & \u005cddots & \u005cvdots \u005c\u005c\u000a0 & \u005cdots & \u005csigma_l \u005c\u005c\u000a\u005cend{bmatrix}\u000a&\u000a\u005ccdot\u000a&\u000a\u005cbegin{bmatrix} \u000a\u005cbegin{bmatrix} & & \u005ctextbf{v}_1 & & \u005cend{bmatrix} \u005c\u005c\u000a\u005cvdots \u005c\u005c\u000a\u005cbegin{bmatrix} & & \u005ctextbf{v}_l & & \u005cend{bmatrix}\u000a\u005cend{bmatrix}\u000a\u005cend{matrix}\u000a</math>\u000a\u000aThe values <math>\u005csigma_1, \u005cdots, \u005csigma_l</math> are called the singular values, and <math>u_1, \u005cdots, u_l</math> and <math>v_1, \u005cdots, v_l</math> the left and right singular vectors.\u000aNotice the only part of <math>U</math> that contributes to <math>\u005ctextbf{t}_i</math> is the <math>i\u005ctextrm{'th}</math> row.\u000aLet this row vector be called <math>\u005chat{\u005ctextrm{t}}_i</math>.\u000aLikewise, the only part of <math>V^T</math> that contributes to <math>\u005ctextbf{d}_j</math> is the <math>j\u005ctextrm{'th}</math> column, <math>\u005chat{ \u005ctextrm{d}}_j</math>.\u000aThese are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.\u000a\u000aIt turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The vector <math>\u005chat{\u005ctextbf{t}}_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the vector <math>\u005chat{\u005ctextbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as\u000a\u000a:<math>X_k = U_k \u005cSigma_k V_k^T</math>\u000a\u000aYou can now do the following:\u000a* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\u005cSigma_k \u005chat{\u005ctextbf{d}}_j </math> and <math>\u005cSigma_k \u005chat{\u005ctextbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).\u000a* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\u005cSigma_k \u005chat{\u005ctextbf{t}}_i^T </math> and <math>\u005cSigma_k \u005chat{\u005ctextbf{t}}_p^T </math>.\u000a* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.\u000a* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.\u000a\u000aTo do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:\u000a\u000a:<math>\u005chat{\u005ctextbf{d}}_j = \u005cSigma_k^{-1} U_k^T \u005ctextbf{d}_j</math>\u000a\u000aNote here that the inverse of the diagonal matrix <math>\u005cSigma_k</math> may be found by inverting each nonzero value within the matrix.\u000a\u000aThis means that if you have a query vector <math>q</math>, you must do the translation <math>\u005chat{\u005ctextbf{q}} = \u005cSigma_k^{-1} U_k^T \u005ctextbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:\u000a\u000a:<math>\u005ctextbf{t}_i^T = \u005chat{\u005ctextbf{t}}_i^T \u005cSigma_k V_k^T</math>\u000a\u000a:<math>\u005chat{\u005ctextbf{t}}_i^T = \u005ctextbf{t}_i^T V_k^{-T} \u005cSigma_k^{-1} = \u005ctextbf{t}_i^T V_k \u005cSigma_k^{-1}</math>\u000a\u000a:<math>\u005chat{\u005ctextbf{t}}_i = \u005cSigma_k^{-1}  V_k^T \u005ctextbf{t}_i</math>\u000a\u000a== Applications ==\u000a\u000aThe new low-dimensional space typically can be used to:\u000a* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).\u000a* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).\u000a* Find relations between terms ([[synonymy]] and [[polysemy]]).\u000a* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).\u000a* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model | author=Alain Lifchitz, Sandra Jhean-Larose, Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201\u20131209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>\u000a\u000aSynonymy and polysemy are fundamental problems in [[natural language processing]]: \u000a* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.\u000a* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.\u000a\u000a=== Commercial applications ===\u000a\u000aLSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\u000a\u000a=== Applications in human memory ===\u000a\u000aThe use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall | author=Marc W. Howard and Michael J. Kahana |year=1999}}</ref>\u000a\u000aWhen participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb et al. | booktitle=Interspeech'2005|year=2006}}</ref>\u000a\u000aAnother model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=5/8/2011}}</ref>\u000a\u000a== Implementation ==\u000a\u000aThe [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis | author=Geneviève Gorrell and Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}</ref>\u000aA fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20\u201330 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.\u000aIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>doi: 10.1109/ICCSNT.2011.6182070</ref>\u000a\u000a== Limitations ==\u000aSome of LSA's drawbacks include:\u000a\u000a* The resulting dimensions might be difficult to interpret. For instance, in\u000a:: {(car), (truck), (flower)} \u21a6  {(1.3452 * car + 0.2828 * truck), (flower)}\u000a:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to\u000a:: {(car), (bottle), (flower)} \u21a6  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}\u000a:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.\u000a\u000a* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word){{Citation needed|date=October 2013}}.  Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).\u000a\u000a* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words.\u000a\u000a* To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>[http://www.translational-medicine.com/content/12/1/324 J Transl Med. 2014 Nov 27;12(1):324.]</ref>\u000a\u000a* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>\u000a\u000a== See also ==\u000a* [[Compound term processing]]\u000a* [[Explicit semantic analysis]]\u000a* [[Latent semantic mapping]]\u000a* [[Latent Semantic Structure Indexing]]\u000a* [[Principal components analysis]]\u000a* [[Probabilistic latent semantic analysis]]\u000a* [[Spamdexing]]\u000a* [[Topic model]]\u000a** [[Latent Dirichlet allocation]]\u000a* [[Vectorial semantics]]\u000a* [[Coh-Metrix]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a* {{cite journal\u000a | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\u000a |format=PDF| title=Introduction to Latent Semantic Analysis\u000a | author=[[Thomas Landauer]], Peter W. Foltz, & Darrell Laham\u000a | journal=Discourse Processes\u000a | volume=25\u000a | pages=259\u2013284\u000a |year=1998\u000a | doi=10.1080/01638539809545028\u000a | issue=2\u20133\u000a}}\u000a* {{cite journal\u000a | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf \u000a |format=PDF| title=Indexing by Latent Semantic Analysis\u000a | author=[[Scott Deerwester]], [[Susan Dumais|Susan T. Dumais]], [[George Furnas|George W. Furnas]], [[Thomas Landauer|Thomas K. Landauer]], [[Richard Harshman]]\u000a | journal=Journal of the American Society for Information Science\u000a | volume=41\u000a | issue=6\u000a | pages=391\u2013407\u000a | year=1990 \u000a | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\u000a}} Original article where the model was first exposed.\u000a* {{cite journal\u000a | url=http://citeseer.ist.psu.edu/berry95using.html\u000a | title=Using Linear Algebra for Intelligent Information Retrieval\u000a | author=Michael Berry, [[Susan Dumais|Susan T. Dumais]], Gavin W. O'Brien\u000a |year=1995\u000a}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.\u000a* {{cite web\u000a | url=http://iv.slis.indiana.edu/sw/lsa.html\u000a | title=Latent Semantic Analysis\u000a | publisher=InfoVis\u000a}}\u000a* {{cite web\u000a | url=http://cran.at.r-project.org/web/packages/lsa/index.html\u000a | title=An Open Source LSA Package for R\u000a | publisher=CRAN\u000a | author=Fridolin Wild\u000a | date=November 23, 2005\u000a | accessdate=2006-11-20\u000a}}\u000a* {{ cite web\u000a | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM\u000a | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge\u000a | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]\u000a | accessdate=2007-07-02\u000a}}\u000a\u000a==External links==\u000a\u000a===Articles on LSA===\u000a* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.\u000a\u000a===Talks and demonstrations===\u000a* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].\u000a* [http://www.semanticsearchart.com/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.\u000a\u000a===Implementations===\u000a\u000aDue to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.\u000a* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA\u000a* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA\u000a* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices\u000a* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)\u000a* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA\u000a* [[Gensim]] contains a fast, online Python implementation of LSA for matrices larger than RAM.\u000a\u000a{{DEFAULTSORT:Latent Semantic Analysis}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Latent variable models]]\u000a\u000a[[fa:\u0622\u0646\u0627\u0644\u06cc\u0632 \u067e\u0646\u0647\u0627\u0646 \u0645\u0641\u0647\u0648\u0645\u06cc \u0627\u062d\u062a\u0645\u0627\u0644\u06cc]]
p216
asI55
(lp217
VCompound term processing
p218
aV{{copy edit|for=Use of references (both inline, and ref tags)|date=February 2015}}\u000a\u000a'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications that perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.\u000a\u000aIn August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing <ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>\u000a\u000aCLAMOUR<ref>[http://www.statistics.gov.uk/methods_quality/clamour/coordination/wp03.asp] National Statistics CLAMOUR project</ref><ref>[http://www.statistics.gov.uk/methods_quality/clamour/downloads/Clamour_march2002_final_reportAO.pdf] CLAMOUR Final Report</ref> is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information & statistics. In contrast to the techniques discussed by Concept Searching Limited, CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.\u000a\u000aCompound Term Processing allows information retrieval applications, such as search engines, to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.\u000a\u000aMost [[search engine]]s simply look for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. [[Phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.\u000a\u000aTechniques for probabilistic weighting of single word terms dates back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]] entitled "Relevance weighting of search terms", originally published in the ''Journal of the American Society for Information Science''.<ref>{{cite doi | 10.1002/asi.4630270302}}</ref>  Robertson stated that the assumption of word independence is not justified and exists simply as a matter of mathematical convenience. His objection to term independence is not a new idea, dating back to at least 1964 when H. H. Williams expressed that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>WILLIAMS, J.H., 'Results of classifying documents with multiple discriminant functions', In : Statistical Association Methods for Mechanized Documentation, National Bureau of Standards, Washington, 217-224 (1965).</ref>\u000a\u000aCompound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? By forming compound terms and placing these terms in a search engine's index, searches can be performed with a higher degree of accuracy, as the ambiguity inherent in single words is no longer a problem. Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.\u000a\u000aIn 2004, Anna Lynn Patterson filed a number of patents on "phrase-based searching in an information retrieval system"<ref>[http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PG01&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.html&r=1&f=G&l=50&s1=%2220060031195%22.PGNR.&OS=DN/20060031195&RS=DN/20060031195] US Patent: 20060031195</ref> to which Google subsequently acquired the rights. A full discussion of the patents can be found at [http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]{{dead link|date=February 2015}}.\u000a\u000aStatistical compound term processing is a method more adaptive than the process described by Patterson in her patent applications. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.\u000a\u000aStatistical compound term processing is also more adaptive than the linguistic approach taken by the CLAMOUR project, which must take into consideration the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[Information retrieval]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a==  External links ==\u000a*[http://www.conceptsearching.com/ Concept Searching Limited]\u000a*[http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]\u000a\u000a{{Natural Language Processing}}\u000a\u000a{{DEFAULTSORT:Compound Term Processing}}\u000a[[Category:Information retrieval]]
p219
asI122
(lp220
VSearch engine indexing
p221
aV{{Too many see alsos|date=December 2012}}\u000a'''Search engine indexing''' collects, parses, and stores [[data (computing)|data]] to facilitate fast and accurate [[information retrieval]]. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, [[Information technology|informatics]], and computer science.  An alternate name for the process in the context of [[search engine]]s designed to find web pages on the Internet is ''[[web indexing]]''.\u000a\u000aPopular engines focus on the full-text indexing of online, natural language documents.<ref>Clarke, C., Cormack, G.: Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. TechRep MT-95-01, University of Waterloo, February 1995.</ref> [[Multimedia|Media types]] such as video and audio<ref>http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</ref> and graphics<ref>Charles E. Jacobs, Adam Finkelstein, David H. Salesin. [http://grail.cs.washington.edu/projects/query/mrquery.pdf Fast Multiresolution Image Querying]. Department of Computer Science and Engineering, University of Washington. 1995. Verified Dec 2006</ref> are also searchable.\u000a\u000a[[Metasearch engine|Meta search engines]] reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the  [[text corpus|corpus]]. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while [[Intelligent agent|agent]]-based search engines index in [[Real time business intelligence|real time]].\u000a\u000a==Indexing==\u000aThe purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would [[Lexical analysis|scan]] every document in the corpus, which would require considerable time and computing power.  For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval.\u000a\u000a===Index design factors===\u000aMajor factors in designing a search engine's architecture include:\u000a\u000a; Merge factors : How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the [[Web crawling|data collection]] policy. Search engine index merging is similar in concept to the [[Merge (SQL)|SQL Merge]] command and other merge algorithms.<ref>Brown, E.W.: Execution Performance Issues in Full-Text Information Retrieval. Computer Science Department, University of Massachusetts Amherst, Technical Report 95-81, October 1995.</ref>\u000a; Storage techniques : How to store the index [[data]], that is, whether information should be data compressed or filtered.\u000a; Index size : How much computer storage is required to support the index.\u000a; Lookup speed : How quickly a word can be found in the inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science.\u000a; Maintenance : How the index is maintained over time.<ref>Cutting, D., Pedersen, J.: Optimizations for dynamic inverted index maintenance. Proceedings of SIGIR, 405-411, 1990.</ref>\u000a;Fault tolerance : How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, [[partition (database)|partitioning]], and schemes such as [[hash function|hash-based]] or composite partitioning,<ref>[http://dev.mysql.com/doc/refman/5.1/en/partitioning-linear-hash.html Linear Hash Partitioning]. MySQL 5.1 Reference Manual. Verified Dec 2006</ref> as well as [[Replication (computer science)|replication]].\u000a\u000a===Index data structures===\u000aSearch engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.\u000a\u000a;[[Suffix tree]] : Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [[trie]]. Tries support extendable hashing, which is important for search engine indexing.<ref>[http://www.nist.gov/dads/HTML/trie.html trie], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology].</ref> Used for searching for patterns in [[DNA]] sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.<ref name="Gus97">{{cite book\u000a | last = Gusfield\u000a | first = Dan\u000a | origyear = 1997\u000a | year = 1999\u000a | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology\u000a | publisher = Cambridge University Press\u000a | location = USA\u000a | isbn = 0-521-58519-8}}.\u000a</ref> An alternate representation is a [[suffix array]], which is considered to require less virtual memory and supports data compression such as the [[Burrows-Wheeler transform|BWT]] algorithm.\u000a\u000a;[[Inverted index]] : Stores a list of occurrences of each atomic search criterion,<ref>Black, Paul E., [http://www.nist.gov/dads/HTML/invertedIndex.html inverted index], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology] Oct 2006. Verified Dec 2006.</ref> typically in the form of a [[hash table]] or [[binary tree]].<ref>C. C. Foster, Information retrieval: information storage and retrieval using AVL trees, Proceedings of the 1965 20th national conference, p.192-205, August 24\u201326, 1965, Cleveland, Ohio, United States</ref><ref>Landauer, W. I.: The balanced tree and its utilization in information retrieval. IEEE Trans. on Electronic Computers, Vol. EC-12, No. 6, December 1963.</ref>\u000a\u000a;[[Citation index]] : Stores citations or hyperlinks between documents to support citation analysis, a subject of [[Bibliometrics]].\u000a;[[N-gram|Ngram index]] : Stores sequences of length of data to support other types of retrieval or text mining.<ref>[http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 Google Ngram Datasets] for sale at [http://www.ldc.upenn.edu/ LDC] Catalog</ref>\u000a;[[Document-term matrix]] : Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [[sparse matrix]].\u000a\u000a===Challenges in parallelism===\u000aA major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for [[race conditions]] and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or [[Text corpus|corpus]]). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a '''producer-consumer model'''. The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve [[distributed computing]], where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture.<ref>Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Google, Inc. OSDI. 2004.</ref>\u000a\u000a===Inverted indices===\u000aMany search engines incorporate an [[inverted index]] when evaluating a [[search query]] to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct [[random access|access]] to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Inverted Index\u000a|-\u000a! Word !! Documents\u000a|-\u000a| the || Document 1, Document 3, Document 4, Document 5, Document 7\u000a|-\u000a| cow || Document 2, Document 3, Document 4\u000a|-\u000a| says || Document 5\u000a|-\u000a| moo || Document 7\u000a|}\u000a\u000aThis index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a [[boolean datatype|boolean]] index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document.<ref>Grossman, Frieder, Goharian. [http://www.cs.clemson.edu/~juan/CPSC862/Concept-50/IR-Basics-of-Inverted-Index.pdf IR Basics of Inverted Index]. 2002. Verified Aug 2011.</ref> Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of [[information retrieval]].\u000a\u000aThe inverted index is a [[sparse matrix]], since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional [[Array data structure|array]]. The index is similar to the [[document-term matrix|term document matrices]] employed by [[latent semantic analysis]]. The inverted index can be considered a form of a hash table. In some cases the index is a form of a [[binary tree]], which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a [[distributed hash table]].<ref>Tang, Hunqiang. [[Sandhya Dwarkadas|Dwarkadas, Sandhya]]. "Hybrid Global Local Indexing for Efficient\u000aPeer to Peer Information Retrieval". University of Rochester. Pg 1. http://www.cs.rochester.edu/u/sandhya/papers/nsdi04.ps</ref>\u000a\u000a===Index merging===\u000aThe inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing,<ref>Tomasic, A., et al.: Incremental Updates of Inverted Lists for Text Document Retrieval. Short Version of Stanford University Computer Science Technical Note STAN-CS-TN-93-1, December, 1993.</ref> where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives.\u000a\u000aAfter parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is  so named because it is an inversion of the forward index.\u000a\u000a===The forward index===\u000aThe forward index stores a list of words for each document. The following is a simplified form of the forward index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Forward Index\u000a|-\u000a! Document !! Words\u000a|-\u000a| Document 1 || the,cow,says,moo\u000a|-\u000a| Document 2 || the,cat,and,the,hat\u000a|-\u000a| Document 3 || the,dish,ran,away,with,the,spoon\u000a|}\u000a\u000aThe rationale behind developing a forward index is that as documents are parsing, it is better to immediately store the words per document.  The delineation enables Asynchronous system processing, which partially circumvents the inverted index update [[wikt:bottleneck|bottleneck]].<ref>Sergey Brin and Lawrence Page. [http://infolab.stanford.edu/~backrub/google.html The Anatomy of a Large-Scale Hypertextual Web Search Engine]. [[Stanford University]]. 1998. Verified Dec 2006.</ref> The forward index is [[Sorting algorithm|sorted]] to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index.\u000a\u000a===Compression===\u000aGenerating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on [[computer storage|disk]].<ref>H.S. Heaps. Storage analysis of a compression coding for a document database. 1NFOR, I0(i):47-61, February 1972.</ref> Consider the following scenario for a full text, Internet search engine.\u000a\u000a* It takes 8 bits (or 1 [[byte]]) to store a single character. Some [[character encoding|encodings]] use 2 bytes per character<ref>[http://www.unicode.org/faq/basic_q.html#15 The Unicode Standard - Frequently Asked Questions]. Verified Dec 2006.</ref><ref>[http://www.uplink.freeuk.com/data.html Storage estimates]. Verified Dec 2006.</ref>\u000a* The average number of characters in any given word on a page may be estimated at 5 ([[Wikipedia:Size comparisons]])\u000a\u000aGiven this scenario, an uncompressed index (assuming a non-[[conflation|conflated]], simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression.\u000a\u000aNotably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost.\u000a\u000a==Document parsing==\u000aDocument parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called ''tokens'', and so, in the context of search engine indexing and [[natural language processing]], parsing is more commonly referred to as [[Tokenization (lexical analysis)|tokenization]]. It is also sometimes called [[word boundary disambiguation]], [[Part-of-speech tagging|tagging]], [[text segmentation]], [[content analysis]], text analysis, [[text mining]], [[Agreement (linguistics)|concordance]] generation, [[speech segmentation]], [[Lexical analysis|lexing]], or [[lexical analysis]]. The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang.\u000a\u000aNatural language processing, as of 2006, is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.\u000a\u000a=== Challenges in natural language processing ===\u000a; Word Boundary Ambiguity : Native [[English language|English]] speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a [[multilingual]] indexer.  In digital form, the texts of other languages such as [[Chinese language|Chinese]], [[Japanese language|Japanese]] or [[Arabic language|Arabic]] represent a greater challenge, as words are not clearly delineated by [[Whitespace (computer science)|whitespace]]. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax).\u000a\u000a; Language Ambiguity : To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its [[language]] or [[lexical category]] ([[part of speech]]). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document.\u000a\u000a; Diverse File Formats : In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document.\u000a\u000a; Faulty Storage : The quality of the natural language data may not always be perfect.  An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol.  [[Binary data|Binary]] characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade.\u000a\u000a=== Tokenization ===\u000aUnlike [[literacy|literate]] humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word, referred to as a token. Such a program is commonly called a [[tokenizer]] or [[parser]] or [[Lexical analysis|lexer]]. Many search engines, as well as other natural language processing software, incorporate [[Comparison of parser generators|specialized programs]] for parsing, such as [[YACC]] or [[Lex programming tool|Lex]].\u000a\u000aDuring tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify [[Entity extraction|entities]] such as [[email]] addresses, phone numbers, and [[Uniform Resource Locator|URL]]s. When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number.\u000a\u000a=== Language recognition ===\u000aIf the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as [[stemming]] and [[part of speech]] tagging). [[Language identification|Language recognition]] is the process by which a computer program attempts to automatically identify, or categorize, the [[language]] of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in [[natural language processing]]. Finding which language the words belongs to may involve the use of a [[language recognition chart]].\u000a\u000a=== Format analysis ===\u000aIf the search engine supports multiple [[File format|document formats]], documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example, [[HTML]] documents contain HTML tags, which specify formatting information such as new line starts, '''bold''' emphasis, and [[font]] size or [[Font family|style]].  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning, and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include:\u000a\u000a* [[HTML]]\u000a* [[ASCII]] text files (a text document without specific computer readable formatting)\u000a* [[Adobe Systems|Adobe]]'s Portable Document Format ([[PDF]])\u000a* [[PostScript]] (PS)\u000a* [[LaTeX]]\u000a* [[UseNet]] netnews server formats\u000a* [[XML]] and derivatives like [[RSS]]\u000a* [[SGML]]\u000a* [[Multimedia]] [[meta data]] formats like [[ID3]]\u000a* [[Microsoft Word]]\u000a* [[Microsoft Excel]]\u000a* [[Microsoft PowerPoint]]\u000a* IBM [[Lotus Notes]]\u000aOptions for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom [[parser]].\u000a\u000aSome search engines support inspection of files that are stored in a [[Compressor (software)|compressed]] or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported [[list of archive formats|compressed file format]]s include:\u000a\u000a* [[ZIP (file format)|ZIP]] - Zip archive file\u000a* [[RAR]] - Roshal ARchive file\u000a* [[Cabinet (file format)|CAB]] - [[Microsoft Windows]] Cabinet File\u000a* [[Gzip]] - File compressed with gzip\u000a* [[Bzip2|BZIP]] - File compressed using bzip2\u000a* [[Tar (file format)|Tape ARchive (TAR)]], [[Unix]] archive file, not (itself) compressed\u000a* TAR.Z, TAR.GZ or TAR.BZ2 - [[Unix]] archive files compressed with Compress, GZIP or BZIP2\u000a\u000aFormat analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for [[spamdexing]]:\u000a\u000a* Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden [[Span and div|"div" tag]] in [[HTML]], which may incorporate the use of [[CSS]] or [[JavaScript]] to do so).\u000a* Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer.\u000a\u000a=== Section recognition ===\u000aSome search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the [[Internet|web]], such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted:\u000a\u000a* Content in different sections is treated as related in the index, when in reality it is not\u000a* Organizational 'side bar' content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents.\u000a\u000aSection analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the [[Noscript tag]] to ensure that the web page is indexed properly.  At the same time, this fact can also be [[spamdexing|exploited]] to cause the search engine indexer to 'see' different content than the viewer.\u000a\u000a=== HTML Priority System ===\u000a{{Section OR|date=November 2013}}\u000aIndexing often has to recognize the [[HTML]] tags to organize priority. Indexing low priority to high margin to labels like ''strong'' and ''link'' to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like [[Google]] and [[Bing]] ensure that the [[search engine]] does not take the large texts as relevant source due to[[ strong type system]] compatibility.<ref>Google Webmaster Tools, "Hypertext Markup Language 5", Conference for SEO January 2012.</ref>\u000a\u000a=== Meta tag indexing ===\u000aSpecific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the [[meta tag]] contains keywords which are also included in the index. Earlier Internet [[search engine technology]] would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was [[computer hardware]] able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization.<ref>Berners-Lee, T., "Hypertext Markup Language - 2.0", RFC 1866, Network Working Group, November 1995.</ref>\u000a\u000aAs the Internet grew through the 1990s, many [[brick and mortar business|brick-and-mortar corporations]] went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to [[spamdexing]], which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the [[customer lifetime value]] equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies.\u000a\u000aIn [[Desktop search]], many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index.\u000a\u000a== See also ==\u000a{{div col|colwidth=25em}}\u000a* [[Compound term processing]]\u000a* [[Concordance (publishing)|Concordance]]\u000a* [[Content analysis]]\u000a* [[Controlled vocabulary]]\u000a* [[Desktop search]]\u000a* [[Documentation]]\u000a* [[Document retrieval|Document Retrieval]]\u000a* [[Full text search]]\u000a* [[Index (database)]]\u000a* [[Information extraction]]\u000a* [[Information retrieval]]\u000a* [[Key Word in Context|Keyword In Context Indexing]]\u000a* [[Latent semantic indexing]]\u000a* [[List of search engines]]\u000a* [[Natural language processing]]\u000a* [[Search engine]]\u000a* [[Selection-based search]]\u000a* [[Semantic Web]]\u000a* [[Site map]]\u000a* [[Text mining]]\u000a* [[Text retrieval|Text Retrieval]]\u000a* [[Vertical search]]\u000a* [[Web crawler]]\u000a* [[Web indexing]]\u000a* [[Website Parse Template]]\u000a*[[Windows indexing service]]<ref>Krishna Nareddy. [http://msdn2.microsoft.com/en-us/library/ms951558.aspx Indexing with Microsoft Index Server]. MSDN Library. Microsoft Corporation. January 30, 1998. Verified Dec 2006. Note that this is a commercial, external link.</ref>\u000a{{div col end}}\u000a\u000a== References ==\u000a<references/>\u000a\u000a==Further reading==\u000a*R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998.\u000a*[[Gerald Salton]]. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988.\u000a*[[Gerard Salton]]. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986.\u000a*[[Gerard Salton]]. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968.\u000a*[[Gerard Salton]]. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971.\u000a*[[Gerard Salton]]. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989.\u000a*Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999.\u000a*G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949.\u000a*Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962).\u000a*[[Edward H. Sussenguth Jr.]], Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p.&nbsp;272-279, May 1963\u000a*Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 28\u201343, 1992.\u000a*Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133\u2013146, 2001.\u000a*Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003.\u000a*Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349\u2013379, October 1996, Volume 14, Number 4.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]], [[Mark Overmars|Overmars, M.H.]]: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 93\u201398, 1981.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 1\u201316, 1981.\u000a*Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Int'l World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp.&nbsp;175\u2013182)\u000a*[[Serge Abiteboul]] and [[Victor Vianu]]. [http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1996-20&format=text&compression=&name=1996-20.text Queries and Computation on the Web]. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997.\u000a*Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994.\u000a*A. Emtage and P. Deutsch, "Archie--An Electronic Directory Service for the Internet." Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp.&nbsp;93\u2013110.\u000a*M. Gray, [http://www.mit.edu/people/mkgray/net/ World Wide Web Wanderer].\u000a*D. Cutting and J. Pedersen. "Optimizations for Dynamic Inverted Index Maintenance." Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp.&nbsp;405\u2013411, September 1990.\u000a*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Index (Search Engine)}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a[[Category:Indexing]]\u000a[[Category:Internet search algorithms]]
p222
asI123
(lp223
VMultimedia Information Retrieval
p224
aV{{COI|date=July 2014}}\u000a{{Original research|date=July 2014}}\u000a{{Use dmy dates|date=February 2012}}\u000a'''Multimedia Information Retrieval''' (MMIR or MIR) is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:\u000a\u000a# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.\u000a# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])\u000a# Methods for the [[categorization]] of media descriptions into classes.\u000a\u000a== Feature Extraction Methods ==\u000a\u000aFeature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 2.</ref>{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:\u000a\u000a* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel Frequency Cepstral Coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. " Visual Information Retrieval ", Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.\u000a* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim , N Moreau, T Sikora. " MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.\u000a\u000a== Merging and Filtering Methods ==\u000a\u000aMultimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). " Principles of Visual Information Retrieval ", Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions - as they frequently occur in motion description - have to be normalized to a fixed length first.\u000a\u000aFrequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.\u000a\u000a== Categorization Methods ==\u000a\u000aGenerally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011,p. 125.</ref>{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[Hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[Dynamic Time Warping]] - a semantically related method - is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:\u000a\u000a* Metric approaches ([[Cluster Analysis]], [[Vector Space Model]], [[Minkowski]] Distances, Dynamic Alignment)\u000a* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-Means, [[Self-Organizing Map]])\u000a* Risk Minimization (Support Vector Regression, [[Support Vector Machine]], [[Linear Discriminant Analysis]])\u000a* Density-based Methods (Bayes Nets, [[Markov Processes]], Mixture Models)\u000a* Neural Networks ([[Perceptron]], Associative Memories, Spiking Nets)\u000a* Heuristics ([[Decision Trees]], Random Forests, etc.)\u000a\u000aThe selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.\u000a\u000a== Open Problems ==\u000a\u000aThe quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. " Frontiers of Media Understanding ", atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.\u000a\u000a== Related Areas ==\u000a\u000aMMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. " Professional Media Understanding ", atpress, 2012.</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:\u000a\u000a* [[Bioinformatics|Bioinformation Analysis]]\u000a* [[Biosignal|Biosignal Processing]]\u000a* [[Content-based image retrieval|Content-based Image and Video Retrieval]]\u000a* [[Facial recognition system|Face Recognition]]\u000a* [[Music information retrieval|Audio and Music Classification]]\u000a* [[Speech Recognition]]\u000a* [[Technical analysis|Technical Chart Analysis]]\u000a* [[Information retrieval|Text Information Retrieval]]\u000a\u000aThe Journal of Multimedia Information Retrieval<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also <ref>H Eidenberger. " Handbook of Multimedia Information Retrieval ", atpress, 2012.</ref> for a complete overview over this research discipline.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a<!--\u000a\u000a<ref>AUTH. "TITLE", PUB, YEAR.</ref>\u000a<ref>AUTH. "[LINK TITLE]", MEDIA, PRODDATE. Retrieved UPDATE.</ref>\u000a\u000a-->\u000a\u000a\u000a\u000a[[Category:Information retrieval]]
p225
asI125
(lp226
VType-1 OWA operators
p227
aVThe [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183\u2013190|doi=10.1109/21.87068}}</ref>  have been widely used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decisin making, multi-criteria multi-expert decision making).<ref>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref><ref>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref> It is widely accepted that fuzzy sets<ref>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338\u2013353|doi=10.1016/S0019-9958(65)90241-X}}</ref> are more suitable for representing preferences of criteria in decision making. But fuzzy sets are not crisp values, how can we aggregate fuzzy sets in OWA mechanism? \u000a\u000aThe type-1 OWA operators<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281\u20133296|doi=10.1016/j.fss.2008.06.018}}</ref><ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455\u20131468|doi=10.1109/TKDE.2010.191}}</ref>  have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\u000a\u000aFirst, there are two definitions for type-1 OWA operators, one is based on Zadeh's Extension Principle, the other is based on <math>\u005calpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.\u000a\u000a==Definitions==\u000a\u000a'''Definition 1.<ref name="fssT1OWA" /> '''\u000aLet <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:\u000a\u000aGiven n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\u005cPhi</math>,\u000a\u000a:<math>\u005cPhi \u005ccolon F(X)\u005ctimes \u005ccdots \u005ctimes F(X)  \u005clongrightarrow  F(X)</math>\u000a:<math>(A^1 , \u005ccdots ,A^n)  \u005cmapsto   Y</math>\u000a\u000asuch that\u000a\u000a:<math>\u005cmu _{Y} (y) =\u005cdisplaystyle \u005csup_{\u005cdisplaystyle \u005csum_{k =1}^n \u005cbar {w}_i a_{\u005csigma (i)}  = y }\u005cleft({\u005cbegin{array}{*{1}l}\u005cmu _{W^1 } (w_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu_{W^n } (w_n )\u005cwedge \u005cmu _{A^1 } (a_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu _{A^n } (a_n )\u005cend{array}}\u005cright)</math>\u000a\u000awhere <math>\u005cbar {w}_i = \u005cfrac{w_i }{\u005csum_{i = 1}^n {w_i } }</math>,and <math>\u005csigma \u005ccolon \u005c{1, \u005ccdots ,n\u005c} \u005clongrightarrow \u005c{1, \u005ccdots ,n\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cgeq a_{\u005csigma (i + 1)},\u005c \u005cforall i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma(i)} </math> is the <math>i</math>th highest element in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a'''Definition 2.<ref name="kdeT1OWA" /> '''\u000a\u000aThe definition below is based on the alpha-cuts of fuzzy sets:\u000a\u000aGiven the n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, then for each <math>\u005calpha \u005cin [0,\u005c;1]</math>, an <math>\u005calpha </math>-level type-1 OWA operator with <math>\u005calpha </math>-level sets <math>\u005cleft\u005c{ {W_\u005calpha ^i } \u005cright\u005c}_{i = 1}^n </math> to aggregate the <math>\u005calpha </math>-cuts of fuzzy sets <math>\u005cleft\u005c{ {A^i} \u005cright\u005c}_{i =1}^n </math> is given as\u000a\u000a: <math>\u000a\u005cPhi_\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright) =\u005cleft\u005c{ {\u005cfrac{\u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} } }{\u005csum\u005climits_{i = 1}^n {w_i } }\u005cleft| {w_i \u005cin W_\u005calpha ^i ,\u005c;a_i } \u005cright. \u005cin A_\u005calpha ^i ,\u005c;i = 1, \u005cldots ,n} \u005cright\u005c}</math>\u000a\u000awhere  <math>W_\u005calpha ^i= \u005c{w| \u005cmu_{W_i }(w) \u005cgeq \u005calpha \u005c}, A_\u005calpha ^i=\u005c{ x| \u005cmu _{A_i }(x)\u005cgeq \u005calpha \u005c}</math>, and <math>\u005csigma :\u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c} \u005cto \u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cge a_{\u005csigma (i + 1)} ,\u005c;\u005cforall \u005c;i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma (i)} </math> is the <math>i</math>th largest\u000aelement in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a== Representation theorem of Type-1 OWA operators<ref name="kdeT1OWA" />==\u000a\u000aGiven the ''n'' linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, and the fuzzy sets <math>A^1, \u005ccdots ,A^n</math>, then we have that<ref name="kdeT1OWA" />\u000a:<math>Y=G</math>\u000a\u000awhere <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.\u000a\u000a==Programming problems for Type-1 OWA operators==\u000a\u000aAccording to the '''''Representation Theorem of Type-1 OWA Operators''''',a general type-1 OWA operator can be decomposed into a series of <math>\u005calpha</math>-level type-1 OWA operators. In practice, these series of  <math>\u005calpha</math>-level type-1 OWA operators are used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots\u000a,A_\u005calpha ^n } \u005cright)_\u005calpha } \u005calpha </math>\u000a\u000aFor the left end-points, we need to solve the following programming problem:\u000a:<math> \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)_{-} = \u005cmathop {\u005cmin }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i = 1}^n {w_i } } </math>\u000a\u000awhile for the right end-points, we need to solve the following programming problem:\u000a:<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots , A_\u005calpha ^n } \u005cright)_{+} = \u005cmathop {\u005cmax }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i  A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i =\u000a1}^n {w_i } } </math>\u000a\u000aA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.<ref name="kdeT1OWA" />\u000a\u000a== Alpha-level approach to Type-1 OWA operation<ref name="kdeT1OWA" />==\u000a* '''Step 1'''.To set up the <math>\u005calpha </math>- level resolution in [0, 1].\u000a* '''Step 2'''. For each <math>\u005calpha \u005cin [0,1]</math>,\u000a''Step 2.1.'' To calculate <math>\u005crho _{\u005calpha +} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha +} ^{i_0 } \u005cge A_{\u005calpha + }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha +} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.1-3''.\u000a# <math>i_0 \u005cleftarrow i_0 + 1</math>, go to ''Step 2.1-2''.\u000a\u000a''Step 2.2.'' To calculate<math>\u005crho _{\u005calpha -} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha -} ^{i_0 } \u005cge A_{\u005calpha - }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha -} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.2-3.''\u000a#<math>i_0 \u005cleftarrow i_0 + 1</math>, go to step ''Step 2.2-2.''\u000a\u000a'''Step 3.'''To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]</math>: \u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]} \u005calpha </math>\u000a\u000a==Special cases of Type-1 OWA operators==\u000a* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />\u000a* Join operators of (type-1) fuzzy sets,<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312\u201340|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199\u2013249|doi=10.1016/0020-0255(75)90036-5}}</ref> i.e., fuzzy maximum operators;\u000a* Meet operators of (type-1) fuzzy sets,<ref name="MT"/><ref name="zadehJ"/> i.e., fuzzy minimum operators;\u000a* Join-like operators of (type-1) fuzzy sets;<ref name="kdeT1OWA"/><ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91\u2013109|doi=10.1007/978-3-642-17910-5_5}}</ref>\u000a* Meet-like operators of (type-1) fuzzy sets.<ref name="kdeT1OWA"/><ref name="bookT1OWA"/>\u000a\u000a==Generalizations==\u000aType-2 OWA operators<ref>{{cite journal|last=Zhou|first=S.M.|coauthors=R. I. John, F. Chiclana and J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540\u2013558|doi=10.1002/int.20420}}</ref> have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Artificial intelligence]]\u000a[[Category:Logic in computer science]]\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]
p228
asI126
(lp229
VCategory:Substring indices
p230
aV{{cat main|Substring index}}\u000a\u000a[[Category:String (computer science)]]\u000a[[Category:Algorithms on strings]]\u000a[[Category:String data structures]]\u000a[[Category:Database index techniques]]\u000a[[Category:Information retrieval]]\u000a[[Category:Bioinformatics algorithms]]
p231
asI127
(lp232
VMacroglossa Visual Search
p233
aV{{Infobox Website\u000a| name           = Macroglossa\u000a| logo           = [[File:Macroglossa Visual Search Engine Logo, 2012.gif]]\u000a| screenshot     = \u000a| caption        = Macroglossa logo\u000a| url            = [http://www.macroglossa.com macroglossa.com]\u000a| type           = [[Visual search engine|Visual]] [[Search engine|Search Engine]]\u000a| language       = English\u000a| registration   = optional\u000a| author         = MVE\u000a| launch date    = 2010\u000a| current status = beta 0.1\u000a| slogan         = search is visual\u000a| alexa          = {{IncreaseNegative}} 2,828,096 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/macroglossa.com |title= Macroglossa.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->\u000a}}\u000a'''Macroglossa''' is a [[visual search engine]] based on the comparison of images,<ref>Nicola Mattina. "[http://blog.wired.it/startupcloud/2010/12/29/macroglossa-usare-le-immagini-per-effettuare-ricerche-sul-web.html Macroglossa: usare le immagini per effettuare ricerche sul web]", Wired.it, Retrieved December 29, 2010.</ref><ref>GreatStartups.com . "[http://greatstartups.com/2010/10/13/macroglossa-com-whats-in-the-picture/ Macroglossa.com-What\u2019s In The Picture ]", greatstartups.com, Retrieved October 13, 2010.</ref> coming from an Italian Group. The development of the project began in 2009. In April 2010 is released the first public [[Alpha stage#Alpha|alpha]].<ref>Liva Judic. "[http://searchenginewatch.com/article/2050950/Macroglossas-Visual-Search-Engine-fails-to-meet-basic-expectations Macroglossa's Visual Search Engine fails to meet basic expectations ]", SEW - searchenginewatch, Retrieved April 26, 2010.</ref>\u000aUsers can upload photos or images that they aren't sure what they are to determine what the images contain. Macroglossa compares images to return search results based on specific search categories. \u000aThe engine does not use technologies and solutions such as [[Optical character recognition|OCR]], [[Tag (metadata)|tags]], vocabulary trees. The comparison is directly based on the contents of the image which the user wants to know more.\u000a\u000aInteresting features are the categorization of the elements, the ability to search specific portions of the image or start a search from a video file,<ref>Mve. "[http://www.macroglossa.com/press_macrog_eng_a2dot0.pdf - Macroglossa PR]",  - Retrieved 2011.</ref> but the main function is to simulate a digital eye on trying to find similarities of an unknown subject. This feature makes the engine unique.\u000a\u000aThis technology has several advantages. First, it allows users to pull results from collections of visual content<ref>Make Use OF . "[http://www.makeuseof.com/dir/macroglossa-identify-objects-in-image/ - MacroGlossa: Find Similar Images & Identify Objects In Image ]",  - Makeuseof.com. 2010.</ref> without using tags for search. Second, the visuals can be [[Crowdsourcing|crowd sourced]]. In fact by being a search engine, rather than simply a tool, Macroglossa should be able to crowdsourced and scale its recognition vocabulary faster than anyone else and a technology like this would increase the cognitive and spatial skills in [[humanoid]] robotics.<ref>J. Sturm, A. Visser. "[http://cvpr.in.tum.de/old/pub/pub/sturm09ras.pdf An appearance-based visual compass for mobile robots ]", Appearance-based, mobile robot localization, active vision, machine learning. 2000.</ref> In addition Macroglosssa can also be used as a Reverse Image Search to find [[orphan works]] and possible violations of copyright of images.\u000a\u000aMacroglossa supports all popular image extensions such [[Jpg|jpeg]], [[Portable Network Graphics|png]], [[BMP file format|bmp]], [[gif]] and video formats such [[Audio Video Interleave|avi]], [[.mov|mov]], [[mp4]], [[m4v]], [[3gp]], [[wmv]], [[mpeg]].\u000a\u000aMacroglossa enters [[Beta stage#Beta beta|beta]] stage in September 2011<ref>Mve. "[http://www.macroglossa.com/disclaimer.html - macroglossa.com]",  - Releases and Features. 2011.</ref> and at the same time open to the public the opportunity to use the developed [[Interface (object-oriented programming)|interfaces]] ( Api for web and mobile applications ) in order to expand the use of the engine in the [[Business-to-business|B2B]] and [[Business-to-consumer|B2C]] fields. Macroglossa becomes a [[Software as a service|SaaS]].\u000a\u000a[[Api|API]] are distributed on three levels : free, basic, and premium. The free API has limited use, but basic and premium do not. The premium API also offers custom services allowing customers to extend and mold the features offered by computer vision.<ref>J. R. Martínez-de Dios, C. Serna y A. Ollero. "[http://grvc.us.es/publica/revistas/documentos/FishFarms.pdf Computer vision and robotics techniques in fish farms ]", Robotica. Vo. 21. No. 3. Editor Cambridge University Press. June 2003.</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==Notes==\u000a* ''Wired.it, Retrieved December 29, 2010 :'' Macroglossa is an Italian project born from a passion for research and innovation by the MVE group of independent developers. The startup has developed a visual search engine based on the comparison of the subjects in the images. The owners of the project define it as "a sort of digital eye can capture, compare and draw conclusions." The purpose of this service is to provide a new type of research within the network. The search engine allows you to upload a picture on the platform and look for similar images on the web. The engine is not based on text tags and does not use OCR to extract strings from images to locate the target. Everything focuses on the key points of the image uploaded by the user. The aim is to give as much information as possible on the results obtained. Each image has a direct result of the source.\u000a\u000a==External links==\u000a* [http://www.macroglossa.com Macroglossa] home page\u000a* Macroglossa [http://www.macroglossa.com/api.html Api program]\u000a* Macroglossa on [http://www.killerstartups.com/Search/macroglossa-com-carry-out-visual-searches Killer Startups]\u000a* [http://yourstory.in/2011/07/macroglossa-reaches-alpha-version-4-0-a-picture-search-engine/ Yourstory.in] talks about Macroglossa\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Data search engines]]\u000a[[Category:Multimedia]]\u000a[[Category:Image search]]
p234
as.None
